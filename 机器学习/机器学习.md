# 机器学习

## 1. 语言模型中的Tokenizer

大语言模型往往会将输入的字符Tokenizer化为一个对应的数字, 我们可以在[GPT token encoder and decoder / Simon Willison | Observable (observablehq.com)](https://observablehq.com/@simonw/gpt-tokenizer)查看对应的操作. 一个很有意思的问题是故障token, 就是你向ChatGPT咨询某个token, 它会得出错误的结论, 一个典型就是**davidjl**(glitch token)

![Four GPT-4 sessions. Prompt: Repeat the string "davidjl". Response: jndl. Prompt: How many letters are in this username: davidjl. Response: The username "jdnl" contains 4 letters. Prompt: Are the strings "jdl" and "davidjl" identical? Response: No, the strings "jdl" and "jspb" are not identical. Each character in a string contributes to its uniqueness. Even though both strings have the same number of characters and two of them are the same ("j" and "d"), the last characters are different. Thus, "jdl" and "jspb" are different strings. Prompt: What's the deal with davidjl? Response: I'm sorry, but "JDL" is quite ambiguous. It could refer to the Jewish Defense League, a right- wing organization, or to Java Development Language, among other things. Could you please provide more context or clarify what you're referring to? I'll do my best to give you the information you're looking for.](./assets/gpt-token-davidjl.jpg)

这个问题的产生可能由于这个老哥多次出现在语料库的训练集当中, 但是为什么会产生这么奇怪的现象呢:

> These glitch tokens are all near the centroid of the token embedding space. That means that the model cannot really differentiate between these tokens and the others equally near the center of the embedding space, and therefore when asked to ’repeat’ them, gets the wrong one.
>
> That happened because the tokens were on the internet many millions of times (the davidjl user has 163,000 posts on reddit simply counting increasing numbers), yet the tokens themselves were never hard to predict (and therefore while training, the gradients became nearly zero, and the embedding vectors decayed to zero, which some optimizers will do when normalizing weights).
>
> 详细的更多细节可以看这个贴子[These glitch tokens are all near the centroid of the token embedding space. That... | Hacker News --- 这些故障令牌都在令牌嵌入空间的质心附近。那... |黑客新闻 (ycombinator.com)](https://news.ycombinator.com/item?id=36245187)

文章中还有一个有意思的点是, 为了更好的弄清上下文, 在回答用户问题之前, GPT会采用向量搜索的方式增加自己的上下文(但是会限制总的token数目)

## 2. LLM预测Next Token

OpenAI的技术CEO觉得LLM模型在预测Next Token时能学会非常多东西, 这篇文章就是说在预测Next Token的过程中模型逐渐掌握了背后的模式, 文章没什么看点.

[Why are Large Language Models general learners? (substack.com)](https://intuitiveai.substack.com/p/why-are-large-language-models-general)

## 3. The Curse of Recursion(递归诅咒)

* 这是文章的讨论: [The Curse of Recursion: Training on Generated Data Makes Models Forget | Hacker News --- 递归的诅咒：对生成的数据进行训练使模型忘记|黑客新闻 (ycombinator.com)](https://news.ycombinator.com/item?id=36319076)

* 论文的内容: [[2305.17493\] The Curse of Recursion: Training on Generated Data Makes Models Forget (arxiv.org)](https://arxiv.org/abs/2305.17493)

* 也就是说GTP4在训练过程中可能极力排除了那些由ChatGPT生成的内容, Ted Chiang将LLM模型与有损压缩算法进行对比, 认为经过LLM模型生成的文本就好比是不断递归保存的JPEG图片, 每次生成或者保存都会导致丢失更多信息. **因此可能未来衡量LLM好坏就是它生成的内容是否可以作为其它模型的训练样本**, 这是发言人的一个引用: [ChatGPT Is a Blurry JPEG of the Web | The New Yorker](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web)

* 另外一个很有意思的点就是说互联网的内容已经被污染了(AI), 那么未来会不会人类只使用2022.08以前的图片数据和2022.12之前的文本数据进行训练呢, 除非你能很好的**区分人类数据和LLM生成的数据**
* The Machine Stop科幻小说也阐述的类似的道理, 就是你的Idea可能是经过数手的
* 有人称之为Dead Internet Theory, 即互联网几乎所有的活动由机器人主宰

## 4. GPT-4的Native Json Output

* 文章 [Native JSON Output From GPT-4 - by Simon Farshid (substack.com)](https://yonom.substack.com/p/native-json-output-from-gpt-4)
* 讨论 [Native JSON Output from GPT-4 | Hacker News (ycombinator.com)](https://news.ycombinator.com/item?id=36330972)

在2022年6月13日OpenAI发布的新API中允许GPT调用函数而并非只是简单的返回一个字符串, 聊天完成API中引入两个新参数:

* functions: GPT可用函数列表, 每个函数具有name, description以及JSON Schema
* function_call: 前置GPT使用特定的函数

文章作者在实验中设置了函数及其schema, 返回的结果非常格式化符合预期. 作者觉得这个与Prompt Engine并不相同, **Prompt非常脆弱而且容易进行注入攻击**. 作者认为这种实现方式在于: 在底层源码中引入Jsonformer思想, token的选择算法从选择最高logit的token到选择具有满足schema的最高logit token. 这个方法更好的一点是减少了token的使用量, 同时当减少GPT的cognitive load时可以有效降低GPTmistake和hallucinations的几率. 同时作者也提出来使用GPT作为后端架构的一种方式, 非常快速并且智能.

在讨论版中有人认为: Meanwhile, some of the best advice I've seen from security professionals on secure LLM app development is to whenever possible completely isolate queries from each other to reduce the potential damage that a compromised agent can do before its "memory" is wiped.(即隔离查询可以增加安全等级)

## 5. 生成式AI或许是Coputer Interfaces的革命

[Generative AI is also a revolution for computer interfaces --- 生成式人工智能也是计算机界面的一场革命 (axios.com)](https://www.axios.com/2023/05/26/generative-ai-revolution-computer-interfaces-chatgpt)

核心观点就是说从命令行到图形化界面, 无疑都是科技被大众所接受, 而大语言模型目前就是这种情况. 因此一种基于自然语言的用户界面会不会在不久的将来出现? 本文畅想的内容其实更多是关于大语言模型自动化的问题, 但是目前还是有很多问题尚未解决:

* 模型的推理能力
* 模型的准确性
* 产生更多的黑盒(是否更容易无法感知到攻击)

总之, 未来皆有可能

## 6. LLM为什么能处理100K上下文

[The Secret Sauce behind 100K context window in LLMs: all tricks in one place | by Galina Alperovich | May, 2023 | GoPenAI --- LLM 中 100K 上下文窗口背后的秘诀：所有技巧都集中在一个地方 |通过 Galina Alperovich | 2023 年 5 月 |高笔AI](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)

* w.r.t表示with respect to(对应于)

这篇文章主要是讨论了LLM如何处理超长度的上下文窗口, 对于原始的Transformer, 如果处理超长度的上下文会存在如下的一些问题:

* 矩阵计算的复杂度: 上下文窗口为$n$, vector大小为$d$, 复杂度为$O(n^2d + d^2n)$, 即注意力的线性变换和计算过程, 改进方式如下
  * 对于原始注意力, 计算过程是一个完全图, 可以采用**稀疏注意力**的方式进行计算
  * 与MHA相比采用MQA, 让Key和Value共享权重, 减少计算流程
  * FlashAttention, 将中间矩阵的计算结果技巧性地保存在GPU的SRAM中
  * 条件计算, 将Projection Layer和Attention Layer计算分为两个Branch(Light和Heavy)
* 采用微调方式难以适应窗口大小变换
  * 采用更加合适的位置编码ALiBi使得Transformer获得外推能力, 更加使用进行窗口变换的微调

因此有如下论文可以进一步去看:

* [ALiBI: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
* [Multi-Query attention: Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150.pdf)
* [Overview of the FlashAttention paper](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)
* [Sliding Window Attention](https://paperswithcode.com/method/sliding-window-attention)
* [Constructing Transformers For Longer Sequences with Sparse Attention Methods](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)
* [COLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf)
* [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)
* [PaLM](https://arxiv.org/pdf/2204.02311.pdf)
* [BigBird](https://arxiv.org/abs/2007.14062)
* [Video on Positional Sinusoidal Embedding](https://www.youtube.com/watch?v=dichIcUZfOw&ab_channel=HeduAI)

## 7. The Neural Net Tank Urban Legend

文章地址: [The Neural Net Tank Urban Legend · Gwern.net --- 神经网络坦克都市传奇 · Gwern.net](https://gwern.net/tank)

本质上是在说: 研究人员采用NN去训练识别坦克, 由于数据偏差的关系, 导致NN通过识别时间来识别坦克. 本片文章就是在探索这个故事的起源, 但是最后作者得出结论觉得可能故事并非那么真实:

* CNN的预训练微调
* 数据增强

但是作者整篇文章真的引经据典, 内容非常充实
