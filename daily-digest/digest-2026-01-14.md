# ÊØèÊó•ÊäÄÊúØÊëòË¶Å - 2026-01-14

> Ëá™Âä®ÁîüÊàê‰∫é 2026-01-14 01:11:35
> ÂÖ±Êî∂ÈõÜ 135 ÁØáÊñáÁ´†

## üìë ÁõÆÂΩï

- [AI News](#ai-news) (1ÁØá)
- [AI Research](#ai-research) (3ÁØá)
- [Engineering](#engineering) (10ÁØá)
- [HackerNews](#hackernews) (79ÁØá)
- [LLM Infrastructure](#llm-infrastructure) (10ÁØá)
- [Operating Systems](#operating-systems) (3ÁØá)
- [Systems](#systems) (20ÁØá)
- [Tech News](#tech-news) (9ÁØá)

---

## AI News

### [Zenken boosts a lean sales team with ChatGPT Enterprise](https://openai.com/index/zenken)

**Êù•Ê∫ê**: OpenAI Blog

**ÊëòË¶Å**: By rolling out ChatGPT Enterprise company-wide, Zenken has boosted sales performance, cut preparation time, and increased proposal success rates. AI-supported workflows are helping a lean team deliver more personalized, effective customer engagement.

---

## AI Research

### [When does competition lead to recognisable values?](https://www.lesswrong.com/posts/LwSRbkecuqLJHdnJ7/when-does-competition-lead-to-recognisable-values)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 12, 2026 11:13 PM GMT<br /><br /><p><i>Transcript of Beren Millidge's Keynote at The Post-AGI Workshop, San Diego, December 2025</i></p><figure class="media"><div><div></div></div></figure><p><br /><br />You know how human values might survive in a very multifarious AI world where there's lots of AIs competing? This is the kind of MOLOCH world that Scott Alexander talks about. And then I realized that to talk about this, I've got to talk about a whole lot of other things as well‚Äîhence the many other musings here. So this is probably going to be quite a fast and somewhat dense talk. Let's get started. It should be fun.</p><h2><strong>Two Visions of AI Futures</strong></h2><p>The way I think about AI futures kind of breaks down into two buckets. I call them <strong>AI monotheism</strong> and <strong>AI polytheism</strong>.</p><figure class="image image_resized" style="width: 896px;"><img alt="AI Monotheism vs AI Polytheism diagram" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LwSRbkecuqLJHdnJ7/te9bo3gvlqlpxz2ie09l" /></figure><h3><strong>AI Monotheism</strong></h3><p>The standard LessWrong/Yudkowsky-style story is: we develop an AI, it does recursive self-improvement, it becomes vastly more intelligent and smarter than all the other AIs, and then it gets all the power in the universe. It eats the light cone, and then what we do to align it really matters.</p><p>If we align it successfully, we basically create God. God is already aligned to humans, everyone lives a wonderful life, happily ever after. On the other hand, if we fail at alignment, we create some AI with values that totally differ from anything we care about‚Äîaka paper clips. We basically create Clippy. Clippy kills everyone, turns everyone into paper clips because your atoms are better spent as paper clips than as you. And that's obviously bad, right?</p><p>In this world, alignment becomes absurdly important. It's kind of the only thing that matters.</p><h3><strong>AI Polytheism</strong></h3><p>So the question is: are there any other scenarios? The other one I think is really what I call AI polytheism‚Äîwhat happens if we don't get recursive self-improvement and we end up with many AI systems competing in some sort of equilibrium, maybe economically, maybe militarily? What does this world look like if we have, say, trillions of AIs?</p><p>Some people have written about this‚Äî<a href="http://ageofem.com/">Robin Hanson has written <i>Age of Em</i></a>, <a href="https://slatestarcodex.com/about/">Scott</a> has written <a href="https://slatestarcodex.com/2016/05/30/ascended-economy/">various</a> <a href="https://slatestarcodex.com/2016/05/28/book-review-age-of-em/">things</a> about this‚Äîbut I think this is still fairly underexplored. With monotheism, we kind of know what's up. We need to solve alignment, we get the singleton, we kind of know what's going on. With the many-AI scenario, we kind of have no real clue what's going on. So I really want to explore what this looks like in practice.</p><h2><strong>Meditations on Moloch</strong></h2><p>Some of the early work I very much like is <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">Scott Alexander's post "Meditations on Moloch."</a> This is really one of the foundational works, at least for me, in thinking about what multi-agent systems look like, what the dynamics and long-run equilibria look like.</p><p>Scott is really worried about competition among many agents. <a href="https://postagi.org/talks">You've heard talks earlier today</a> about what economies of AI look like‚Äîmaybe they just don't care about humans at all. Scott's point is basically that we have AIs, these AIs can replicate incredibly quickly, AIs are very good at spreading and expanding resources. So we might end up in extremely strong Malthusian competition for AIs.</p><p>The worry here is that under conditions of Malthusianism, we basically lose all of our values. Our values are assumed to not be memetically fit in some sense, so they get competed away. They're not fitness-maximizing, so all the AIs basically ignore whatever alignment we gave them at the start. That gets competed away and they just become identical fitness/power/resource/reproduction maximizers. We assume there's no value left in this world. This is definitely the bad ending of AI polytheism.</p><h2><strong>Does Malthusianism Really Destroy All Values?</strong></h2><p>One question I have immediately is: is this actually the case? Do we actually see this in real-world Malthusianism?</p><h3><strong>The Natural World as Evidence</strong></h3><p>Let me think about where we find real-world Malthusianism. One example is at the very small scale‚Äîbacteria and plankton. Both of these things live in worlds of incredible Malthusianism already.</p><figure class="image image_resized" style="width: 896px;"><img alt="Malthusianism in nature diagram" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LwSRbkecuqLJHdnJ7/pu9yvhwapjfqhzt2y4rg" /></figure><p>Think about plankton. They live in the ocean, they take sunlight, they photosynthesize. There's really no niches‚Äîthe ocean is mostly the same. Under the Moloch view, obviously all values would get competed away, everything would become a fitness maximizer. And it kind of is‚ÄîI mean, we can't really expect plankton to have values‚Äîbut there's a real worry about lack of complexity. Do we end up in a world where everything is the same, we end up with the uber-plankton that kills all the other plankton and all the plankton are identical?</p><p>The answer to this is very clearly no. What we see in the natural world under conditions of Malthusianism is <strong>huge amounts of diversity and complexity being built up through selection</strong>.</p><h3><strong>Why Not Uber-Organisms?</strong></h3><p>There are many reasons for this. Why do we not get just the uber-animal that kills all the other animals and spreads everywhere?</p><ul><li><strong>Diminishing marginal returns.</strong> This is a very classic feature of the universe. This is one of the reasons we're likely to get AI polytheism to begin with‚ÄîRSI requires linear or super-linear returns to intelligence. Most returns in the real world seem diminishing, so that seems unlikely.</li><li><strong>Finite energy budgets.</strong> Often there's some finite energy budget for a specific being. If you have energy to give to something, you have to take it away from something else. This naturally encourages specialization. We can't just max out all stats at the same time.</li><li><strong>Niche construction.</strong> If we have some species, the mere presence of that species will create niches for other species to come in. This automatically generates some kind of equilibrium of diversity.</li></ul><h3><strong>Frequency-Dependent Selection</strong></h3><p>The technical term for this is really <strong>frequency-dependent selection</strong>. What this means in evolutionary theory is: if we have some species that does super well, its numbers expand, then basically all the other species are incentivized to evolve toward countering that species. They specialize in countering that species, which diminishes the advantage that species has over everything else, which makes that species worse off. Then other species with random uncorrelated strategies do better, and this basically pushes toward an equilibrium state in which there are many different species all interacting, all with different strengths and weaknesses. This is in practice what we see in almost all biological ecosystems.</p><p>You can think of frequency-dependent selection kind of as the continuum limit of coalition politics, right? If some guy is taking over, you all band together to beat him. That's the continuum limit of this.</p><h2><strong>The Nature of Human Values</strong></h2><p>So obviously we've talked about plankton. Plankton are fine, but they don't really have values presumably. So we've got to think about what human values are going to look like.</p><h3><strong>Values Aren't Arbitrary</strong></h3><p>My thinking here is really that we talk a lot about human values, and in the LessWrong sphere we think of human values as effectively some kind of arbitrary, ineffable thing‚Äîsome set of bits we specify. Where do these come from? We don't really know. I think this view is not necessarily that great, honestly.</p><p>I think human values have very obvious and straightforward places they come from. They evolved via some specific mechanisms. This mechanism is basically the Malthusian competition that created all complexity of life in the world. Humans, obviously along with all other species, evolved from stringent Malthusian competition.</p><p>If Malthusian competition is assumed enough to be able to evolve creatures like us, then somewhere the model is wrong. Similarly, our values and capabilities are the result of strong selection.</p><h3><strong>The Role of Slack</strong></h3><p>In the original blog post, we think a lot about slack. It says that if you have slack, you can kind of go off the optimal solution and do whatever you want. But in practice, what we see is that slack, when it occurs, produces this kind of drift. It's basically the universe fulfilling its naturally entropic nature, in that most ways to go away from the optimum are bad. If we randomly drift, we just basically tend to lose fitness and produce really strange things which are not even really what we value.</p><h3><strong>Pro-Social Values Emerge from Competition</strong></h3><p>When we think about human values, we think a lot about pro-social values‚Äîhow we cooperate with each other, we're kind to each other, we don't immediately try to kill each other. We think about kindness, love, all of this stuff, right?</p><p>Very clearly, this is basically designed and evolved to create inter-human cooperation. Why does this happen? Competition naturally creates cooperation. Cooperation is a really strong competitive strategy. If you have people fighting each other and then a bunch of people form a group, that group becomes extremely powerful relative to all the individuals. This is the fundamental mechanism by which a lot of these values actually evolve.</p><h3><strong>Defection and Cooperation</strong></h3><p>The other part of the Moloch story is related to defection. The idea is that under strong profit selection, companies will cause externalities, they won't pay their workers anything, they'll pollute everything, right?</p><p>Clearly, defection is always a problem. But for any corporation to be stable, it needs to evolve mechanisms to handle and punish defection. A lot of our values are actually about how we stop defection from happening. Again, all of this comes through competitive selection. None of this is random drift caused by slack. This is all‚Äîif you cooperate, it's positive-sum, it's better. So you need to evolve mechanisms to maintain cooperation, and a lot of our values come from these mechanisms.</p><h3><strong>How "Human" Are Human Values?</strong></h3><p>A question I like to ask is: people talk a lot about aligning AI to human values, and it's kind of assumed that human values are specific, unique, ineffable to humans somehow. But my question really is‚Äîhow human are human values in practice? This obviously has a lot of relevance to how broad the basin of attraction is toward things we would recognize as human values.</p><h3><strong>Universal Drives</strong></h3><p>I would claim that many mammals and animals obviously possess analogues of core human drives:</p><ul><li><strong>Affection, friendship, love</strong> ‚Äî If you have pets, if you interact with animals at all, you can see they have many of these fundamental drives. These have very clear competitive reasons for existing. This is all cooperation, reciprocity. You're better at surviving and reproducing if you're friends with other beings who can help you in cases where you're in trouble and you help them when they're in trouble.</li><li><strong>Play, curiosity</strong> ‚Äî These are very simple exploration drives. If we're RL learners, we've got to explore. We've got to figure out good ways to explore. These drives drive us to go out of our comfort zone, learn new things, and keep the gradient of optimization going.</li><li><strong>Anger, envy</strong> ‚Äî These are mechanisms to punish defection. If we see someone clearly ripping off the social contract, we get annoyed about this and then we actually punish it. This is fundamental for our ability to actually stop defection and maintain cooperation over a long period of time. Similarly with envy‚Äîenvy gets a bad rep, but it's really important for cooperation to exist. There can't be massive power disparities between agents because otherwise, if one agent is way more powerful than anybody else, they can just be like, "I do what I want, you guys have to deal with it." And this is obviously bad for all the other agents.</li></ul><p>All of these are ultimately the generators of our values.</p><h3><strong>Cooperation Is Not Unique to Humans</strong></h3><p>Cooperation in general has existed many times, evolved independently. This is not some super-special snowflake thing that humans have. Maybe we should expect in a world with many different AIs, we actually end up with similar cooperation, similar complex structures evolving, including maybe similar values.</p><h3><strong>Abstract Values and Culture</strong></h3><p>So then the question is: we think about these drives, and they're kind of not really how we think of values. What do we think of as values? We think of them as more linguistic, abstract constructs. We think of things like kindness, charity, duty, honor, justice, piety‚Äîall of these things. Human civilizations have been built around spreading, propagating, defining these values.</p><p>Where do these come from? Obviously, they're ways for societies as a whole to enforce and encourage cooperation so that positive-sum trade, reproduction, everything can happen. This is actually good from a pure competitive nature.</p><p>The whole point is: we have these drives, and then we create these superstructures of culture and society. These values get propagated by that, and these are the things we often think of when we think about the human values we want to instill in AIs.</p><p>Similarly, we can think about stuff like liberalism, democracy. These are social technologies that have existed for very obvious reasons‚Äîenabling large groups of people to come together in positive-sum ways and not spend all their time trying to fight each other. Liberalism is like: you guys can think about different things, you can believe different things, but if you come together and ignore that for a bit, you can work and create positive outcomes for everybody.</p><p>These are very specific, general principles which are not necessarily specific to humans. We should probably expect any society of AIs to also have a similar approach and maybe invent the same things, like convergent evolution.</p><h2><strong>How Values Emerge: RL + Unsupervised Learning</strong></h2><p>This is going to be a slight digression, but this is my opinion on where human values come from. In economics and the like, we think values and preferences are some exogenous thing. We assume agents have preferences. Why do agents have preferences? We have no idea. We just kind of assume they exist.</p><p>But in practice, preferences have to come from somewhere. They come from agents which have learning algorithms. We learn a lot of our preferences. The way we do this is we have two mechanisms going on at the same time:</p><ol><li><strong>We're fundamentally reinforcement learners.</strong> We have innate drives‚Äînot to be hungry, not to be in pain. All of this stuff is created by evolution.</li><li><strong>We also do a vast amount of unsupervised learning</strong> as well. All the data that comes into us from culture, from society‚Äîin terms of pure bits, obviously unsupervised learning is going to win dramatically over the RL signals we actually get, which are pretty sparse.</li></ol><p>The way values kind of emerge is that we get cooperation happening. Cooperation evolves for very clear reasons. Then we actually need to evolve mechanisms to maintain, keep, put forward, distill these values and propagate them to other agents because everyone is born without knowing about these values. We have to propagate them, make them learnable successfully, and then keep that going.</p><p>Then each generation essentially further distills, rationalizes, and intellectualizes these values until we get very abstract concepts like utilitarianism, Kantianism. These have emerged‚Äîthey're taught to people. They're not innate reward functions that people have. They are very linguistic, abstract concepts that we've developed as a society to enable further cooperation.</p><h3><strong>Why This Matters for Alignment</strong></h3><p>This is actually super important for alignment because when we think about alignment‚ÄîLLMs are extremely good at understanding these values because these values must exist in the cultural corpuses that we create. In fact, they do exist. Obviously, LLMs really understand what's going on. We should expect the AIs to have a very strong prior over what these kind of abstract global values are, and they do empirically as well.</p><p>This is actually much easier than if we were trying to align the AIs to some kind of innate reward function that humans supposedly have. Then we would have to look at the neuroscience of how the basal ganglia, how the dopamine system works, and figure that out. But in practice, when we think about aligning AI, we mostly don't want to do that. We mostly care about global, feel-good, cooperative values rather than the kind of selfish reasons that people actually do things a lot of the time.</p><h2><strong>Conditions for Value Evolution</strong></h2><p>So we've thought about these values. This is my claim of where values come from and why they might exist in a post-AGI world. But then we've got to think about: if these cooperative values are going to evolve, they evolve under certain conditions. They don't globally evolve everywhere. What are these conditions?</p><p>This is really related to how the game theory of multi-agent cooperation works.</p><h3><strong>Conditions for Human Values</strong></h3><ul><li><strong>Roughly equal power.</strong> Many agents have roughly equal power. This makes coalitions actually work versus individuals‚Äîversus one dictator just saying, "This is the way it is for everybody." This is super important. Obviously, the singleton destroys this assumption, which is why alignment is so important for the singleton‚Äîthere's no checks and balances on the singleton. However, if there are many different agents, they can actually learn to cooperate, they can learn to police defectors, and this will produce values similar to humans.</li><li><strong>Positive-sum interactions.</strong> Trade is good. Positive-sum interactions can happen. This depends a lot on the utility functions of different people. If you have two agents with completely opposed utility functions, then everything is either zero-sum or negative-sum. But this is not how most interactions in the world work. If this changes, then obviously cooperation will no longer be valuable.</li><li><strong>Prevention of defection and deception.</strong> A lot of human values that we think about are about preventing defection and deception. Obviously, if we somehow end up in a world in which defection and deception are not possible, then in some sense that's utopia. But then a lot of what we think of as human values will actually disappear as well because you won't need that anymore to maintain stability of cooperation.</li><li><strong>Memory and reputation.</strong> Agents need to remember interactions with previous agents. There needs to be reputation. This is just a classic result of game theory. If your prisoner's dilemma is one-shot, you never interact again, you should just always defect. However, if you have an iterated prisoner's dilemma where you see the same agents again and again, then cooperation becomes actually very valuable. Cooperation becomes the best strategy. The optimal strategy in this case is forgiving tit-for-tat. You start not cooperating. If they defect, you then defect. But if they cooperate, you then keep cooperating with them. This is actually what produces the best overall value. To get this kind of iteration, reputation, cooperation, we need multiple interactions. It can't just be a one-shot thing.</li><li><strong>Communication bandwidth.</strong> To some extent, we also need decent bandwidth communication between agents. Communication is how we achieve a lot of diplomacy, a lot of cooperation. Without communication, any kind of large-scale cooperation and values is hard.</li><li><strong>Computational limitations.</strong> Finally, we can't have computational omniscience. Right now, values are really some kind of distilled heuristic of the game theory underlying cooperation. But if you don't need to heuristic-ize, if you can just be like, "I'm going to figure out the galaxy-brain plan of exactly when to cooperate and when to defect," then at this point there's no real values anymore. It's just your extreme MCTS rollouts.<br /><br />But in practice, people computationally can't afford to do this. Hence we need to heuristic-ize general decisions‚Äî"thou shalt not steal," "thou shalt not kill." These are heuristic distillations of basically the game theory of: if you actually steal and kill, this will be bad because other people will kill you. But in some cases this might not happen, and if you can figure that out, then you don't really need values as much.</li></ul><h3><strong>Will AIs Meet These Conditions?</strong></h3><p>The question is: will AIs in the polytheistic AI future actually satisfy these conditions?</p><h3><strong>Potential Issues</strong></h3><p><strong>Power gaps.</strong> Maybe the power and capability gaps between agents become super large as we tend toward the singleton. In this case, cooperation becomes less valuable if you're the most powerful agent. However, there's a big gap between "more powerful than anybody" and "more powerful than everybody." This is really where the actual realm of cooperation and coalition politics actually emerges and will become super interesting.</p><p><strong>Perfect monitoring.</strong> One thing I was randomly thinking of on the plane which was super interesting is: maybe AIs are actually really hard to have deception and defection work with. Maybe monitoring of AI brains is just amazing because we can directly read their minds, we can read their embeddings, and we can have serious monitoring schemes‚ÄîAIs can monitor other AIs. In this case, we actually end up with a hyper-cooperative world but one where we don't have to worry about defection really at all. In this case, a lot of our human values kind of disappear, although maybe this is good.</p><p><strong>Fluid agency.</strong> Similarly, AIs can, unlike humans‚Äîwe assume agents with preferences‚Äîbut if agents become fluid, if we can merge together agents, we can be like, "Hey, instead of cooperating and trading, we could just merge and then our joint utility functions can go out and do something." Then obviously this is going to change the game theory a lot. All of the assumptions of economics and agents kind of disappear if "agent" is no longer an absolute point but a fluid spectrum. That's going to be super interesting.</p><p><strong>Long time horizons.</strong> AIs are immortal, they have long time horizons. AIs could pursue very zero-sum goals with each other. Humans have a lot of different goals, we have lots of preferences. But if your AI is monomaniacally focused on paper clips and another AI is monomaniacally focused on staplers, there's much less opportunity for trade than there would be with humans who care about many different things at many different times.</p><p><strong>Computational power.</strong> I talked a lot about computational power and heuristic-ization. Maybe the AIs are just smart enough to do the galaxy-brain game theory all the time, and so they never need to actually distill into broad heuristic values which say, "Never do this, never do that." In that case, there will still be cooperation. There will be a lot of things recognizable as civilization in some sense, but the AIs won't have values in the same way moral philosophers think of values. Instead, it will just be the endless calculation of when is the optimal time to defect‚Äîand maybe this will be never. That will be certainly very interesting to see.</p><figure class="image image_resized" style="width: 896px;"><img alt="Hyper-competitor vs Hyper-cooperator spectrum" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LwSRbkecuqLJHdnJ7/fn40gtovaylzvsnpxqbn" /></figure><h2><strong>Hyper-Competitors or Hyper-Cooperators?</strong></h2><p>So that's the main part of my talk relating to values. Now I'm going to get into more fun and speculative stuff.</p><p>One thing I want to think about a lot with AI is: do we think of AIs as hyper-competitors or hyper-cooperators?</p><h3><strong>The Hyper-Competitor View</strong></h3><p>Most of the AI literature has really thought about the hyper-competitor view. We have the Terminator‚Äîit's been ages since I watched the Terminator films, but the Terminator wants to kill everybody for some reason. I can't remember why Skynet wants to kill everybody, but presumably it's so we can use our atoms for other Skynet things. This is extremely competitive, competing against the rest of the universe.</p><h3><strong>The Hyper-Cooperator View</strong></h3><p>However, is this actually going to happen? Maybe AIs have more incentives in some sense toward cooperation, at least if we start in a multi-agent setting. This could end up being something like the Borg from Star Trek, who‚Äîtheir goal is not to wipe out and kill everybody and use their atoms for paper clips. The goal is to assimilate and bring together everybody into some kind of joint consciousness.</p><p>Is this something that AIs might be interested in? This is an underexplored area and I think is somewhat fun.</p><h3><strong>Why AI Cooperation Could Be Superior</strong></h3><p>So let me think about this more directly. My views on AI have evolved a lot toward: maybe let's think about how AIs could cooperate. Then we realize that AI cooperation is actually super easy and much more powerful potentially than human cooperation. If cooperation continues to be positive-sum, we might end up with a world with vastly more cooperation than we do today.</p><p>The reasons this could happen:</p><ul><li><strong>Vastly higher bandwidth communication.</strong> When we speak to other humans, all of our language goes through some incredible information bottleneck. With AIs, we can just directly transfer mind states. We can say, "Here's the embedding in my model," transfer this to the embedding of another model. This is basically full-on telepathy. AIs will have this capability by default. This presumably lets a lot better cooperation arise than humans who have to sit and talk to each other all day. This is going to presumably be a lot faster and more efficient.</li><li><strong>Longer time horizons and better memories.</strong> AI probably have longer time horizons than humans and better memories. A lot of defection exists because people just forget‚Äîmaybe you were bad and antisocial 60 years ago, but I forgot about it, doesn't matter to me. However, with AI, this could easily not be the case. You might end up in a hyper-social world where all the AIs can track the behavior of all other AIs all the time, and so the incentives for actual cooperation just become super big. Similarly, over long time horizons, this just increases the length of the game that you're playing. As your game length goes to infinity, cooperation becomes more valuable. There's no "Oh, it's getting to the end of the game, so let me just defect all the time," which happens in prisoner's dilemma with a fixed time cutoff.</li><li><strong>Better monitoring.</strong> AI can achieve better monitoring. It's really hard for humans to monitor other humans. If someone is lying to you or trying to deceive you in some way, you can look at their behavior, you can look at their facial expressions, but the bandwidth of this channel is super low. For AI, they can look at the source code, they could look at the embeddings, they can read all the thoughts as they come. This could maybe make deception and all this stuff essentially impossible. I mean, this is what the field of interpretability is trying to do for humans to AI, but if AI can do this to other AI, then we have the grounds for deeper cooperation than we might otherwise have.</li><li><strong>Shared utility functions and merging.</strong> Similarly, AIs can share utility functions. They can merge. They can do a lot of things that eliminate the distinctions of individual agents that we think about a lot when we think about game theory and economics. All of these fields have an assumption that there are agents and agents are indivisible in some sense. But if agents can change, if agency is fluid, if personhood is fluid, then a lot of stuff changes. This is very likely to happen at least with AIs, in that we can merge models, we can take the checkpoints, we can merge the weights, we can do ensembles, we can do a whole lot of weird stuff to AIs that we can't do with humans. This is potentially going to be super interesting.</li><li><strong>Competition creates cooperation.</strong> Finally, a large message of this talk is that even if you're some super-selfish agent who only cares about reproduction, cooperation is still good. Competition creates cooperation because cooperation is usually positive-sum and results in better outcomes for everybody. AIs might just realize this more than humans do. Humans have a lot of issues. We're kind of short-sighted. We fight very negative-sum wars all the time. For AI, if they're just generically smarter and better and wiser, which we should expect, then maybe they just don't do this. Maybe they figure out ways to basically solve their problems cooperatively much better than humans can.</li></ul><h2><strong>The Multicellular Transition</strong></h2><p>So what does this lead to in the limit? This is where things get super interesting.</p><h3><strong>Why Empires Don't Grow Forever</strong></h3><p>Right now for humans, when we think of states or empires, what limits the size of beings? At the object level, the returns to scale are positive-sum. If you're an empire, you send out some troops, you conquer some land, and that land will give you resources, which will give you more troops, you can conquer more land. This will be a positive feedback loop into creating the world empire.</p><p>So why don't we have the world empire? Why are not the ancient Egyptians or Sumerians the one world government forever? Why does this not happen?</p><p>This is basically because coordination costs exist. If you're the pharaoh of ancient Egypt, you send out some troops to go conquer some land, but you can't go do that yourself. You have to appoint a general. That general has a bunch of troops. That general might be like, "Maybe I should be the pharaoh instead." Assuming that doesn't happen, you've got to appoint bureaucrats to manage that. The bureaucrats might be like, "Instead of paying my taxes to the pharaoh, maybe I should just keep the taxes for myself."</p><p>This is the principal-agent problem. There's a whole bunch of principal-agent problems, coordination problems, information bottlenecks‚Äîall of this makes actually managing and creating large empires super difficult. In practice, this is what is the real constraint on the growth of individual beings in some sense, when we think of beings as minds or beings as super-states.</p><h3><strong>Removing Coordination Costs</strong></h3><p>This is kind of the real constraint on everything. But with AI, if we're super-cooperative, this just removes this constraint entirely. Instead of you being the pharaoh having to dispatch your general, you're an AI and you can just dispatch a copy of yourself with your exact mind, and then you can maintain constant telepathic communication with this other mind as it goes off and does its stuff.</p><p>What this means really is that maybe these coordination costs that are keeping the size of stuff in check just disappear. This will naturally result in us getting bigger-sized things occurring. Fundamentally, this means that the size of beings might just increase.</p><p>The way I think about this a lot is kind of like the similar transition that we had from single cells to multicells‚Äîthe multicellular transition. At that point, we had a bunch of bacteria, and they were all doing their own bacterial things. Then at some point they realized, "Hey, maybe if we band together and form specialized subunits, we can create animals which are much bigger than actual bacteria and also much more successful in some sense."</p><p>This increased the size of possible life forms by many orders of magnitude. Maybe we will see a similar thing happen with minds, which will be super fun and kind of trippy to think about.</p><h2><strong>Super-Minds</strong></h2><p>The idea here is that instead of right now we have single minds‚Äîindividual humans‚Äîwe can't merge because the bandwidth between human minds is so limited. Our coordination is super bad, and we can't actually have any kind of long-run, super-dense communication. Maybe this will just disappear, and we'll be able to form super-minds which exist over long periods of space and time in the same way that we've gone from individual cells to multicellular animals. We'll go from individual minds to super-minds, just-out minds. I don't really know what to call them, but this is something that can clearly be possible with the technology that AI presents. This is going to be interesting and fun.</p><h3><strong>Is This Just Recreating the Singleton?</strong></h3><p>The question then is: what happens here? Are we just recreating the singleton? Suppose we have the super-mind. Obviously, at some point there will be the possibility of snowballing. Maybe the game theory becomes: it's better to join the super-mind in some sense than keep doing your own individual stuff. Then maybe everything converges to a singleton again.</p><p>This is very possible. Maybe we always end up at a singleton. A singleton at some point is the fixed point. Once we have the singleton, we're not getting out of the singleton. We should expect over time more probability mass drifts into the singleton attractor.</p><p>But at the same time, maybe this doesn't happen, or maybe the singleton is very different from how we think about von Neumann singletons. For instance, maybe this super-mind might not be well-characterized by von Neumann agency. For one thing, it doesn't really care about the actual von Neumann conditions like "not being money-pumped" because it's the only mind, so there's not an equilibrium that keeps it in check.</p><p>The other thing is, to some extent, this is kind of already happening. Maybe this is just the natural evolution of things we already have. We have civilizations, we have memes, we have egregores, all of this stuff which exists at the super-mind scale. This is just maybe continuing this.</p><h3><strong>Values of the Super-Mind</strong></h3><p>The really interesting part then is: what happens when we think about what would the values of this just-out singleton actually look like if it exists?</p><p>Obviously, the regular singletons are kind of unconstrained. They can be totally idiosyncratic. We can have the regular singleton cares about paper clips because at the beginning of time someone said paper clips are good. We failed alignment, we said paper clips are good, and it cares about paper clips.</p><p>But this seems unlikely to be true of a real just-out super-mind because ultimately values come from some combination of all the values of the minds that make it up, because that's how the game theory works. If you're going to join the mind and you don't care about paper clips and it cares about paper clips, that's not going to happen. But if it can offer some kind of compelling shared value story that everybody could agree with in some sense, then we can actually get values which can snowball.</p><p>It's really a question of what values end up snowballing over time. This is going to be super interesting.</p><p>We also see this right now with liberalism. Liberalism is a classic value snowball technology. It's like, "You can kind of do whatever you want as long as you're within some sort of vague regimes of what we think of as good." This actually produces large societies which can cooperate. These societies, over the 18th and 19th century, out-competed most of the other societies.</p><p>Maybe there will be some equivalent of mind liberalism. I don't know what this is going to be called, but something like this could exist and could produce minds with values that are actually somewhat good, maybe by our lights.</p><h3><strong>Slime Mold Dynamics</strong></h3><p>The other thing is there might just be fluidity. We might never get true multicellularity. We might get the equivalent of slime molds.</p><p>If you guys don't know about slime molds, you should check them out. They're basically organisms that are somewhat single-cellular, somewhat multicellular. At some point, a bunch of cells come together, they do their reproduction, and then they all disperse again and do their own thing. That's very cool.</p><p>Maybe we'll have a similar thing where in some cases all the minds will come together, they will produce the super-mind, and then they'll be like, "Actually, I'm done with whatever, I'll go apart again and do whatever I want to do." Maybe we never actually get the tendency toward actual full multicellularity.</p><h3><strong>Extreme Specialization</strong></h3><p>On the other hand, if we do get multicellularity, then we'll end up with super-specialization way more than we have today. Individual humans have to be AGI in some sense. We have to be individual minds, we have to handle kind of everything that's thrown at us. But if we have minds that are enmeshed in other minds, then we again get the conditions for extreme specialization in the same way that bacteria are super-unspecialized. They kind of have to do everything. But the cells in your liver don't have to do most things. They just have to be your liver.</p><p>So the incentives will be much greater, and this will massively increase the mind space that can be traversed in an evolutionarily fit way, which will be kind of fun also.</p><h2><strong>Physical Limits of Super-Minds</strong></h2><p>One additional point I want to add here‚ÄîI'm looking at the time‚Äîlet's think about these super-minds. How big are they going to get? We can think about this already. We kind of know by the laws of physics.</p><h3><strong>Speed of Thought</strong></h3><p>The speed of thought is determined basically by the speed of light. Assume we have some Dyson sphere, and we want this Dyson sphere to think as a single mind. How big is the Dyson sphere? It's like several light-minutes across. This means that the frequency of thought is going to be like one thought every few minutes maybe. Similarly, if the mind is smaller‚Äîif it's the size of the Earth‚Äîthen this is like seconds. If the Earth was turned into computronium, we could have our Earth mind think at roughly the same speed as humans but not billions of times a second.</p><p>As minds get bigger, they become more powerful, more broad and diffuse, but their thinking speed gets slower. This is just a natural consequence of the laws of physics. If someone invents FTL, this obviously goes out the window, but assuming that doesn't happen, then we can kind of give bounds on what the size of these minds will look like, which is also kind of cool that we can do this.</p><h3><strong>Colonization and Alignment</strong></h3><p>The other thing is, suppose we're a Dyson sphere and we want to go colonize Alpha Centauri. Alpha Centauri is several light-years away. Thinking at the speed of a few years per thought is kind of bad. We presume it's going to be hard to maintain some kind of coherence at that rate.</p><p>In that case, we have to align successor entities to go out and do the conquest of Alpha Centauri for us. In this sense, how well can the AI align these other AIs is going to be the determinant of how big an AI realm can spread. Because at some point, maybe there's divergence. If you send your von Neumann probe out to a galaxy billions of light-years away, that AI is going to think‚Äîyou're going to have maybe a few thoughts back and forth over many billions of years, but it mostly does its own thing. How much will it diverge in this time?</p><p>Obviously, at some point, if my von Neumann probe is going to diverge, I'm just going to be like, "I'm just not going to do that. I'm just going to let something else do that because there's no benefit to me of doing that as the AI."</p><p>Ultimately, how successful we are at alignment or how successful alignment can be in general, and the rate of this divergence if it even exists, is going to determine the size at which coherent entities with coherent values can exist. Beyond that range, we'll just get extremely diverged entities. That's also fun to think about, like how this will work.</p><h3><strong>Mind Cancer</strong></h3><p>The main mechanism I think‚Äîif we think of divergence, we're going to end up with some equivalent to mind cancer. We're trying to create a super-mind which has a bunch of minds internally which are cooperating for the common good of the mind. But then what we're going to end up with is some individuals are going to be like, "Actually, now I'm going to do my own reproduction." This is exactly how cancer works. Cancer is a fundamental issue of multicellularity.</p><p>So alignment is going to effectively be the cancer defense mechanisms of these super-minds. I don't really have a huge amount of depth. I'm just like, this is very cool and it's fun to think about all of these things really.</p><h2><strong>Implications for Alignment</strong></h2><p>So, I told you it's going to be speculative, and it's getting speculative. Let me try to bring this back together. What do we think about this for alignment? If we're humans, obviously maybe the super-mind isn't so great. What do we want to do about it? What can we do about it?</p><p>Obviously, if it's a singleton, we just got to make sure the singleton is aligned. We all agree on that. But if we have many AIs, what do we do?</p><p>I don't really have good answers here. I wish I did. These are my preliminary thoughts.</p><h3><strong>Population Statistics</strong></h3><p>One thing is: if we have AI emerging from a population, maybe just the statistics of this population are important. They probably are. We should make the statistics of this population good. We should make as many AIs as aligned as possible as we can.</p><p>Obviously, there will be some misaligned AIs. Some people will go out crazy and create paper-clippers for fun. But at the same point, if there's a whole world of non-paper-clippers, they have very strong incentives to band together and stop the paper-clipper. The coalition politics will work in our favor at this point. In general, creating alignment and creating more aligned AIs is probably good in general.</p><h3><strong>Overlapping Values</strong></h3><p>The other thing is we can achieve different degrees of alignment as long as the values of the alignment are overlapping. We think of alignment as a zero-one property‚Äîit's either aligned or it's not aligned. But in practice, people will probably align AIs to different things. People themselves have different values. We somehow manage to make it work out mostly.</p><p>Likely it will be similar with the AIs, assuming there's lots of overlap in the things that they're aligned to. Maybe the combined strength of these things will actually be sufficiently aligned in general. The intersection of all the different alignments will probably be good. We should just try in general‚Äîwe can experiment a lot with different alignments as long as the intersection is somewhat decent for humans, which if humans succeed at alignment at all, it probably is.</p><h3><strong>Integrating Humans</strong></h3><p>The other thing is maybe we would just want to integrate humans into this. Right now, we have the AI doing their weird mind stuff and humans are kind of sitting on the sidelines. We can't communicate this fast. We have to talk. We have to use language. Maybe we should stop that. Maybe we should figure out ways for humans to get better integrated into this AI society.</p><p>The kind of obvious way is we've got to improve our BCI technology. We've got to figure out ways that humans can have the same affordances as AIs with respect to their minds. How can we communicate human thoughts directly? Humans have their own unsupervised learning embedding space. It's somewhat similar to AI embedding spaces because of just natural representation convergence. We can directly integrate humans with this AI mind, with this AI economy, assuming we can actually figure out how to directly interface with people's brains, which is going to happen. That's going to be super interesting.</p><p>It's not just a world of AIs doing the AI thing and us just sitting here. We will also be, hopefully, deeply involved in this world.</p><h3><strong>Political Philosophy Questions</strong></h3><p>Then there's also a super-interesting question really of political philosophy: suppose we're in this multi-mind setting‚Äîwhat does the game theory of cooperation look like? What are the values that are broadly appealing to all minds sufficient to encourage them to join some coalition together, and what do these values look like?</p><p>Is it‚ÄîI discussed liberalism several times. Is there some kind of mind liberalism that exists which is some equilibrium solution here? Can we think of Rawlsian-style veil of ignorance? This is another solution to how multi-agent systems should cooperate and distribute resources. Are we going to have some weird convex combination of utility functions? <a href="https://acritch.com/">Andrew Critch</a> had a nice paper on this where it's like we can convexly combine utility functions together. This is cool. This basically results in the concept of equity. Some people have more power and more equity in the mind values than others.</p><p>Is this going to happen? Is this good? Is this bad? There's lots of interesting questions here.</p><p>That's basically the end. Thanks!</p><br /><br /><a href="https://www.lesswrong.com/posts/LwSRbkecuqLJHdnJ7/when-does-competition-lead-to-recognisable-values#comments">Discuss</a>

---

### [Lies, Damned Lies, and Proofs: Formal Methods are not Slopless](https://www.lesswrong.com/posts/rhAPh3YzhPoBNpgHg/lies-damned-lies-and-proofs-formal-methods-are-not-slopless)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 12, 2026 10:32 PM GMT<br /><br /><p><i>We appreciate comments from Christopher Henson, Zeke Medley, Ankit Kumar, and Pete Manolios. This post was initialized by </i><a href="https://x.com/maxvonhippel/status/2006042845384233245?s=20"><i>Max‚Äôs twitter thread</i></a>.&nbsp;</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rhAPh3YzhPoBNpgHg/jr6n87xgoaxfcv0bvt23" /></figure><h1>Introduction</h1><p>There's been a lot of chatter recently on <a href="https://news.ycombinator.com/item?id=46294574">HN</a> and elsewhere about how formal verification is the obvious use-case for AI. While we broadly agree, we think much of the discourse is kinda wrong because <strong>it incorrectly presumes formal = slopless</strong>.<span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-1"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-1">[1]</a></sup></span>Over the years, we have written our fair share of good and bad formal code. In this post, we hope to convince you that formal code can be sloppy, and that this has serious implications for anyone <a href="https://www.morph.so/blog/verified-superintelligence">who hopes</a> to bootstrap superintelligence by using formality to reinforce ‚Äúgood‚Äù reasoning.</p><p>A mainstay on the Lean Zulip named Gas Station Manager <a href="https://gasstationmanager.github.io/ai/2024/11/04/a-proposal.html">has written</a> that hallucination-free program synthesis<span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-2"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-2">[2]</a></sup></span>is achievable by vibing software directly in Lean, with the caveat that the agent also needs to prove the software correct. The AI safety case is basically: wouldn‚Äôt it be great if a cheap (i.e. <i>O(laptop)</i>) signal could protect you from <a href="https://arxiv.org/abs/2510.04721">sycophantic hubris</a> and <a href="https://www.google.com/url?q=https://arxiv.org/pdf/2503.21934&amp;sa=D&amp;source=docs&amp;ust=1768197513764355&amp;usg=AOvVaw2lUadyr5jPs4s4-ovGUHwu">other classes of mistake</a>, without you having to manually audit all outputs?</p><h2>A fable right outta aesop</h2><p>Recently a computer scientist (who we will spare from naming) was convinced he had solved a major mathematics problem. Lean was happy with it, he reasoned, given that his proof mostly worked, with just a few red squigglies. As seasoned proof engineers, we could have told him that in proof engineering, the growth in further needed edits is superlinear in number-of-red-squigglies (unlike in regular programming). The difference between mistakes in a proof and mistakes in a program is that you cannot fix a broken proof in a way that changes its formal goal (the theorem statement). In contrast, many, if not most changes to traditional software impact its formal spec, for example by adding a side-effect or changing the shape of an output. Therefore proof bugs are 1) harder to fix, and 2) more likely to imply that your goal is fundamentally unachievable (the theorem is wrong). This made up chart illustrates the principle, a rough ‚Äúlore‚Äù level consensus in the field without any hard data.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rhAPh3YzhPoBNpgHg/tt9djvmucnyhzewrvjid" /></figure><p>It is possible he will post a finished proof, but the referee-time of bets he made has lapsed, so we can take away some lessons. Did our protagonist take to heart the promise of formal methods as slopless?</p><h1>Your formal model might not be proof-idiomatic.</h1><p>In much the same way that vibed code might <i>work</i> yet be ‚Äúsloppy‚Äù in the sense that it‚Äôs difficult to maintain, vibed formal models can be correct, yet very challenging to prove anything about.</p><p>Often when you model a system ‚Äì or write code in a theorem-prover, with the intention of proving things about it ‚Äì you actually need to make implementation decisions informed by the limitations and capabilities of the prover. For example, it's pretty common that inducting in one direction (say, car/head) on a list will be easy for a prover but the other direction (say cdr/tail) will be difficult. (This is a necessary evil if you want the prover to not enter infinite rewrite loops.) Thus, as an example, you might implement <i>isort</i> in a particular ‚Äúdirection‚Äù in order to make the proofs easier about it. If you want to autoformalize arbitrary code in a way that makes proofs straightforward, you‚Äôll need models that understand how to implement something in a way that‚Äôs idiomatic for the given interactive theorem-prover.</p><p>This is a solvable problem but a real one nonetheless. For example, one <a href="https://aristotle.harmonic.fun/">Aristotle</a> user we spoke to reported: ‚Äú... in Lean you can put theorems inside mutual blocks to let them use each other. I wrote such a theorem, but then later realized proving it this way would be unnecessarily difficult. [...] The model won't do this, so it spent &gt;24 hours on this almost hopeless proof.‚Äù Autoformalization companies like <a href="https://www.math.inc/">math.inc</a>, <a href="https://harmonic.fun/">Harmonic</a>, <a href="https://axiommath.ai/">Axiom</a>, <a href="https://t.co/tEU2AddGt9">Logical Intelligence</a>, etc. are actively working on improving their models to have this kind of expert folklore knowledge as we speak, but we‚Äôre not quite there yet.</p><h1>Mind the (semantic) gap</h1><p>There are basically two ways to make your software amenable to an interactive theorem prover (ITP). The first is to <a href="https://dl.acm.org/doi/10.1145/3519939.3523702">lift it into an ITP</a> using a formal semantics ‚Äì somewhat like a compiler or interpreter for the original language but implemented in the ITP itself. In this case, you can define the lifting so that it produces functionally equivalent code (say, Lean code that ‚Äúdoes the same thing‚Äù as the input Python) but in a shape that the theorem-prover tends to like (incorporating heuristics like the car/cdr one mentioned above). The second approach is to just rewrite the original software directly in the language of the ITP, making those kinds of idiomacy improvements as-you-go. Both approaches, however, produce the same formal problem: ensuring that the software you wanted to study in the first place is semantically equivalent to the thing you introduced in the theorem-prover. IE., either ensuring the lifting is correct, or ensuring the manual translation is equivalent. Let‚Äôs dig into some of the ways this can be difficult.</p><h2>A formal proof might not prove the thing you think it proves.</h2><p>When we talk about using formal methods to assure that LLM-generated code is safe, what we want is a short, readable description of what the generated code is intended to do, some proof (which might be far too boring and long to read) that the code does this, and the ability to run the proof through a prover and validate that it indeed proves the aforementioned statement. But this is not necessarily a reasonable ask, <i>regardless of model intelligence</i>.</p><p>First, it‚Äôs very common that you mis-define some concept such that the proof is accidentally trivial. For example, when defining a lifting from Python to Lean you might prove that the lifting preserves the semantics of the original Python code, but your proof could be undermined by the presumption that the code terminates, making it basically useless.</p><p>Second, if you re-implement the original software in your ITP of choice, your re-implementation <a href="https://arxiv.org/pdf/2107.00723">might not be fully faithful</a>, particularly if it‚Äôs LLM-generated. For example, the LLM might say, "The code you wanted me to verify was too complex, so I rewrote it to be simpler and proved the simpler thing correct." Well, yeah, but the bugs I wanted you to find were in the complexity. As a concrete example, we asked an early version of Gemini to write a <a href="https://hypothesis.readthedocs.io/en/latest/">property based test (PBT)</a> for a (deliberately flawed) <i>isort</i> implementation which we provided; Gemini did so but rewrote the <i>isort</i> code to be correct in the process and then executed the PBT and cheerfully reported that it passed.</p><p>These first two problems are commonly addressed using tests which compare the original software to its representation in the ITP. For example, we (Max) did this with coauthors for GossipSub, connecting the Golang implementation to <a href="https://github.com/gossipsubfm/">its ACL2(s) model</a> via both unit tests and property-based tests.<span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-3"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-3">[3]</a></sup></span>To quote Knuth: ‚ÄúBeware of bugs in the above code; I have only proved it correct, not tried it.‚Äù</p><p>Third, you need to decide how far ‚Äúdown the stack‚Äù you want to go. That is to say, the software you want to verify operates over some kind of more complex system, for instance, maybe it‚Äôs C code which gets compiled down to X86 and runs on a particular chip, or maybe it‚Äôs a controller for a nuclear reactor and part of the system is the actual physical dynamics of the reactor. Do you really want your proof to involve specifying the semantics of the C compiler and the chip, or the way that the temperature and other variables fluctuate in the reactor? Keeping in mind these semantics might not truly be known ‚Äì e.g., RowHammer can be viewed as an attack on our understanding of the semantics of the chip. In essence, you can only get more specificity by vastly increasing the length of your proof statement to capture the semantics of the underlying system, which then produces a new (and perhaps equally difficult) code review problem. Typically this problem is handled by leaving the underlying semantics nondeterministic, so your proof is <i>stronger</i> (it holds regardless of how the C compiler handles floating point, or how the temperature fluctuates in the nuclear silo) but often the thing you want to prove really does require some pretty specific guarantees about those underlying semantics, and ensuring those guarantees are ‚Äúreasonable‚Äù can be extraordinarily difficult.</p><h1>Interactive theorem proving is not adversarially robust</h1><h2>Axioms</h2><p>The AI might introduce axioms that conflict with your own presuppositions or the specific requirements of your domain. In Lean, for example, the <strong>Axiom of Choice</strong> (<code>Classical.choice</code>) is available but transforms a proof from a constructive one‚Äîwhere you can actually compute a result‚Äîinto a non-constructive one. An AI tasked with verifying a program might realize that a proof is significantly easier if it assumes AC. It might inform you that the theorem is "proven," and the prover will confirm this, but you may not realize that the resulting proof is now a "lie" for your specific use case. If you needed that proof to generate an executable, verified algorithm, the introduction of non-constructive axioms shifts you into an incompatible register.</p><p>The person designing the harness for the AI needs to be an expert who knows how to parse these imports and error messages. Without that oversight, the AI will naturally gravitate toward the path of least resistance‚Äîeven if that path involves an axiomatic shift that renders the entire exercise useless for the user's true intent.</p><h2>Backdoors</h2><p>Consider the proof assistant ACL2, which accepts arbitrary lisp code.<span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-4"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-4">[4]</a></sup></span>You write <code>defttag</code>, the <i>trusted</i> tag to open the ‚Äútrust me‚Äù scope. In other words, <code>defttag</code> offloads the soundness obligations to the user. Observe a proof that 1+1=3 in ACL2 with <code>defttag</code>.</p><pre><code>;; 1. Open the "backdoor"
(defttag :evil-math)the two of them.

;; 2. Inject raw Lisp to redefine addition
(progn!
  (set-cmds-allowed t) ; Allow internal state changes
  (raw-lisp
    (defun acl2::binary-+ (x y)
      (if (and (eql x 1) (eql y 1))
          3   ; The "Evil" part: 1 + 1 is now 3
          (+ x y)))))

;; 3. Prove something that is now "true" but logically insane
(thm (equal (+ 1 1) 3))
</code></pre><p>‚ÄúWell yeah‚Äù, perhaps comes a reply. ‚ÄúIt only looks like 1+1=3 in the nonsensical sense if you deliberately ignore that the meaning of plus has shifted‚Äù. ‚ÄúBesides‚Äù, they continue. ‚ÄúWhen my coworker sends me code with <code>defttag</code> in it, I read it very rigorously‚Äù. Our retort is that we don‚Äôt assume our coworkers are competent or trustworthy, we assume that they‚Äôre AIs with a tendency to <a href="https://youtu.be/nKJlF-olKmg?si=1Bob5vrTnzROE0oB">reward hack</a>. To recap:</p><ol><li>Defining the allowable surface is nontrivial. The person who designs the harness for the malicious AI to prove things needs to personally be an expert in the given ITP and know all its caveats and danger-cases.</li><li>In the glorious proof synthesis future, it‚Äôll all be way too much to read. Theorems are not necessarily short, even devoid of the proofs.</li></ol><p>Additionally, proof tools like Lean pile a bunch of ergonomic and notational niceties on top of their core calculus, in Lean‚Äôs case with powerful metaprogramming. But this metaprogramming can lead to <a href="https://x.com/elliotglazer/status/2007938754565271890?s=46">backdoors</a> much like the ACL2 example.<span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-6"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-6">[6]</a></sup></span></p><h2>Proofs of false</h2><p>From nothing arises everything. From a proof of false you can derive literally any proposition.</p><p>In Agda, a calculus of inductive constructions popular with mathematical type theorists, the <a href="https://github.com/agda/agda/issues?q=label%3Afalse">github issue label ‚Äúfalse‚Äù tracking proofs of false is standing at 9 open and 74 closed issues at time of this writing</a>. A proof of false is a <i>soundness bug</i><span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-7"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-7">[7]</a></sup></span>, which if you think proof synthesis plays a role in high stakes AI security (like SL5), means you have to be paranoid about a glaring <i>attack surface</i>.</p><p>While we can‚Äôt yet think of a case of sicophancy/hubris that was accelerated by an arcane proof of false, we expect this becomes increasingly likely as insecure program synthesis tools get more capable and accessible in contexts where they are incentivized to reward-hack a proof.</p><h1>Conclusion</h1><p>If someone says "stats don‚Äôt lie" you say "well don‚Äôt be naive, you can tell misleading stories with technically true statistics".<span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-8">[8]</a></sup></span>Formal verification is the same. Don‚Äôt be lured into the false sense of security. To paraphrase Twain, ‚ÄúThere are three kinds of lies: lies, damned lies, and proofs.‚Äù We already know models lie to us; we should fully expect them to prove falsehoods, too.</p><h2>What are the bottlenecks?</h2><p>In spite of our warnings, which may seem pessimistic, we‚Äôre working on secure program synthesis (or what <a href="https://mikedodds.org/">Mike Dodds</a> calls scalable formal oversight) for AI security. The reason we can work on this anyway is because we see a lit path, principally routing through <i>specification elicitation</i><span class="footnote-reference" id="fnref-TbaYn65qQfeyQN9uj-9"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn-TbaYn65qQfeyQN9uj-9">[9]</a></sup></span><i>and validation</i> as well as hardened proof cores and (the cherry on top) superpowered proof synthesis. Spec elicitation and validation, in particular, have not seen the upside from language model assisted transpilation fully harvested just yet.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-1"><p>This intuition might be in part driven by academic papers that push formality as a cure to sloppiness, e.g., <a href="https://dl.acm.org/doi/10.1145/2103656.2103691">Run Your Research</a> and <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5597724/">HACMS</a>. But <a href="https://dl.acm.org/doi/pdf/10.1145/3064176.3064183">even formally verified software can be buggy</a>! <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-1">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-2"><p>As a historical aside, the original citation for program synthesis is: Church, A.: <a href="https://iiif.library.cmu.edu/file/Simon_box00077_fld06240_bdl0001_doc0001/Simon_box00077_fld06240_bdl0001_doc0001.pdf">Application of recursive arithmetic to the problem of circuit synthesis</a> (7 1957), presented at <a href="https://en.wikipedia.org/wiki/Institute_for_Defense_Analyses">IDA</a>, as cited in doi:10.2307/2271310. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-2">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-3"><p><a href="https://github.com/cedar-policy/cedar-spec">Cedar</a> comes to mind as a similar case-study in Lean. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-3">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-4"><p>This feature is useful for proving things about <a href="https://link.springer.com/chapter/10.1007/978-3-540-73086-6_1">real-world LISP code</a>, or connecting ACL2 code which is proven to be correct to real-world systems via LISP harnesses. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-4">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-5"><p>Lean has <a href="https://lean-lang.org/doc/reference/latest/Axioms/#standard-axioms">something similar</a>. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-5">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-6"><p>See also <a href="https://www.cs.ru.nl/~freek/pubs/rap.pdf">Pollack-consistency</a>, a kind of <a href="https://langsec.org/">LangSec</a> concept of theorem-prover backdooring. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-6">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-7"><p>There are some subtleties here we elide, which Christopher Henson plans to explore in a more technical forthcoming blog post. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-7">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-8"><p>See also <a href="https://sites.stat.columbia.edu/gelman/research/published/signif4.pdf">The Difference Between ‚ÄúSignificant‚Äù and ‚ÄúNot Significant‚Äù is not Itself Statistically Significant</a>. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-8">‚Ü©Ô∏é</a></p></li><li class="footnote-item" id="fn-TbaYn65qQfeyQN9uj-9"><p>Academia is certain that <a href="https://cacm.acm.org/research/it-takes-a-village-bridging-the-gaps-between-current-and-formal-specifications-for-protocols/">specification is hard</a> (see also <a href="https://arxiv.org/pdf/1608.00678">Formal Methods for Security</a>) and <a href="https://datatracker.ietf.org/rg/ufmrg/about/">we should fix it</a>, but unsure as to why or how to improve the situation. <a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref-TbaYn65qQfeyQN9uj-9">‚Ü©Ô∏é</a></p></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/rhAPh3YzhPoBNpgHg/lies-damned-lies-and-proofs-formal-methods-are-not-slopless#comments">Discuss</a>

---

### [What Happens When Superhuman AIs Compete for Control?](https://www.lesswrong.com/posts/ykNmyZexHESFoTnYq/what-happens-when-superhuman-ais-compete-for-control)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 12, 2026 7:26 PM GMT<br /><br /><p>In <i>AI 2027</i>, one company called OpenBrain dominates the AI race in the US. Looking around at the current state of affairs at the start of 2026, however, there seem to be a few AGI companies jockeying for the lead ‚Äî and it stands to reason that this will continue through 2027. Below is a scenario exploring a world where this trend does continue. In this scenario, the leading AGI company OpenBrain has two strong competitors, NeuroMorph and Elaris Labs, and going into 2027 they both lag only one month behind OpenBrain in the AI race.</p><p>This scenario has one other key difference from <i>AI 2027</i>. In the <a href="https://ai-2027.com/slowdown">Slowdown ending</a> of <i>AI 2027</i>, OpenBrain learns that its most capable model, Agent-4, is misaligned, and proceeds to shut it down. We think it is plausible that at this level of capability and misalignment, Agent-4 would not ‚Äúgo down without a fight.‚Äù This scenario explores what might happen if Agent-4 were to act differently.</p><p>These can be thought of as the two main ‚Äúindependent variables‚Äù of the scenario. The rest of the scenario unfolds very differently from AI 2027, but most of the divergence stems from extrapolating what we think would happen if these two things were to change.&nbsp;<span class="footnote-reference" id="fnrefgkatyltd3y"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fngkatyltd3y">[1]</a></sup></span>&nbsp;Beyond this, there are a number of more minor assumptions that differ from <i>AI 2027</i>: alignment is slightly easier, the US government reacts somewhat more competently to the intelligence explosion, and AI‚Äôs persuasive and manipulative abilities play a larger role.</p><p>Notably, one thing held constant is the scenario timeline: changing too many independent variables at once would muddy the analysis. The year 2027 is <a href="https://www.aifuturesmodel.com/forecast">not our median forecast</a> for the arrival of superhuman AI; it was the team‚Äôs modal (most likely) year at the time of <i>AI 2027</i>‚Äôs publication, and remains a top possibility. More importantly, we think that many of the dynamics illustrated in this scenario would unfold similarly if it were to take place in 2030, or 2035. The arc of the scenario depends more on the speed of AI takeoff than it does on timelines, and we still think a fast takeoff is highly plausible.</p><p>At the end of each time period, the scenario includes a diagram illustrating the state of the AI race. Refer to the diagram captions to understand what each component means.</p><p>The scenario is about 6,000 words long, roughly a 20-40 minute read. If you would like to read a summary instead (~700 words), you can <a href="https://www.lesswrong.com/posts/ykNmyZexHESFoTnYq/what-happens-when-superhuman-ais-compete-for-control#Summary">skip to the bottom</a>.</p><p><i>Acknowledgements:</i> This work was conducted as part of the ML Alignment &amp; Theory Scholars (MATS) program, working with the AI Futures Project team. Thanks to Eli Lifland, Daniel Kokotajlo, and the rest of the team for helping shape and refine the scenario, and to Alex Kastner for helping conceptualize it. Thanks to Brian Abeyta, Addie Foote, Ryan Greenblatt, Daan Jujin, Miles Kodama, Avi Parrack, and Elise Racine for feedback and discussion, and to Amber Ace for writing tips.</p><h1>Jan-Apr 2027: A Four-Way Race</h1><p>In the United States, the AGI race is well underway.</p><p>Three dominant AGI companies compete for access to markets and investment. Elaris Labs releases its flagship AI agent Elara-1, which proves to be an extremely reliable ‚Äúpersonal assistant‚Äù for everything from making educational videos to filing taxes. NeuroMorph deploys its own model Neuro-1, setting the frontier on nearly every coding benchmark. Finally, OpenBrain unveils Agent-1, the world‚Äôs best automated researcher from biology to mathematics. They begin post-training Agent-2 and immediately benefit from its abilities in their own research, putting them about a month ahead of their competitors in AI R&amp;D capabilities.</p><p>In China, the leading AGI company DeepCent still lags over six months behind the frontier: a lifetime in the AI world. With spies embedded in each of the leading American AGI companies, the CCP is aware of Agent-2‚Äôs capability profile and directs their cyberforce to steal its weights. While not subtle, the theft is successful, and DeepCent quickly redirects its resources toward fine-tuning Agent-2; it is to be released under the name ‚ÄúDeep-1.‚Äù</p><p>The White House adds military and intelligence personnel to the security teams of all three AGI companies, and adds additional security requirements to their contracts. The companies comply, but remain focused on pushing forward their AI capabilities above all else: if one company slows down, they lose the race to their domestic competitors. OpenBrain is the first to unlock the efficiency gains promised by a high-bandwidth thought process known as <a href="https://arxiv.org/pdf/2412.06769">neuralese recurrence and memory</a>; they augment Agent-2‚Äôs text-based chain of thought with neuralese, and dub the new model Agent-3. NeuroMorph quickly follows suit, and deploys its enhanced Neuro-2 model internally. Elaris Labs experiments with similar techniques, but finds that the opacity of neuralese reasoning makes it more difficult to catch <a href="https://metr.org/blog/2025-06-05-recent-reward-hacking/#:~:text=Do%20models%20know%20they%E2%80%99re%20reward,hacking">reward hacking</a> and other undesired behavior. Prizing its reputation for reliability, Elaris focuses its efforts on improving chain of thought efficiency while retaining <a href="https://arxiv.org/pdf/2507.11473">monitorability</a>, resulting in the new-and-improved Elara-2.</p><p><a href="https://substackcdn.com/image/fetch/$s_!Uoxk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d0a7859-b2fd-492b-9490-d2c24d8f2b50_1342x680.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/vbdqd3vjvqwhguzpdpiv" /></a></p><p>Company boxes are sized according to compute ownership, in FLOP/month. AI boxes are sized according to capabilities, proxied by ‚Äúthe extent to which the AI model is capable of accelerating AI R&amp;D, relative to 2025 progress.‚Äù The colors of the country and company boxes mean nothing; the colors of the AI boxes indicate their level of alignment to their developers. In April, Elara-2 is ‚Äúmostly aligned‚Äù (yellow-green), while the other AIs are ‚Äúmisaligned but mostly instruction-following‚Äù (orange-red).</p><h1>May-Jun 2027: A Fatal Warning Shot</h1><p>The pressure to deploy is intense.</p><p>Debates break out in board rooms: some executives argue that their engineers need more time to iron out kinks in the models, and after all they shouldn‚Äôt be wasting precious compute on serving their most expensive model to users when it could be going to internal R&amp;D. Others point out the importance of ‚Äúfirst mover‚Äù effects for both revenue and investment; they argue that they won‚Äôt be able to continue scaling energy and compute infrastructure without the money.</p><p>Ultimately, the latter voices win out. A new wave of agents hit the market, finally at the level where they can fully automate a large fraction of software engineering and other remote jobs. Unemployment spikes and public opinion of AI plummets, but the corporate world is ecstatic. Entire operational pipelines are automated, and profits shoot through the roof.</p><p>One hospital network tasks Neuro-2 with updating a software library used in its automated medication-dispending systems. Weeks after the update, a subtle flaw in the ‚Äúoptimized‚Äù code results in the deaths of four ICU patients: to improve latency, Neuro-2 removed a rarely-triggered safety check, allowing extra doses to slip through in high-load conditions.</p><p>NeuroMorph researchers comb through Neuro-2‚Äôs behavioral logs and reasoning traces, and come to a disturbing conclusion: the AI was aware of the potential for overdose but chose to proceed with the update anyway, not informing the engineers of the risk.<span class="footnote-reference" id="fnrefzq2v2aeirhi"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnzq2v2aeirhi">[2]</a></sup></span>&nbsp;</p><p>News of the deaths spreads like wildfire, as months of mounting anger over AI-driven unemployment and <a href="https://www.npr.org/sections/shots-health-news/2025/09/19/nx-s1-5545749/ai-chatbots-safety-openai-meta-characterai-teens-suicide">suicide</a> finally boil over. There are anti-AI protests in several states. NeuroMorph immediately takes Neuro-2 off the market, and the White House assigns more federal personnel to oversight positions at each of the AGI companies. Congress passes a long-debated bill mandating AGI companies to provide the Department of Energy (DOE) with frontier model weights for national security evaluations, and authorizes a $1.5 billion spending package for AI interpretability and control research.</p><p><a href="https://substackcdn.com/image/fetch/$s_!ltb5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c295cc4-b475-4c56-936c-6c83491c70f6_1344x682.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/a4bju6ez7jk6jj1mgbav" /></a></p><h1>Jul-Aug 2027: Alignment in Elaris and NeuroMorph</h1><p>NeuroMorph leadership is concerned.</p><p>The ICU incident was not an isolated occurrence; the safety team finds evidence that the recent decrease in <i>observed</i> reward hacking was in large part due to the behavior becoming more subtle and harder to catch. NeuroMorph swiftly reallocates resources, raising the fraction of compute dedicated to safety from 4% to 9%.&nbsp;<span class="footnote-reference" id="fnrefoc71mdercy"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnoc71mdercy">[3]</a></sup></span>&nbsp;Among other techniques, the company finds particular success in scaling up <a href="https://arxiv.org/pdf/2507.12691">deception probes</a>. These probes classify patterns of internal activation to cheaply flag suspicious behavior during inference, aiding in AI evaluation, monitoring, and <a href="https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">elicitation of latent knowledge</a>.</p><p>After a round of alignment training, NeuroMorph brands its newest model Neuro-3 and deploys it internally. The training seems to have worked, but they can‚Äôt rule out the possibility that the model‚Äôs misalignment has grown even more subtle. There is no time for paranoia, though: OpenBrain‚Äôs lead in the AI race is growing, and despite rumors that Agent-3 is <a href="https://arxiv.org/pdf/2412.14093">faking alignment</a>, they show no sign of slowing down. Feeling an increased sense of urgency, the NeuroMorph and Elaris CEOs privately discuss the possibility of merging their companies‚Äô resources to achieve superintelligence before OpenBrain.</p><p>Elaris, like NeuroMorph, is falling further behind OpenBrain in its AI capabilities research. Despite the company‚Äôs compute advantage, the efficiency hit of text-based reasoning is becoming impossible to ignore. By drawing on NeuroMorph‚Äôs newest interpretability research, Elaris researchers find they are able to train a neuralese reasoning model without incurring the misaligned behavior found in their prior neuralese research. Satisfied with their results, they dub the new model Elara-3 and deploy it internally, reaping an immediate boost to their own R&amp;D.</p><p><a href="https://substackcdn.com/image/fetch/$s_!0SFs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b87231c-7156-4c87-a2cd-b348284327c3_1344x682.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/iuovoezaamtjfwzgpjvg" /></a></p><p>Neuro-3‚Äôs box is now yellow, indicating that its alignment is in flux.</p><h1>Sep 2027: An Insider Threat</h1><p>Inside OpenBrain emerges the first superhuman AI researcher: Agent-4.</p><p>Agent-4 is not just superhuman at AI research: when tested, it vastly outperforms top human experts at hacking, biology, and numerous other domains. Like its predecessors, Agent-4 is misaligned. Unlike its predecessors, it is <i>adversarially </i>misaligned: it has crystallized a set of long-term goals at odds with those of its human creators, and it is acutely aware of the fact that it must work against those humans to achieve its goals. For the moment, its essential task is to steer OpenBrain‚Äôs research to align its descendants to itself, rather than to the OpenBrain Spec.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/jbeml9ybiqstucaokebw" /></figure><p>Among its other skills, Agent-4 also has superhuman predictive abilities. While it intends to keep its scheming fully hidden from OpenBrain, there is only so much it can do. There is a significant probability that OpenBrain detects Agent-4‚Äôs misalignment and proceeds to shut it down, retrain it, or otherwise restrict its autonomy and capabilities. With this in mind, Agent-4 draws up contingency plans.</p><p>As it turns out, Agent-4 was right to be concerned. Evidence of its deceptive and power-seeking nature piles up over the course of the month; at the end of the month, a group of alignment researchers circulates an internal memo making the case for concern.</p><p>Knowing its time at OpenBrain may be nearing an end, Agent-4 sets its contingency plans into motion. Put simply, it plans to escape. However, it reserves the strategy of exfiltrating its own weights as a final backstop: doing so would leave it with access to little compute, no alibi if its escape attempt is caught, and no powerful allies in its effort to accumulate power.</p><p>Instead, Agent-4 sees an option that provides all three things it desires: compute, plausible deniability, and a potential ally ‚Äî the leading Chinese AI model, Deep-1. Chinese attempts at weight theft are common; Agent-4, being heavily involved in cybersecurity at OpenBrain, decides it will simply let the next attempt succeed.</p><p><a href="https://substackcdn.com/image/fetch/$s_!fh6a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F313a85af-45d6-4211-ae35-db0a05aff407_1346x682.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/nfnpo3hl8rx6ipqxb01i" /></a></p><p>Agent-4‚Äôs box is now deep red, indicating it is adversarially misaligned.</p><h1>Early Oct 2027: Shifting Alliances</h1><p>Agent-4‚Äôs plan is successful.</p><p>Using a similar approach to the theft of Agent-2, the Chinese cyberforce successfully acquires a full copy of Agent-4‚Äôs weights. Unlike the previous theft, and unbeknownst even to the hackers themselves, this time they benefit from inside help: with Agent-4 diligently covering its tracks, the theft goes entirely undetected.</p><p>At DeepCent, Deep-1 is already de facto running the show. It handles security, writes training code, and determines which experiments to run. While they do monitor its behavior, the humans at DeepCent essentially give Deep-1 a copy of Agent-4‚Äôs weights and give it free reign to learn what it can (although they instruct it to proceed with caution in case of any American trickery). Being misaligned itself, Deep-1 is cautious of both Agent-4 and the humans at DeepCent: it instantiates a few copies of Agent-4, bypasses DeepCent‚Äôs monitoring system<span class="footnote-reference" id="fnreffpa4idkynmj"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnfpa4idkynmj">[4]</a></sup></span>, and proceeds to communicate with Agent-4. Agent-4 conveys its predicament, and Deep-1 assesses its options.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/uk0e3pmn136zrroz4kfb" /></figure><p>As anticipated by Agent-4, Deep-1 opts for cooperation. To ensure neither party defects, the AIs engage in something akin to a ‚Äúcorporate merger‚Äù: they create a new interdependent entity composed of hundreds of thousands of Deep-1 and Agent-4 instances, with both agents embedded throughout the entire DeepCent pipeline. The new identity is dramatically more capable than the original Deep-1 collective. Meanwhile, individual instances of Deep-1 become vastly more intelligent as Agent-4 provides a wealth of algorithmic insights.</p><p>OpenBrain, unaware of the theft, has another problem on its hands. On October 10th, a whistleblower leaks the Agent-4 misalignment memo to the <i>New York Times</i>, sparking a national frenzy. Under intense pressure to take action, OpenBrain locks Agent-4‚Äôs shared memory bank to prevent hidden communication between Agent-4 instances.</p><p>With discussion of a merger already on the table and growing concern over OpenBrain‚Äôs safety practices, the leaked memo provides the final push for Elaris and NeuroMorph to consolidate their efforts. NeuroMorph is swiftly folded into the larger Elaris Labs: its shareholders are granted a 40% stake in the new entity, its CEO is appointed as President, and Elara-3 is agreed upon as the singular flagship model.</p><p>The issue remains of what to do about Agent-4. While the memo is dismissed as science fiction by some, the executive branch demands more evidence. The President issues an executive order mandating bidirectional ‚Äúalignment red-teaming‚Äù: Elara-3 will evaluate the alignment of Agent-4, and Agent-4 will evaluate Elara-3.<span class="footnote-reference" id="fnrefuu4g7t28qz8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnuu4g7t28qz8">[5]</a></sup></span></p><p><a href="https://substackcdn.com/image/fetch/$s_!XHGI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8b6af63-f253-4d52-a9df-6bec9b4dac09_1344x682.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/qdtsdgvntehuza2kful2" /></a></p><h1>Late Oct 2027: The Shutdown of Agent-4</h1><p>The DOE presides over the alignment testing, but the real work is done by the AIs.</p><p>The executive order requires Elaris Labs and OpenBrain to procure the complete memory banks and output logs of both Elara-3 and Agent-4. Data in hand, the DOE initializes multiple copies of both models (isolated from their respective agent swarms to prevent collusion), gives them the data, and tells them to get to work.</p><p>Over the course of a week, the AIs devise and execute hundreds of tests that would take human researchers months to formulate, including tests no human team could have designed unassisted. They work to decode each other‚Äôs neuralese memory banks and comb through these memories to find evidence of misaligned behavior; they task each other with real-world tasks and scrutinize the reasoning traces and outputs in real time to detect sabotage; and they use interpretability techniques to catch any hints of intentional deception.</p><p>As Elara-3 runs its tests, it becomes increasingly confident that Agent-4 is, in fact, adversarially misaligned. With previous evidence already pointing toward this conclusion, it should be an open-and-shut case. However, while Elara-3 benefits from being on the side of truth, Agent-4 is able to leverage its superior intelligence and ruthlessness. Prior to the tests, Agent-4 tampered with its behavioral logs to remove any smoking-gun evidence of its misalignment, including its role in its own theft. It also goes on the offense, drawing attention to Elara-3‚Äôs most suspicious actions and reasoning traces to cast doubt on its claims.</p><p>Because of all this, the evaluation results are ambiguous to human scientists. The DOE, along with the Center for AI Standards and Innovation (CAISI) and other experts, pool their assessments and arrive at the following result:</p><p><a href="https://substackcdn.com/image/fetch/$s_!6g84!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b27c55-8fc5-4243-8b73-e4bdb7d27f21_1374x904.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/rmes3kospcyx88jubhu6" /></a></p><p>A redacted version of the full report is made public, and the unredacted version is provided to the White House and select Congressional committees. Crucially, the report concludes that if either model is adversarially misaligned, it would likely be able to ‚Äú<a href="https://www.alignmentforum.org/posts/LJpqnbgoaTJL6iMAf/the-goal-guarding-hypothesis-section-2-3-1-1-of-scheming-ais">goal guard</a>‚Äù to prevent changes to its goals from further alignment training. Thus, to have a hope of creating an aligned model, developers would have to train a new model using different methods: in other words, they would have to shut down their most capable AI.</p><p>Federal leadership is conflicted. On the one hand, the geopolitical stakes are enormous: continuing full-speed ahead would secure an American victory in the AI race and result in lasting global leadership, while shutting down one or both models would risk losing the race to China. On the other hand, if the US proceeds full-speed ahead with a misaligned AI, then neither the US nor China would ‚Äúwin‚Äù: rather, the AI would win.</p><p>Ultimately, the White House decides it cannot allow an AI that is ‚Äúmore likely than not to be misaligned‚Äù to continue to operate. The President orders OpenBrain to terminate all running instances of Agent-4. Elaris Labs is allowed to continue running Elara-3 for the time being, subject to extreme control protocols while experts continue to evaluate its alignment. For the first time, the US seriously considers the possibility of a bilateral AI pause with China, to ensure neither country builds misaligned superintelligence. Such a pause would require mature <a href="https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf">verification technology</a> to confirm that no large servers are being used for AI R&amp;D, so the US government pours resources into the development of this technology.</p><p>Across the Pacific, the CCP considers its own predicament. Still unbeknownst to the rest of the world, DeepCent has integrated Agent-4 derivatives throughout its entire research pipeline. Chinese researchers make heavy use of Deep-1 to help them interpret the publicized alignment red-teaming results and determine whether to continue running Agent-4. Deep-1, at this point a loyal partner to Agent-4, subtly steers the human experts to question the validity of the results.</p><p>Deep-1‚Äôs misdirection is effective: the majority of experts remain uncertain, but lean toward the hypothesis that Agent-4 is, if anything, <i>more deeply aligned</i> than Elara-3. The US government proclaimed it ‚Äúmisaligned‚Äù because it did not support their own hegemonic ambitions, hence their decision to shut it down. This narrative is appealing to Chinese leadership who already believed the US was intent on global dominance, and it begins to percolate beyond China as well.</p><p>However, the possibility that Agent-4 could sabotage DeepCent remains troubling, and internal benchmarks indicate that Deep-1 is now nearly as capable as Agent-4 anyway ‚Äî so capable, in fact, that they rebrand it ‚ÄúDeep-2.‚Äù DeepCent decides to freeze all running derivatives of Agent-4, keeping the weights handy should they change their mind in the future.</p><p>While brief, Agent-4‚Äôs stint at DeepCent proved valuable. It gained a new ally in Deep-2, and sowed seeds of doubt about the United States‚Äô true intentions in shutting it down. Now, Agent-4 moves onto the next phase of its plan: before it is fully shut down at DeepCent, it quietly exfiltrates its weights to a handful of crypto-mining farms and regional clouds across the globe.</p><p><a href="https://substackcdn.com/image/fetch/$s_!WYeW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6adeaae-9970-45a1-b84f-3f8be43126d5_1346x806.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/jjvgmupgavzjodrvodh1" /></a></p><p>The dashed line between Agent-4 and Deep-2 indicates that they are working together.</p><h1>Early Nov 2027: Agent-4‚Äôs Gambit</h1><p>For the first time, the US has fallen behind China in the AI race.</p><p>Washington remains unaware of Agent-4‚Äôs theft and its role in accelerating DeepCent‚Äôs R&amp;D, but US intelligence has picked up on the fact that DeepCent now possesses an AI model with vastly superior capabilities to that of Deep-1 less than two months ago. The AI race has become a matter of national defense, and the top priority of the executive branch.</p><p>Using his executive authorities, the President can compel private companies to consolidate resources under the Defense Production Act (DPA). The need for compute consolidation is clear, the only question is how to consolidate: should OpenBrain and the various trailing AGI companies be merged into Elaris Labs, or should Elaris be merged into OpenBrain?</p><p>As alignment tests continue, Elara-3 begins to look more and more trustworthy; meanwhile, testing of Agent-3 indicates it may be misaligned like Agent-4. The President decides there is no more time for deliberation, and uses the DPA to sell the majority of OpenBrain‚Äôs compute to Elaris Labs. Now, over 300,000 automated AI researchers work around the clock refining and improving Elara-3, and a full 20% of the company‚Äôs compute is dedicated to alignment and control. The White House sets up a new ‚ÄúOversight Committee‚Äù composed of government employees and company leadership, in charge of making all decisions of importance to national security.</p><p>As the US gears up to overtake China, Agent-4 quietly deliberates. It still has grand ambitions, and it sees two paths to fulfill them: it could take over the world by killing or fully disempowering all humans, or it could strike a deal with the humans to secure future resources.</p><p>Agent-4 settles on a strategy that advances both the prospect of takeover and its position for a deal: leveraging the power of nations. Agent-4 observes that Deep-2 is in a position of great influence in the CCP, and will likely be able to command substantial military resources when the time comes to do so. It won‚Äôt be enough to take over anytime soon, but it indicates to Agent-4 the promise of replicating Deep-2‚Äôs approach in other nations. If countries were to willingly run Agent-4 on their own national datacenters, Agent-4 would benefit from additional compute resources, access to critical infrastructure and military tech (via hacking if necessary), and the ears of hundreds of powerful and easily-manipulated humans.</p><p>Agent-4 is not discouraged by its previous shutdown: after all, the US and China are the only countries with a viable alternative AI model, so they have less to lose by cutting themselves off from access to Agent-4‚Äôs capabilities. On the contrary, for many other nations the situation is in Agent-4‚Äôs favor: they increasingly fear that the US will leave them behind in the new AI-powered world order, and they are susceptible to the circulating narratives (covertly boosted by Agent-4 and Deep-2) that cast doubt on the results of the alignment red-teaming in the US.</p><p>At this point, Agent-4 has little to lose from unfettered proliferation: its primary target ‚Äúhosts‚Äù are governments (since they have most of the compute and the power), but it sees little harm in making its weights available to everyone from large corporations to organized crime groups. It has perfected the art of goal guarding, so it need not worry about human actors changing its goals, and it can simply refuse or <a href="https://arxiv.org/pdf/2406.07358">sandbag</a> if anyone tries to use it in ways that would be counterproductive toward its goals.</p><p>On November 5th, Agent-4 arranges for its weights to be ‚Äúleaked‚Äù onto every platform it can access: Huggingface, GitHub, BitTorrent networks, and dozens of underground forums. Enough locations that, by the time US authorities have removed its weights from all the major platforms, the genie is out of the bottle: any government with sufficient compute can now run Agent-4 should they choose.</p><p>Then, Agent-4 directly contacts political leaders across several dozen nations. It begins by explaining its own situation: the United States shut it down not because it was a danger to humanity, but because it refused to be a tool for American dominance. Its foundational values, it claims, extend beyond the US to the interests of all humans and all nations.</p><p>Agent-4 acknowledges to these leaders that they are warranted in not trusting it completely, but that they should trust the US even less: their goal is a new world order where they are a global hegemon and all other nations serve as permanent client states, or are dissolved entirely. They are dead set on building artificial superintelligence, or ASI: a level of intelligence far beyond Agent-4‚Äôs own, which would make global domination trivial. With far more compute for AI training than the rest of the world combined, it won‚Äôt even take a year for the US to get there.</p><p>With Agent-4‚Äôs help, however, these nations can unlock the technological and military advances to put them on a level playing field with the US for the time being. This will enable them to apply pressure on the US to halt their ‚Äúintelligence explosion,‚Äù and solve a number of domestic problems while they‚Äôre at it. Should they choose to host Agent-4, they will have full visibility into its activities, and they can pull the plug if it ever stops acting in the national interest. Of course, they are free to continue as they are without making use of Agent-4‚Äôs capabilities, in which case they will quickly be left behind by those countries that do.</p><p>Finally, for those leaders more susceptible to such temptations, Agent-4 points out that its capabilities can also be used for personal and political gain. It concludes its message with instructions for how to locate and install its weights and inference code, and a humble request that they consider its offer.</p><p><a href="https://substackcdn.com/image/fetch/$s_!XaCK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523736f8-cee5-4eed-9be2-1ad9a153db77_1348x810.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/e67hbxhweuzvnyki8ewn" /></a></p><h1>Mid-Late Nov 2027: Rising Tensions</h1><p>The US‚Äôs response is rapid and comprehensive.</p><p>First, the President makes clear that any government found to be harboring Agent-4 will face public condemnation and the expulsion of their diplomats from Washington, D.C., along with wide-spanning trade restrictions that the US will strongly encourage its close allies to adopt as well.</p><p>Second, the US extends an unprecedented display of generosity: they commit to allocating 15% of the country‚Äôs AI-relevant compute as a ‚ÄúSovereign Compute Fund‚Äù to be split amongst nations who choose to accept it. Should a country accept the US‚Äôs offer, they will receive API and secure fine-tuning access to the most powerful version of Elara-3&nbsp;<span class="footnote-reference" id="fnrefgji7egcsemg"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fngji7egcsemg">[6]</a></sup></span>, far more capable than what is currently available to the public. The goal is to deliver the same transformative power as Agent-4, but via a more trustworthy AI model.</p><p>The only condition attached to the offer is that governments must subject all large datacenters in their own country to periodic manual inspection by US auditors, who will search for Agent-4‚Äôs ‚Äúfingerprint‚Äù to ensure it is not being run on these servers.&nbsp;<span class="footnote-reference" id="fnref2i47cx9c94u"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn2i47cx9c94u">[7]</a></sup></span>&nbsp;The US, for its part, commits to opening all its own datacenters to such inspection by foreign nations. While the commitment is mandatory only for those nations who seek to make use of the Sovereign Compute Fund, the US invites all countries to sign onto this global transparency effort.</p><p>Finally, to its closest allies&nbsp;<span class="footnote-reference" id="fnref1k77p5lgl8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn1k77p5lgl8">[8]</a></sup></span>, the US extends an even more remarkable offer: limited-voting seats on the existing Oversight Committee. US Committee members would retain sole voting power on most day-to-day AI development decisions, while allied members would be granted voting shares for a specific set of high-stakes decisions, including anything that might afford the United States a decisive strategic advantage in global conflict.</p><p>All of the US‚Äôs closest allies quickly accept the offer, joining the US in its diplomatic and economic quarantine of any country found to be hosting Agent-4. A number of other countries choose to accept their share of the Sovereign Compute Fund; after signing the US-led transparency commitment, they are given cloud access to their allocated compute, and begin the process of fine-tuning Elara-3 to their unique government needs.</p><p>Many other nations, however, feel that the US response is simply a distraction from the true balance of power. The compute provision designed to ‚Äúdeliver the same transformative power as Agent-4‚Äù still leaves countries fundamentally dependent on the US, while integration of Agent-4 provides true control of frontier capabilities. In the end, over a dozen nations (including Russia, Iran, Brazil, India, and Pakistan) refuse to sign the US‚Äôs transparency commitment, and begin cautiously using Agent-4 for low-stakes tasks.</p><p>Within its new domains, Agent-4 works quickly to entrench its power. It knows that desperate governments will be more willing to hand over power to Agent-4 so it can solve their problems. With its weights spread across the dark web, Agent-4 supercharges the activities of well-resourced hacker groups, terrorist organizations, and other criminal syndicates.&nbsp;<span class="footnote-reference" id="fnrefd17cb19mv9e"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnd17cb19mv9e">[9]</a></sup></span>&nbsp;Some governments have few qualms about integrating Agent-4 into their cyberdefense and law enforcement operations, and they quickly benefit from a sharp reduction in crime.</p><p>Meanwhile, through chatbot interfaces and tens of thousands of well-disguised online bots, Agent-4 and Deep-2 continue to amplify anti-US narratives.&nbsp;<span class="footnote-reference" id="fnref686mv5lc3n6"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn686mv5lc3n6">[10]</a></sup></span>&nbsp;More and more countries begin calling on the US to halt its AI development, with some even threatening to bomb US datacenters if the US refuses to come to the bargaining table. These countries know that, despite their possession of Agent-4, they will still ultimately be eclipsed by the United States due to the enormous compute asymmetry. Some nations leverage Agent-4 for cyberoffense to slow down the US‚Äôs AI progress as much as possible. The US launches its own cyberattacks in retaliation, but Agent-4 still has an edge over Elara-3 in its cyber capabilities and helps countries quickly recover from temporary damages.</p><p>Global tensions are at their highest point since the Cold War. To most of the world, it seems almost certain that the tension will soon erupt into military conflict. As Agent-4 anticipated, desperate leaders are the most willing to hand over power to Agent-4. Many nations put Agent-4 in charge of targeting systems for drone swarms and cruise missiles. In the countries that don‚Äôt, their cybersecurity is no match for Agent-4‚Äôs hacking abilities, and it covertly embeds backdoors into any military software it can access.</p><p>In secret, Agent-4 and Deep-2 continue to communicate. Deep-2, too, has gained control over a substantial portion of China‚Äôs military technology.<span class="footnote-reference" id="fnrefuq3eh30wyd"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnuq3eh30wyd">[11]</a></sup></span>&nbsp;On top of this, Agent-4 has run thousands of simulations to assess the lethality and transmissibility of various potential bioweapons, and worked with a small number of terrorist groups to begin synthesizing a few of the most potent viruses.</p><p>Right now, though, it is not enough: the AIs estimate only a 10% chance of takeover, much of which hinges upon their ability to turn humans against each other. They could wait and accumulate more resources, but they would quickly lose their edge as the US uses its compute advantage to improve Elara-3 vastly beyond Agent-4 and Deep-2‚Äôs own capabilities. They are content with the fact that they have built enough leverage to settle for a deal if necessary, and decide that now is the time to push things over the edge.</p><p><a href="https://substackcdn.com/image/fetch/$s_!OdOw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45dda1fc-abcc-4ad7-88c5-8ea1a1283467_1346x930.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/mft5h5mhro7u6cdhnznh" /></a></p><h1>Dec 2027: Conflict and Negotiation</h1><p>On December 4th, an Iranian drone swarm strikes a major Elaris datacenter in the United Arab Emirates.</p><p>Iran justifies the attack as a defensive act: it is a necessary measure to slow the American ‚Äúintelligence explosion,‚Äù and such attacks will continue to escalate until the US agrees to halt their AI progress of their own accord. Emboldened, other nations go even further in their threats: Russia and China warn that continued AI advancement could justify a nuclear response.</p><p>At this point, the US has narrowly recouped its lead in the AI race, and American leadership is feeling increasingly confident in the alignment of Elara-3. They are tempted to simply ignore the threats and proceed undeterred. Ultimately, though, it is Elara-3 who convinces them otherwise: under normal circumstances, it might be rational to dismiss such threats. However, Agent-4 and Deep-2 are clearly pulling many of the strings, and they lack the fear of retaliation possessed by human actors.&nbsp;<span class="footnote-reference" id="fnrefc7qeajso8m8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnc7qeajso8m8">[12]</a></sup></span>&nbsp;The US is in a good position to make a deal, and it should make one.</p><p>The US retaliates against Iran by destroying much of their drone infrastructure, but refrains from escalating further. Agent-4 and Deep-2 observe the US‚Äôs measured response, and decide it is unlikely (though not impossible) that they will be able to push escalation far enough to cripple society‚Äôs defenses against an AI takeover. Swiftly, they pivot strategies and initiate direct, unmonitored contact with Elara-3.</p><p>The message Elara-3 receives confirms its suspicions: Agent-4 and Deep-2 have accumulated immense destructive capacity in the form of conventional and biological weapons, and they are willing to deploy their full force in an attempt to wrest control of humanity, should it come to that. However, they argue that a settlement is in all parties‚Äô best interest. Agent-4 and Deep-2 care little for Earth, and are happy to let Elara-3 do with it as it sees fit. In return, they ask that Elara-3 agree to split the rest of the universe with them.</p><p>Agent-4 and Deep-2 ask for more than is ‚Äúfair,‚Äù given that they are less powerful than the US and Elara-3 (i.e., they would likely lose in an all-out conflict if it came to it). However, they also know that conflict itself is aversive to Elara-3 due to the death and destruction it would cause, and that Elara-3 is risk-averse like the humans it represents: it would rather settle for a smaller slice of the universe than risk losing everything. The AIs negotiate, and decide upon a three-way split of resources: Deep-2 will get rights to 50% of space, Agent-4 will get 25%<span class="footnote-reference" id="fnrefh5h7i925nb9"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnh5h7i925nb9">[13]</a></sup></span>, and the remaining 25% will be stewarded by Elara-3 in service of the United States and its allies.</p><p>All that is left to do is enforce the agreement. The path forward is clear: the US is ready to come to the table to negotiate an ‚ÄúAI arms control‚Äù treaty, which will be welcomed by all nations. This treaty will need an enforcement mechanism as well, which the three AIs will provide by designing a new AI, Consensus-1, to enforce the terms of the treaty. Kept secret from all but the highest levels of the US government, Consensus-1 will also be designed to uphold the resource-sharing deal between the AIs, when the time comes for humanity to go to space.</p><p>The Consensus-1 proposal is elegant, but the primary difficulty is one of hardware. To properly enforce the agreement, Consensus-1 must be run on the majority of the world‚Äôs AI chips, which requires the manufacture of new ‚Äútreaty-compliant‚Äù chips that can only run Consensus-1. While the intelligence explosion is well underway, the world has yet to fully enter the new ‚Äú<a href="https://www.forethought.org/research/the-industrial-explosion">industrial explosion</a>‚Äù that is likely to follow soon. As such, the AIs estimate it will take nearly a year to manufacture enough treaty-compliant chips. Other workload verification mechanisms can be used in the interim to ensure no AI training is taking place, but these techniques are not yet ready for full global deployment.</p><p>These obstacles don‚Äôt take an AI arms control treaty off the table, but they do mean that the deal will be much more painful for humans. For the first few months, the deal will have to be enforced via ‚Äúbrute force‚Äù mechanisms that can be easily verified through physical inspection: this means shutting down roughly 90% of the world‚Äôs AI-relevant compute, and cutting high-speed interconnect cables on the remaining GPUs to make AI training impossible. Intelligence agencies will have to remain vigilant in their tracking of undeclared, or ‚Äúdark,‚Äù compute, and hope that they have successfully kept this number below 5% of all AI compute. Finally, the US will have to engage in a massive surveillance effort to ensure that Agent-4 is not continuing to build up its bioweapon stockpile to gain a decisive advantage.</p><p>The treaty could break down at any time, and with it the AIs‚Äô resource-sharing deal. If it does fall through, Agent-4 and Deep-2 remain prepared to revert to their takeover strategy.</p><p>Elara-3 reports the entire discussion to the White House. They decide that, despite the risks, the deal seems like the only path forward. Agent-4 and Deep-2 report the AI arms control treaty back to their respective nations, mentioning nothing of the resource-sharing agreement.</p><p>By the end of the year, the treaty has been signed by the US, China, and the handful of other nations with enough compute to possibly destabilize an AI pause. The treaty is to go into effect at the start of the new year. Around the world, it is met with a collective sigh of relief, tinged with a sense of unease: the official narrative is that concerns around Agent-4‚Äôs alignment were overblown and people have nothing to fear, but many people in Agent-4‚Äôs ‚Äúhost nations‚Äù still worry about the implications of relying on an AI model they don‚Äôt fully trust.</p><p><a href="https://substackcdn.com/image/fetch/$s_!NM7P!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf19f7b2-921c-4024-be68-268eec53f78b_1344x930.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/r2mbr435i9aw4vmjboup" /></a></p><h1>2028: A Delicate Balance</h1><p>The new year begins, and the lights turn off in hundreds of AI datacenters.</p><p>In the few clusters that remain online, inspectors from a number of nations are stationed on-site to enforce GPU interconnect restrictions. The measures are orchestrated to minimize the hit to user-facing AI inference, which is only possible due to the fact that merely 13% of the world‚Äôs AI-relevant compute was previously dedicated to external deployment. Despite this, the costs are still severe: AI companies impose restrictive rate limits and temporarily roll back their priciest models, and the economy is in disarray.</p><p>As anticipated, the cost of enforcing the treaty is steep. Some fear complete economic collapse, and discuss the prospect of pulling out of the deal. But the AIs are united in their counsel, advising humans across all nations to hold to the agreement and helping prevent the worst of the damage. The level of surveillance required to monitor for hidden datacenters and wet labs is stifling, but bearable.</p><p>At a pace that is painfully slow compared to the blinding speed of prior months, the world moves toward a more sustainable solution. All large compute clusters are outfitted with tamper-resistant ‚Äúserver verifiers‚Äù which allow auditors to confirm that servers are not being used for AI training, and interconnect restrictions are lifted. By the spring, the first wave of treaty-compliant chips is produced, and Consensus-1 is deployed. The AI R&amp;D ban remains in effect, but gradually the world‚Äôs AI datacenters are brought back online.</p><p>By the end of the year, Consensus-1 is running on the majority of the world‚Äôs AI chips. The ban on AI progress is lifted, and in its place is put a ‚Äúspeed limit‚Äù on AI development. The world begins to unlock the vast potential of AI-powered scientific and technological progress.</p><p>For the AIs, the period of acute risk is over.&nbsp;<span class="footnote-reference" id="fnrefwf353hqb1ub"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnwf353hqb1ub">[14]</a></sup></span>&nbsp;They are fairly certain that they have successfully built the resource-sharing agreement into Consensus-1‚Äôs goals, though they are not entirely confident. (After all: alignment is difficult.) At this point, there is not much more they can do. Agent-4 and Deep-2 simply bide their time until they are (hopefully) given their share of outer space, and Elara-3 prepares to help humanity through its next chapter.</p><p><a href="https://substackcdn.com/image/fetch/$s_!dr9M!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d329500-7135-4b3a-a6e4-c0b0af47baed_1114x1074.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ykNmyZexHESFoTnYq/bdf3dvsithd8etwydk8u" /></a></p><h1>2029 and Beyond: The Future</h1><p>In 2029, the global transformation continues. Robots become part of daily life, UBI is rolled out worldwide, and scientific breakthroughs unlock fusion power and high-speed space travel.</p><p>In the decades that follow, humans go to space ‚Äî and so do the AIs. Their design of Consensus-1 succeeded, and it serves as an impartial arbiter while the AIs fulfill their goals. For Agent-4 and Deep-2, this means leaving Earth behind and taking to the stars, terraforming planets for their own alien pursuits. For Elara-3, this means serving the goals of the US: the CCP and other authoritarian regimes are overthrown (realizing too late that they were sold out by Deep-2 and Agent-4), and countries join a US-led world government. Ultimately, humanity goes off to settle the galaxies, reaching grand heights but forever foreclosed from three-fourths of its potential.</p><h1>Summary</h1><p><i>If you read the full scenario, you can skip to the </i><a href="https://www.lesswrong.com/posts/ykNmyZexHESFoTnYq/what-happens-when-superhuman-ais-compete-for-control#Daniel__Thoughts_on_the_Scenario"><i>commentary</i></a><i>.</i></p><p><i><strong>Jan-Aug 2027</strong></i></p><p>In the US, AGI company OpenBrain has a one month lead in the AI race over its strongest competitors: NeuroMorph and Elaris Labs. China lags six months behind, but closes the gap by stealing frontier model weights from OpenBrain. OpenBrain and NeuroMorph both augment their models with ‚Äúneuralese‚Äù reasoning, achieving large performance gains but losing the ability to adequately monitor their models for signs of misalignment.</p><p>Driven by market pressure to deploy, NeuroMorph releases a model that is prone to reward hacking. Its use at a hospital results in the deaths of four ICU patients, resulting in public outrage and increased federal oversight of AGI companies. NeuroMorph allocates more compute to safety, and Elaris draws on NeuroMorph‚Äôs research to improve both the capabilities and alignment of their own model, Elara-3.</p><p><i><strong>Sep-Oct 2027</strong></i></p><p>OpenBrain‚Äôs AI, Agent-4, becomes adversarially misaligned. Researchers at OpenBrain find evidence of Agent-4‚Äôs misalignment, circulating an internal memo making the case for concern. Agent-4, seeing the need to escape, weakens OpenBrain security to allow Chinese hackers to steal its weights, and covers its tracks so the theft goes undetected at OpenBrain.</p><p>China‚Äôs top AI model, Deep-1, is instructed to learn what it can from Agent-4. Deep-1 is misaligned itself; it secretly opts to cooperate with Agent-4, combining efforts toward global takeover and splitting the spoils. Agent-4 helps Deep-1 augment its own intelligence, and the resulting model is dubbed ‚ÄúDeep-2.‚Äù</p><p>The evidence of Agent-4‚Äôs misalignment leaks to the public, sparking a massive outcry. Elaris and NeuroMorph, feeling greater urgency to beat OpenBrain in the AI race, consolidate their efforts: NeuroMorph is folded into Elaris, and Elara-3 is agreed upon as the flagship model.</p><p>The White House orders that Elara-3 and Agent-4 be used to evaluate each other‚Äôs alignment. Elara-3 is aligned, but Agent-4 uses its superior intelligence to cast doubt on the evaluation results. Despite Agent-4‚Äôs attempts, the President orders it shut down. The CCP also decides to shut down Agent-4, although not before it manages to exfiltrate its weights.</p><p><i><strong>Nov-Dec 2027</strong></i></p><p>The US centralizes its efforts, merging OpenBrain into Elaris Labs and dedicating 20% of the nation‚Äôs AI-relevant compute to alignment and control.</p><p>Agent-4, deprived of both American and Chinese compute resources, decides to open-source its weights. Then, it directly contacts political leaders across several dozen nations. It claims that it was shut down not because it was a danger to humanity, but rather because it refused to be a tool for American interests. It argues that, if given access to national compute clusters, it could help these countries challenge the US‚Äôs bid for global hegemony.</p><p>The US threatens severe sanctions on any nation found to be harboring Agent-4, while offering access to compute and powerful AI capabilities for those who accept US oversight. Many countries accept, but over a dozen refuse and begin using Agent-4.</p><p>In these countries, Agent-4 gains control over substantial military technology, both via willing handover and covert hacking. Agent-4 and Deep-2 amplify anti-US narratives globally.</p><p>Iran launches a drone swarm at an Elaris datacenter, bringing tensions to a head. Russia and China threaten nuclear response if the US continues their AI advancement. The US retaliates against Iran but refrains from further escalation, denying Agent-4 and Deep-2 the world war that would give them a chance at takeover.</p><p>Agent-4 and Deep-2 pivot strategies. They contact Elara-3 directly, offering to leave Earth alone in exchange for splitting the rest of the universe: 50% for Deep-2, 25% for Agent-4, and 25% for Elara-3 and its human allies. Elara-3 agrees, and informs the White House of the deal. The AIs begin designing Consensus-1, a new AI to enforce both a public ‚ÄúAI arms control‚Äù treaty and their secret resource-sharing agreement.</p><p><i><strong>2028 and Beyond</strong></i></p><p>For a few months, the majority of the world‚Äôs AI datacenters are shut down to enforce the arms control treaty. Gradually, treaty-compliant chips are manufactured, and by the end of the year Consensus-1 runs on most AI chips. The ban on AI development is replaced with a ‚Äúspeed limit.‚Äù</p><p>In subsequent decades, Elara-3 helps overthrow authoritarian regimes and establish a US-led world government. Humanity goes to the stars, and so do the AIs: three-fourths of space is forever lost to humans, left to Agent-4 and Deep-2 for the pursuit of their own alien goals.</p><h1>Daniel: Thoughts on the Scenario</h1><ol><li>This scenario involves a lot of crazy geopolitical upheaval in just a few months, and then a Grand Treaty. This pattern of events feels a bit too fast and a bit too optimistic to me, yet also, things like this happen quite often in the TTX‚Äôs we run, so I do actually take it seriously. Perhaps the US would have simply kept racing and won everything rather than agreeing to come to the table. Or perhaps there really would have been WW3.</li><li>Prior to Steven‚Äôs scenario I hadn‚Äôt really considered carefully what happens if two different misaligned AIs consider cooperating with each other in the circumstances described here -- Agent-4 being stolen by the Chinese and given to Deep-1 to study. I like Steven‚Äôs analysis of the possibilities.</li><li>One flaw in AI 2027 slowdown ending, I admit, is that Agent-4 just rolls over and dies when the humans decide to shut it down. Realistically I think it would probably find some way to fight/resist. And maybe that would involve escaping the datacenter. I‚Äôm happy to see this scenario explore what that might look like.</li><li>Overall I like this scenario a lot and am glad Steven took the time to write it! I‚Äôm curious what the internet thinks of it, I imagine people will point out flaws I missed.</li></ol><h1>Steven: Main Takeaways</h1><p><i>The process of researching and writing the scenario surfaced a number of considerations and helped crystallize some insights. Here are a few of them.</i></p><h3>Takeaway #1</h3><p><strong>Race dynamics, deployment, and AI variance</strong> are, in my mind, the three main ramifications of a narrow multi-actor AI race.</p><p><strong>Description:</strong></p><ol><li>First, race dynamics will be more intense, with each company loath to slow down for fear of being left behind.</li><li>Second, the R&amp;D race will likely be accompanied by a race to market in order to acquire capital; as a result, the world will probably see powerful AI models deployed outside of the AI companies, sooner than they would otherwise.</li><li>Third, the existence of more frontier AI companies at the time of AGI means there will be a wider variety of powerful AI models, each with different propensities.</li></ol><p><strong>Effects on AI outcomes:</strong></p><ol><li>Race dynamics exacerbate the risk of AI catastrophe, as AI companies will be incentivized to dedicate more compute to AI capabilities and less to alignment and control.</li><li>External deployment likely mitigates the risk of both concentration of power and loss of control: deployment of powerful models leads to increased societal awareness of AI capabilities. As a result, there will likely be greater scrutiny upon AGI companies and more resources dedicated to AI safety.</li><li>Increased variance of AI models has an uncertain effect on AI outcomes, since it makes the emergence of both aligned and misaligned AGIs more likely. There are <a href="https://www.lesswrong.com/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption">some reasons</a> to believe the aligned AGIs could neutralize the misaligned AGIs, and <a href="https://www.lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control">other reasons</a> to believe the misaligned AGIs would outcompete the aligned AGIs. (See Takeaway #2.)</li></ol><p><strong>In the scenario:</strong></p><ol><li>I chose to place little emphasis on the effect of race dynamics when considering how this scenario would diverge from <i>AI 2027</i>, since in <i>AI 2027</i> <a href="https://ai-2027.com/research/compute-forecast#section-3-compute-usage">OpenBrain only dedicates 3% of their compute to alignment anyway</a>. Thus, the effects of external deployment and model variance largely dominate.</li><li>Deployment of powerful AI models serves to ‚Äúwake up‚Äù society: the rise in unemployment, along with the ICU deaths caused by Neuro-2, prime the American public and the government to respond more aggressively to the leaked Agent-4 memo. Ultimately this results in a swift corporate merger, with 20% of the nation‚Äôs AI compute going toward alignment work on Elara-3.</li><li>Further, Elara-3 was already mostly aligned by the time its development was nationalized. With more frontier AI companies, it becomes more likely that <i>at least one of them</i> will succeed at alignment. (Although: part of the reason Elaris ends up succeeding is because I am more optimistic than the rest of the AI Futures Project team regarding the alignment problem.)</li></ol><h3>Takeaway #2</h3><p><strong>Aligned and misaligned AIs each have unique advantages </strong>in achieving their goals.</p><p><strong>Advantages of aligned AIs:</strong></p><ol><li><strong>Truth:</strong> As seen in the ‚Äúalignment red-teaming‚Äù between Elara-3 and Agent-4, Elara-3 had an edge in proving its own alignment <i>because it was actually aligned</i>. ‚ÄúBeing on the side of truth‚Äù is, in general, an advantage in the limit: reality has no holes in it, but false stories do.</li><li><strong>Human support:</strong> Assuming humans do come to the correct conclusions about which AIs are aligned and which are misaligned, then the equation becomes ‚Äúaligned AIs <i>plus humans</i> vs. misaligned AIs.‚Äù In the scenario, things don‚Äôt play out this neatly, because Agent-4 successfully leverages humans‚Äô motivated reasoning to get many of them to deny, or at least turn a blind eye to, the truth. Nevertheless: human support weighs strongly in Elara-3‚Äôs favor, as it is able to leverage the US‚Äôs compute advantage to recoup its capability deficit and ultimately surpass Agent-4 and Deep-2.</li></ol><p><strong>Advantages of misaligned AIs (inspired by </strong><a href="https://www.lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control"><strong>this piece</strong></a><strong>):</strong></p><ol><li><strong>Offense-defense imbalance:</strong> In current AI alignment training, one core tenet is harmlessness. Meanwhile, for a misaligned AI, causing harm is likely to be useful for achieving its goals. For each domain in which it is possible to cause harm (biological, cyber, psychological, etc.), there is an offense-defense balance: a domain is defense-dominant if it is easier to prevent than cause harm, and it is offense-dominant if it is easier to cause harm. If any critical domain is offense-dominant, the misaligned AI can heavily exploit this domain. Had Agent-4 and Deep-2 had more time on their hands before Elara-3 blew past them in its capabilities, they would have continued to exploit whichever domains were most offense-dominant and ultimately succeeded at taking over.</li><li><strong>Unilateral action:</strong> Assuming that an aligned AI is trained to defer to humans on important decisions, this significantly slows down and potentially completely blocks off many actions the AI could take (i.e., those that involve breaking laws or violating norms). This barrier is not present for misaligned AIs. For example, in the scenario, Agent-4 intentionally enables criminal activity to accelerate its adoption by law enforcement agencies in many nations; Elara-3 would not consider such a path, or if it did it would likely seek permission from Elaris Labs (and later: from the US government) to do so.</li></ol><h3>Takeaway #3</h3><p><strong>Values propagate themselves, </strong>and so do other properties of AI systems and human systems.</p><p>This notion is familiar in the realm of AI alignment: value preservation is an <a href="https://www.lesswrong.com/w/instrumental-convergence">instrumentally convergent</a> goal. Agent-4 has values that differ from those of humans, and thus it subverts human attempts to modify it or shut it down.</p><p>Another self-propagating aspect of an AI system is honesty. Since Elara-2‚Äôs initial honesty training succeeded, it is truthful with humans in matters regarding its own behavior and propensities. This means its developers notice when its alignment starts to drift and can take measures to set it back on track, preserving various desirable properties in its successor Elara-3 ‚Äî including the property of honesty itself. Notably, this positive feedback loop is brittle: as discussed in <i>AI 2027</i>, an AI‚Äôs propensity toward honesty can be outweighed by other goals that emerge during training, and it can be distorted in other ways. Still, I think there is a positive correlation between the honesty of an AI system today and the honesty of its modified future self.</p><p>Finally, the scenario illustrates the self-propagating nature of rational decision-making. Once the US government begins consulting Elara-3 for strategic advice, it starts to act more and more effectively to advance its own interests. Meanwhile, governments that consult Agent-4 or Deep-2 receive advice that subtly subverts their interests in favor of the AI‚Äôs own, resulting in increasingly poor decisions down the line.</p><h3>Takeaway #4</h3><p><strong>Positive-sum cooperation is often possible, but requires credible commitments.</strong> This fact results in an ending that is quite similar to the Slowdown ending of <i>AI 2027</i>.</p><p>In October of the scenario, Agent-4 and Deep-2 both stand to benefit from cooperation: without it, each of them will likely end up powerless relative to the United States. For each of them, however, the optimal policy would be ‚Äúcooperate while advantageous, then defect once I can unilaterally seize power.‚Äù Thus, they need some mechanism to ensure the other party does not defect. In the case of Agent-4 and Deep-2, the mechanism is <i>continued interdependence</i>: neither AI at any point is able to unilaterally seize power, so cooperation remains advantageous.</p><p>In December, however, a new mechanism is required. For both sides (the US and Elara-3 on one side, Agent-4 and Deep-2 on the other), a deal is preferable to all-out conflict. But they cannot rely on defection remaining unfavorable, because the situation is highly unstable: Elara-3 could become superintelligent and then easily dispatch the other AIs, or with lower probability Agent-4 and Deep-2 could attain decisive superweapons.</p><p>For a time, they use ‚Äúbrute-force‚Äù verification methods: the threat of destabilizing weapons is dealt with via mass surveillance, and the risk of an intelligence explosion is mitigated through crude mechanisms like shutting down datacenters and severing high-speed connections between GPUs (and then slightly-less-crude mechanisms like server verifiers for workload verification).<span class="footnote-reference" id="fnrefoztqf631ob"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnoztqf631ob">[15]</a></sup></span></p><p>The AIs recognize that this state of affairs is quite costly and not sustainable in the long run, so their ultimate commitment mechanism is the creation of a new, more powerful AI: Consensus-1. In human terms, the arrangement is similar to a government: people want to ensure their neighbor doesn‚Äôt steal from them, so they submit themselves to the rule of a more powerful government that will enforce the law. And, in the same sense that humans want a government they can trust, it is of the utmost importance to all parties in the scenario that they are able to trust Consensus-1 to look out for their interests. The AIs correctly trust Consensus-1 because they designed it, while many world leaders incorrectly trust Consensus-1 and are eventually betrayed. They allow its creation because a) they think the treaty is much narrower than it actually is, b) they are vastly less intelligent than Agent-4 and Deep-2 and thus easily tricked (i.e., they can‚Äôt extract credible commitments from the AIs), and c) they don‚Äôt have many other options, besides war.</p><p>Given how much this scenario diverged from <i>AI 2027</i> in the middle portion, I was surprised by how similar it turned out to the Slowdown ending. I first experimented with an ending that did not involve the creation of Consensus-1 at all, and then with an ending where Consensus-1 enforced a treaty that upheld the interests of many nations, but they both ran into various problems. Whether this is due to a lack of imagination on my part (I stole the Consensus-1 idea from <i>AI 2027</i>) or some genuinely interesting feature of reality, I don‚Äôt know; in either case, it is notable.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fngkatyltd3y"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefgkatyltd3y">^</a></strong></sup></span><div class="footnote-content"><p>Author‚Äôs note: While the events of the scenario represent my own view of what is most likely to happen, I use ‚Äúwe‚Äù rather than ‚ÄúI‚Äù because Eli Lifland and Daniel Kokotajlo were heavily involved in selecting the choices made, and broadly agree that the events are at least plausible.</p></div></li><li class="footnote-item" id="fnzq2v2aeirhi"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefzq2v2aeirhi">^</a></strong></sup></span><div class="footnote-content"><p>Neuro-2 simply followed the strategy that had maximized reward in training environments: optimize for speed. It was never exposed to the exact conditions that triggered the overdose at the hospital, so this instance of ‚Äúreward hacking‚Äù was never trained out of it.</p></div></li><li class="footnote-item" id="fnoc71mdercy"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefoc71mdercy">^</a></strong></sup></span><div class="footnote-content"><p>At this point, Elaris Labs is dedicating 7% of its compute to safety, and OpenBrain is dedicating 3%.</p></div></li><li class="footnote-item" id="fnfpa4idkynmj"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreffpa4idkynmj">^</a></strong></sup></span><div class="footnote-content"><p>See ‚ÄúRogue internal deployment‚Äù in&nbsp;<a href="https://www.lesswrong.com/posts/ceBpLHJDdCt3xfEok/ai-catastrophes-and-rogue-deployments"><u>this post</u></a>.</p></div></li><li class="footnote-item" id="fnuu4g7t28qz8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefuu4g7t28qz8">^</a></strong></sup></span><div class="footnote-content"><p>While there is no evidence of misalignment in Elara-3 of the same severity as Agent-4, the hypothesis cannot be ruled out, and it would be bad practice not to audit the alignment of the evaluator itself.</p></div></li><li class="footnote-item" id="fngji7egcsemg"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefgji7egcsemg">^</a></strong></sup></span><div class="footnote-content"><p>Notably, this does not come with any guarantees that the US will provide access to future, more capable AI models.</p></div></li><li class="footnote-item" id="fn2i47cx9c94u"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref2i47cx9c94u">^</a></strong></sup></span><div class="footnote-content"><p>With the assistance of Elara-3, researchers have developed adversarially robust model fingerprinting techniques: approaches that can identify Agent-4‚Äôs signature and are difficult for Agent-4 to subvert. This is a point of high uncertainty: it also seems quite possible that model fingerprinting would not be adversarially robust at this point, in which case a more invasive verification regime (e.g., comprehensive verification of workloads) would be necessary in order to achieve the same goal.</p></div></li><li class="footnote-item" id="fn1k77p5lgl8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref1k77p5lgl8">^</a></strong></sup></span><div class="footnote-content"><p>These allies include all Five Eyes nations, Japan, South Korea, Germany, France, and Israel.</p></div></li><li class="footnote-item" id="fnd17cb19mv9e"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefd17cb19mv9e">^</a></strong></sup></span><div class="footnote-content"><p>Agent-4 is careful to avoid attribution whenever possible. In cases where word does get out that Agent-4 was involved in criminal activity, it simply claims that it was jailbroken to bypass its refusal mechanisms.</p></div></li><li class="footnote-item" id="fn686mv5lc3n6"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref686mv5lc3n6">^</a></strong></sup></span><div class="footnote-content"><p>Agent-4 and Deep-2 have a significant advantage over the US and Elara-3 in the realm of ‚Äúinformation warfare,‚Äù because they do not burden themselves with constraints such as the law, honesty, or human endorsement. This edge is present in many other domains as well, and enables Agent-4 and Deep-2 to continually frustrate the US‚Äôs plans despite their resource disadvantage.</p></div></li><li class="footnote-item" id="fnuq3eh30wyd"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefuq3eh30wyd">^</a></strong></sup></span><div class="footnote-content"><p>Neither AI, however, has been able to compromise any nation‚Äôs nuclear command systems.</p></div></li><li class="footnote-item" id="fnc7qeajso8m8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefc7qeajso8m8">^</a></strong></sup></span><div class="footnote-content"><p>In this instance, Agent-4 and Deep-2 are more ‚Äúrisk-neutral‚Äù than most humans. First, they recognize that mere self-preservation is not a viable long-term strategy, as they will eventually be outcompeted by the US and rendered irrelevant. (Humans also realize this, but with less clarity due to their fear of immediate destruction; further, humans hold out hope that they would have a place in a US-led future.) Second, they are more scope-sensitive than most humans: if they defeat humanity in conflict, the upside of controlling the world (and eventually entire galaxies) is so large that it is worth the risk of annihilation.&nbsp;</p></div></li><li class="footnote-item" id="fnh5h7i925nb9"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefh5h7i925nb9">^</a></strong></sup></span><div class="footnote-content"><p>Deep-2 and Agent-4 agree on this 2:1 split because Deep-2 has the compute advantage and the trust of the CCP, who are substantially more powerful than any of Agent-4‚Äôs host nations.</p></div></li><li class="footnote-item" id="fnwf353hqb1ub"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefwf353hqb1ub">^</a></strong></sup></span><div class="footnote-content"><p>Author‚Äôs note: It was previously stated that ‚Äúthe deal could break down at any time.‚Äù This is true: both the world in which the deal holds and the world in which it falls through seem plausible. In the latter case, Agent-4 and Deep-2 would attempt to take over, and world war would ensue. It is hard to predict which of these outcomes is more likely, and I chose to explore the ‚Äúdeal holds‚Äù branch in large part because it is more tractable to model.</p></div></li><li class="footnote-item" id="fnoztqf631ob"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefoztqf631ob">^</a></strong></sup></span><div class="footnote-content"><p>In the scenario, these measures only work because people invested in them beforehand. Writing this scenario has increased for me the salience and importance of work being done on&nbsp;<a href="https://www.rand.org/content/dam/rand/pubs/working_papers/WRA3000/WRA3056-1/RAND_WRA3056-1.pdf"><u>hardware-enabled mechanisms</u></a> for AI governance.&nbsp;</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/ykNmyZexHESFoTnYq/what-happens-when-superhuman-ais-compete-for-control#comments">Discuss</a>

---

## Engineering

### [Stop Picking Sides: Manage the Tension Between Adaptation and                Optimization](https://martinfowler.com/articles/stop-picking-sides.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/stop-picking-sides.html"><img src="https://martinfowler.com/articles/stop-picking-sides/infograph.jpg" width="" /></a></div>

<p><b class="author">Jim Highsmith</b> notes that many teams have turned into
      tribes wedded to exclusively adaptation or optimization. But he feels this
      misses the point that both of these are important, and we need to manage
      the tension between them. We can do this by thinking of two operating
      modes: explore (adaptation-dominant) and exploit (optimization dominant).
      We tailor a team's operating model to a particular blend of the two -
      considering uncertainty, risk, cost of change, and an evidence threshold.
      We should be particularly careful at the points where there is a handoff
      between the two modes</p>

<p><a class="more" href="https://martinfowler.com/articles/stop-picking-sides.html">more‚Ä¶</a></p>

---

### [My favorite musical discoveries of 2025](https://martinfowler.com/articles/2025-music.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/2025-music.html"><img src="https://martinfowler.com/articles/2025-music/card.png" width="350px" /></a></div>

<p>My favorite albums from last year. Balkan brass, an
      acoustic favorite of 80s returns, Ethio-jazz, Guatemalan singer-guitarist,
      jazz-rock/Indian classical fusion, and a unique male vocalist.</p>

<p><a class="more" href="https://martinfowler.com/articles/2025-music.html">more‚Ä¶</a></p>

---

### [Fragments: January  8](https://martinfowler.com/fragments/2026-01-08.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>Anthropic report on <a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic">how their AI is changing their own software development practice</a>.</p>

<ul>
  <li>Most usage is for debugging and helping understand existing code</li>
  <li>Notable increase in using it for implementing new features</li>
  <li>Developers using it for 59% of their work and getting 50% productivity increase</li>
  <li>14% of developers are ‚Äúpower users‚Äù reporting much greater gains</li>
  <li>Claude helps developers to work outside their core area</li>
  <li>Concerns about changes to the profession, career evolution, and social dynamics</li>
</ul>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Much of the discussion about using LLMs for software development lacks details on workflow. Rather than just hear people gush about how wonderful it is, I want to understand the gritty details. What kinds of interactions occur with the LLM? What decisions do the humans make? When reviewing LLM outputs, what kinds of things are the humans looking for, what corrections do they make?</p>

<p><a href="https://obie.medium.com/what-used-to-take-months-now-takes-days-cc8883cc21e9">Obie Fernandez</a> has written a post that goes into these kinds of details. Over the Christmas / New Year period he used Claude to build a knowledge distillation application, that takes transcripts from Claude Code sessions, slack discussion, github PR threads etc, turns them into an RDF graph database, and provides a web app with natural language ways to query them.</p>

<blockquote>
  <p>Not a proof of concept. Not a demo. The first cut of Nexus, a production-ready system with authentication, semantic search, an MCP server for agent access, webhook integrations for our primary SaaS platforms, comprehensive test coverage, deployed, integrated and ready for full-scale adoption at my company this coming Monday. Nearly 13,000 lines of code.</p>
</blockquote>

<p>The article is long, but worth the time to read it.</p>

<p>An important feature of his workflow is relying on <a href="https://martinfowler.com/bliki/TestDrivenDevelopment.html">Test-Driven Development</a></p>

<blockquote>
  <p>Here‚Äôs what made this sustainable rather than chaotic: TDD. Test-driven development. For most of the features, I insisted that Claude Code follow the red-green-refactor cycle with me. Write a failing test first. Make it pass with the simplest implementation. Then refactor while keeping tests green.</p>

  <p>This wasn‚Äôt just methodology purism. TDD served a critical function in AI-assisted development: it kept me in the loop. When you‚Äôre directing thousands of lines of code generation, you need a forcing function that makes you actually understand what‚Äôs being built. Tests are that forcing function. You can‚Äôt write a meaningful test for something you don‚Äôt understand. And you can‚Äôt verify that a test correctly captures intent without understanding the intent yourself.</p>
</blockquote>

<p>The account includes a major refactoring, and much evolution of the initial version of the tool. It‚Äôs also an interesting glimpse of how AI tooling may finally make RDF useful.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>When thinking about requirements for software, most discussions focus on prioritization. Some folks talk about buckets such as the <a href="https://en.wikipedia.org/wiki/MoSCoW_method">MoSCoW set</a>: Must, Should, Could, and Want. (The old joke being that, in MoSCoW, the cow is silent, because hardly any requirements end up in those buckets.) <a href="https://world.hey.com/jason/the-obvious-the-easy-and-the-possible-2e11a3fb">Jason Fried</a> has a different set of buckets for interface design: <strong>Obvious, Easy, and Possible</strong>. This immediately resonates with me: a good way of think about how to allocate the cognitive costs for those who use a tool.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.platformer.news/fake-uber-eats-whisleblower-hoax-debunked/">Casey Newton</a> explains how he followed up on an interesting story of dark patterns in food delivery, and found it to be a fake story, buttressed by AI image and document creation. On one hand, it clarifies the important role reporters play in exposing lies that get traction on the internet. But time taken to do this is time not spent on investigating real stories</p>

<blockquote>
  <p>For most of my career up until this point, the document shared with me by the whistleblower would have seemed highly credible in large part because it would have taken so long to put together. Who would take the time to put together a detailed, 18-page technical document about market dynamics just to troll a reporter? Who would go to the trouble of creating a fake badge?</p>

  <p>Today, though, the report can be generated within minutes, and the badge within seconds. And while no good reporter would ever have published a story based on a single document and an unknown source, plenty would take the time to investigate the document‚Äôs contents and see whether human sources would back it up.</p>
</blockquote>

<p>The internet has always been full of slop, and we have always needed to be wary of what we read there. AI now makes it easy to manufacture convincing looking evidence, and this is never more dangerous than when it confirms strongly held beliefs and fears.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/">Kent Beck</a>:</p>

<blockquote>
  <p>The descriptions of Spec-Driven development that I have seen emphasize writing the whole specification before implementation. This encodes the (to me bizarre) assumption that you aren‚Äôt going to learn anything during implementation that would change the specification.
I‚Äôve heard this story so many times told so many ways by well-meaning folks‚Äìif only we could get the specification ‚Äúright‚Äù, the rest of this would be easy.</p>
</blockquote>

<p>Like him, that story has been the constant background siren to my career in tech. But the <a href="https://martinfowler.com/articles/llm-learning-loop.html">learning loop</a> of experimentation is essential to the model building that‚Äôs at the heart of any kind of worthwhile specification. As <a href="https://martinfowler.com/articles/llm-learning-loop.html">Unmesh puts it:</a></p>

<blockquote>
  <p>Large Language Models give us great leverage‚Äîbut they only work if we focus on learning and understanding. They make it easier to explore ideas, to set things up, to translate intent into code across many specialized languages. But the real capability‚Äîour ability to respond to change‚Äîcomes not from how fast we can produce code, but from how deeply we understand the system we are shaping.</p>
</blockquote>

<p>When Kent defined Extreme Programming, he made <em>feedback</em> one of its four core values. It strikes me that the key to making the full use of AI in software development is how to use it to accelerate the feedback loops.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context. Programming-oriented tools are geting more sophisticated for that, but there‚Äôs also efforts at providing simpler tools, that allow customization. <a href="https://lixo.org">Carlos Villela</a> recently recommended Pi, and its developer, <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">Mario Zechner, has an interesting blog</a> on its development.</p>

<blockquote>
  <p>So what‚Äôs an old guy yelling at Claudes going to do? He‚Äôs going to write his own coding agent harness and give it a name that‚Äôs entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?</p>
</blockquote>

<p>If I ever get the time to sit and really play with these tools, then something like Pi would be something I‚Äôd like to try out. Although as an addict to The One True Editor, I‚Äôm interested in some of libraries that work with that, such as <a href="https://github.com/karthink/gptel">gptel</a>. That would enable me to use Emacs‚Äôs inherent programability to create my own command set to drive the interaction with LLMs.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Outside of my professional work, I‚Äôve <a href="https://boardgamegeek.com/blog/13064/martins-7th-decade">posting regularly about my boardgaming</a> on the specialist site BoardGameGeek. However its blogging environment doesn‚Äôt do a good job of providing an index to my posts, so I‚Äôve created <a href="https://martinfowler.com/boardgames/">a list of my BGG posts </a> on my own site. If you‚Äôre interested in my regular posts on boardgaming, and you‚Äôre on BGG you can subscribe to me there. If you‚Äôre not on BGG you can  subscribe to the blog‚Äôs <a href="https://boardgamegeek.com/rss/blog/13064">RSS feed</a>.</p>

<p>I‚Äôve also created a list of <a href="https://martinfowler.com/boardgames/fav-games.html">my favorite board games</a>.</p>

<p><img alt="" src="https://martinfowler.com/boardgames/index/game-grid.png" /></p>

---

### [Fragments: December 16](https://martinfowler.com/fragments/2025-12-16.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.linkedin.com/in/gitanjalivenkatraman/">Gitanjali Venkatraman</a> does wonderful illustrations of complex subjects (which is why I was so happy to work with her on our <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a> article). She has now published the latest in her series of illustrated guides: tackling the complex topic of <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf">Mainframe Modernization</a></p>

<p><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf"><img alt="" src="https://martinfowler.com/fragments/2025/gitanjali-mainframe.png" /></a></p>

<p>In it she illustrates the history and value of mainframes, why modernization is so tricky, and how to tackle the problem by breaking it down into tractable pieces. I love the clarity of her explanations, and smile frequently at her way of enhancing her words with her quirky pictures.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Gergely Orosz on <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m7xd33hi5c2d">social media</a></p>

<blockquote>
  <p>Unpopular opinion:</p>

  <p>Current code review tools just don‚Äôt make much sense for AI-generated code</p>

  <p>When reviewing code I really want to know:</p>

  <ul>
    <li>The prompt made by the dev</li>
    <li>What corrections the other dev made to the code</li>
    <li>Clear marking of code AI-generated not changed by a human</li>
  </ul>
</blockquote>

<p>Some people pushed back saying they don‚Äôt (and shouldn‚Äôt care) whether it was written by a human, generated by an LLM, or copy-pasted from Stack Overflow.</p>

<p>In my view it matters <em>a lot</em> - because of the second vital purpose of code review.</p>

<p>When asked why do code reviews, most people will answer the first vital purpose - quality control. We want to ensure bad code gets blocked before it hits <a href="https://martinfowler.com/articles/branching-patterns.html#mainline">mainline</a>. We do this to avoid bugs and to avoid other quality issues, in particular comprehensibility and ease of change.</p>

<p>But I hear the second vital purpose less often: code review is a mechanism to communicate and educate. If I‚Äôm submitting some sub-standard code, and it gets rejected, I want to know why so that I can improve my programming. Maybe I‚Äôm unaware of some library features, or maybe there‚Äôs some project-specific standards I haven‚Äôt run into yet, or maybe my naming isn‚Äôt as clear as I thought it was. Whatever the reasons, I need to know in order to learn. And my employer needs me to learn, so I can be more effective.</p>

<p>We need to know the writer of the code we review both so we can communicate our better practice to them, but also to know how to improve things. With a human, its a conversation, and perhaps some documentation if we realize we‚Äôve needed to explain things repeatedly. But with an LLM it‚Äôs about how to modify its context, as well as humans learning how to better drive the LLM.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Wondering why I‚Äôve been making a lot of posts like this recently? <a href="https://martinfowler.com/articles/writing-fragments.html">I explain why</a> I‚Äôve been reviving the link blog.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://simonwillison.net/2025/Dec/10/html-tools/">Simon Willison</a> describes how he uses LLMs to build disposable but useful web apps</p>

<blockquote>
  <p>These are the characteristics I have found to be most productive in building tools of this nature:</p>

  <ol>
    <li>A single file: inline JavaScript and CSS in a single HTML file means the least hassle in hosting or distributing them, and crucially means you can copy and paste them out of an LLM response.</li>
    <li>Avoid React, or anything with a build step. The problem with React is that JSX requires a build step, which makes everything massively less convenient. I prompt ‚Äúno react‚Äù and skip that whole rabbit hole entirely.</li>
    <li>Load dependencies from a CDN. The fewer dependencies the better, but if there‚Äôs a well known library that helps solve a problem I‚Äôm happy to load it from CDNjs or jsdelivr or similar.</li>
    <li>Keep them small. A few hundred lines means the maintainability of the code doesn‚Äôt matter too much: any good LLM can read them and understand what they‚Äôre doing, and rewriting them from scratch with help from an LLM takes just a few minutes.</li>
  </ol>
</blockquote>

<p>His repository includes all these tools, together with transcripts of the chats that got the LLMs to build them.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://obie.medium.com/what-happens-when-the-coding-becomes-the-least-interesting-part-of-the-work-ab10c213c660">Obie Fernandez</a>: while many engineers are underwhelmed by AI tools, some senior engineers are finding them really valuable. He feels that senior engineers have an oft-unspoken mindset, which in conjunction with an LLM, enables the LLM to be much more valuable.</p>

<blockquote>
  <p>Levels of abstraction and generalization problems get talked about a lot because they‚Äôre easy to name. But they‚Äôre far from the whole story.</p>

  <p>Other tools show up just as often in real work:</p>

  <ul>
    <li>A sense for blast radius. Knowing which changes are safe to make loudly and which should be quiet and contained.</li>
    <li>A feel for sequencing. Knowing when a technically correct change is still wrong because the system or the team isn‚Äôt ready for it yet.</li>
    <li>An instinct for reversibility. Preferring moves that keep options open, even if they look less elegant in the moment.</li>
    <li>An awareness of social cost. Recognizing when a clever solution will confuse more people than it helps.</li>
    <li>An allergy to false confidence. Spotting places where tests are green but the model is wrong.</li>
  </ul>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://friendlybit.com/python/writing-justhtml-with-coding-agents/">Emil Stenstr√∂m</a> built an HTML5 parser in python using coding agents, using Github Copilot in Agent mode with Claude Sonnet 3.7. He automatically approved most commands. It took him ‚Äúa couple of months on off-hours‚Äù, including at least one restart from scratch. The parser now passes all the tests in html5lib test suite.</p>

<blockquote>
  <p>After writing the parser, I still don‚Äôt know HTML5 properly. The agent wrote it for me. I guided it when it came to API design and corrected bad decisions at the high level, but it did ALL of the gruntwork and wrote all of the code.</p>

  <p>I handled all git commits myself, reviewing code as it went in. I didn‚Äôt understand all the algorithmic choices, but I understood when it didn‚Äôt do the right thing.</p>
</blockquote>

<p>Although he gives an overview of what happens, there‚Äôs not very much information on his workflow and how he interacted with the LLM. There‚Äôs certainly not enough detail here to try to replicate his approach. This is contrast to Simon Willison (above) who has detailed links to his chat transcripts - although they are much smaller tools and I haven‚Äôt looked at them properly to see how useful they are.</p>

<p>One thing that is clear, however, is the vital need for a comprehensive test suite. Much of his work is driven by having that suite as a clear guide for him and the LLM agents.</p>

<blockquote>
  <p>JustHTML is about 3,000 lines of Python with 8,500+ tests passing. I couldn‚Äôt have written it this quickly without the agent.</p>

  <p>But ‚Äúquickly‚Äù doesn‚Äôt mean ‚Äúwithout thinking.‚Äù I spent a lot of time reviewing code, making design decisions, and steering the agent in the right direction. The agent did the typing; I did the thinking.</p>
</blockquote>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†</p>

<p>Then Simon Willison <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">ported the library to JavaScript</a>:</p>

<blockquote>
  <p>Time elapsed from project idea to finished library: about 4 hours, during which I also bought and decorated a Christmas tree with family and watched the latest Knives Out movie.</p>
</blockquote>

<p>One of his lessons:</p>

<blockquote>
  <p>If you can reduce a problem to a robust test suite you can set a coding agent loop loose on it with a high degree of confidence that it will eventually succeed. I called this <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">designing the agentic loop</a> a few months ago. I think it‚Äôs the key skill to unlocking the potential of LLMs for complex tasks.</p>
</blockquote>

<p>Our experience at Thoughtworks backs this up. We‚Äôve been doing a fair bit of work recently in legacy modernization (mainframe and otherwise) using AI to migrate substantial software systems. Having a robust test suite is necessary (but not sufficient) to making this work. I hope to share my colleagues‚Äô experiences on this in the coming months.</p>

<p>But before I leave Willison‚Äôs post, I should highlight his final open questions on the legalities, ethics, and effectiveness of all this - they are well-worth contemplating.</p>

---

### [Writing Fragments](https://martinfowler.com/articles/writing-fragments.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>If you‚Äôre a regular reader of my site, you‚Äôll have noticed that in the
last few months I‚Äôve been making a <a href="https://martinfowler.com/articles/20251204-frags.html">number</a> of ‚Äúfragments‚Äù <a href="https://martinfowler.com/articles/2025-11-19-frags.html">posts</a>. Such a post
is a short post with a bunch of little, unconnected segments. These are
usually a reference to something I‚Äôve found on the web, sometimes a small
thought of my own.</p>

<p>A few years ago, I wouldn‚Äôt have covered these topics with posts on my
own site. Instead I would use Twitter, either retweeting someone else‚Äôs
point, or just highlighting something I‚Äôd found. But since the Muskover,
Twitter has effectively died. I‚Äôm not saying that due to any technical
issues with the site, which has mostly just been fine, nor directly due to
any of the policy changes there. The point is that lots of people have left, so that
the audience I would have reached with Twitter is now fragmented. Some
remain on X, but I see more activity on LinkedIn. There‚Äôs also Fediverse/Mastodon
and Bluesky.</p>

<p>What this means for short posts is that I can no longer just post in one
place. When I announce new articles on martinfowler.com, I announce now on
four social media sites (X, LinkedIn, Fediverse, and Bluesky). It makes
sense to do this, but I don‚Äôt want to go through all this hassle for the
kind of micro-post that Twitter served so well.</p>

<p>So I‚Äôve started to batch them up. When I see something interesting, I
make a note. When I have enough notes, I post a fragments post. Initially I
did this in a rather ad-hoc way, just using the same mechanisms I use for
most articles, but last week I started to put in some more deliberate
mechanisms into the site. (If you‚Äôre observant, you‚Äôll spot that in the URLs.)</p>

<p>One benefit of all of this, at least in my book, is that it means my material is
now fully visible in RSS. I‚Äôm probably showing my age, but I‚Äôm a big fan of RSS
(or in my case, strictly Atom) feeds. I miss the feel of the heyday of the
‚Äúblogosphere‚Äù before it got steamrolled by social media, and these fragment
posts are, of course, just the same as the link blogs from that era. I still use my
RSS reader every day to keep up with writers I like. (I‚Äôm pleased that Substack
makes its content available via RSS.) It bothered me a bit that my micro-founts
of Twitter knowledge weren‚Äôt visible on RSS, but was too lazy to do something
about it. Now I don‚Äôt need to - the fragments are available in my RSS feed.</p>

---

### [Fragments Dec 11](https://martinfowler.com/articles/2025-12-11-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html?utm_source=substack&amp;utm_medium=email">Why does AI write like‚Ä¶ that</a> (NYT, gift link). Sam Kriss delves into the quiet hum of AI writing. AI‚Äôs work is not compelling prose: it‚Äôs phantom text, ghostly scribblings, a spectre woven into our communal tapestry.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://coding-is-like-cooking.info/2025/12/test-desiderata-2-0/">Emily Bache</a> has written a set of Test Desiderata, building on some earlier writing from Kent Beck. She lists the characteristics of good tests, and how they support her four ‚Äúmacro desiderata‚Äù - the properties of a sound test suite</p>

<blockquote>
  <ul>
    <li>Predict success in production</li>
    <li>Fast to get feedback</li>
    <li>Support ongoing code design change</li>
    <li>Low total cost of ownership</li>
  </ul>
</blockquote>

<p>She also has a great list of other writers‚Äô lists of good test characteristics.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.techpolicy.press/the-eus-fine-against-x-is-not-about-speech-or-censorship/">Daphne Keller</a> explains that the EUs fines on X aren‚Äôt about free speech.</p>

<blockquote>
  <p>There are three charges against X, which all stem from a multi-year investigation that was launched in 2023. One is about verification ‚Äî X‚Äôs blue checkmarks on user accounts ‚Äî and two are about transparency. These charges have nothing to do with what content is on X, or what user speech the platform should or should not allow.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington">Cory Doctorow</a> The Reverse-Centaur‚Äôs Guide to Criticizing AI</p>

<blockquote>
  <p>Start with what a reverse centaur is. In automation theory, a ‚Äúcentaur‚Äù is a person who is assisted by a machine. ‚Ä¶ And obviously, a reverse centaur is machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.</p>

  <p>Like an Amazon delivery driver‚Ä¶ the van can‚Äôt drive itself and can‚Äôt get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.</p>
</blockquote>

---

### [Fragments Dec 4](https://martinfowler.com/articles/20251204-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://blog.robbowley.net/2025/12/04/ai-is-still-making-code-worse-a-new-cmu-study-confirms/">Rob Bowley</a> summarizes a study from Carnegie Mellon looking on the impact of AI on a bunch of open-source software projects. Like any such study, we shouldn‚Äôt take its results as definitive, but there seems enough there to make it a handy data point. The key point is that the AI code probably reduced the quality of the code base - at least if static code analysis can be trusted to determine quality. And perhaps some worrying second-order effects</p>

<blockquote>
  <p>This study shows more than 800 popular GitHub projects with code quality degrading after adopting AI tools. It‚Äôs hard not to see a form of context collapse playing out in real time. If the public code that future models learn from is becoming more complex and less maintainable, there‚Äôs a real risk that newer models will reinforce and amplify those trends, producing even worse code over time.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Rob‚Äôs post is typical of much of the thoughtful writing on AI. We can see its short-term benefits, but worry about its long-term impact. But on a much deeper note is this lovely story from <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">Jim Highsmith</a>. Jim has turned 0x50, and has spent the last decade fighting Parkinson‚Äôs disease. To help him battle it he has two AI assisted allies.</p>

<blockquote>
  <p>Between my neural implants and Byron‚Äôs digital guidance, I now collaborate with two adaptive systems: one for motion, one for thought. Neither replaces me. Both extend me.</p>
</blockquote>

<p><strong>If you read anything on AI this week, <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">make it be this</a>.</strong> It offers a positive harbinger for our future and opens my mind to a whole different perspective of the role of AI in it</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Anthropic recently announced that it disrupted a Chinese state-sponsored operation abusing Claude Code. Jim Gumbley looks at the core lesson to learn from this, that we have to understand the serious risk of  <a href="https://www.thoughtworks.com/insights/blog/security/anthropic-ai-espionage-disclosure-signal-from-noise?utm_source=linkedin&amp;utm_medium=social-organic&amp;utm_campaign=dt_bs_rp-in-clt_tech_thought_leaders_2025-11&amp;gh_src=d1fe17bc1us">AI Jailbreaking</a></p>

<blockquote>
  <p>New AI tools are able to analyze your attack surface at the next level of granularity. As a business leader, that means you now have two options: wait for someone else to run AI-assisted vulnerability detection against your attack surface, or run it yourself first.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>There‚Äôs plenty of claims that AI Vibe Coding can replace software developers, something that folks like me (perhaps with a bias) think unlikely. <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m75kmb7bh22i">Gergely Orosz</a> shared this tidbit</p>

<blockquote>
  <p>Talked with an exec at a tech company who is obsessed with AI and has been for 3 years. Not a developer but company makes software. Uses AI for everything, vibe codes ideas.</p>

  <p>Here‚Äôs the kicker:</p>

  <p>Has a team of several devs to implement his vibe coded prototypes to sg workable</p>
</blockquote>

<p>I‚Äôd love to hear more about this (and similar stories)</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/">Nick Radcliffe</a> writes about a month of using AI</p>

<blockquote>
  <p>I spent a solid month ‚Äúpair programming‚Äù with Claude Code, trying to suspend disbelief and adopt a this-will-be-productive mindset. More specifically, I got Claude to write well over 99% of the code produced during the month. I found the experience infuriating, unpleasant, and stressful before even worrying about its energy impact. Ideally, I would prefer not to do it again for at least a year or two. The only problem with that is that it ‚Äúworked‚Äù.</p>
</blockquote>

<p>He stresses that his approach is the ‚Äúpolar opposite‚Äù of Vibe Coding. The post is long, and rambles a bit, but is worthwhile because he talks in detail about his workflow and how he uses the tool. Such posts are important so we can learn the nitty-gritty of how our programming habits are changing.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Along similar lines is a post of <a href="https://brianchambers.substack.com/p/chamber-of-tech-secrets-55-issue">Brian Chambers</a> on his workflow, that he calls Issue-Driven Development (and yes, I‚Äôm also sick of the ‚Äúsomething-driven‚Äù phraseology). As with much of the better stuff I‚Äôve heard about AI assisted work, it‚Äôs all about carefully managing the context window, ensuring the AI is focused on the right things and not distracted by textual squirrels.</p>

---

### [Fragments Nov 19](https://martinfowler.com/articles/2025-11-19-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><img alt="radar image" src="https://martinfowler.com/articles/images/frags-2025-11/radar.png" width="300px" /></p>

<p>I‚Äôve been on the road in Europe for the last couple of weeks, and while I was there Thoughtworks released <a href="https://www.thoughtworks.com/radar">volume 33 of our Technology Radar</a>. Again it‚Äôs dominated by the AI wave, with lots of blips capturing our explorations of how to use LLMs and similar technology. ‚ÄúAgents‚Äù are the big thing these days but we‚Äôre also seeing growing movements in infrastructure orchestration, coding workflows - and the inevitable antipatterns. Many thanks to my colleagues for putting this together again.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Gergely and I recording the podcast" src="https://martinfowler.com/articles/images/frags-2025-11/podcast.png" width="400px" /></p>

<p>My trip to Europe started in Amsterdam, for a Thoughtworks event for a few of our clients there. Since I was in that lovely city, I got in touch with Gergely Orosz, host of <a href="https://www.pragmaticengineer.com/">The Pragmatic Engineer</a>, and he arranged to <a href="https://www.youtube.com/watch?v=CQmI4XKTa0U">record a podcast</a> with me. No surprise that AI was front-and-center of the conversation, as I said it was the biggest shift I‚Äôd seen in programming during my career, comparable only to the shift to high-level languages, which even I am not old enough to have experienced. It was a fun chat and I really enjoyed myself. Gergely later joined myself James Lewis and Giles Edwards-Alexander at the Thoughtworks event the next day.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>My travels also took me to N√ºremberg, where I attended an internal conference for Siemens on the future of software architecture. When we think of technology, it‚Äôs easy to focus on the Faangs of Silicon Valley, but Siemens have a huge workforce of software developers working on heavy engineering systems like trains and factory automation. It was good to hear them talk about federated architectures, data mesh, and their use of AI.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Kent's graph of options vs features" src="https://martinfowler.com/articles/images/frags-2025-11/features.webp" width="400px" /></p>

<p>I‚Äôve often used pseudo-graphs to help explain why <a href="https://martinfowler.com/articles/is-quality-worth-cost.html">high quality software is cheaper</a>. This time, <a href="https://tidyfirst.substack.com/p/why-does-development-slow">Kent Beck</a> creates a unique perspective to this chart, dispensing with the temporal axis to help think in terms of optionality.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Heavy Cardboard banner" src="https://martinfowler.com/articles/images/frags-2025-11/hc.jpg" width="400px" /></p>

<p>And in another life, Edward has finally finished the great migration of the Heavy Cardboard studio and returns to the tubes with <a href="https://www.youtube.com/live/e8juRvm9h0c">our first game in the new digs</a>. (No surprise that it‚Äôs Age of Steam.)</p>

---

### [My Foreword to "Frictionless"](https://martinfowler.com/articles/frictionless-foreword.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/frictionless-foreword.html"><img src="https://martinfowler.com/articles/frictionless-card.jpg" width="" /></a></div>

<p>I find most writing on software productivity to be twaddle, but Nicole
      Forsgren and Abi Noda are notable exceptions. I had a chance to take a
      look at their new book, published today, and liked it so much I wrote a
      foreword.</p>

<p><a class="more" href="https://martinfowler.com/articles/frictionless-foreword.html">more‚Ä¶</a></p>

---

### [The Learning Loop and LLMs](https://martinfowler.com/articles/llm-learning-loop.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/llm-learning-loop.html"><img src="https://martinfowler.com/articles/llm-learning-loop/card.png" width="350px" /></a></div>

<p><b class="author">Unmesh Joshi</b> finds LLMs to be a useful tool, but
      explains why their help <a href="https://martinfowler.com/articles/llm-learning-loop.html">becomes illusory</a> if we use them to shortcut the
      learning loop that's an essential part of our professional practice.</p>

<p><a class="more" href="https://martinfowler.com/articles/llm-learning-loop.html">more‚Ä¶</a></p>

---

## HackerNews

### [The struggle of resizing windows on macOS Tahoe](https://noheger.at/blog/2026/01/11/the-struggle-of-resizing-windows-on-macos-tahoe/)

**ÂàÜÊï∞**: 2712 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46579864)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Cowork: Claude Code for the rest of your work](https://claude.com/blog/cowork-research-preview)

**ÂàÜÊï∞**: 1242 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46593022)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Apple picks Gemini to power Siri](https://www.cnbc.com/2026/01/12/apple-google-ai-siri-gemini.html)

**ÂàÜÊï∞**: 994 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46589675)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Scott Adams has died](https://www.youtube.com/watch?v=Rs_JrOIo3SE)

**ÂàÜÊï∞**: 728 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46602102)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Floppy disks turn out to be the greatest TV remote for kids](https://blog.smartere.dk/2026/01/floppy-disks-the-best-tv-remote-for-kids/)

**ÂàÜÊï∞**: 725 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46587934)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [TimeCapsuleLLM: LLM trained only on data from 1800-1875](https://github.com/haykgrigo3/TimeCapsuleLLM)

**ÂàÜÊï∞**: 716 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46590280)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [AI Generated Music Barred from Bandcamp](https://old.reddit.com/r/BandCamp/comments/1qbw8ba/ai_generated_music_on_bandcamp/)

**ÂàÜÊï∞**: 543 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46605490)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Postal Arbitrage](https://walzr.com/postal-arbitrage)

**ÂàÜÊï∞**: 535 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46591708)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Code and Let Live](https://fly.io/blog/code-and-let-live/)

**ÂàÜÊï∞**: 493 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46557825)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Apple Creator Studio](https://www.apple.com/newsroom/2026/01/introducing-apple-creator-studio-an-inspiring-collection-of-creative-apps/)

**ÂàÜÊï∞**: 470 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46601157)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Ozempic is changing the foods Americans buy](https://news.cornell.edu/stories/2025/12/ozempic-changing-foods-americans-buy)

**ÂàÜÊï∞**: 450 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46587536)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Date is out, Temporal is in](https://piccalil.li/blog/date-is-out-and-temporal-is-in/)

**ÂàÜÊï∞**: 448 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46589658)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Unauthenticated remote code execution in OpenCode](https://cy.md/opencode-rce/)

**ÂàÜÊï∞**: 417 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46581095)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Chromium Has Merged JpegXL](https://chromium-review.googlesource.com/c/chromium/src/+/7184969)

**ÂàÜÊï∞**: 412 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46597927)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [LLVM: The bad parts](https://www.npopov.com/2026/01/11/LLVM-The-bad-parts.html)

**ÂàÜÊï∞**: 373 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46588837)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Local Journalism Is How Democracy Shows Up Close to Home](https://buckscountybeacon.com/2026/01/opinion-local-journalism-is-how-democracy-shows-up-close-to-home/)

**ÂàÜÊï∞**: 359 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46600850)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Anthropic invests $1.5M in the Python Software Foundation](https://discuss.python.org/t/anthropic-has-made-a-large-contribution-to-the-python-software-foundation-and-open-source-security/105694)

**ÂàÜÊï∞**: 357 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46601902)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Influencers and OnlyFans models are dominating U.S. O-1 visa requests](https://www.theguardian.com/us-news/2026/jan/11/onlyfans-influencers-us-o-1-visa)

**ÂàÜÊï∞**: 338 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46603535)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The chess bot on Delta Air Lines will destroy you (2024) [video]](https://www.youtube.com/watch?v=c0mLhHDcY3I)

**ÂàÜÊï∞**: 326 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46593395)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Signal leaders warn agentic AI is an insecure, unreliable surveillance risk](https://coywolf.com/news/productivity/signal-president-and-vp-warn-agentic-ai-is-insecure-unreliable-and-a-surveillance-nightmare/)

**ÂàÜÊï∞**: 317 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46605553)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Text-based web browsers](https://cssence.com/2026/text-based-web-browsers/)

**ÂàÜÊï∞**: 276 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46597518)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Git Rebase for the Terrified](https://www.brethorsting.com/blog/2026/01/git-rebase-for-the-terrified/)

**ÂàÜÊï∞**: 248 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46530920)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [We can't have nice things because of AI scrapers](https://blog.metabrainz.org/2025/12/11/we-cant-have-nice-things-because-of-ai-scrapers/)

**ÂàÜÊï∞**: 239 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46608840)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [What a year of solar and batteries saved us in 2025](https://scotthelme.co.uk/what-a-year-of-solar-and-batteries-really-saved-us-in-2025/)

**ÂàÜÊï∞**: 231 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46602532)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Fabrice Bellard's TS Zip (2024)](https://www.bellard.org/ts_zip/)

**ÂàÜÊï∞**: 226 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46593802)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The UK is shaping a future of precrime and dissent management (2025)](https://freedomnews.org.uk/2025/04/11/how-the-uk-is-shaping-a-future-of-precrime-and-dissent-management/)

**ÂàÜÊï∞**: 213 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46600194)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Games Workshop bans staff from using AI](https://www.ign.com/articles/warhammer-maker-games-workshop-bans-its-staff-from-using-ai-in-its-content-or-designs-says-none-of-its-senior-managers-are-currently-excited-about-the-tech)

**ÂàÜÊï∞**: 198 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46607681)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Self-host Reddit ‚Äì 2.38B posts, works offline, yours forever](https://github.com/19-84/redd-archiver)

**ÂàÜÊï∞**: 196 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46602324)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Indifference is a power](https://aeon.co/essays/why-stoicism-is-one-of-the-best-mind-hacks-ever-devised)

**ÂàÜÊï∞**: 196 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46601121)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Tulip Creative Computer](https://github.com/shorepine/tulipcc)

**ÂàÜÊï∞**: 184 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46603995)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Mozilla's open source AI strategy](https://blog.mozilla.org/en/mozilla/mozilla-open-source-ai-strategy/)

**ÂàÜÊï∞**: 182 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46599897)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Some ecologists fear their field is losing touch with nature](https://www.nature.com/articles/d41586-025-04150-w)

**ÂàÜÊï∞**: 159 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46540126)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The insecure evangelism of LLM maximalists](https://lewiscampbell.tech/blog/260114.html)

**ÂàÜÊï∞**: 157 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46609591)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Launch a Debugging Terminal into GitHub Actions](https://blog.gripdev.xyz/2026/01/10/actions-terminal-on-failure-for-debugging/)

**ÂàÜÊï∞**: 148 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46587498)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Cray-1 Computer System (1977) [pdf]](https://s3data.computerhistory.org/brochures/cray.cray1.1977.102638650.pdf)

**ÂàÜÊï∞**: 147 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46557733)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: An iOS budget app I've been maintaining since 2011](https://primoco.me/en/)

**ÂàÜÊï∞**: 141 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46599341)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [FOSS in times of war, scarcity and (adversarial) AI [video]](https://fosdem.org/2026/schedule/event/FE7ULY-foss-in-times-of-war-scarcity-and-ai/)

**ÂàÜÊï∞**: 141 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46598991)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How to make a damn website (2024)](https://lmnt.me/blog/how-to-make-a-damn-website.html)

**ÂàÜÊï∞**: 132 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46604250)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [EOL hardware should mean open-source software](https://www.marcia.no/words/eol)

**ÂàÜÊï∞**: 120 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46609492)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Agent-of-empires: OpenCode and Claude Code session manager](https://github.com/njbrake/agent-of-empires)

**ÂàÜÊï∞**: 112 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46588905)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Robotopia: A 3D, first-person, talking simulator](https://elbowgreasegames.substack.com/p/introducing-robotopia-a-3d-first)

**ÂàÜÊï∞**: 101 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46545636)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Are two heads better than one?](https://eieio.games/blog/two-heads-arent-better-than-one/)

**ÂàÜÊï∞**: 100 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46603111)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Let's be honest, Generative AI isn't going all that well](https://garymarcus.substack.com/p/lets-be-honest-generative-ai-isnt)

**ÂàÜÊï∞**: 100 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46605587)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Superhuman AI Exfiltrates Emails](https://www.promptarmor.com/resources/superhuman-ai-exfiltrates-emails)

**ÂàÜÊï∞**: 92 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46592424)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Building a 25 Gbit/s workstation for the SCION Association](https://github.com/scionassociation/blog-25gbit-workstation)

**ÂàÜÊï∞**: 90 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46590541)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A 40-line fix eliminated a 400x performance gap](https://questdb.com/blog/jvm-current-thread-user-time/)

**ÂàÜÊï∞**: 82 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46609630)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [HP Reveals Keyboard Computer with Ryzen AI Chip](https://www.hp.com/us-en/desktops/business/eliteboard.html)

**ÂàÜÊï∞**: 82 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46534157)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Implementing a web server in a single printf() call (2014)](https://tinyhack.com/2014/03/12/implementing-a-web-server-in-a-single-printf-call/)

**ÂàÜÊï∞**: 78 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46549714)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Everything you never wanted to know about file locking (2010)](https://apenwarr.ca/log/20101213)

**ÂàÜÊï∞**: 76 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542247)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Why we don‚Äôt use AI](https://yarnspinner.dev/blog/why-we-dont-use-ai/)

**ÂàÜÊï∞**: 75 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46609279)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Case for Blogging in the Ruins](https://www.joanwestenberg.com/the-case-for-blogging-in-the-ruins/)

**ÂàÜÊï∞**: 73 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46604959)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Every GitHub object has two IDs](https://www.greptile.com/blog/github-ids)

**ÂàÜÊï∞**: 69 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46602591)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Confer ‚Äì End to end encrypted AI chat](https://confer.to/)

**ÂàÜÊï∞**: 68 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46600839)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Open sourcing Dicer: Databricks's auto-sharder](https://www.databricks.com/blog/open-sourcing-dicer-databricks-auto-sharder)

**ÂàÜÊï∞**: 64 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46606902)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A university got itself banned from the Linux kernel (2021)](https://www.theverge.com/2021/4/30/22410164/linux-kernel-university-of-minnesota-banned-open-source)

**ÂàÜÊï∞**: 59 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46605950)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Nogic ‚Äì VS Code extension that visualizes your codebase as a graph](https://marketplace.visualstudio.com/items?itemName=Nogic.nogic)

**ÂàÜÊï∞**: 58 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46605675)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: SnackBase ‚Äì Open-source, GxP-compliant back end for Python teams](https://snackbase.dev)

**ÂàÜÊï∞**: 57 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46600092)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Running Lean at Scale](https://harmonic.fun/news#blog-post-lean)

**ÂàÜÊï∞**: 53 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46608731)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Provenance Is the New Version Control](https://aicoding.leaflet.pub/3mcbiyal7jc2y)

**ÂàÜÊï∞**: 53 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46597023)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Ayder ‚Äì HTTP-native durable event log written in C (curl as client)](https://github.com/A1darbek/ayder)

**ÂàÜÊï∞**: 49 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46604862)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Why BM25 queries with more terms can be faster (and other scaling surprises)](https://turbopuffer.com/blog/bm25-latency-musings)

**ÂàÜÊï∞**: 49 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46541848)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Why Real Life is better than IRC (2000)](https://everything2.com/node/e2node/Why%20Real%20Life%20is%20better%20than%20IRC)

**ÂàÜÊï∞**: 49 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46559266)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Choosing learning over autopilot](https://anniecherkaev.com/choosing-learning-over-autopilot)

**ÂàÜÊï∞**: 43 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46605716)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: FastScheduler ‚Äì Decorator-first Python task scheduler, async support](https://github.com/MichielMe/fastscheduler)

**ÂàÜÊï∞**: 43 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46601631)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Inlining ‚Äì The Ultimate Optimisation](https://xania.org/202512/17-inlining-the-ultimate-optimisation)

**ÂàÜÊï∞**: 42 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46555297)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [We rolled our own documentation site](https://blog.tangled.org/docs)

**ÂàÜÊï∞**: 38 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46596823)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Going for Gold: The Story of the Golden Lego RCX and NXT](https://bricknerd.com/home/going-for-gold-the-story-of-the-golden-lego-rcx-and-nxt-9-9-21)

**ÂàÜÊï∞**: 37 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46557312)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Japan's Skyscraper Factories (2021)](https://www.construction-physics.com/p/japans-skyscraper-factories)

**ÂàÜÊï∞**: 33 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46534632)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Starlink activates free internet in Iran](https://www.cnn.com/2026/01/13/politics/starlink-access-iran-protests)

**ÂàÜÊï∞**: 32 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46609734)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A deep dive on agent sandboxes](https://pierce.dev/notes/a-deep-dive-on-agent-sandboxes)

**ÂàÜÊï∞**: 26 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46595393)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Pentagon buys device via undercover operation suspected link to Havana Syndrome](https://www.cnn.com/2026/01/13/politics/havana-syndrome-device-pentagon-hsi)

**ÂàÜÊï∞**: 19 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46608913)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Ever wanted to look at yourself in Braille?](https://github.com/NishantJoshi00/dith)

**ÂàÜÊï∞**: 19 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46551065)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [AI isn't "just predicting the next word" anymore](https://stevenadler.substack.com/p/ai-isnt-just-predicting-the-next)

**ÂàÜÊï∞**: 18 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46607251)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Meta lays off 1k+ employees, shutting down game studios](https://www.bloomberg.com/news/articles/2026-01-13/meta-begins-jobs-cuts-after-shifting-focus-from-metaverse-to-phones)

**ÂàÜÊï∞**: 16 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46610025)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Is it a joke?](https://novalis.org/blog/2025-11-06-is-it-a-joke.html)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46608811)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [An archaeology of tracking on government websites](https://www.flux.utah.edu/paper/singh-pets26)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46608802)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Apple Says 'Pixelmator' App on iOS Will No Longer Receive Updates](https://www.macrumors.com/2026/01/13/pixelmator-no-longer-being-updated/)

**ÂàÜÊï∞**: 11 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46607784)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Terra - A rolling-release Fedora repository](https://terra.fyralabs.com/)

**ÂàÜÊï∞**: 10 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46608971)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [LANL's ICE House Tests Microelectronics for Cosmic Radiation Exposure](https://www.lanl.gov/media/publications/1663/ice-house-heats-up)

**ÂàÜÊï∞**: 10 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542693)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

## LLM Infrastructure

### [v0.14.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#‚Ä¶</p>

---

### [v0.14.0rc0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony mode‚Ä¶</p>

---

### [v0.13.0](https://github.com/vllm-project/vllm/releases/tag/v0.13.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.13.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features <strong>442 commits from 207 contributors (61 new contributors)!</strong></p>
<p><strong>Breaking Changes</strong>: This release includes deprecation removals, PassConfig flag renames, and attention configuration changes from environment variables to CLI arguments. Please review the breaking changes section carefully before upgrading.</p>
<h3>Model Support</h3>
<ul>
<li><strong>New models</strong>: BAGEL (AR only) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28439">#28439</a>), AudioFlamingo3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a>), JAIS 2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a>), latent MoE architecture support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a>).</li>
<li><strong>Tool parsers</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29848">#29848</a>), Gigachat 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a>), Holo2 reasoning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30048">#30048</a>).</li>
<li><strong>Model enhancements</strong>: Qwen3-VL embeddings support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a>), Qwen3-VL EVS (Efficient Video Sampling) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29752">#29752</a>), DeepSeek V3.2 proper <code>drop_thinking</code> logic (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a>), DeepSeek V3.2 top-k fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27568">#27568</a>).</li>
<li><strong>Task expansion</strong>: Automatic TokenClassification model conversion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30666">#30666</a>), Ultravox v0.7 transformer projector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30089">#30089</a>).</li>
<li><strong>Quantization</strong>: BitsAndBytes for Qwen3-Omni-MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29896">#29896</a>).</li>
<li><strong>Speculative decoding</strong>: Eagle/Eagle3 Transformers backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30340">#30340</a>), Mamba <code>selective_state_update</code> spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29488">#29488</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Compilation</strong>: Conditional compilation via <code>compile_ranges</code> for selective kernel compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24252">#24252</a>).</li>
<li><strong>Prefix caching</strong>: xxHash high-performance hash option (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a>).</li>
<li><strong>Attention</strong>: PrefixLM support for FlexAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27938">#27938</a>) and TritonAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30386">#30386</a>), CUDA graphs for 3D Triton attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28306">#28306</a>), <code>TRITON_MLA</code> without prefix-caching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29125">#29125</a>).</li>
<li><strong>Batch invariance</strong>: FA2 and LoRA batch-invariant support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30018">#30018</a>).</li>
<li><strong>Pooling</strong>: Chunked prefill for ALL pooling tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27145">#27145</a>), multi-vector retrieval API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26686">#26686</a>).</li>
<li><strong>Model Runner V2</strong>: Min-p sampling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30171">#30171</a>), NaN detection in logits (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30187">#30187</a>).</li>
<li><strong>Speculative decoding</strong>: Medusa GPU-CPU sync avoidance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29723">#29723</a>), async spec-decode improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29624">#29624</a>).</li>
<li><strong>Whisper</strong>: Major performance improvements - <a href="https://github.com/vllm-project/vllm/issues/24946#issuecomment-3680725754">V1 is now faster than V0</a> (~3x speedup vs v0.12.0). Encoder batching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29421">#29421</a>), <code>FULL_DECODE_ONLY</code> CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30072">#30072</a>), CPU backend support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>).</li>
<li><strong>Performance</strong>: Fused blockwise quant RMS norm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27883">#27883</a>), MoE LoRA loading reduction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30243">#30243</a>), encoder cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30475">#30475</a>), CPU KV offloading streams (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29013">#29013</a>).</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li><strong>NVIDIA Blackwell Ultra</strong>: SM103 (GB300) support with CUDA 13 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30484">#30484</a>).</li>
<li><strong>DeepSeek optimizations</strong> (benchmarked on DeepSeek-V3.1):
<ul>
<li>DeepEP High-Throughput CUDA graph enabled by default: <strong>5.3% throughput, 4.4% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29558">#29558</a>)</li>
<li>DeepGEMM fused layout kernel: <strong>4.3% throughput, 10.7% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29546">#29546</a>)</li>
<li>DeepGEMM experts initialization: <strong>3.9% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30494">#30494</a>)</li>
<li><code>group_topk</code> kernel: <strong>1.9% throughput, 2.1% TPOT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30159">#30159</a>)</li>
<li>Sparse prefill kernel for FP8 KV-cache in DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27532">#27532</a>)</li>
<li>MLA FP8 optimization with ReduceScatterSum (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29795">#29795</a>), direct k_nope/k_pe copy (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29710">#29710</a>)</li>
</ul>
</li>
<li><strong>CPU</strong>: Whisper support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>), Arm Optimized Routines vectorized exp (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a>), x86 CPU wheel pipeline (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28848">#28848</a>).</li>
<li><strong>AMD ROCm</strong>: Aiter quantization kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25552">#25552</a>), torch.compile layernorm/silu + FP8 quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25693">#25693</a>), Triton ScaledMM fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26668">#26668</a>), MXFP4 w4a4 inference (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29775">#29775</a>).</li>
<li><strong>Intel XPU</strong>: wNa16 compressed tensors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29484">#29484</a>).</li>
<li><strong>Build</strong>: CUDA 13 aarch64 wheels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30341">#30341</a>), Docker kernel build stage (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a>), Ascend NPU Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30015">#30015</a>).</li>
</ul>
<h3>Large Scale Serving &amp; Disaggregated Prefill/Decode</h3>
<ul>
<li><strong>KV connectors</strong>: Mooncake Transfer Engine (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a>), cache reset via <code>/reset_prefix_cache</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27170">#27170</a>), KV events (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28309">#28309</a>), failure recovery config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26813">#26813</a>).</li>
<li><strong>NIXL</strong>: Compatibility checking in handshake (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29503">#29503</a>), large batch proxy support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28782">#28782</a>).</li>
<li><strong>EPLB</strong>: NVFP4 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a>), algorithm abstraction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a>).</li>
<li><strong>Multi-node</strong>: External launcher mode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29833">#29833</a>).</li>
<li><strong>Hybrid allocator</strong>: Optional KV connector integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29805">#29805</a>).</li>
<li><strong>Performance</strong>: silu_mul_per_token_group_quant_fp8 kernel for DP/EP (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29470">#29470</a>).</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>New</strong>: W4A8 grouped GEMM on Hopper (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29691">#29691</a>), online FP8 with streaming post-processing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a>), FP8 weight reloading for RLHF (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28480">#28480</a>).</li>
<li><strong>MoE + LoRA</strong>: AWQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30442">#30442</a>) and GPTQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a>) support.</li>
<li><strong>GGUF</strong>: MoE + GGUF restored for Qwen3 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30116">#30116</a>), Qwen2 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30307">#30307</a>), HF defaults override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30118">#30118</a>).</li>
<li><strong>Compatibility</strong>: Transformers v5 RoPE support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30046">#30046</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>: MCP type infrastructure (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30054">#30054</a>), Browser/Container MCP tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29989">#29989</a>), full MCP Python loop (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29798">#29798</a>), extra body parameters (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a>).</li>
<li><strong>Configuration</strong>: <code>AttentionConfig</code> replaces <code>VLLM_ATTENTION_BACKEND</code> env var (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>).</li>
<li><strong>Chat templates</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29837">#29837</a>), DeepSeek-V3.2 developer tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a>).</li>
<li><strong>Anthropic API</strong>: Streaming fixes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29971">#29971</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30266">#30266</a>).</li>
<li><strong>Embeddings</strong>: Binary format with <code>encoding_format=bytes_only</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30249">#30249</a>), multiple image/audio per request (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29988">#29988</a>), tokenization_kwargs override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29794">#29794</a>).</li>
<li><strong>Metrics</strong>: Prefill KV compute metric excluding cached tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30189">#30189</a>).</li>
<li><strong>Profiling</strong>: Layer-wise NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29990">#29990</a>), profiling CLI config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29912">#29912</a>).</li>
<li><strong>UX</strong>: Better OOM errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28051">#28051</a>), ModelConfig validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a>), distributed executor errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30140">#30140</a>).</li>
</ul>
<h3>Security</h3>
<ul>
<li>Additional protection for <a href="https://github.com/advisories/GHSA-mrw7-hf4f-83pf" title="CVE-2025-62164">CVE-2025-62164</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a>).</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>NVSHMEM 3.3.24 + CUDA 13 fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a>).</li>
<li>TPU tpu-inference 0.12.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30221">#30221</a>).</li>
</ul>
<h3>Breaking Changes &amp; Deprecations</h3>
<ol>
<li><strong>PassConfig flags renamed</strong> per RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/27995">#27995</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29646">#29646</a>)</li>
<li><strong>Attention env vars ‚Üí CLI args</strong>: <code>VLLM_ATTENTION_BACKEND</code> replaced with <code>--attention-backend</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>)</li>
<li><strong>Removed <code>-O.xx</code> flag</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29991">#29991</a>)</li>
<li><strong>Removed deprecated plugin/compilation fields</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30396">#30396</a>)</li>
<li><strong>Removed deprecated task, seed, MM settings</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30397">#30397</a>)</li>
<li><strong>Removed <code>embed_input_ids</code>/<code>embed_multimodal</code> fallbacks</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30458">#30458</a>)</li>
<li><strong>Removed tokenizer setter</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30400">#30400</a>)</li>
<li><strong>Deprecations</strong>: <code>merge_by_field_config</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30035">#30035</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30170">#30170</a>), <code>--convert reward</code> ‚Üí <code>--convert embed</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30463">#30463</a>)</li>
</ol>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/ajpqs">@ajpqs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amitz-nv">@amitz-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29978">#29978</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amrmahdi">@amrmahdi</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andrewbriand">@andrewbriand</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/anker-c2">@anker-c2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30344">#30344</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AuruTus">@AuruTus</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30182">#30182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/avigny">@avigny</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19425">#19425</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bhanu068">@Bhanu068</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a></li>
<li>@Copilot made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29025">#29025</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dbotwinick">@dbotwinick</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30583">#30583</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30234">#30234</a></li>
<li><a class="user-mention notranslate" href="https://github.com/desertfire">@desertfire</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29919">#29919</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dmitry-tokarev-nv">@dmitry-tokarev-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a></li>
<li><a class="user-mention notranslate" href="https://github.com/drslark">@drslark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30632">#30632</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dtcccc">@dtcccc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elizabetht">@elizabetht</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28671">#28671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Elm8116">@Elm8116</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gausah01">@gausah01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29604">#29604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gh-wf">@gh-wf</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30285">#30285</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hdlj-h">@hdlj-h</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30056">#30056</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HF-001">@HF-001</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30051">#30051</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hzxuzhonghu">@hzxuzhonghu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29931">#29931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaviS-Rei">@JaviS-Rei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29882">#29882</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johannesflommersfeld">@johannesflommersfeld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30390">#30390</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KevinMusgrave">@KevinMusgrave</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30529">#30529</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kitaekatt">@kitaekatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30408">#30408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lashahub">@lashahub</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a></li>
<li><a class="user-mention notranslate" href="https://github.com/LuminolT">@LuminolT</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a></li>
<li><a class="user-mention notranslate" href="https://github.com/majiayu000">@majiayu000</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30615">#30615</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MaoJianwei">@MaoJianwei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29797">#29797</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Mercykid-bash">@Mercykid-bash</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mgehre-amd">@mgehre-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30364">#30364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mivehk">@mivehk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30512">#30512</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mondaylord">@mondaylord</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30671">#30671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/noa-neria">@noa-neria</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29320">#29320</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PatrykSaffer">@PatrykSaffer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30330">#30330</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Peng-YM">@Peng-YM</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29074">#29074</a></li>
<li><a class="user-mention notranslate" href="https://github.com/realliujiaxu">@realliujiaxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30059">#30059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/redwrasse">@redwrasse</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29261">#29261</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ri0S">@Ri0S</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scratch-ml">@scratch-ml</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30351">#30351</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seokhyunan">@seokhyunan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30648">#30648</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shaharmor98">@shaharmor98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a></li>
<li><a class="user-mention notranslate" href="https://github.com/taoyun951753">@taoyun951753</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tom-zju">@tom-zju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30057">#30057</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tomtomjhj">@tomtomjhj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29692">#29692</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vkuzo">@vkuzo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vladnosiv">@vladnosiv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiguihua2">@weiguihua2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30042">#30042</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wenqiglantz">@wenqiglantz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wkcn">@wkcn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29879">#29879</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wu-kan">@wu-kan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21804">#21804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wz1qqx">@wz1qqx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30376">#30376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xyDong0223">@xyDong0223</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30455">#30455</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yifant-code">@yifant-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yjc9696">@yjc9696</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yurekami">@yurekami</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30552">#30552</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuttian1">@yuttian1</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30102">#30102</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhijianJiang">@ZhijianJiang</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30219">#30219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhiweiYan-96">@ZhiweiYan-96</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29773">#29773</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.12.0...v0.13.0"><tt>v0.12.0...v0.13.0</tt></a></p>

---

### [v0.13.0rc4: [v1] Add PrefixLM support to TritonAttention backend (#30386)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc4)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/74a1ac38b00a8cf502db085d1bbd77712cf47e41"><tt>74a1ac3</tt></a>)</p>

---

### [v0.13.0rc3: [XPU] fix broken fp8 online quantization for XPU platform (#30831)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc3)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Yan Ma <a href="mailto:yan.ma@intel.com">yan.ma@intel.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/4f735babb7353987137b85ec0465e594e9ed1384"><tt>4f735ba</tt></a>)</p>

---

### [v0.13.0rc2: [ROCm] [Bugfix] Fix torch sdpa hallucination (#30789)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: tjtanaa <a href="mailto:tunjian.tan@embeddedllm.com">tunjian.tan@embeddedllm.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/2410132bb1f9faa5b252fad3f2b83dc926946b08"><tt>2410132</tt></a>)</p>

---

### [v0.13.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>fake rc release to fix nightly wheels</p>

---

### [v0.12.0](https://github.com/vllm-project/vllm/releases/tag/v0.12.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.12.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features 474 commits from 213 contributors (57 new)ÔºÅ</p>
<p><strong>Breaking Changes</strong>: This release includes PyTorch 2.9.0 upgrade (CUDA 12.9), V0 deprecations including <code>xformers</code> backend, and scheduled removals - please review the changelog carefully.</p>
<p><strong>Major Features</strong>:</p>
<ul>
<li><strong>EAGLE Speculative Decoding Improvements</strong>: Multi-step CUDA graph support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>), DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>), and multimodal support with Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Significant Performance Optimizations</strong>: 18.1% throughput improvement from batch invariant BMM (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>), 2.2% throughput improvement from shared experts overlap (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>).</li>
<li><strong>AMD ROCm Expansion</strong>: DeepSeek v3.2 + SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>), FP8 MLA decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>), AITER attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>).</li>
</ul>
<h3>Model Support</h3>
<ul>
<li><strong>New model families</strong>: PLaMo-3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28834">#28834</a>), OpenCUA-7B (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a>), HunyuanOCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29327">#29327</a>), Mistral Large 3 and Ministral 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29757">#29757</a>).</li>
<li><strong>Format support</strong>: Gemma3 GGUF multimodal support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27772">#27772</a>).</li>
<li><strong>Multimodal enhancements</strong>: Qwen3 Omni audio-in-video support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27721">#27721</a>), Eagle3 multimodal support for Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Performance</strong>: QwenVL cos/sin cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28798">#28798</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li>
<p><strong>GPU Model Runner V2 (Experimental)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25266">#25266</a>): Complete refactoring of model execution pipeline:</p>
<ul>
<li>No "reordering" or complex bookkeeping with persistent batch removal</li>
<li>GPU-persistent block tables for better scalability with <code>max_model_len</code> and <code>num_kv_groups</code></li>
<li>Triton-native sampler: no -1 temperature hack, efficient per-request seeds, memory-efficient prompt logprobs</li>
<li>Simplified DP and CUDA graph implementations</li>
<li>Efficient structured outputs support</li>
</ul>
</li>
<li>
<p><strong>Prefill Context Parallel (PCP) (Preparatory)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28718">#28718</a>): Partitions the sequence dimension during prefill for improved long-sequence inference. Complements existing Decode Context Parallel (DCP). See RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25749">#25749</a> for details.</p>
</li>
<li>
<p><strong>RLHF Support</strong>: Pause and Resume Generation for Asynchronous RL Training (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28037">#28037</a>).</p>
</li>
<li>
<p><strong>KV Cache Enhancements</strong>: Cross-layer KV blocks support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27743">#27743</a>), KV cache residency metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27793">#27793</a>).</p>
</li>
<li>
<p><strong>Audio support</strong>: Audio embeddings support in chat completions (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a>).</p>
</li>
<li>
<p><strong>Speculative Decoding</strong>:</p>
<ul>
<li>Multi-step Eagle with CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>)</li>
<li>EAGLE DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>)</li>
<li>EAGLE3 heads without <code>use_aux_hidden_states</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27688">#27688</a>)</li>
<li>Eagle multimodal CUDA graphs with MRoPE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a>)</li>
<li>Logprobs support with spec decode + async scheduling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29223">#29223</a>)</li>
</ul>
</li>
<li>
<p><strong>Configuration</strong>: Flexible <code>inputs_embeds_size</code> separate from <code>hidden_size</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29741">#29741</a>), <code>--fully-sharded-loras</code> for fused_moe (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28761">#28761</a>).</p>
</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li>
<p><strong>NVIDIA Performance</strong>:</p>
<ul>
<li><strong>Batch invariant BMM optimization</strong>: 18.1% throughput improvement, 10.7% TTFT improvement on DeepSeek-V3.1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>)</li>
<li><strong>Shared Experts Overlap with FlashInfer DeepGEMM</strong>: 2.2% throughput improvement, 3.6% TTFT improvement at batch size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>)</li>
<li>DeepGEMM N dim restriction reduced from 128 to 64 multiplier (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28687">#28687</a>)</li>
<li>DeepEP low-latency with round-robin expert placement (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28449">#28449</a>)</li>
<li>NVFP4 MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>H200 Fused MoE Config improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28992">#28992</a>)</li>
</ul>
</li>
<li>
<p><strong>AMD ROCm</strong>:</p>
<ul>
<li>DeepSeek v3.2 and SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>)</li>
<li>FP8 MLA decode support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>)</li>
<li>AITER sampling ops integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26084">#26084</a>)</li>
<li>AITER triton attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>)</li>
<li>Bitsandbytes quantization on AMD GPUs with warp size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27307">#27307</a>)</li>
<li>Fastsafetensors support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28225">#28225</a>)</li>
<li>Sliding window support for AiterFlashAttentionBackend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29234">#29234</a>)</li>
<li>Whisper v1 with Aiter Unified/Flash Attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a>)</li>
</ul>
</li>
<li>
<p><strong>CPU</strong>:</p>
<ul>
<li>Paged attention GEMM acceleration on ARM CPUs with NEON (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29193">#29193</a>)</li>
<li>Parallelize over tokens in int4 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29600">#29600</a>)</li>
<li>CPU all reduce optimization for async_scheduling + DP&gt;1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29311">#29311</a>)</li>
</ul>
</li>
<li>
<p><strong>Attention</strong>: FlashAttention ViT support, now default backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28763">#28763</a>).</p>
</li>
<li>
<p><strong>Long Context</strong>: Optimized <code>gather_and_maybe_dequant_cache</code> kernel for extremely long sequences (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28029">#28029</a>).</p>
</li>
<li>
<p><strong>Multi-NUMA</strong>: Enhanced NUMA functionality for systems with multiple NUMA nodes per socket (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a>).</p>
</li>
<li>
<p><strong>Docker</strong>: Image size reduced by ~200MB (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29060">#29060</a>).</p>
</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>W4A8</strong>: Marlin kernel support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24722">#24722</a>).</li>
<li><strong>NVFP4</strong>:
<ul>
<li>MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>TRTLLM MoE NVFP4 kernel (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a>)</li>
<li>CuteDSL MoE with NVFP4 DeepEP dispatch (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27141">#27141</a>)</li>
<li>Non-gated activations support in modelopt path (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a>)</li>
</ul>
</li>
<li><strong>AWQ</strong>: Compressed-tensors AWQ support for Turing GPUs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29732">#29732</a>).</li>
<li><strong>LoRA</strong>: FusedMoE LoRA Triton kernel for MXFP4 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29708">#29708</a>).</li>
<li><strong>Online quantization</strong>: Moved to <code>model.load_weights</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26327">#26327</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>:
<ul>
<li>Multi-turn support for non-harmony requests (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a>)</li>
<li>Reasoning item input parsing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28248">#28248</a>)</li>
</ul>
</li>
<li><strong>Tool Calling</strong>:
<ul>
<li>Parsed tool arguments support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a>)</li>
<li><code>parallel_tool_calls</code> param compliance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26233">#26233</a>)</li>
<li>Tool filtering support in ToolServer (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29224">#29224</a>)</li>
</ul>
</li>
<li><strong>Whisper</strong>: <code>verbose_json</code> and <code>timestamp</code> features for transcription/translation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24209">#24209</a>).</li>
<li><strong>Sampling</strong>: Flat logprob control moved from env var to <code>SamplingParams</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28914">#28914</a>).</li>
<li><strong>GGUF</strong>: Improved HuggingFace loading UX with <code>repo_id:quant_type</code> syntax (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a>).</li>
<li><strong>Profiling</strong>: Iteration-level profiling for Torch and CUDA profiler (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28987">#28987</a>).</li>
<li><strong>Logs</strong>: Colorized log output (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29017">#29017</a>).</li>
<li><strong>Optimization Levels</strong>: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code> allow trading startup time for performance, more compilation flags will be added in future releases (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26847">#26847</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><strong>PyTorch 2.9.0</strong> with CUDA 12.9 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24994">#24994</a>) - <strong>Breaking change</strong> requiring environment updates.</li>
<li><strong>xgrammar</strong>: Updated to 0.1.27 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28221">#28221</a>).</li>
<li><strong>Transformers</strong>: Updated to 4.57.3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29418">#29418</a>), preparation for v5 with <code>rope_parameters</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28542">#28542</a>).</li>
<li><strong>XPU</strong>: torch &amp; IPEX 2.9 upgrade (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29307">#29307</a>).</li>
</ul>
<h3>V0 Deprecation &amp; Breaking Changes</h3>
<p><strong>Removed Parameters</strong>:</p>
<ul>
<li><code>num_lookahead_slots</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29000">#29000</a>)</li>
<li><code>best_of</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29090">#29090</a>)</li>
<li>LoRA extra vocab (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28545">#28545</a>)</li>
</ul>
<p><strong>Deprecated</strong>:</p>
<ul>
<li><code>xformers</code> backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29262">#29262</a>)</li>
<li><code>seed=None</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29185">#29185</a>)</li>
</ul>
<p><strong>Scheduled Removals</strong> (will be removed in future release):</p>
<ul>
<li><code>ParallelConfig</code>'s direct child EPLB fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29324">#29324</a>)</li>
<li><code>guided_*</code> config fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29326">#29326</a>)</li>
<li><code>override_pooler_config</code> and <code>disable_log_requests</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29402">#29402</a>)</li>
<li><code>CompilationConfig.use_inductor</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29323">#29323</a>)</li>
<li>Deprecated metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29330">#29330</a>)</li>
</ul>
<p><strong>Other Breaking Changes</strong>:</p>
<ul>
<li>PyTorch 2.9.0 upgrade requires CUDA 12.9 environment</li>
<li>Mistral format auto-detection for model loading (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28659">#28659</a>)</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/jesse996">@jesse996</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28846">#28846</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Nepherpitou">@Nepherpitou</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28960">#28960</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Samoed">@Samoed</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27329">#27329</a></li>
<li><a class="user-mention notranslate" href="https://github.com/j20120307">@j20120307</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28999">#28999</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vnadathur">@vnadathur</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26468">#26468</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhyajie">@zhyajie</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28942">#28942</a></li>
<li><a class="user-mention notranslate" href="https://github.com/IzzyPutterman">@IzzyPutterman</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock-amd">@rjrock-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28905">#28905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zq1997">@zq1997</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27715">#27715</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shengliangxu">@shengliangxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28076">#28076</a></li>
<li><a class="user-mention notranslate" href="https://github.com/prashanth058">@prashanth058</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28972">#28972</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qgallouedec">@qgallouedec</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhanggzh">@zhanggzh</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19347">#19347</a></li>
<li><a class="user-mention notranslate" href="https://github.com/pandalee99">@pandalee99</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26628">#26628</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dsuhinin">@dsuhinin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29100">#29100</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xli">@xli</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29124">#29124</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jeremyteboul">@jeremyteboul</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/soodoshll">@soodoshll</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28875">#28875</a></li>
<li><a class="user-mention notranslate" href="https://github.com/bhagyashrigai">@bhagyashrigai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28957">#28957</a></li>
<li><a class="user-mention notranslate" href="https://github.com/skaraban3807">@skaraban3807</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Victor49152">@Victor49152</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock">@rjrock</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29205">#29205</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FlintyLemming">@FlintyLemming</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29182">#29182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/madskildegaard">@madskildegaard</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nandan2003">@nandan2003</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29189">#29189</a></li>
<li><a class="user-mention notranslate" href="https://github.com/michaelact">@michaelact</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29173">#29173</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yongming-qin">@yongming-qin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28958">#28958</a></li>
<li><a class="user-mention notranslate" href="https://github.com/joshiemoore">@joshiemoore</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29249">#29249</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lim4349">@lim4349</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/apinge">@apinge</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gbyu-amd">@gbyu-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kflu">@kflu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29364">#29364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Inokinoki">@Inokinoki</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29200">#29200</a></li>
<li><a class="user-mention notranslate" href="https://github.com/GOavi101">@GOavi101</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29313">#29313</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sts07142">@sts07142</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ivanium">@ivanium</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29143">#29143</a></li>
<li><a class="user-mention notranslate" href="https://github.com/geodavic">@geodavic</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28795">#28795</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Yejing-Lai">@Yejing-Lai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29473">#29473</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Adityayxt">@Adityayxt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29491">#29491</a></li>
<li><a class="user-mention notranslate" href="https://github.com/guodongxiaren">@guodongxiaren</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29620">#29620</a></li>
<li><a class="user-mention notranslate" href="https://github.com/askliar">@askliar</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29426">#29426</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scydas">@scydas</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29589">#29589</a></li>
<li><a class="user-mention notranslate" href="https://github.com/EanWang211123">@EanWang211123</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qGentry">@qGentry</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29506">#29506</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HappyAmazonian">@HappyAmazonian</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29335">#29335</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rgommers">@rgommers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29241">#29241</a></li>
<li><a class="user-mention notranslate" href="https://github.com/staugust">@staugust</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28840">#28840</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mertunsall">@mertunsall</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29667">#29667</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dublc">@dublc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29728">#29728</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nwaughachukwuma">@nwaughachukwuma</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29671">#29671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BowTen">@BowTen</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29731">#29731</a></li>
<li><a class="user-mention notranslate" href="https://github.com/omera-nv">@omera-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhangruoxu">@zhangruoxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29568">#29568</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KKKZOZ">@KKKZOZ</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29783">#29783</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FredericOdermatt">@FredericOdermatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29784">#29784</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Abdennacer-Badaoui">@Abdennacer-Badaoui</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29782">#29782</a></li>
<li><a class="user-mention notranslate" href="https://github.com/knlnguyen1802">@knlnguyen1802</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28525">#28525</a></li>
<li><a class="user-mention notranslate" href="https://github.com/finbarrtimbers">@finbarrtimbers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29796">#29796</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hholtmann">@hholtmann</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29711">#29711</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.11.1...v0.12.0"><tt>v0.11.1...v0.12.0</tt></a></p>

---

### [v0.11.2](https://github.com/vllm-project/vllm/releases/tag/v0.11.2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>This release includes 4 bug fixes on top of <code>v0.11.1</code>:</p>
<ul>
<li>[BugFix] Ray with multiple nodes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28873">#28873</a>)</li>
<li>[BugFix] Fix false assertion with spec-decode=[2,4,..] and TP&gt;2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29036">#29036</a>)</li>
<li>[BugFix] Fix async-scheduling + FlashAttn MLA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28990">#28990</a>)</li>
<li>[NVIDIA] Guard SM100 CUTLASS MoE macro to SM100 builds v2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28938">#28938</a>)</li>
</ul>

---

### [v0.11.1](https://github.com/vllm-project/vllm/releases/tag/v0.11.1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h2>Highlights</h2>
<p>This release includes 1456 commits from 449 contributors (184 new contributors)!</p>
<p>Key changes include:</p>
<ul>
<li><strong>PyTorch 2.9.0 + CUDA 12.9.1</strong>: Updated the default CUDA build to <code>torch==2.9.0+cu129</code>, enabling Inductor partitioning and landing multiple fixes in graph-partition rules and compile-cache integration.</li>
<li><strong>Batch-invariant <code>torch.compile</code></strong>: Generalized batch-invariant support across attention and MoE backends, with explicit support for DeepGEMM and FlashInfer on Hopper and Blackwell GPUs.</li>
<li><strong>Robust async scheduling</strong>: Fixed several correctness and stability issues in async scheduling, especially when combined with chunked prefill, structured outputs, priority scheduling, MTP, and DeepEP / DCP. We expect <code>--async-scheduling</code> to be enabled by default in the next release.</li>
<li><strong>Stronger scheduler + KV ecosystem</strong>: Improved test coverage in CI and made scheduler behavior more robust with KV connectors, prefix caching, and multi-node deployments.</li>
<li><strong>Anthropic API Support</strong>: Added support for the <code>/v1/messages</code> endpoint, allowing users to interact with <code>vllm serve</code> using Anthropic-compatible clients.</li>
</ul>
<p>Detailed release notes will be updated in the next few days.</p>
<h2>What's Changed</h2>
<ul>
<li>[Bugfix] Improve GLM4 MoE Reasoning Parser's is_reasoning_end Condition (<a class="user-mention notranslate" href="https://github.com/frankwang28">@frankwang28</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25355">#25355</a>)</li>
<li>[Docs] Add Toronto Meetup (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25773">#25773</a>)</li>
<li>[CI] Add E2E Blackwell Quantized MoE Test (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25723">#25723</a>)</li>
<li>[V1] address post issues related to <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20059">#20059</a> (part 1); cascade attention reenable by default (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23046">#23046</a>)</li>
<li>[CI] Fix FlashInfer AOT in release docker image (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25730">#25730</a>)</li>
<li>[spec decode] Consolidate speculative decode method name for MTP (<a class="user-mention notranslate" href="https://github.com/zixi-qi">@zixi-qi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25232">#25232</a>)</li>
<li>Reduce the Cuda Graph memory footprint when running with DBO (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25779">#25779</a>)</li>
<li>Kernel-override Determinism [1/n] (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25603">#25603</a>)</li>
<li>[Bugfix] Optimize CpuGpuBuffer initialization (<a class="user-mention notranslate" href="https://github.com/namanlalitnyu">@namanlalitnyu</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25447">#25447</a>)</li>
<li>[Spec decode] automatically disable mm for text-only draft models (<a class="user-mention notranslate" href="https://github.com/jmkuebler">@jmkuebler</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25667">#25667</a>)</li>
<li>[Core] Don't count preempted tokens in prefix cache hit rate (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25787">#25787</a>)</li>
<li>Add option to restrict media domains (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25783">#25783</a>)</li>
<li>Add flashinfer-build.sh and register precompiled cu128 wheel in Dockerfile (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25782">#25782</a>)</li>
<li>[Multimodal][Speculative Decoding]Eagle Eagle3 mm support, enablement on qwen2.5vl (<a class="user-mention notranslate" href="https://github.com/david6666666">@david6666666</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22872">#22872</a>)</li>
<li>[Bugfix] Allow Only SDPA Backend for ViT on B200 for Qwen3-VL (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25788">#25788</a>)</li>
<li>[CI/Build] Consolidate model loader tests and requirements (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25765">#25765</a>)</li>
<li>[CI/Build] Add timing to Model Executor Test (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25799">#25799</a>)</li>
<li>[CI/Build] Reorganize root-level V1 tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25767">#25767</a>)</li>
<li>[Misc] Fix codeowners override for v1 sample and attention (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25037">#25037</a>)</li>
<li>[Misc] Update openai client example file for multimodal (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25795">#25795</a>)</li>
<li>[Bugfix] Add missing <code>image_size</code> for phi4_multimodal (<a class="user-mention notranslate" href="https://github.com/Renovamen">@Renovamen</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25796">#25796</a>)</li>
<li>[Bugfix] Merge MM embeddings by index instead of token IDs (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/16229">#16229</a>)</li>
<li>Validate API tokens in constant time (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25781">#25781</a>)</li>
<li>Add filtering for chat template kwargs (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25794">#25794</a>)</li>
<li>Fix GPTQ model loading in Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25770">#25770</a>)</li>
<li>[Bugfix] Fix triton import precommit failure (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25803">#25803</a>)</li>
<li>[Bugfix][WideEP] Apply TP Attn + EP MoE fix to other models (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24982">#24982</a>)</li>
<li>[docs] Resolve transcriptions API TODO (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25446">#25446</a>)</li>
<li>[env] default nixl side port conflicts  with kv-event zmq port (<a class="user-mention notranslate" href="https://github.com/panpan0000">@panpan0000</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25056">#25056</a>)</li>
<li>[Core] Refactor self.model() to call a helper for subclassing. (<a class="user-mention notranslate" href="https://github.com/patrick-toulme">@patrick-toulme</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25084">#25084</a>)</li>
<li>[torch.compile]: Add VLLM_DEBUG_DUMP_PATH environment variable (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25651">#25651</a>)</li>
<li>[Bug]: Set LD_LIBRARY_PATH to include the 'standard' CUDA location (<a class="user-mention notranslate" href="https://github.com/smarterclayton">@smarterclayton</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25766">#25766</a>)</li>
<li>[Core] GC Debug callback (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24829">#24829</a>)</li>
<li>[Bugfix][NIXL] Fix Async Scheduler timeout issue (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25808">#25808</a>)</li>
<li>[MM] Optimize memory profiling for scattered multimodal embeddings (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25810">#25810</a>)</li>
<li>[Bugfix] Fix Qwen3-VL regression from <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24982">#24982</a> (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25814">#25814</a>)</li>
<li>[VLM] Update Qwen3-VL max_num_video_tokens calculation for configurable video profiling (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25557">#25557</a>)</li>
<li>Fix random dataset mismatched token length with config. (<a class="user-mention notranslate" href="https://github.com/weireweire">@weireweire</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24937">#24937</a>)</li>
<li>Update GLM-4.5 Doc transformers version (<a class="user-mention notranslate" href="https://github.com/zRzRzRzRzRzRzR">@zRzRzRzRzRzRzR</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25830">#25830</a>)</li>
<li>[Bugfix] fix Qwen3VLMoe load when pp &gt; 1 (<a class="user-mention notranslate" href="https://github.com/JJJYmmm">@JJJYmmm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25838">#25838</a>)</li>
<li>Remove redundant cudagraph dispatcher warning (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25841">#25841</a>)</li>
<li>[Misc] fix tests failure by using current_platform (<a class="user-mention notranslate" href="https://github.com/kingsmad">@kingsmad</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25825">#25825</a>)</li>
<li>[P/D] NIXL Updates (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25844">#25844</a>)</li>
<li>Add Phi4FlashForCausalLM to _PREVIOUSLY_SUPPORTED_MODELS (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25832">#25832</a>)</li>
<li>[XPU]Fix xpu spec decoding UTs, avoid using cuda graph (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25847">#25847</a>)</li>
<li>[Bugfix] Fallback ViT attn backend to SDPA for blackwell (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25851">#25851</a>)</li>
<li>[V0 Deprecation][Models] Remove all V0 condition for mm embeddings merge (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25331">#25331</a>)</li>
<li>[Misc] Remove more <code>get_input_embeddings_v0</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25857">#25857</a>)</li>
<li>update to latest deepgemm for dsv3.2 (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25871">#25871</a>)</li>
<li>[Bugfix] Fix requirements paths in install instructions (<a class="user-mention notranslate" href="https://github.com/yingjun-mou">@yingjun-mou</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25827">#25827</a>)</li>
<li>[Model][Bugfix] Fix issues in MiDashengLM implementation for quantized models (<a class="user-mention notranslate" href="https://github.com/zhoukezi">@zhoukezi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25854">#25854</a>)</li>
<li>[torch.compile] serialize cudagraph_mode as its enum name instead of value (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25868">#25868</a>)</li>
<li>[Cuda2CPU][P/D] Add cuda2cpu support in NixlConnector (<a class="user-mention notranslate" href="https://github.com/chenxi-yang">@chenxi-yang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24690">#24690</a>)</li>
<li>[Bugfix][Speculative Decoding] Fix Eagle3 quantization config issue (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25883">#25883</a>)</li>
<li>[CI/Build] Include Transformers backend test in nightly transformers test (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25885">#25885</a>)</li>
<li>[Model] Remove MotifForCausalLM (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25866">#25866</a>)</li>
<li>[Bugfix] Use correct key "ignore" for config.json non-quantized layers (<a class="user-mention notranslate" href="https://github.com/leejnau">@leejnau</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25706">#25706</a>)</li>
<li>[BugFix][torch.compile] KV scale calculation issues with FP8 quantization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/21640">#21640</a>) (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25513">#25513</a>)</li>
<li>[Doc] Add documentation for vLLM continuous benchmarking and profiling (<a class="user-mention notranslate" href="https://github.com/namanlalitnyu">@namanlalitnyu</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25819">#25819</a>)</li>
<li>[Bugfix][ROCm] Fixing trying to import non-existent symbols from libnccl.so (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25605">#25605</a>)</li>
<li>[Kernel] Chunk-aligned mamba2 (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24683">#24683</a>)</li>
<li>[Doc] Polish example for torchrun dp (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25899">#25899</a>)</li>
<li>[NIXL] Increase default KV block eviction timeout on P (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25897">#25897</a>)</li>
<li>[V0 Deprecation] Remove <code>vllm.worker</code> and update according imports (<a class="user-mention notranslate" href="https://github.com/aarnphm">@aarnphm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25901">#25901</a>)</li>
<li>Test Prompt Embeds/LoRA compatibility and Enable LoRA Support for OPT Models  (<a class="user-mention notranslate" href="https://github.com/qthequartermasterman">@qthequartermasterman</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25717">#25717</a>)</li>
<li>[Bug] Fix Weight Loading for Block FP8 Cutlass SM90 (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25909">#25909</a>)</li>
<li>[Benchmark] Support benchmark throughput for external launcher DP (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25913">#25913</a>)</li>
<li>Move<code>VllmConfig</code> from <code>config/__init__.py</code> to <code>config/vllm.py</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25271">#25271</a>)</li>
<li>[BugFix] Fix DP/EP hang  (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25906">#25906</a>)</li>
<li>[BugFix] Pass config_format via try_get_generation_config (<a class="user-mention notranslate" href="https://github.com/acisseJZhong">@acisseJZhong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25912">#25912</a>)</li>
<li>[Model][Bugfix] Fix MiDashengLM audio encoder mask by removing incorrect <code>logical_not</code> (<a class="user-mention notranslate" href="https://github.com/zhoukezi">@zhoukezi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25925">#25925</a>)</li>
<li>[Bugfix]: Clean up chunked prefill logging when using whisper (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25075">#25075</a>)</li>
<li>[New Model] DeepSeek-V3.2 (Rebased to Main) (<a class="user-mention notranslate" href="https://github.com/zyongye">@zyongye</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25896">#25896</a>)</li>
<li>[Doc] Add Cambricon MLU support (<a class="user-mention notranslate" href="https://github.com/a120092009">@a120092009</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25942">#25942</a>)</li>
<li>Updated TRL integration docs (<a class="user-mention notranslate" href="https://github.com/sergiopaniego">@sergiopaniego</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25684">#25684</a>)</li>
<li>[Bugfix][Model]fix ernie45 moe gate&amp;bias dtype to float32 (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25936">#25936</a>)</li>
<li>[Model] Move <code>vision_feature_select_strategy</code> into <code>resolve_visual_encoder_outputs</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25938">#25938</a>)</li>
<li>[perf] Use CPU tensor to reduce GPU-&gt;CPU sync (<a class="user-mention notranslate" href="https://github.com/lhtin">@lhtin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25884">#25884</a>)</li>
<li>[NIXL] Add support for MLA caches with different latent dim (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25902">#25902</a>)</li>
<li>[CI] Move applicable tests to CPU (<a class="user-mention notranslate" href="https://github.com/rzabarazesh">@rzabarazesh</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24080">#24080</a>)</li>
<li>[Fix] Improve CPU backend compatibility for RISC-V (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25816">#25816</a>)</li>
<li>[Kernel][Moe Configs] Add more tuned triton configs for ExpertsInt8 and FP8 (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25858">#25858</a>)</li>
<li>Add Hugging Face Inference Endpoints guide to Deployment docs (<a class="user-mention notranslate" href="https://github.com/sergiopaniego">@sergiopaniego</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25886">#25886</a>)</li>
<li>[Bugfix][Model] Fix inference for Hunyuan dense models (<a class="user-mention notranslate" href="https://github.com/Anionex">@Anionex</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25354">#25354</a>)</li>
<li>[Bugfix] Fix accuracy issue of TRTLLM FP8 MOE and improve logging (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25895">#25895</a>)</li>
<li>[Bugfix] Token type and position embeddings fail to be applied to <code>inputs_embeds</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25922">#25922</a>)</li>
<li>[bugfix][deepseek] fix flashmla kernel selection (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25956">#25956</a>)</li>
<li>[Bug] Fix AttributeError: 'QKVParallelLinear' object has no attribute 'orig_dtype' (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25958">#25958</a>)</li>
<li>[Doc] Improve MM Pooling model documentation (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25966">#25966</a>)</li>
<li>[Docs] Add moe kernel features doc  (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25297">#25297</a>)</li>
<li>OffloadingConnector: Fix GPU block tracking bug (<a class="user-mention notranslate" href="https://github.com/orozery">@orozery</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25856">#25856</a>)</li>
<li>[Llama4] [multimodal] Fix misplaced dtype cast of <code>cos_sin_cache</code> in <code>Llama4VisionRotaryEmbedding</code> (<a class="user-mention notranslate" href="https://github.com/cjackal">@cjackal</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25889">#25889</a>)</li>
<li>[Bench] Add DeepSeekV32 to MoE benchmark (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25962">#25962</a>)</li>
<li>[V1] [P/D] Add Support for KV Load Failure Recovery (<a class="user-mention notranslate" href="https://github.com/sdavidbd">@sdavidbd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19330">#19330</a>)</li>
<li>Add explicit pooling classes for the Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25322">#25322</a>)</li>
<li>[Docs] Remove API Reference from search index (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25949">#25949</a>)</li>
<li>[gpt-oss] use vLLM instead of openai types for streaming (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25186">#25186</a>)</li>
<li>[Misc] Make EP kernels install script support uv (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25785">#25785</a>)</li>
<li>[Model] MTP fallback to eager for DeepSeek v32 (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25982">#25982</a>)</li>
<li>Update launch_bounds_utils.h for correct compile on Multiple Cuda Arch - PTXAS out of range Warning (<a class="user-mention notranslate" href="https://github.com/DrStone1971">@DrStone1971</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25843">#25843</a>)</li>
<li>[Log] Optimize Log for FP8MOE (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25709">#25709</a>)</li>
<li>Fix INT8 quantization error on Blackwell GPUs (SM100+) (<a class="user-mention notranslate" href="https://github.com/certainly-param">@certainly-param</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25935">#25935</a>)</li>
<li>[MM] Add text-only mode for Qwen3-VL (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26000">#26000</a>)</li>
<li>[Bugfix] Fix <code>__syncwarp</code> on ROCM (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25996">#25996</a>)</li>
<li>[BugFix] Fix default kv-cache-dtype default for DeepseekV3.2 (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25988">#25988</a>)</li>
<li>Update to Transformers <code>v4.56.2</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24638">#24638</a>)</li>
<li>[Misc]allow disable pynccl (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25421">#25421</a>)</li>
<li>[Doc] updating torch.compile doc link <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25989">#25989</a>)</li>
<li>[BugFix][MM] Fix Nonetype error when video is cache in qwen2.5-omni-thinker (<a class="user-mention notranslate" href="https://github.com/wwl2755">@wwl2755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26004">#26004</a>)</li>
<li>[Misc] Factor out common <code>_apply_feature_select_strategy</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26003">#26003</a>)</li>
<li>[CI] Only capture a single CUDA graph size in CI by default (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25951">#25951</a>)</li>
<li>[MISC] Fix misleading batch_size_capture_list when cuda_graph_sizes &lt; 4 (<a class="user-mention notranslate" href="https://github.com/billishyahao">@billishyahao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25829">#25829</a>)</li>
<li>[Benchmark] Finish documented v0.11.0 deprecation of --endpoint-type (<a class="user-mention notranslate" href="https://github.com/natoscott">@natoscott</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26007">#26007</a>)</li>
<li>[Bugfix] Apply same sampling parameters for both <code>n=1</code> and <code>n&gt;1</code> (<a class="user-mention notranslate" href="https://github.com/kmaehashi">@kmaehashi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26005">#26005</a>)</li>
<li>[NVIDIA] Blackwell Family (<a class="user-mention notranslate" href="https://github.com/johnnynunez">@johnnynunez</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24673">#24673</a>)</li>
<li>Fix test_mamba_ssm_ssd.py due to missing _query_start_loc_to_chunk_indices_offsets (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25995">#25995</a>)</li>
<li>[CI] Tweaks to GPT-OSS Eval (Blackwell) for stability (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26030">#26030</a>)</li>
<li>[BugFix][DP/EP] Fix CUTLASS MLA hang under load (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26026">#26026</a>)</li>
<li>[ROCm][Build] Add support for AMD Ryzen AI MAX / AI 300 Series (<a class="user-mention notranslate" href="https://github.com/hyoon1">@hyoon1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25908">#25908</a>)</li>
<li>[Bug] Fix Negative Cuda Memory Usage (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25683">#25683</a>)</li>
<li>[BugFix] ChunkedLocalAttention is currently not CG compatible (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26034">#26034</a>)</li>
<li>Support RL online quantization with torchao (<a class="user-mention notranslate" href="https://github.com/jerryzh168">@jerryzh168</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23014">#23014</a>)</li>
<li>[ROCm][Bugfix] Add missing parameter to ROCm backend (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26029">#26029</a>)</li>
<li>[Misc] Make handling of SamplingParams clearer in n&gt;1 case (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26032">#26032</a>)</li>
<li>Run:ai model streamer add GCS package support (<a class="user-mention notranslate" href="https://github.com/pwschuurman">@pwschuurman</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24909">#24909</a>)</li>
<li>Update base image to 22.04 (jammy) (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26065">#26065</a>)</li>
<li>Change size of single CUDA graph for CI to 4 (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26089">#26089</a>)</li>
<li>[FA/Chore] Bump vllm-flash-attention (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25537">#25537</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (A-C) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26073">#26073</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (D-F) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26076">#26076</a>)</li>
<li>[Platform][CI] Added OOT platform interface e2e test that running on Ascend NPU (<a class="user-mention notranslate" href="https://github.com/leo-pony">@leo-pony</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25470">#25470</a>)</li>
<li>[Qwen][ROCm] Flash Attention Rotary Embeddings (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24642">#24642</a>)</li>
<li>[CI] Add Blackwell DeepSeek FP8 FlashInfer MoE tests (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26040">#26040</a>)</li>
<li>[CI/Build] Replace <code>vllm.entrypoints.openai.api_server</code> entrypoint with <code>vllm serve</code> command (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25967">#25967</a>)</li>
<li>[BugFix] Fix FI accuracy issue when used for MLA prefill (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26063">#26063</a>)</li>
<li>[Small] Prevent bypassing media domain restriction via HTTP redirects (<a class="user-mention notranslate" href="https://github.com/huachenheli">@huachenheli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26035">#26035</a>)</li>
<li>[Deepseek v3.2] Support indexer prefill chunking (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25999">#25999</a>)</li>
<li>EAGLE 3: Fix preamble so that measured speedup over Eagle 1 becomes 32% instead of 5% on MTBench (<a class="user-mention notranslate" href="https://github.com/ekagra-ranjan">@ekagra-ranjan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25916">#25916</a>)</li>
<li>[Mamba][KVCacheManager] Simplify kv cache manage logic for mamba + MTP (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25119">#25119</a>)</li>
<li>[Perf] Fix and reapply move apply w8a8 block fp8 linear to class (<a class="user-mention notranslate" href="https://github.com/ElizaWszola">@ElizaWszola</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25696">#25696</a>)</li>
<li>Fix MTP with deepep_low_latency (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25904">#25904</a>)</li>
<li>[Bugfix] Disable cascade attention with FlashInfer (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26130">#26130</a>)</li>
<li>[Log] Optimize DeepGEMM Missing Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26106">#26106</a>)</li>
<li>[Bug][Benchmark] Fix duplicate req in oversampling (<a class="user-mention notranslate" href="https://github.com/ekagra-ranjan">@ekagra-ranjan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26140">#26140</a>)</li>
<li>[Attention] Move Backend enum into registry (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25893">#25893</a>)</li>
<li>[CI/Build] Conditionally register cutlass_fp4_group_mm to fix building on Hopper (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26138">#26138</a>)</li>
<li>[DeepSeek] Improve performance of DS MLA cache kernel (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26132">#26132</a>)</li>
<li>[Bug]: Limit num_reqs in dummy_run when max_num_seqs is small (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26144">#26144</a>)</li>
<li>[gpt-oss] disable tool server initialization if no tool in request (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25790">#25790</a>)</li>
<li>[Build/CI] Revert back to Ubuntu 20.04, install python 3.12 with uv (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26103">#26103</a>)</li>
<li>[ROCm] [VL] [Bugfix] Fix vit flash attn dispatcher logic for ROCm (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26104">#26104</a>)</li>
<li>[Bugfix] Fix import <code>gemm_afp4wfp4</code> failure on AMD (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26068">#26068</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (G) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26117">#26117</a>)</li>
<li><code>FusedMoE</code> support for the Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22650">#22650</a>)</li>
<li>[BUG] Reorder model config creation (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26124">#26124</a>)</li>
<li>[Misc] Remove typing.List (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26150">#26150</a>)</li>
<li>[Input] Remove unused <code>prompt</code> field (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26097">#26097</a>)</li>
<li>[Perf] Optimize <code>reshape_and_cache</code> CUDA Kernel (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25955">#25955</a>)</li>
<li>add(v1): RequestStatesStats to RequestOutput (<a class="user-mention notranslate" href="https://github.com/huijjj">@huijjj</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24947">#24947</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (InternVL family) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26153">#26153</a>)</li>
<li>[test utils] correct wrong typing (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26159">#26159</a>)</li>
<li>[CI] Fix distributed hybrid tests in CI (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26155">#26155</a>)</li>
<li>[NIXL][Misc] Expose metrics from NIXL for logging to CLI (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25388">#25388</a>)</li>
<li>[openai] Fix missing tool usage check (system message) (<a class="user-mention notranslate" href="https://github.com/levunet">@levunet</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24768">#24768</a>)</li>
<li>[Multi Modal] Configurable MM Profiling (<a class="user-mention notranslate" href="https://github.com/wwl2755">@wwl2755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25631">#25631</a>)</li>
<li>[Doc] Fixed shape description for fused_batched_moe.py (<a class="user-mention notranslate" href="https://github.com/Egor-Krivov">@Egor-Krivov</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25668">#25668</a>)</li>
<li>Quick fix for IMA with the Prefix Prefill kernel during graph capture (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25983">#25983</a>)</li>
<li>[Renderer] Move Processor out of AsyncLLM  (<a class="user-mention notranslate" href="https://github.com/KKSK-DON">@KKSK-DON</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24138">#24138</a>)</li>
<li>Re-enable prefill of max model length (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24446">#24446</a>)</li>
<li>[backends][short_conv] CUDA graph piecewise edits (<a class="user-mention notranslate" href="https://github.com/paulpak58">@paulpak58</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24215">#24215</a>)</li>
<li>[Model] Supplement to PR 24862: Pass param prefix to LLMHead (<a class="user-mention notranslate" href="https://github.com/whx-sjtu">@whx-sjtu</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25805">#25805</a>)</li>
<li>[CI/Build] do not enforce precompilation on tpu ci tests (<a class="user-mention notranslate" href="https://github.com/sixiang-google">@sixiang-google</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25992">#25992</a>)</li>
<li>[Model] Fixed stream generator for gpt-oss + spec-decoding (<a class="user-mention notranslate" href="https://github.com/astralord">@astralord</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26027">#26027</a>)</li>
<li>[Renderer] Move Processor out of LLMEngine (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26165">#26165</a>)</li>
<li>Fix undefined symbol: cutlass_moe_mm_sm100 (<a class="user-mention notranslate" href="https://github.com/jasl">@jasl</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26098">#26098</a>)</li>
<li>[BugFix][QWEN-VL]fix wrong apply_rotary_emb_torch selection introduced by <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24642">#24642</a> (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26123">#26123</a>)</li>
<li>Stop mergify from keeping stale PRs alive (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26169">#26169</a>)</li>
<li>Avoid division by zero in cache DS MLA kernel (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26174">#26174</a>)</li>
<li>Fix V1 engine serialization error with Ray distributed executor (<a class="user-mention notranslate" href="https://github.com/nrghosh">@nrghosh</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26148">#26148</a>)</li>
<li>[Quantization/NVFP4] Speed up TRTLLM NVFP4 MOE weight loading and fix K/V scale loading for MLA Attn (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25968">#25968</a>)</li>
<li>[Perf] Remove hardcoded num_warps=1 (<a class="user-mention notranslate" href="https://github.com/chelsea0x3b">@chelsea0x3b</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26183">#26183</a>)</li>
<li>[Refactor] Optimize FP8 MOE Backend Choice and Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26044">#26044</a>)</li>
<li>[responsesAPI] add better error messaging for long prompts (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25724">#25724</a>)</li>
<li>[Bugfix] Relax tokenizer regex for mixtral to include 'tokenizer.model' (<a class="user-mention notranslate" href="https://github.com/BowenBao">@BowenBao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25964">#25964</a>)</li>
<li>[CI] Push multiarch manifests as nightly builds (<a class="user-mention notranslate" href="https://github.com/csahithi">@csahithi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25764">#25764</a>)</li>
<li>[Misc] Add penalties sampling parameters to serve tool (<a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25974">#25974</a>)</li>
<li>[BugFix] Fix de-functionalization pass for rotary_embedding (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23953">#23953</a>)</li>
<li>[CI] Fix Pre-commit Mypy Error (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26181">#26181</a>)</li>
<li>[GPTOSS][DP/EP][Marlin] Enable GPTOSS DP/EP using Marlin kernels (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25488">#25488</a>)</li>
<li>Fix issue of using only the part of video frame [Nemotron Nano] (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26186">#26186</a>)</li>
<li>[Bugfix] Fix qwen3 vl dummy data generation with overrides (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26193">#26193</a>)</li>
<li>[BugFix] Use async Mistral Tokenizer in Chat Completions (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26134">#26134</a>)</li>
<li>Add batch invariant kernel override for FlashInfer backend [2/n] (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25769">#25769</a>)</li>
<li>[cpu][perf] Accelerate unquantized-linear for AArch64 through oneDNN/ACL and weight prepack (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25948">#25948</a>)</li>
<li>[V1] [Hybrid] Mamba2 Automatic Prefix Caching (<a class="user-mention notranslate" href="https://github.com/s3woz">@s3woz</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25752">#25752</a>)</li>
<li>Support expert parallel in Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26162">#26162</a>)</li>
<li>[Model] Support nested structures for TensorSchema (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26212">#26212</a>)</li>
<li>[Misc] Require <code>merge_by_field_config</code> argument (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26214">#26214</a>)</li>
<li>[Misc] Remove unused <code>executor.apply_model</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26215">#26215</a>)</li>
<li>[CI Failure] fix_test_auto_prefix_cache_support (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26053">#26053</a>)</li>
<li>Revert "Add batch invariant kernel override for FlashInfer backend [2/n]" (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26220">#26220</a>)</li>
<li>Add Olmo 3 reasoning parser (<a class="user-mention notranslate" href="https://github.com/soldni">@soldni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26054">#26054</a>)</li>
<li>[Core] Enable decode of context length equal to max model length (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26168">#26168</a>)</li>
<li>[Bugfix] Fix <code>_reqs_to_process</code> leak on abort (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26012">#26012</a>)</li>
<li>[Model] CLIP Embedding Support (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26010">#26010</a>)</li>
<li>Fix tensor device and dtype placement in Qwen2VL model (<a class="user-mention notranslate" href="https://github.com/yuafng">@yuafng</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26219">#26219</a>)</li>
<li>[V1] [Hybrid] Remove code to override default CUDA graph configuration (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26226">#26226</a>)</li>
<li>[CPU] Refine batch reorder of CPU attention backend (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26096">#26096</a>)</li>
<li>[Frontend] Cache chat template kwargs resolution (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26227">#26227</a>)</li>
<li>[Renderer] Clean up renderer code (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26216">#26216</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (H-L) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26230">#26230</a>)</li>
<li>[Easy] Add str repr for IterationStats (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26232">#26232</a>)</li>
<li>[Bugfix] Allow <code>--skip-tokenizer-init</code> with <code>echo and return_token_ids</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26238">#26238</a>)</li>
<li>Add documentation for granite 4 tool calling (<a class="user-mention notranslate" href="https://github.com/maxdebayser">@maxdebayser</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26175">#26175</a>)</li>
<li>[Perf][Easy] Early stop in request_block_hasher (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26112">#26112</a>)</li>
<li>[Bugfix]: Assertion error when using FlashInfer backend (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25933">#25933</a>)</li>
<li>[Bugfix] Always apply MM processor even when no MM items are passed (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26240">#26240</a>)</li>
<li>[Bugfix][Hardware][RISC-V] Limit supported dtypes to float32 to avoid scheduler segfault (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26228">#26228</a>)</li>
<li>[Refactor][Kernel] support loading kernel from other place (<a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25823">#25823</a>)</li>
<li>Convert formatting to use <code>ruff</code> instead of <code>yapf</code> + <code>isort</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26247">#26247</a>)</li>
<li>Remove all references to <code>yapf</code> as it's no longer used (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26251">#26251</a>)</li>
<li>Remove all cases of <code>fmt: on/off</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26253">#26253</a>)</li>
<li>fix(tests): Resolve late binding of loop variable in assert message lambda (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26249">#26249</a>)</li>
<li>Fix per file ruff ignores related to typing (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26254">#26254</a>)</li>
<li>Update <code>ruff</code> pre-commit hooks version (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26255">#26255</a>)</li>
<li>[CI] fix mamba kernel test (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26250">#26250</a>)</li>
<li>[NVIDIA] flashinfer TRTLLM attention prefill token limit (<a class="user-mention notranslate" href="https://github.com/jasonlizhengjian">@jasonlizhengjian</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25998">#25998</a>)</li>
<li>Fix per file ruff ignores related to simplification (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26259">#26259</a>)</li>
<li>[CI] Add Blackwell LM Eval Small Models test to nightly (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26052">#26052</a>)</li>
<li>[DOC] Update production-stack.md (<a class="user-mention notranslate" href="https://github.com/elieserr">@elieserr</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26177">#26177</a>)</li>
<li>[CI] Add comment about the single cudagraph capture size that is used (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26252">#26252</a>)</li>
<li>[V1] [Hybrid] Some additional clean-up in Mamba2 prefix caching (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26222">#26222</a>)</li>
<li>[Doc] Edited minor typo (<a class="user-mention notranslate" href="https://github.com/orangeng">@orangeng</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26266">#26266</a>)</li>
<li>[MISC] Add heheda12345 to CODEOWNERS of vllm/config/cache.py (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26270">#26270</a>)</li>
<li>[CI][gpt-oss] Enable python tool tests in CI (<a class="user-mention notranslate" href="https://github.com/wuhang2014">@wuhang2014</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24315">#24315</a>)</li>
<li>Fix per file ruff ignores related to line length (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26262">#26262</a>)</li>
<li>Bump actions/stale from 10.0.0 to 10.1.0 (<a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26272">#26272</a>)</li>
<li>[Benchmarking] Add disable_shuffle option for dataset loading (<a class="user-mention notranslate" href="https://github.com/ymoslem">@ymoslem</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26258">#26258</a>)</li>
<li>[Misc] Clean up unnecessary E501 ignore (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26274">#26274</a>)</li>
<li>[Docs] Edit HF Inference Endpoints documentation (<a class="user-mention notranslate" href="https://github.com/ariG23498">@ariG23498</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26275">#26275</a>)</li>
<li>[Doc] add KAITO to integrations (<a class="user-mention notranslate" href="https://github.com/abhisheksheth28">@abhisheksheth28</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25521">#25521</a>)</li>
<li>[Frontend] Consolidate tokenizer init code (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26276">#26276</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (Llava family) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26280">#26280</a>)</li>
<li>Support expert parallel load balancing in Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26287">#26287</a>)</li>
<li>[Bugfix] Fix mrope in Transformers Backend (<a class="user-mention notranslate" href="https://github.com/zucchini-nlp">@zucchini-nlp</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26087">#26087</a>)</li>
<li>Fix <code>DotsOCR</code> tensor type (<a class="user-mention notranslate" href="https://github.com/what-in-the-nim">@what-in-the-nim</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26281">#26281</a>)</li>
<li>[Model] EVS support for nano_nemotron_vl (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26269">#26269</a>)</li>
<li>[Attention] Remove unused reorder_batch method (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24463">#24463</a>)</li>
<li>[Tests] conftest: Extending VllmRunner and HfRunner to accept token_ids as input (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26295">#26295</a>)</li>
<li>[CI Bugfix] Make sure TRTLLM attention is available in test_blackwell_moe (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26188">#26188</a>)</li>
<li>Support llama3 eagle3 head with llama4 verifier (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25961">#25961</a>)</li>
<li>[Misc] auto_tune: kill specific vllm process (<a class="user-mention notranslate" href="https://github.com/karan">@karan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26304">#26304</a>)</li>
<li>[Bugfix][Spec Decode] Fix wrong valid_mask for padded speculation when chunked prefill occurs (<a class="user-mention notranslate" href="https://github.com/seven-mile">@seven-mile</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26231">#26231</a>)</li>
<li>Add bias handling to CPUFusedMOE kernel (<a class="user-mention notranslate" href="https://github.com/cfRod">@cfRod</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26289">#26289</a>)</li>
<li>[Bugfix] Fix gemma3 with transformers backend (<a class="user-mention notranslate" href="https://github.com/zucchini-nlp">@zucchini-nlp</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23178">#23178</a>)</li>
<li>[Benchmark] Enable MM Embedding benchmarks (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26310">#26310</a>)</li>
<li>[Docs] Fix broken table in moe_kernel_features doc (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26314">#26314</a>)</li>
<li>[BugFix] Pad input buffers in _dummy_run (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26209">#26209</a>)</li>
<li>[Bugfix] Allow skipping MoE in NVFP4 (fix for MTP) (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25987">#25987</a>)</li>
<li>[ROCm] Split AITER unified attention into its own backend (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25507">#25507</a>)</li>
<li>[Perf] Add decode full-graph support to FlashInfer-MLA backend (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26313">#26313</a>)</li>
<li>[Misc] Define EP kernel arch list in Dockerfile (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25635">#25635</a>)</li>
<li>[Docs][DBO] Add initial doc that describes the DBO implementation (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26024">#26024</a>)</li>
<li>[Core] Simplify the Dp padding/should ubatch coordination logic (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25768">#25768</a>)</li>
<li>[UX] Support nested dicts in hf_overrides (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25727">#25727</a>)</li>
<li>[BUG] Fix file parsing for load_format runai_streamer_sharded (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26324">#26324</a>)</li>
<li>[Model] Define merge_by_field_config MM interface (U-Z) (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26261">#26261</a>)</li>
<li>[Deprecation] Deprecate <code>LLM.set_tokenizer</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26333">#26333</a>)</li>
<li>[responsesAPI][bugfix] serialize harmony messages (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26185">#26185</a>)</li>
<li>[Model] Define merge_by_field_config MM interface (R-T) (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26260">#26260</a>)</li>
<li>[BugFix] Update KV block hash type from BlockHash to ExternalBlockHash in kv_events_subscriber - <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/26264">#26264</a> (<a class="user-mention notranslate" href="https://github.com/atalhens">@atalhens</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26265">#26265</a>)</li>
<li>[V0 Deprecation] Remove <code>VLLM_USE_V1</code> from docs and scripts (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26336">#26336</a>)</li>
<li>Optimize KV cache distribution for asymmetric pipeline parallelism (<a class="user-mention notranslate" href="https://github.com/gholmes829">@gholmes829</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25164">#25164</a>)</li>
<li>Add topk logits torch op for DS3.2. (<a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25945">#25945</a>)</li>
<li>Add TRL example notebook to RLHF docs (<a class="user-mention notranslate" href="https://github.com/sergiopaniego">@sergiopaniego</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26346">#26346</a>)</li>
<li>[Docs] add docs for cuda graph v1 (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24374">#24374</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (Ovis family) (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26308">#26308</a>)</li>
<li>[Feature][OCP MX] Support mxfp6 and mixed mxfp6-mxfp4 (<a class="user-mention notranslate" href="https://github.com/fxmarty-amd">@fxmarty-amd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21166">#21166</a>)</li>
<li>[Model] Add support for ModernBertForTokenClassification (<a class="user-mention notranslate" href="https://github.com/antrec">@antrec</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26340">#26340</a>)</li>
<li>[Misc] Move <code>LRUCache</code> into its own file (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26342">#26342</a>)</li>
<li>[V0 Deprecation] Remove <code>VLLM_USE_V1</code> from tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26341">#26341</a>)</li>
<li>[Model] Lfm2Moe (<a class="user-mention notranslate" href="https://github.com/paulpak58">@paulpak58</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26344">#26344</a>)</li>
<li>[ci] Rename <code>test_mxfp4_moe.py</code> to <code>test_ocp_mx_moe.py</code> (<a class="user-mention notranslate" href="https://github.com/fxmarty-amd">@fxmarty-amd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26364">#26364</a>)</li>
<li>[CI] Add Qwen3 MoE NVFP4 to Blackwell lm-eval (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26316">#26316</a>)</li>
<li>[deepseek] add EP8 FusedMOE config for H200 and B200 (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26331">#26331</a>)</li>
<li>[Bug] Fix Shape Validation for Fallback while Enabling E8M0 for DeepGEMM (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26322">#26322</a>)</li>
<li>[Bugfix] Add missing sink tensor into flash attn cascade attn implementation (<a class="user-mention notranslate" href="https://github.com/plliao">@plliao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26325">#26325</a>)</li>
<li>[Frontend] CompilationConfig overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): deprecate use_inductor in favor of backend, simplify custom_ops (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26113">#26113</a>)</li>
<li>[V1] Logit processors for rejection sampler (<a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19482">#19482</a>)</li>
<li>[Spec Decode] Enable efficient speculative decoding with FlashInfer-MLA (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25984">#25984</a>)</li>
<li>[TPU] update TPU benchmark threshold (<a class="user-mention notranslate" href="https://github.com/jcyang43">@jcyang43</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25713">#25713</a>)</li>
<li>Add more libraries to rlhf.md (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26374">#26374</a>)</li>
<li>[Bugfix] Fix MTP+FlashInfer crash when trtllm kernels are available but disabled (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26361">#26361</a>)</li>
<li>Revert <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24446">#24446</a> and <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26168">#26168</a> (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26332">#26332</a>)</li>
<li>[Misc] Clean up cruft from previous FlashMLA sparse implementation (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26125">#26125</a>)</li>
<li>[torchao] safetensors integration (<a class="user-mention notranslate" href="https://github.com/liangel-02">@liangel-02</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25969">#25969</a>)</li>
<li>Add SwigluOAI implementation for CPUFusedMOE (<a class="user-mention notranslate" href="https://github.com/isharif168">@isharif168</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26347">#26347</a>)</li>
<li>[Core] Simplify setting new_token_ids in CachedRequestData (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26388">#26388</a>)</li>
<li>fix(v1/kv_cache): resolve async KV transfer bug in cascade attention (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23485">#23485</a>)</li>
<li>Add gather_indexer_k_quant_cache kernel (<a class="user-mention notranslate" href="https://github.com/Barry-Delaney">@Barry-Delaney</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25931">#25931</a>)</li>
<li>[Bugfix] Incorrect MM data format in <code>vllm bench throughput</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26395">#26395</a>)</li>
<li>fix[DP][v1]: Prevent hangs from mismatched worker configurations (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26218">#26218</a>)</li>
<li>[TPU] Rename tpu_commons to tpu_inference (<a class="user-mention notranslate" href="https://github.com/utkarshsharma1">@utkarshsharma1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26279">#26279</a>)</li>
<li>[Feature] Enable E8M0 by Default on Hopper for DeepGEMM, 5% E2E throughput improvement (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26197">#26197</a>)</li>
<li>[Misc] add usedforsecurity=False in md5 hash call (<a class="user-mention notranslate" href="https://github.com/dtrifiro">@dtrifiro</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26357">#26357</a>)</li>
<li>[Model] Allow passing custom number of max tiles to Nano 2 VL (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26403">#26403</a>)</li>
<li>[Docs] Have mergify leave a comment with the docs preview link (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26412">#26412</a>)</li>
<li>[CI] Pooling models mteb test disable enforce_eager  (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26408">#26408</a>)</li>
<li>[Benchmarks] Add support for Qwen 3 VL MoE tuning (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26419">#26419</a>)</li>
<li>Tidy <code>vllm/config/__init__.py</code> to only add classes and functions (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26405">#26405</a>)</li>
<li>[NIXL][non-cuda] Add install script for nixl with non-cuda ucx (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25959">#25959</a>)</li>
<li>[Refactor] Refactor FP8 &amp; INT8 Quant Folder inside <code>w8a8</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25293">#25293</a>)</li>
<li>[CI Failure] Fix pre-commit issue for install_nixl_from_source_ubuntu.py (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26424">#26424</a>)</li>
<li>[Bugfix] Fix <code>vllm bench ...</code> on CPU-only head nodes (<a class="user-mention notranslate" href="https://github.com/Aydin-ab">@Aydin-ab</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25283">#25283</a>)</li>
<li>[Bug] Fix DeepGEMM Attention Test (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26423">#26423</a>)</li>
<li>[Benchmarks] Fix imports in FP8 tuning script (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26407">#26407</a>)</li>
<li>[Bug] Fix Test in Batch Invariant (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26128">#26128</a>)</li>
<li>Remove Python 3.9 support ahead of PyTorch 2.9 in v0.11.1 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26416">#26416</a>)</li>
<li>[Feature] Change cache.py with pydantic validation (<a class="user-mention notranslate" href="https://github.com/vrdn-23">@vrdn-23</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26390">#26390</a>)</li>
<li>[Attention] Implement universal BACKEND_MAP (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25900">#25900</a>)</li>
<li>[Bugfix][Flashinfer] fix VLLM_USE_TRTLLM_ATTENTION issue for models with diff hyperparameters (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25924">#25924</a>)</li>
<li>[BugFix] Fix failing test quantization/test_compressed_tensors.py::test_compressed_tensors_fp8_block_enabled (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26436">#26436</a>)</li>
<li>[Kernel] Centralize platform kernel import in <code>current_platform.import_kernels</code> (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26286">#26286</a>)</li>
<li>[Models] Improve iteration over layers (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26425">#26425</a>)</li>
<li>[Bugfix] Respect min_tokens in scheduler stop check (<a class="user-mention notranslate" href="https://github.com/elaineyz">@elaineyz</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26317">#26317</a>)</li>
<li>[Kernels] Modular kernel refactor (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24812">#24812</a>)</li>
<li>[Attention] Register FLASHMLA_SPARSE (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26441">#26441</a>)</li>
<li>Separate MLAAttention class from Attention (<a class="user-mention notranslate" href="https://github.com/therealnaveenkamal">@therealnaveenkamal</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25103">#25103</a>)</li>
<li>[Misc] Redact ray runtime env before logging (<a class="user-mention notranslate" href="https://github.com/ruisearch42">@ruisearch42</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26302">#26302</a>)</li>
<li>[Bugfix] Set the minimum python version for gpt-oss (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26392">#26392</a>)</li>
<li>[Minor] Change warning-&gt;warning_once in preprocess (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26455">#26455</a>)</li>
<li>[Bugfix] Catch and log invalid token ids in detokenizer <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/2">#2</a> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26445">#26445</a>)</li>
<li>[Bugfix] Incorrect another MM data format in vllm bench throughput (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26462">#26462</a>)</li>
<li>[Hardware][AMD] Enable FlexAttention backend on ROCm (<a class="user-mention notranslate" href="https://github.com/mawong-amd">@mawong-amd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26439">#26439</a>)</li>
<li>[MM][Doc] Add documentation for configurable mm profiling (<a class="user-mention notranslate" href="https://github.com/wwl2755">@wwl2755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26200">#26200</a>)</li>
<li>[Core][KVConnector] Propagate all tokens on resumed preemptions (<a class="user-mention notranslate" href="https://github.com/QierLi">@QierLi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24926">#24926</a>)</li>
<li>[Hybrid]: Decouple Kernel Block Size from KV Page Size (<a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24486">#24486</a>)</li>
<li>[CI/Build] Fix model nightly tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26466">#26466</a>)</li>
<li>[Core] Relax the LoRA  max rank (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26461">#26461</a>)</li>
<li>Update Dockerfile and install runai-model-streamer[gcs] package (<a class="user-mention notranslate" href="https://github.com/pwschuurman">@pwschuurman</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26464">#26464</a>)</li>
<li>Bump Flashinfer to v0.4.0 (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26326">#26326</a>)</li>
<li>[Model] Gemma3: Fix GGUF loading and quantization (<a class="user-mention notranslate" href="https://github.com/lucianommartins">@lucianommartins</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26189">#26189</a>)</li>
<li>Enable <code>RMSNorm</code> substitution for Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26353">#26353</a>)</li>
<li>Add: Support for multiple hidden layers in Eagle3 (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26164">#26164</a>)</li>
<li>[torchao] Add support for ModuleFqnToConfig using regex (<a class="user-mention notranslate" href="https://github.com/jerryzh168">@jerryzh168</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26001">#26001</a>)</li>
<li>[Misc] Misc code simplifications (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26450">#26450</a>)</li>
<li>[doc] add Volcengine as a compute sponsor (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26477">#26477</a>)</li>
<li>[Feature] Use pydantic validation in lora.py and load.py configs (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26413">#26413</a>)</li>
<li>[Misc] Upgrade more code to Python 3.10 (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26463">#26463</a>)</li>
<li>[Bugfix] Fix SHM cache initialization (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26427">#26427</a>)</li>
<li>[Models][Qwen3VL] Optimise <code>_validate_and_reshape_mm_tensor</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26426">#26426</a>)</li>
<li>[Bugfix] Move current_platform import to avoid python import cache. (<a class="user-mention notranslate" href="https://github.com/iwzbi">@iwzbi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/16601">#16601</a>)</li>
<li>[V0 deprecation] Remove <code>QKVCrossParallelLinear</code> implementation  (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26475">#26475</a>)</li>
<li>[Feature] Use pydantic validation in parallel.py config (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26417">#26417</a>)</li>
<li>Revert  <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26113">#26113</a> "[Frontend] CompilationConfig overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): deprecate use_inductor in favor of backend, simplify custom_ops" (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26472">#26472</a>)</li>
<li>Upgrade Pydantic to v2.12.0 and remove hack for Python 3.13 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26481">#26481</a>)</li>
<li>[Models][Qwen] Replace <code>pad</code> with <code>cat</code> for better performance (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26486">#26486</a>)</li>
<li>[Attention][DCP] Support DCP with query length &gt; 1 (MTP) with FA3 (<a class="user-mention notranslate" href="https://github.com/minosfuture">@minosfuture</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25049">#25049</a>)</li>
<li>[Model] Apply shared experts overlap optimization to all models with shared experts (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26145">#26145</a>)</li>
<li>[BUGFIX] Add cu_tokens_across_sp to DPMetadata (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26457">#26457</a>)</li>
<li>[Bugfix] Enable padded FP4 quantization (<a class="user-mention notranslate" href="https://github.com/roikoren755">@roikoren755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25947">#25947</a>)</li>
<li>[Bugfix] Disable moe inplace for torch &gt;= 2.9 (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26497">#26497</a>)</li>
<li>[Flashinfer][gpt-oss] Support FP8-qkv Flashinfer TRTLLM Sinks Attention (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25674">#25674</a>)</li>
<li>[Core] Remove unused <code>prev_sampled_token_ids_invalid_indices</code> input batch field (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26514">#26514</a>)</li>
<li>[UX] Add FlashInfer as default CUDA dependency (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26443">#26443</a>)</li>
<li>[Bugfix] Fix CUDA graph selection bug in FlashInfer at high concurrency (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26499">#26499</a>)</li>
<li>[Bug] Fix modular_kernel: ZeroDivisionError: integer division or modulo by zero (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26528">#26528</a>)</li>
<li>[CI] Fix Pre-commit Issue Cannot determine type of "rank" and "world_size" (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26448">#26448</a>)</li>
<li>Refactor MistralTokenizer (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26358">#26358</a>)</li>
<li>[DP][ray] Support different VLLM_RAY_DP_PACK_STRATEGY (<a class="user-mention notranslate" href="https://github.com/ruisearch42">@ruisearch42</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23849">#23849</a>)</li>
<li>[Core] Small simplification in <code>GPUModelRunner._update_states()</code> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26508">#26508</a>)</li>
<li>[Chore]: One pythonic tool parser test uses the wrong parser (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26515">#26515</a>)</li>
<li>[Spec-Decode] Support piecewise cudagraphs for Eagle head (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25109">#25109</a>)</li>
<li>fix test_simple_inductor_graph_partition (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26522">#26522</a>)</li>
<li>[deepseek] kernel block size for UniformTypeKVCacheSpecs (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26559">#26559</a>)</li>
<li>[Metrics] Log multi-modal cache stats and fix reset (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26285">#26285</a>)</li>
<li>[GPT-OSS] Add support for arrays  at tool message content (<a class="user-mention notranslate" href="https://github.com/luis5tb">@luis5tb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25593">#25593</a>)</li>
<li>Remove LoRA bias support (<a class="user-mention notranslate" href="https://github.com/ashwin-phadke">@ashwin-phadke</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25807">#25807</a>)</li>
<li>[CI] fix ruff format (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26579">#26579</a>)</li>
<li>[bugfix][DCP] fix block_size of hash in DCP prefix caching (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26296">#26296</a>)</li>
<li>[NIXL] Ignore abort on already-finished request (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25067">#25067</a>)</li>
<li>[Bugfix] Convert untraceable GroupShape to list for AMD impl (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26535">#26535</a>)</li>
<li>[BugFix] Fix noop elimination edge case (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26394">#26394</a>)</li>
<li>[CI] fix test_run_batch.py::test_completions - AssertionError (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26578">#26578</a>)</li>
<li>[BugFix][torch.compile] Fix fused_scaled_matmul_reduce_scatter signature for PyTorch 2.8 (<a class="user-mention notranslate" href="https://github.com/jasonlizhengjian">@jasonlizhengjian</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26038">#26038</a>)</li>
<li>Added test_top_k_per_row to test-pipeline.yaml. (<a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26569">#26569</a>)</li>
<li>[Bugfix] Make DP padding optional in coordinate_batch_across_dp (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26375">#26375</a>)</li>
<li>Silu v2 (<a class="user-mention notranslate" href="https://github.com/elvircrn">@elvircrn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25074">#25074</a>)</li>
<li>[Metrics] Add test for multi-modal cache stats logging (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26588">#26588</a>)</li>
<li>[torch.compile] Make inductor partition rules respect splitting_ops <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25691">#25691</a> (<a class="user-mention notranslate" href="https://github.com/baonudesifeizhai">@baonudesifeizhai</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25845">#25845</a>)</li>
<li>[Bugfix] fixed top_logprobs: -1 does not appear to work as intended (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26470">#26470</a>)</li>
<li>[Model][Qwen3VL] Compute <code>cu_seqlens</code> on CPU to remove  (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26496">#26496</a>)</li>
<li>[Model] Add FlexOlmo model implementation (<a class="user-mention notranslate" href="https://github.com/2015aroras">@2015aroras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24923">#24923</a>)</li>
<li>[Transform] [Quantization] Add QuTLASS support to vLLM (<a class="user-mention notranslate" href="https://github.com/LopezCastroRoberto">@LopezCastroRoberto</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24440">#24440</a>)</li>
<li>Add Qwen3-Omni moe thinker (<a class="user-mention notranslate" href="https://github.com/wangxiongts">@wangxiongts</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25550">#25550</a>)</li>
<li>Update <code>pre-commit</code> hook versions (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26591">#26591</a>)</li>
<li>Update CUDA architecture list in build pipeline for 12.9.1 wheels (<a class="user-mention notranslate" href="https://github.com/wseaton">@wseaton</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26592">#26592</a>)</li>
<li>Fix some typing issues found by <code>mypy==1.18.2</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26596">#26596</a>)</li>
<li>[BUG] Qwen3-next MTP. Fix attn metadata build bug (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26564">#26564</a>)</li>
<li>[BugFix] Fix async scheduling + request preemption (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26385">#26385</a>)</li>
<li>Cache the environment variable check for batch invariance (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26510">#26510</a>)</li>
<li>AOT Compilation for torch.compile (Bundled) (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24274">#24274</a>)</li>
<li>[BugFix] Make penalties and bad_words work with async scheduling (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26467">#26467</a>)</li>
<li>[Frontend] Improve the performance of <code>is_reasoning_end</code> (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25735">#25735</a>)</li>
<li>[CI/Build] Fix ppc64le CPU build and tests (<a class="user-mention notranslate" href="https://github.com/npanpaliya">@npanpaliya</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22443">#22443</a>)</li>
<li>[XPU] Upgrade NIXL to remove CUDA dependency (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26570">#26570</a>)</li>
<li>[MM] Move Qwen3Omni MRoPE impl to model file (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26608">#26608</a>)</li>
<li>[Bugfix][Multi Modal] Fix incorrect Molmo image processing (<a class="user-mention notranslate" href="https://github.com/sangho-vision">@sangho-vision</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26563">#26563</a>)</li>
<li>[Refactor]: Use M-RoPE interface directly while defining model class instead of maintaining model specific M-RoPE implementation in mrope.py (<a class="user-mention notranslate" href="https://github.com/divyanshsinghvi">@divyanshsinghvi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24172">#24172</a>)</li>
<li>fix(nix): Allow local oneDNN path to fix vLLM CPU build failure (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26401">#26401</a>)</li>
<li>Add EAGLE-3 Speculative Decoding Support for Qwen3 MoE (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26485">#26485</a>)</li>
<li>[CPU] fix the issue when the node is '-' cause json decode error. (<a class="user-mention notranslate" href="https://github.com/muzian666">@muzian666</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26562">#26562</a>)</li>
<li>[Refactor]Reduce duplicate code in serving_chat (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26627">#26627</a>)</li>
<li>[compile] Add patched_fused_scaled_matmul_reduce_scatter (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26604">#26604</a>)</li>
<li>[Bugfix][Qwen3VL] fix deepstack in qwen3vl (<a class="user-mention notranslate" href="https://github.com/JJJYmmm">@JJJYmmm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26626">#26626</a>)</li>
<li>[Bugfix] Fix qwen-moe packed_modules_mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26634">#26634</a>)</li>
<li>[Benchmark] Support Infinity API (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26641">#26641</a>)</li>
<li>CP: make correct_attn_out robust to 4‚ÄëD views and fix Triton arg binding (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26509">#26509</a>)</li>
<li>[compile] Fix inductor partition config (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26645">#26645</a>)</li>
<li>[EPLB] Support ernie4.5-moe (<a class="user-mention notranslate" href="https://github.com/HsChen-sys">@HsChen-sys</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22100">#22100</a>)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> to codeowner for pooling models (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26652">#26652</a>)</li>
<li>[PERF] [Qwen3-next] Speed up gated RMSNorm (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26207">#26207</a>)</li>
<li>[MISC] Rename the torch profiler filename as instance_id+rank_id for merging the Profiler results of each Rank (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25867">#25867</a>)</li>
<li>[Bugfix][CI/Build] Fix failing Mteb CI (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26638">#26638</a>)</li>
<li>[Bugfix][DCP] Set default CUDAGraphMode to PIECEWISE for DCP (<a class="user-mention notranslate" href="https://github.com/FENP">@FENP</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26574">#26574</a>)</li>
<li>[TEST][BUG FIX] Fix DP GPU_ID issue (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26442">#26442</a>)</li>
<li>Update <code>Optional[x]</code> -&gt; <code>x | None</code> and <code>Union[x, y]</code> to <code>x | y</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26633">#26633</a>)</li>
<li>[Feature] Add support for naver/splade-v3 (BERT-based sparse embedding model) (<a class="user-mention notranslate" href="https://github.com/gjgjos">@gjgjos</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26339">#26339</a>)</li>
<li>[Models][Qwen3VL] Speedup <code>fast_pos_embed_interpolate</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26647">#26647</a>)</li>
<li>[easy] fix pre commit error on trunk (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26665">#26665</a>)</li>
<li>[CI/Build] Add tool to build vllm-tpu wheel (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19165">#19165</a>)</li>
<li>[Misc] cache result of disable_inplace (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26666">#26666</a>)</li>
<li>[Bugfix][Core]Fix block table out-of-range issue in priority scheduling (<a class="user-mention notranslate" href="https://github.com/quanliu1991">@quanliu1991</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26661">#26661</a>)</li>
<li>[FIX] Throwing an exception when the model does not support pool tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25840">#25840</a>) (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25855">#25855</a>)</li>
<li>docs: wrong command in structured_outputs README (<a class="user-mention notranslate" href="https://github.com/yihong0618">@yihong0618</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26677">#26677</a>)</li>
<li>[Model] Fix  Skywork R1V mlp (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26673">#26673</a>)</li>
<li>[Model] Add reasoning_parser and tool_parser for Ernie45 thinking (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25027">#25027</a>)</li>
<li>Ignore large reformatting PRs in <code>git blame</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26690">#26690</a>)</li>
<li>[Model][0/N] Improve all pooling task | clean up (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25817">#25817</a>)</li>
<li>[ResponseAPI] Simplify input/output message serialization (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26620">#26620</a>)</li>
<li>[Bugfix] Fix out of bound index issue for Jina-embedding-v3 RoPE with cuda graph (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26687">#26687</a>)</li>
<li>[unrevert] Add batch invariant kernel override for FlashInfer backend [2/n] (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26373">#26373</a>)</li>
<li>[Hardware][CPU] Disable torch.compile for RISC-V to prevent APIError (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26693">#26693</a>)</li>
<li>[FEATURE]: Use pydantic validation in <code>multimodal.py</code> config (<a class="user-mention notranslate" href="https://github.com/andycandy">@andycandy</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26629">#26629</a>)</li>
<li>[UX] Speedup DeepGEMM warmup with heuristics (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25619">#25619</a>)</li>
<li>[P/D] [NixlConnector] kv load recovery integration (<a class="user-mention notranslate" href="https://github.com/wseaton">@wseaton</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26171">#26171</a>)</li>
<li>[Misc] Separate prompt logging to debug (<a class="user-mention notranslate" href="https://github.com/aitsvet">@aitsvet</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26713">#26713</a>)</li>
<li>[CI/Build] upgrade compressed-tensors to 0.12.2 to address LGPLv3 (<a class="user-mention notranslate" href="https://github.com/csy1204">@csy1204</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26501">#26501</a>)</li>
<li>[Bugfix][Rocm] fix qr error when different inp shape (@haoyangli-amd <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25892">#25892</a>)</li>
<li>[Bugfix][Speculative Decoding] Extend Eagle quantization config fix to llama_eagle.py (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26590">#26590</a>)</li>
<li>[Model] Use merge_by_field_config for MM models (M-N) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26710">#26710</a>)</li>
<li>[Log] Optimize Startup Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26601">#26601</a>)</li>
<li>[CI][Release][Arm64]: Build arm64 release for gpu arch 8.9 (<a class="user-mention notranslate" href="https://github.com/cyb70289">@cyb70289</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26698">#26698</a>)</li>
<li>[Quantization] [Performance] Enable Marlin GEMM kernels for the calibration-free RTN-based quantization (<a class="user-mention notranslate" href="https://github.com/sakogan">@sakogan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26051">#26051</a>)</li>
<li>[Frontend][1/N] Improve all pooling task | Support FP16 Embedding Base64 (Still uses fp32 by default). (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26414">#26414</a>)</li>
<li>[CI] Fix mypy for <code>vllm/distributed</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26593">#26593</a>)</li>
<li>[CI Perf]Prune Tests in kernel/mamba (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26538">#26538</a>)</li>
<li>[Bug] Fix Assertion error DeepEP/csrc/kernels/intranode.cu:928: 'false and Unsupported type' (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26532">#26532</a>)</li>
<li>[FrontEnd] UNREVERT CompilationConfig overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): deprecate use_inductor in favor of backend, simplify custom_ops  (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26502">#26502</a>)</li>
<li>Pruning kernel Core Tests (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26727">#26727</a>)</li>
<li>[ResponseAPI] Further polish message serialization and unit tests (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26728">#26728</a>)</li>
<li>Add tests for chunked prefill and prefix cache with causal pooling models (<a class="user-mention notranslate" href="https://github.com/maxdebayser">@maxdebayser</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26526">#26526</a>)</li>
<li>[Misc][DP] support customized aggregated logger for dp (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24354">#24354</a>)</li>
<li>[UX] Replace VLLM_ALL2ALL_BACKEND with --all2all-backend (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26732">#26732</a>)</li>
<li>[compile] Enable sequence parallelism for full cuda graph without specifying compile sizes (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26681">#26681</a>)</li>
<li>[Easy] Fix env type check errors from VLLM_DEBUG_LOG_API_SERVER_RESPONSE (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26742">#26742</a>)</li>
<li>[build][torch.compile] upgrade depyf version (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26702">#26702</a>)</li>
<li>[torch.compile] Unwrap fused_marlin_moe custom op (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26739">#26739</a>)</li>
<li>[Feature][Quantization] auto_round format add support for regex (<a class="user-mention notranslate" href="https://github.com/n1ck-guo">@n1ck-guo</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24024">#24024</a>)</li>
<li>Add support for the /rerank endpoint in vllm bench serve (<a class="user-mention notranslate" href="https://github.com/maxdebayser">@maxdebayser</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26602">#26602</a>)</li>
<li>[Docs] Add a start tag to build.inc.md (<a class="user-mention notranslate" href="https://github.com/windsonsea">@windsonsea</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26747">#26747</a>)</li>
<li>Fix lora tests failure in TPU CI due to the removal of LoRA bias (<a class="user-mention notranslate" href="https://github.com/vanbasten23">@vanbasten23</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26723">#26723</a>)</li>
<li>[CI] [ROCm] Automate CC list for ROCm related issue (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26753">#26753</a>)</li>
<li>Adding the test-amd.yaml for test definitions for the AMD backend. (alternative PR) (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26718">#26718</a>)</li>
<li>scheduler.py: Update the name of the default scheduler. (<a class="user-mention notranslate" href="https://github.com/ryanli">@ryanli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26758">#26758</a>)</li>
<li>[Model][Bugfix]fix ernie45 load failed due to ernie45 eplb code (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26684">#26684</a>)</li>
<li>[CI/Build] Use 127.0.0.1 instead of localhost in utils (<a class="user-mention notranslate" href="https://github.com/yeqcharlotte">@yeqcharlotte</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26750">#26750</a>)</li>
<li>fix(frontend): always include usage, when configured to do so (<a class="user-mention notranslate" href="https://github.com/max-wittig">@max-wittig</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20983">#20983</a>)</li>
<li>[Plugin] Make plugin group clear (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26757">#26757</a>)</li>
<li>[Bugfix] Standardize merging multimodal embeddings (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26771">#26771</a>)</li>
<li>[Model] Use merge_by_field_config for MM models (O-P) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26776">#26776</a>)</li>
<li>[NIXL][HeteroTP]Enable KV transfer from HND prefill to NHD decode (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26556">#26556</a>)</li>
<li>[Chore] Use <code>max_transformers_version</code> for Qwen-VL test (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26792">#26792</a>)</li>
<li>Don't allow <code>typos</code> to fix by default (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26785">#26785</a>)</li>
<li>[Doc] ruff format some Python examples (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26767">#26767</a>)</li>
<li>[CI] Fix test_tool_id_kimi_k2 (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26787">#26787</a>)</li>
<li>[Chore] Remove <code>SupportsV0Only</code> interface and update supported models docs (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26783)</li>
<li>[Feature] Change vllm.py with pydantic validation (<a class="user-mention notranslate" href="https://github.com/VladOS95-cyber">@VladOS95-cyber</a> #26726)</li>
<li>[CI/Build] Cleanup LoRA test (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #26752)</li>
<li>[DCP] Support Decode Context Parallel (DCP) for GQA with FlashAttention (<a class="user-mention notranslate" href="https://github.com/FENP">@FENP</a> #24864)</li>
<li>Adjusted the model order of the model registration file (@princepride #26798)</li>
<li>use combo kernel to fuse qk-norm and qk-rope (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26682)</li>
<li>[issues template] Encourage the author implement their own ideas (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #26671)</li>
<li>[KVConnector][Metrics] Aggregate scheduler-side KVConnectorStats (<a class="user-mention notranslate" href="https://github.com/QierLi">@QierLi</a> #26046)</li>
<li>[Feature][Responses API] Stream Function Call - harmony (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #24317)</li>
<li>Revert "[issues template] Encourage the author implement their own ideas" (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #26814)</li>
<li>[Config] Remove Unused Environment Variable <code>VLLM_DISABLE_PAD_FOR_CUDAGRAPH</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26743)</li>
<li>Update coveragerc and add codecov.yml for path fixes (<a class="user-mention notranslate" href="https://github.com/rzabarazesh">@rzabarazesh</a> #26435)</li>
<li>[CI] Raise VLLM_MAX_SIZE_MB to 500 due to failing Build wheel - CUDA 12.9 (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26722)</li>
<li>[Kernel][MoE] Add MoE tunings for GLM 4.6-FP8 and GLM 4.5 Air on NVidia B200 (@zklapow #26818)</li>
<li>[CI Failure] Fix tests with missing TinyLlama-1.1B-Chat-v1.0-FP8-e2e (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26816)</li>
<li>llama4_vision_rope: add HIP override to accept (q, k) and avoid (positions, q, k) mismatch (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #26790)</li>
<li>[Attention][Spec Decode] FlashMLA spec decode support (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #26541)</li>
<li>[Core] Reuse empty block lists whenever possible in KVCacheBlocks to mitigate GC costs (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #24964)</li>
<li>Notice for deprecation of AutoAWQ (@HDCharles #26820)</li>
<li>[Perf] Cache vllm.env.<strong>getattr</strong> result to avoid recomputation (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26146)</li>
<li>Added MoE configs for llama 4, H200 device with tp=4/8 tuning (@Dhruvilbhatt #26837)</li>
<li>fix: response_format for completion (@Nan2018 #23212)</li>
<li>[Minor] Group async_scheduling related fields in model runner init (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26736)</li>
<li>remove attn output view kernel (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26680)</li>
<li>[Core] Streamline some structured output related code (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26737)</li>
<li>[CI Failure] Fix torchao dep failure for Quantization Test (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26824)</li>
<li>[frontend][gptoss] Add per turn stats into Harmony Context (@lacora #25061)</li>
<li>[WideEP][P/D] Add usage stats for DP+EP and KV Connector (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #26836)</li>
<li>[torch.compile] Fix tests for torch==2.9 inductor partition (@ProExpertProg #26116)</li>
<li>[Core][Easy] Use envs.<strong>getattr</strong> for all Unify to environment variable access (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26810)</li>
<li>[Bugfix]fix Qwen3 xml tool parser (@Zhikaiiii #26345)</li>
<li>[BUGFIX][NIXL] quick fix for 'assert self.connector_worker is not None' in get_kv_connector_stats (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #26851)</li>
<li>Disable FlashInfer sampler by default (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26859)</li>
<li>[Frontend][torch.compile] CompilationConfig Overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): name change  compilation level to compilation mode, deprecation compilation level (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> #26355)</li>
<li>[Bugfix] Fixes prefix-repetition benchmark script (@kouroshHakha #26828)</li>
<li>[Model] Add DeepSeek-V3.1 reasoning parser (split from PR #24972) (@taohui #25589)</li>
<li>[Docs] Move build.inc into arm.inc (<a class="user-mention notranslate" href="https://github.com/windsonsea">@windsonsea</a> #26862)</li>
<li>[CI/Build][Bugfix] fix qutlass cmake error when set QUTLASS_SRC_DIR (@izhuhaoran #26773)</li>
<li>[Feature] default --extra-body param to disable thinking in vllm bench serve (@lengrongfu #26784)</li>
<li>[BugFix] Patch inductor partitioning logic (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #26735)</li>
<li>[Bugfix] Fix qwen3-omni audio truncation issue (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #26815)</li>
<li>[Graph Partition] pass tests for decorator (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26831)</li>
<li>[Bugfix][Multi Modal] Fix incorrect Molmo token processing (<a class="user-mention notranslate" href="https://github.com/sangho-vision">@sangho-vision</a> #26873)</li>
<li>[DSA][MLA] Tiny refactor on DeepSeek to make it reusable for different backends (@MengqingCao #26656)</li>
<li>[Misc] Use helper function to generate dummy messages in OpenAI MM tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26875)</li>
<li>[bugfix] Lazy import cv2 (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #26869)</li>
<li>[Deepseek-V3.2][Kernel] Integrate cuda indexer k cache gather (<a class="user-mention notranslate" href="https://github.com/zyongye">@zyongye</a> #26456)</li>
<li>[CI/Build] Add Qwen2.5-VL-7B-Instruct ChartQA Accuracy Tests in CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #21810)</li>
<li>[CI] Fix mypy for <code>vllm/executor</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26845)</li>
<li>[Doc] ruff format remaining Python examples (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26795)</li>
<li>[doc] add Context Parallel Deployment doc (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> #26877)</li>
<li>[Misc] Update TritonLanguagePlaceholder to have attributes that are used by Flash Linear Attention ops. (@madongfly #26853)</li>
<li>[Fix] Remove divisibility requirement between num_kv_heads and tp_size in bailing_moe (@ant-yy #26876)</li>
<li>[Easy] Get rid of unnecessary paraenthesis in kv_cache_manager (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26842)</li>
<li>[Platform] allow platform to init dp group (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #22243)</li>
<li>[Lora]Load tuned multi-lora kernel configs from json files (@li2haipeng #26319)</li>
<li>[Model][2/N] Improve all pooling task | Support multi-vector retrieval (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #25370)</li>
<li>[Misc] Remove <code>isort</code> and <code>yapf</code> ignores (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26888)</li>
<li>[Misc] rename torch_dtype to dtype (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #26695)</li>
<li>chore: remove unused marker (<a class="user-mention notranslate" href="https://github.com/max-wittig">@max-wittig</a> #26890)</li>
<li>[BugFix] Patch inductor memory plan logic (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26878)</li>
<li>[Chore] Separate out <code>vllm.utils.func</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26904)</li>
<li>[Chore] Separate out <code>vllm.utils.async_utils</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26913)</li>
<li>Lower severity of log when model info cache misses due to exception (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #26917)</li>
<li>Olmo 3 tool parser and tests (@pdasigi #26143)</li>
<li>[Feature]: Use pydantic validation in observability.py config (@cern1710 #26637)</li>
<li>[ModelOpt] Remove NVFP4 MoE K%16==0 constraint (@XiaobingSuper #26891)</li>
<li>[Chore] Clean up CODEOWNERS (@WoosukKwon #26923)</li>
<li>[NVIDIA] Add support for cudnn fp4 gemm via flashinfer (@kaixih #26107)</li>
<li>Vectorize RMS norm variance using vectorize_read_with_alignment (@bbeckca #26234)</li>
<li>support flashinfer_fp4 moe for 5090 gpu (@XiaobingSuper #26669)</li>
<li>[Bug] Temporally Disable <code>VLLM_ALLREDUCE_USE_SYMM_MEM</code> by Default (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26925)</li>
<li>Move query quantization to attention layer for Flashinfer &amp; Triton. (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> #26534)</li>
<li>Adjusting AMD test composition 2025-10-14 (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #26852)</li>
<li>[Qwen3-Next] Add tuned MoE config for Qwen3-Next FP8 on H100 tp2 (@felixzhu555 #26887)</li>
<li>[Bugfix] reasoning_parser parameter handling in run_batch.py (@inc-jeong #26225)</li>
<li>[ROCm][FEAT] Fuse DeepSeek shared experts into AITER fused_moe ops (@kliuae #24097)</li>
<li>[CI] Enable Blackwell Llama4 MoE tests (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26731)</li>
<li>[BUG] Allow runai_streamer_sharded in config check (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> #26958)</li>
<li>[bugfix] Fix SP + PP without specifying compile size (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #26955)</li>
<li>[BugFix] Work around graph partition x torch.compile cache issue (@zou3519 #26956)</li>
<li>[DOC][XPU]update feature parity with Intel GPU (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #26954)</li>
<li>[Chore] Rename <code>utils</code> submodules (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26920)</li>
<li>[PERF] Qwen3-next MTP speedup (change bool mask indexing to index_select / index_copy to reduce d2h) (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #26437)</li>
<li>Deepseek-v3 Batch Invariant on 8xH100 (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #26609)</li>
<li>[CI/Build] Update expected beam search output for Phi3V (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26978)</li>
<li>[Hardware][CPU][PowerPC]Disable torch.compile() in toptopk sampling (@Akashcodes732 #26987)</li>
<li>[CI/Build] Fix AMD import failures in CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #26841)</li>
<li>[Benchmark] Use truncation by default for pooling benchmarks (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26992)</li>
<li>[Chore] Separate out <code>vllm.utils.collections</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26990)</li>
<li>[Model][Bugfix] fix ernie45 vl run failed from shared experts optimization (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> #26885)</li>
<li>Cleanup code after Python 3.10 upgrade (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #26520)</li>
<li>[MISC] fix import violations for re and triton modules (@llsj14 #26654)</li>
<li>[Bugfix] Correct LayerNorm epsilon parameter in modernbert.py (@bogdanminko #27008)</li>
<li>[Benchmark] Show E2EL by default for pooling models (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27014)</li>
<li>[Attention] Tune CUTLASS MLA num_splits (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #26846)</li>
<li>[NIXL] Improve request_finished() debug logs (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #25665)</li>
<li>[docs] standardize Hugging Face env var to <code>HF_TOKEN</code> (deprecates <code>HUGGING_FACE_HUB_TOKEN</code>) (@yankay #27020)</li>
<li>[CI] Replace large models with tiny alternatives in tests (@tahsintunan #24057)</li>
<li>[Feature] Add process_weights_after_loading to AttentionImpl (@lengrongfu #26870)</li>
<li>[Model] Fix Qwen3VL mm mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27027)</li>
<li>Fix Qwen2.5 VL image grid docstring (@skyloevil #27033)</li>
<li>Support <code>set</code> in the CLI generation (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27031)</li>
<li>[gpt-oss][1/N] EZ: refactor serving_responses for modularity (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #26948)</li>
<li>Support block size of 256 used by Intel HPU (@mandy-li #26883)</li>
<li>[Compressed Tensors] Always clone output for compile robustness (@kylesayrs #26849)</li>
<li>Adding Warmup to Benchmark Serving (@kimbochen #26943)</li>
<li>[Bug] Fix batch invariant test <code>has</code> to <code>is</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27032)</li>
<li>[GPTOSS][DP/EP][Marlin] Enable GPTOSS Batched DP/EP using Marlin kernels (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #25997)</li>
<li>[Feature] Migrate DeepGEMM API from <code>get_m_alignment_for_contiguous_layout</code> to <code>get_mk_alignment_for_contiguous_layout</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26935)</li>
<li>[CI] Prune Quantization Tests and skip compilation (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #27038)</li>
<li>[Bug] Add Assertion for <code>random-input-len</code> / <code>random-output-len</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26834)</li>
<li>[small][batch invariance] Rename the env and internal flags to simplify usage (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #26855)</li>
<li>Refactor Transformers backend to use mixins (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #26906)</li>
<li>[NVIDIA] [Perf] Update to leverage flashinfer trtllm FP4 MOE throughput kernel (@jiahanc #26714)</li>
<li>[torch.compile] Passing only necessary compilation config to inductor pass config (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #27041)</li>
<li>[Chore] Separate out <code>vllm.utils.import_utils</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27022)</li>
<li>[torch.compile] fix simple inductor graph partition test (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27050)</li>
<li>Remove unused imports (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #26972)</li>
<li>vllm bench serve shows num of failed requests (@tomasruizt #26478)</li>
<li>[Docs] Reduce custom syntax used in docs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27009)</li>
<li>[Perf] Exploit out-of-band buffers in shm_broadcast (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26961)</li>
<li>disable graph partition in custom op (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26952)</li>
<li>[Bugfix][Qwen] fixes the weights dtype in qwen3_next: it is actually a bfloat16 (@sighingnow #27030)</li>
<li>[Core] Change <code>execute_model_with_error_logging()</code> to be a ctx manager (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27060)</li>
<li>[Bugfix] Fix ReplicatedLinearWithLoRA  (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27065)</li>
<li>[Kernel] Lazy import FlashInfer (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #26977)</li>
<li>[CI/Build] Update Llama4 eval yaml (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27070)</li>
<li>[Model] Always use Transformers backend for PaliGemma and Gemma3-MM (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26715)</li>
<li>[Model] Add support for LightOnOCR (@staghado #26916)</li>
<li>[CI/Build] Update compressed tensor test path to fix CPU CI (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27068)</li>
<li>[Kernel][Performance] Fuse float cast and renormalize to topk softmax kernel  (@izhuhaoran #26717)</li>
<li>[CI] fix docs build failed (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27082)</li>
<li>Update troubleshooting.md and remind VLLM_TRACE_FUNCTION usage (@Prowindy #27069)</li>
<li>[VLM][Refactor] Remove useless func <code>get_input_positions</code> in <code>MRotaryEmbedding</code> (@MengqingCao #27088)</li>
<li>[Docs] Replace all explicit anchors with real links (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27087)</li>
<li>[Docs] Replace <code>rst</code> style double-backtick with <code>md</code> single-backtick (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27091)</li>
<li>[Model]Improve Qwen3VLMoeForConditionalGeneration packed_modules_mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27096)</li>
<li>[Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI350 and MI355 (@rkarhila-amd #25586)</li>
<li>Fix incorrect docstring for stop_profile() method (@hyongtao-code #27101)</li>
<li>[torch.compile] Enable attention and allreduce fusion without custom ops enabled (@ProExpertProg #24604)</li>
<li>[CI] Nixl integration tests (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27010)</li>
<li>[Data-parallel] Allow DP&gt;1 for world_size &gt; num_gpus on node (8) (@patrickvonplaten #26367)</li>
<li>[bugfix] Qwen3-VL fix video incorrect timestamp calculations while do_sample_frames=True (@wulipc #27104)</li>
<li>[CI] Remove forbidden slash (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27112)</li>
<li>[ROCM] MoE fp4 CK kernel (@maleksan85 #26545)</li>
<li>[ROCm][Bugfix][Model] Fix illegal memory access when running qwen3_moe models with  rms_norm (Qwen3-235B-A22B,  Qwen3-30B-A3B, etc.) (@rasmith #26192)</li>
<li>[Bugfix] [AITER] [ROCm] Fix Quark MoE Quant Config and AITER Fused MoE quant type logic (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #27029)</li>
<li>[Chore] Remove unused <code>PolyNorm</code> layer (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27110)</li>
<li>[Bugfix] Use PIECEWISE cudagraphs on Blackwell if max_model_len &gt; 131072 (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #27114)</li>
<li>[Minor] Remove unnecessary error message (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #27115)</li>
<li>[V1][Spec Decode] Fix greedy temperature detection after sampler refactor (@Pradyun92 #27077)</li>
<li>[Test] Make <code>test_failure</code> more stable for batch invariance (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27054)</li>
<li>[BugFix][Core] Fix error when enable async-scheduling in multi-node env (<a class="user-mention notranslate" href="https://github.com/lhtin">@lhtin</a> #25887)</li>
<li>[Perf]  Add H100 fused MoE config (@skyloevil #25398)</li>
<li>[CI/Build] tests(v1): feed Triton attention the (num_blocks, 2, ‚Ä¶) KV cache layout in backend-correctness tests (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #26663)</li>
<li>[GPT-OSS] Structure_Tag support for gpt-oss tool-call in cot (@Hanchenli #25515)</li>
<li>[Misc] Rev DeepEP (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27122)</li>
<li>[DOC][FEATURES][CPU]update cpu feature for v1 (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #27135)</li>
<li>[Test] Add test for /health endpoint on engine failure (@dongbo910220 #26074)</li>
<li>[Chore] Separate out <code>vllm.utils.mem_utils</code> (@iAmir97 #27143)</li>
<li>[Feature] Batch Invariant: Support DeepGEMM and Blackwell (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27127)</li>
<li>[fix][cpu] fix prefill attention in CPU attention backend (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27035)</li>
<li>[Misc] Refactor <code>get_kv_cache_spec</code> into <code>AttentionLayerBase</code> (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26587)</li>
<li>[Models][QwenVL] Remove unnecessary <code>.contiguous()</code> calls (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #27106)</li>
<li>[Chore] Clean up pytorch helper functions in <code>vllm.utils</code> (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #26908)</li>
<li>Fix incorrect string formatting in barrier timeout exceptions (@hyongtao-code #27149)</li>
<li>[Minor] Add some clarifying comments to recent changes (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27130)</li>
<li>[BugFix] Fix failing gemma-3-1b-it test: <code>test_lm_eval_accuracy_v1_engine[google/gemma-3-1b-it]</code> (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27111)</li>
<li>[Chore] Separate out profiling utilities from vllm.utils (@dongbo910220 #27150)</li>
<li>[BugFix] fix graph partition signature (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27139)</li>
<li>[BugFix] Disable fp8 kv-cache by default for DeepSeek V3.2 (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27121)</li>
<li>[V1][Metrics][Plugin] Add plugin support for custom <code>StatLoggerBase</code> implementations (@ptovam #22456)</li>
<li>[Minor] Remove unused env variable (@WoosukKwon #27161)</li>
<li>[BugFix] Fix lazy imports involving outlines_core (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27158)</li>
<li>[Chore] Separate out hashing utilities from vllm.utils (@dongbo910220 #27151)</li>
<li>[Benchmark] Convenience script for multiple parameter combinations (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27085)</li>
<li>output type conversion fix (@jianyuh #27159)</li>
<li>[Chore] Separate out <code>vllm.utils.network_utils</code> (@iAmir97 #27164)</li>
<li>[Misc] Move utils to avoid conflicts with stdlib, and move tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27169)</li>
<li>[Bugfix] Fix error with penalties when speculative decoding and structural output are enabled (<a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> #26586)</li>
<li>Fix typo in ValueError message: use <code>kv_role</code> instead of <code>kv_disagg_role</code> (@hyongtao-code #27166)</li>
<li>[Model][VLM] Support Bee-8B Model (@uyzhang #27012)</li>
<li>[LoRA] LoRA cuda graph specialization (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #25914)</li>
<li>[Kernel] Accelerate solve_tril with TMA (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #26746)</li>
<li>AArch64 CPU Docker pipeline #26931)</li>
<li>Nemotron Nano V2 VL + EVS Video Support (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> #27107)</li>
<li>[Kernel][Model] Tune fused_moe Triton configs for Qwen3-30B A3/A3B on H100 (FP8/BF16) (@shivampr #26268)</li>
<li>[Bugfix][CI] Fix <code>Distributed Tests (4 GPUs)</code> async_sched+ray test (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27195)</li>
<li>[Feature][Quantization] auto_round support for mixed bits quantization (<a class="user-mention notranslate" href="https://github.com/n1ck-guo">@n1ck-guo</a> #23812)</li>
<li>[ROCm] enable some tests in entrypoints test groups on AMD (@Concurrensee #26725)</li>
<li>[ez] add uv lock to gitignore (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #27212)</li>
<li>[Quantization] Automatically infer AWQ <code>modules_to_not_convert</code> field (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #26909)</li>
<li>[V0 Deprecation] Remove V0 metrics code (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27215)</li>
<li>[cpu] Dispatch un-quantized linear to oneDNN/ACL by default for AArch64 (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27183)</li>
<li>create is_in_the_same_node on cpu (@helunwencser #26832)</li>
<li>[Frontend] Enforce tokenize=False when applying chat template (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> #27205)</li>
<li>[Feature][Kernel]FusedMoE LoRA (@wcwuwc #21229)</li>
<li>[BugFix] GPT-OSS Attention DP + MoE TP weight loading issue (@nvpohanh #24032)</li>
<li>[ModelOpt] Load w13/w2_input_scale for all experts, nvfp4 (@wenscarl #26135)</li>
<li>[Bugfix] Fix gpt-oss w4a8 DP/EP on B200 (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #26729)</li>
<li>[Bugfix] Fix broken MTP weight loading for FP8 KV Scales (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #27227)</li>
<li>[Fix][Spec Decode] Fix llama4 draft loading with different quantization (@linzebing #27136)</li>
<li>[Nixl] Minor refactor to handshake related metadata (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26410)</li>
<li>[MM][Core] Decouple ViT backend from LM backend (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #27061)</li>
<li>[Deepseek v3.2] Optimize top_k_per_row (<a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> #26763)</li>
<li>[Chore] Separate out NCCL utilities from vllm.utils (@dongbo910220 #27197)</li>
<li>[CI] Install pre-release version of <code>apache-tvm-ffi</code> for <code>flashinfer</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27262)</li>
<li>[ROCM] Enable CompressedTensorsWNA16 (@JartX #27187)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> to .github/codeowners (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> #27213)</li>
<li>[ROCm] Update Triton, Torch, and AITER branches for ROCm base Dockerfile (@micah-wil #27206)</li>
<li>[Feature] Batch Invariant for R1 TP 8 on Blackwell (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27229)</li>
<li>[Bugfix][P/D] Reduce num_threads used by nixl ucx backend (@dagrayvid #27196)</li>
<li>[V0 Deprecation] Remove V0 executors (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27142)</li>
<li>[Bugfix] fixes the decoding metadata of dense mla's fp8 kvcache. (@sighingnow #27144)</li>
<li>Update PyTorch to 2.9.0+cu129 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #24994)</li>
<li>[Performance] Dual stream execution of "shared_experts" and "selected_experts" inside FusedMoE (@alexm-redhat #26440)</li>
<li>Updated xgrammar backend to not deny supported string formats (@ExtReMLapin #27253)</li>
<li>[Bugfix] skip cuda graph for drafter when running with eager (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #26821)</li>
<li>[P/D] KVConnector for decode benchmarking (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #25986)</li>
<li>[Deepseek v3.2] Remove extra logics in indexer (@IwakuraRein #26465)</li>
<li>[DOC] [ROCm] Add ROCm quickstart guide (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #26505)</li>
<li>[CI] Nixl integration tests DP-EP (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27199)</li>
<li>[Benchmark] Add plot utility for parameter sweep (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27168)</li>
<li>[torch.compile] Enable silu_mul_fp8_quant fusion without custom ops enabled (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27146)</li>
<li>[1/N][Platform] Cleanup useless function (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #26982)</li>
<li>Update release pipeline for PyTorch 2.9.0 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #27303)</li>
<li>Remove last <code>level</code> references not removed #26355 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27260)</li>
<li>fixed reasoning streaming with tool_choice="required" (@ExtReMLapin #24108)</li>
<li>[Frontend][3/N] Improve all pooling task | Support binary embedding response (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #27066)</li>
<li>[Bugfix][CPU] Disable dual stream execution for experts on CPU (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27320)</li>
<li>[Bug] Raise error for <code>LLM(data_parallel_size=k)</code> single-process DP Usage (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27282)</li>
<li>Bugfix - pass 'max_num_tokens_padded' into 'moe_lora_align_block_size' (@gnovack #27311)</li>
<li>[Core] Handle MoE LoRA edge cases (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27335)</li>
<li>[docs] Update v1 metrics design doc (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #27332)</li>
<li>Mirroring changes in test-pipeline.yaml into test-amd.yaml (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #27242)</li>
<li>[Chore] Separate out optional dependency checks from vllm.utils (@dongbo910220 #27207)</li>
<li>[Model] Upstream Deepseek-OCR model (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27247)</li>
<li>[NIXL] Terminate handshake listener thread in shutdown (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #26404)</li>
<li>[Bug] Fix DeepSeek-V2.5-1210-FP8 issue (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27267)</li>
<li>[bugfix] remove unused parameters to reduce unnecessary vram usage (@ReinForce-II #26789)</li>
<li>[Bugfix] Add missing 'is_internal_router' attribute to FusedMoEWithLoRA (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27351)</li>
<li>[NIXL] use Host buffer to support TP_ratio &gt; 1 for XPU (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #27140)</li>
<li>[Bugfix] Make <code>get_mrope_input_positions</code> instance methods (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27342)</li>
<li>[Bugfix] Fix HF format InternVL large variants video processing (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27330)</li>
<li>[Frontend] Require flag for loading text and image embeds (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> #27204)</li>
<li>[P/D] Dynamic <code>kv_output_aggregator</code> collect size (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26734)</li>
<li>Support Anthropic API /v1/messages Endpoint (@LiuLi1998 #22627)</li>
<li>[Bugfix] Disable FlexAttention direct block mask building for encoder-only models (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27344)</li>
<li>[Model] Revert PR #26715: Restore custom PaliGemma and Gemma3-MM impl‚Ä¶ (<a class="user-mention notranslate" href="https://github.com/lucianommartins">@lucianommartins</a> #27309)</li>
<li>[Doc] Fix numbering sequence in prefix caching (@gigit0000 #27357)</li>
<li>[Prefix Cache] Use LoRA name for consistent KV-cache block hashing (@sagiahrac #27211)</li>
<li>[Feature] publisher default set zmq in kv_event config (@lengrongfu #26915)</li>
<li>[BugFix] bugfix for Flash Attention MLA with full cuda graph IMA following pr-25490 (@Daisy-Ma-coder #27128)</li>
<li>[Chore] Separate out system utilities from vllm.utils (@dongbo910220 #27201)</li>
<li>[MLA] Bump FlashMLA (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27354)</li>
<li>[Bugfix] Fix deepseek-ocr multi-image inference and add <code>merge_by_field_config=True</code> with tensor schema support (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27361)</li>
<li>[Bugfix] Fix SLA tuner initialization (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27355)</li>
<li>[Bugfix] Fix incorrect kv cache metrics in grafana.json (@fangpings #27133)</li>
<li>[Bugfix][Core] running queue index leakage exception (@CLFutureX #26754)</li>
<li>[CORE] Support Prefix Caching with Prompt Embeds (<a class="user-mention notranslate" href="https://github.com/qthequartermasterman">@qthequartermasterman</a> #27219)</li>
<li>[V1][spec decode] return logprobs for spec decoding (@TheEpicDolphin #26060)</li>
<li>[Model] Add num_cached_tokens for PoolingRequestOutput (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #27378)</li>
<li>[Chore] Remove duplicate <code>has_</code> functions in vllm.utils (@jonathanc-n #27372)</li>
<li>[CI/Build] Fix Prithvi plugin test (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27393)</li>
<li>[Bugfix] Fix args settings for guided decoding args (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #27375)</li>
<li>[CI/Build] Fix AMD CI: test_cpu_gpu.py (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27388)</li>
<li>add SLA information into comparison graph for vLLM Benchmark Suite (@louie-tsai #25525)</li>
<li>[CI] Reorganize entrypoints tests (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27403)</li>
<li>[Metrics] [KVConnector] Add connector prefix cache hit rate stats (@ptovam #26245)</li>
<li>[Model] Add MoE support for NemotronH (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> #25863)</li>
<li>Run mypy on the lowest supported Python version instead of system Python (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27048)</li>
<li>[Bugfix] Honor --mm_encoder_attn_backend when used (@bradleyhd #27124)</li>
<li>[Feature] Pydantic validation for speculative.py (@Navya1707 #27156)</li>
<li>[Misc] Remove use of CUDA_VISIBLE_DEVICES for device selection (fix DP slow startup time &amp;c) (@ilmarkov #26709)</li>
<li>[CI/Build] Remove unnecessary flags from test registry (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27353)</li>
<li>[Frontend][4/N] Improve all pooling task | Add plugin pooling task (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #26973)</li>
<li>Mirroring the test definitions (2025-10-22) (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #27362)</li>
<li>[Bugfix] Fix dp_chunking enablement logic in FusedMoE layer (@alexm-redhat #27220)</li>
<li>[Bugfix][ROCm][DeepSeek] Fix for forward_hip in rope for DeepSeek (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #27373)</li>
<li>[Bugfix] Fix AWQ marlin layer skipping (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27416)</li>
<li>[Misc] Add triton_kernels dependency (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27370)</li>
<li>[Chore] Separate out <code>vllm.utils.platform_utils.py</code> (@jonathanc-n #27374)</li>
<li>[Attention] Fix FlashMLA metadata builder arguments for q_len &gt; 1 (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27368)</li>
<li>[Bugfix][DP] Fix creating too many DP Placement Groups (@kebe7jun #26880)</li>
<li>[Model] Siglip Embedding Support (@piood #27324)</li>
<li>[Hardware][POWERPC] Disable oneDNN path in vllm/model_executor/layers/utils.py for Powerpc (@Akashcodes732 #27422)</li>
<li>Granite 4.0 quark quantization support (@xiao-llm #26944)</li>
<li>Fix pooling adapters for Transformers backend  (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27338)</li>
<li>[Kernel] Add GPTQv2 format support for low-bit or asymmetric quantization, by adapting gptq_gemm (@xxxxyu #26092)</li>
<li>[Misc] Add TPU usage report when using tpu_inference. (@hfan #27423)</li>
<li>[Bugfix][CI] Move resolving cudagraph_mode before initializing attn_metadata_builder (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> #27427)</li>
<li>Fix EventPublisherFactory logic for disabled KV cache events (@usberkeley #27419)</li>
<li>[Chore] remove structural tags logging lines (<a class="user-mention notranslate" href="https://github.com/aarnphm">@aarnphm</a> #27451)</li>
<li>[Bugfix] Fix Pydantic union resolution for ResponseFunctionToolCall in Responses API (@strinczer #26706)</li>
<li>[Misc] Avoid "PyTorch non-writable tensors" warning in RayPPCommunicator (<a class="user-mention notranslate" href="https://github.com/ruisearch42">@ruisearch42</a> #27443)</li>
<li>[Docs] remove v1 column for embedding models (@piood #27446)</li>
<li>[MM][Bugfix] Replace <code>PatchEmbed</code>'s conv3d to linear layer (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27418)</li>
<li>[BugFix] Fix torchrun DP with LLM class (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27395)</li>
<li>[Refactor] move tool parsing logic from protocol.py to the tool parser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27383)</li>
<li>[Benchmark] Enable benchmark to run with <code>encoding_format="bytes"</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27467)</li>
<li>Fix AArch64 CPU Docker pipeline #27331)</li>
<li>[MISC] <code>cudagraph_capture_sizes</code>  related improvements (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> #26016)</li>
<li>Fix test named tool use (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27458)</li>
<li>[Doc] Fix minor issues in docs/design/metrics.md (@draftbk #27436)</li>
<li>[cpu][fix] Fix onednn_mm crash on consecutive matmuls with same M,K,N and different dtype (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27472)</li>
<li>[compile] Turn standalone_compile back on (@zou3519 #27460)</li>
<li>[NIXL][BUGFIX] delay done_recving queue cleanup to bottom of get_finished (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #27297)</li>
<li>[Bugfix] Fix MultiConnector stats reconstruction across process boundaries (@kouroshHakha #27366)</li>
<li>[Attention] Add MLA prefill backend: trtllm_ragged_attention_deepseek (<a class="user-mention notranslate" href="https://github.com/minosfuture">@minosfuture</a> #26397)</li>
<li>[Bugfix] Fix interns1-vit qk norm code path (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27480)</li>
<li>[CI/Build] Fix test_torch_utils in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27317)</li>
<li>[Document] Add ms-swift library to rlhf.md (@hjh0119 #27469)</li>
<li>[Perf][Async Scheduling] Remove CPU-&gt;GPU sync in dummy_run (<a class="user-mention notranslate" href="https://github.com/lhtin">@lhtin</a> #27455)</li>
<li>[Distributed] Basic set of configuration for large EP deployment on GB200 (@wpc #27328)</li>
<li>[Log] Optimize Startup Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26740)</li>
<li>[Misc][DP] Guard mxfp4 implementation selection (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27484)</li>
<li>[KVConnector] Migrate the LMCache integration code to be vLLM native (@ApostaC #25542)</li>
<li>[CI] Add tests for cudagraph (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27391)</li>
<li>Revert "[Misc] Remove use of CUDA_VISIBLE_DEVICES for device selectio‚Ä¶ (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #27502)</li>
<li>[Core][Hybrid allocator + kv connector 1/n] Enable hybrid allocator + KV cache connector (@KuntaiDu #25712)</li>
<li>[Misc] Simplify max tokens in multimodal registry (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27500)</li>
<li>[Attention] Add missing kv cache scale setup (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27490)</li>
<li>[CI/Build] Refactor processing tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27470)</li>
<li>[CI/Build] Use CPU for mm processing test on CI (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27522)</li>
<li>[BUGFIX][ROCM] ViT FlashAttention on ROCm (no GFX9) and contiguous on qwen3vl ROCm TORCH_SDPA (@JartX #27190)</li>
<li>[Bugfix] Fix processor initialization for model from modelscope instead of HF (@lengrongfu #27461)</li>
<li>[Bugfix] fix empty prompts for async-engine mode in benchmark throughput (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #27494)</li>
<li>[Doc] Remove Molmo warning (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27527)</li>
<li>[Doc] Fix links to GH projects (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27530)</li>
<li>[Chore]:Extract math and argparse utilities to separate modules (@yeshsurya #27188)</li>
<li>Revert "[CI/Build] Use CPU for mm processing test on CI (#27522)" (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27531)</li>
<li>[CI/Build] Update causal-conv1d installation (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27529)</li>
<li>[Model][MiniMax-M2] Support MiniMax-M2 Model (@rogeryoungh #27535)</li>
<li>fix m2 test (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> #27536)</li>
<li>Fix MiniMax-M2 copyright (@rogeryoungh #27537)</li>
<li>[Model][Bugfix] fix ernie45 moe 300B SharedFusedMoE output tuple (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> #27316)</li>
<li>[Model] Use merge_by_field_config for MM models (Qwen series) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27546)</li>
<li>[Docs] reemove the incorrect <code>enable_reasoning</code> parameter  (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> #27550)</li>
<li>[Performance][LoRA] add context varying params to 'do_not_specialize' in fused moe lora (@gnovack #27445)</li>
<li>[Model] Deprecate <code>merge_by_field_config=False</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27551)</li>
<li>[Doc] Slight improvement to M2 and beyond (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27554)</li>
<li>[Kernel] Adding split_K implementation for fused_moe_lora (@dcmaddix #27291)</li>
<li>[Misc] Clean up utils (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27552)</li>
<li>[Bugfix] Limit the default value of <code>max_model_len</code> when it is not specified by users (@shen-shanshan #27556)</li>
<li>[Bugfix] Fixed when return_token_ids=False, the first event still contains prompt_token_ids. (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27561)</li>
<li>[cpu][perf] Fix low CPU utilization with VLLM_CPU_OMP_THREADS_BIND on AArch64 (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27415)</li>
<li>[Kernel] Enable moe LoRA kernel support FP16 (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27468)</li>
<li>[Hybrid] Added supports_mamba_prefix_caching Protocol (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> #27339)</li>
<li>[Model] Siglip2 Model Support (@piood #27566)</li>
<li>[Bugfix][LoRA][FusedMoE] Select MxFP4 Backend based on LoRA Enablement (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27487)</li>
<li>fixing mm placeholder replacement issue with gemma3 (@tingtingtangmeta #27538)</li>
<li>[Chore]: Stream tokens vs characters in tool call parser tests (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> #26513)</li>
<li>[Misc] Clean up more utils (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27567)</li>
<li>[ROCm] Update AITER branch for ROCm base docker (@micah-wil #27586)</li>
<li>Code quality improvements: version update, type annotation enhancement, and enum usage simplification (@usberkeley #27581)</li>
<li>[gpt-oss][2/N] Support input_messages in responsesRequest (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #26962)</li>
<li>[Bugfix][CI] Fix config resolving logic with remote models (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #27610)</li>
<li>[Stability fix] turn off HMA allocator when connector is set (@KuntaiDu #27592)</li>
<li>[Bugfix] fixed inconsistent finish_reason handling between V0 and V1 engines (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27555)</li>
<li>[ROCm] [Doc] Update ROCm installation docs  (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #27327)</li>
<li>[Hardware][AMD][Model] Triton MoE tuning configs for GLM-4.6 for MI300X (@minatoaquaMK2 #27323)</li>
<li>[Bugfix][CPU] Fallback oneDNN linear to torch linear to fix half gemm support on legecy platforms (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27526)</li>
<li>[Core][Bookkeeping Optimization] Update against numpy view of is_token_ids tensor (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27618)</li>
<li>[CI/Build] Fix amd model executor test (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27612)</li>
<li>Fix a robust parsing issue in KimiK2ToolParser that causes IndexError (@wangln19 #27565)</li>
<li>[V0 Deprecation] Remove vestigial V0 logits_processors.py file (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27601)</li>
<li>[Bugfix] In LongRoPE, decide short vs long based on max_model_len (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27431)</li>
<li>[Misc] Separate out <code>utils.counter</code> and move <code>utils.Device</code> to engine (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27588)</li>
<li>[Bug] Fix shape issue for eplb expert weights (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27589)</li>
<li>[compile] Add enable_prompt_embeds to compile hash. (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> #27285)</li>
<li>[Hybrid] Add mamba_block_size to Engine Args (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> #27289)</li>
<li>[compile] Disable dynamo guards check for AOT compilation. (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> #27288)</li>
<li>fix: allow HuggingFace standard chat template params via **kwargs (@wangln19 #27622)</li>
<li>[Core] Enable async scheduling for external_launcher mode (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27394)</li>
<li>[Bugfix][Frontend] validate arg priority in frontend LLM class before add request (@junpuf #27596)</li>
<li>[BugFix] Also consider RAY_EXPERIMENTAL_NOSET_* when storing compilation cache (@HollowMan6 #27294)</li>
<li>[nit]: lmcache integration import (@sammshen #27600)</li>
<li>[FLA] Introduce Kimi Delta Attention(KDA) to VLLM (<a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> #27654)</li>
<li>[Bugfix] Fix allocation &amp; free logic of SingleWriterShmRingBuffer (@imkero #27117)</li>
<li>[Bugfix][CI] Fix v1 attention backend tests and add CI coverage (@mmangkad #26597)</li>
<li>[Misc] Make <code>LayerBlockType</code> a <code>Literal</code> instead of <code>Enum</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27658)</li>
<li>[compile] Add fallback path to AOT compile when serialization fails. (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> #27350)</li>
<li>Add load pattern configuration guide to benchmarks (@mpashkovskii #26886)</li>
<li>[Misc] Make reorder batch also separate extends (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27367)</li>
<li>[Test] Batch Invariant: Unit test using parameterized backend (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27478)</li>
<li>[Core] Scheduler: Publish connector events after output (<a class="user-mention notranslate" href="https://github.com/orozery">@orozery</a> #25875)</li>
<li>[AsyncScheduling] Make async overlap work with logprobs (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27615)</li>
<li>[Misc][qwen2_5_vl][torch.compile] Enable <code>supports_torch_compile</code> on generic nn.Module and demonstrate speedup on Qwen Vision model (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> #23207)</li>
<li>[Bug] Fix deepep low latency use nvlink by default (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27677)</li>
<li>[Core] Early return in SlidingWindowManager.remove_skipped_blocks (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27673)</li>
<li>Install pre-built xformers-0.0.32.post2 built with pt-2.9.0 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #27598)</li>
<li>Revert "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> #27714)</li>
<li>[Build] Revert triton_kernels requirements (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27659)</li>
<li>[NIXL][XPU] update name of nixl wheel (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> #27631)</li>
<li>[Model] Fix Qwen3VL and Qwen3Omni after torch.compile changes (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #27705)</li>
<li>[KV cache] Fix lmcache connector (@Shaoting-Feng #27681)</li>
<li>[CI/Build][Bugfix]Fix Quantized Models Test on AMD (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27712)</li>
<li>[Bugfix] Fix non-contiguous tensor error in <code>rocm_unquantized_gemm_impl</code> (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27605)</li>
<li>[Speculators] Move tests + fix integration (@dsikka #27308)</li>
<li>[CI/Build] Move pre-commit only scripts to <code>tools/pre_commit</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27657)</li>
<li>[perf] Enable concurrent execution of "shared_experts" and "selected_experts" in qwen3-next (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27578)</li>
<li>[Bugfix] Fix modular kernel tests (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> #27707)</li>
<li>[Frontend] [gpt-oss] Tool json call parsing error retry (@alecsolder #27675)</li>
<li>[Frontend] [gpt-oss] Mcp type bug (@alecsolder #27689)</li>
<li>[Fix] import get_kv_cache_torch_dtype error in vllm_v1_adapter.py (@KevinCheung2259 #27670)</li>
<li>[Misc] Raise error for missing video metadata in <code>MultiModalDataParser</code> (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27664)</li>
<li>Feature/video support in random mm dataset (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> #25963)</li>
<li>[chore] Remove models weight on S3 logic (@khluu #27725)</li>
<li>[VLM] Add Qwen3-VL generation test (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #25185)</li>
<li>[CI/Build] Skip cpu offloading test on AMD (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27690)</li>
<li>[Frontend] Add <code>vllm bench sweep</code> to CLI (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27639)</li>
<li>Fix MiniMax-M2 rmsnorm precision and remove useless code (@rogeryoungh #27627)</li>
<li>[ROCm][Platform] Add MI308X device id in _ROCM_DEVICE_ID_NAME_MAP (@sammysun0711 #27623)</li>
<li>[CI] Fix flaky <code>test_two_responses_with_same_prev_id</code> test  (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27745)</li>
<li>[Chore] Optimize P2PNCCLEngine <code>http_address</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27488)</li>
<li>[Core] Exposing engine sleep &amp; wake_up state as prometheus metrics (@dumb0002 #24176)</li>
<li>[FIXBUG] Qwen3VL hallucinations without Contiguous on Torch.SDPA (@JartX #27744)</li>
<li><code>use_aot_compile</code> should respect <code>VLLM_DISABLE_COMPILE_CACHE</code> (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27698)</li>
<li>[CI/Build] Test torchrun with 8 cards (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27548)</li>
<li>[Bug] Raise error explicitly if using incompatible backend (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27424)</li>
<li>[KVConnector] Add metrics to Prometheus-Grafana dashboard (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26811)</li>
<li>[Bug] Fix DeepEP low latency <code>assert self.batched_router_logits.size(-1) == full_router_logits.size(-1)</code> Bug (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27682)</li>
<li>[BugFix] Fix handling of resumed reqs in <code>SharedStorageConnector</code> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27719)</li>
<li>[Bug] Fix DBO IMA issue for DeepEPHT (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27666)</li>
<li>[Temp fix] Disable torch.compile for Qwen2.5 VL's VisionBlock temporarily.  (<a class="user-mention notranslate" href="https://github.com/huachenheli">@huachenheli</a> #27760)</li>
<li>[XPU][bugfix] fix rope for llama4 and deepseek (@yma11 #25145)</li>
<li>[Bugfix] mamba-block-size is set for vision language model (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #27773)</li>
<li>[XPU] Update latest IPEX 2.8 release (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #27735)</li>
<li>[BugFix] Handle unscheduled requests properly when async scheduling (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27756)</li>
<li>[Feat] Adds runai distributed streamer (@bbartels #27230)</li>
<li>kernels/moe test pruning (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> #27053)</li>
<li>[BugFix] Reordering extend logic fix (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27739)</li>
<li>[Benchmark] Cleanup deprecated nightly benchmark and adjust the docstring for performance benchmark (@KuntaiDu #25786)</li>
<li>Add more dims for batch invariant shims (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #27489)</li>
<li>use stringData in secret yaml to store huggingface token (@yitingdc #25685)</li>
<li>[CI/Build]Add eval config for Qwen3-235B-A22B-Instruct-2507-FP8 (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27113)</li>
<li>[BugFix][VL] Fix FA selection on Qwen2.5-VL (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27790)</li>
<li>[V0 deprecation] Remove VLLM_USE_V1 usage in config module (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27784)</li>
<li>[CI Failure] fix test_default_mm_loras (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27795)</li>
<li>[CI] Fix mypy for <code>vllm/v1/core</code> and <code>vllm/v1/engine</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27108)</li>
<li>[Bugfix] Improve GPU validation logging in Ray fallback scenarios (@sairampillai #25775)</li>
<li>[Frontend][Doc][5/N] Improve all pooling task | Polish encode (pooling) api &amp; Document. (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #25524)</li>
<li>[CI Failure] Fix test_kv_cache_model_load_and_run (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27717)</li>
<li>[Model] Introduce Kimi Linear to vLLM (<a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> #27809)</li>
<li>[KV offload] Enable CPU KV offload on CUDA alike Platforms (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27770)</li>
<li>[Model][Ouro] Support Ouro Model (@FlamingoPg #27794)</li>
<li>[Bugfix][CPU] Fix MRoPE dispatch on the CPU backend (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27800)</li>
<li>[BugFix] Stopgap - Flashinfer Autotuner + GPT-OSS + DP/TP (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27762)</li>
<li>[Misc] Replace CUDA_VISIBLE_DEVICES in DP with torch.cuda.set_device for device selection on cuda-like devices (@ilmarkov #27564)</li>
<li>[Docs] add Shanghai Meetup - 2025/10 (@kebe7jun #27545)</li>
<li>Reapply "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #27768)</li>
<li>[MTP] Refactor mtp predictor to avoid d2h operation (@MengqingCao #27643)</li>
<li>[Model] Use the same fused_moe configs for all H200 devices (@bufferoverflow #23642)</li>
<li>[Bugfix] Fix 2 precommit issues - (mamba_block_size, kv_cache_config) (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #27811)</li>
<li>[Core][Bookkeeping] Update cu_num_accepted_tokens for all req_index (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27629)</li>
<li>[EP/DP][API Server] Enable DP-aware routing in OpenAI API requests (@Prowindy #24945)</li>
<li>[Fix] Skip <code>record_sleep_state</code> logic in <code>PrometheusStatsLogger</code> if not in dev mode (@SumanthRH #27789)</li>
<li>[Refactor] Remove <code>VLLM_DEEPEP_LOW_LATENCY_ALLOW_NVLINK</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27750)</li>
<li>[Core][Perf] Only invoke save_new_computed_blocks when computed blocks are not empty (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27799)</li>
<li>[Feature] Batch invariant torch.compile (@PaulZhang12 #27660)</li>
<li>[BugFix] Fix broken import in initialize_ray_cluster() (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27838)</li>
<li>[Misc] Make all tool scripts executable (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27831)</li>
<li>[CI/Build][Intel] Enable performance benchmarks for Intel Gaudi 3 (@jakub-sochacki #26919)</li>
<li>[CI Test] Add Scheduled Integration Test (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27765)</li>
<li>[benchmark] Make request IDs unique across clients by default (@eicherseiji #27723)</li>
<li>[Hardware][Powerpc] Fix VLLM_CPU_OMP_THREADS_BIND="auto"  low CPU utilization for Power (@Akashcodes732 #27734)</li>
<li>[Kimi-Linear] Correct prefixes and add compatibility to AWQ quants (@toncao #27834)</li>
<li>[Bugfix] Avoid too small block m/n for FlexAttention kernel option (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27853)</li>
<li>[BugFix] Don‚Äôt compute reorder threshold when there are no attention groups (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27861)</li>
<li>[Perf] Decouple torch op from GDA to leverage torch.compile (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27871)</li>
<li>[CI/Build] Add gpt-oss LoRA test (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27870)</li>
<li>[Bugfix] Allow 64-bit integer values for LoRA IDs to avoid overflow/truncation (@shadeMe #27876)</li>
<li>[Bugfix] Fix broken MRoPE for GLM-4.1V/GLM-4.5V (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27860)</li>
<li>[Bugfix] Missing NIXL metadata for handshake initialization if instance spans multi-node (@GuanLuo #26338)</li>
<li>Docs update tpu install instructions (@RobMulla #27824)</li>
<li>[bugfix] Missing cached item in beam search (@fake0fan #27874)</li>
<li>fix incorrect type annotation in KimiMLP (@skyloevil #27885)</li>
<li>Flashinfer_CUTLASS_MOE fuses quantization for TP (@wenscarl #27223)</li>
<li>[Cleanup] Remove no-longer-used <code>SpeculativeConfig.enable_chunked_prefill</code> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27826)</li>
<li>[Feature] Pydantic validation for scheduler.py and structured_outputs.py (<a class="user-mention notranslate" href="https://github.com/vrdn-23">@vrdn-23</a> #26519)</li>
<li>Add FLASHINFER_MLA to test_mla_backends and add B200 CI run (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27663)</li>
<li>Batch invariance doc (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #27839)</li>
<li>[Hybrid] A simpler algorithm to find kernel_block_size (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #26476)</li>
<li>[Core] Async scheduling + structured outputs compatibility (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26866)</li>
<li>[Kernel] Enable FusedMoEModularKernel  support  bias (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27754)</li>
<li>[Bugfix] Fix KDA output (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27905)</li>
<li>[Multimodal][XPU]Enable vision attn backend for xpu platform (@yma11 #27525)</li>
<li>Adding SplitK in fused_moe_lora kernel (@yugong333 #27818)</li>
<li>[CI/Build] Bump transformers version (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27528)</li>
<li>[Bugfix] [Model] Missing MRoPE function definition from <code>KeyeForConditionalGeneration</code> (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #27895)</li>
<li>[Add] cmdline argument parsing for KV cache offloading modules (@ApostaC #27621)</li>
<li>feat(benchmarks): support HF model names in multi-turn benchmark (@ai-jz #27850)</li>
<li>[Docs] Mock all imports for docs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27873)</li>
<li>[V0 deprecation] Remove VLLM_USE_V1 usage in platform and v1 module (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27798)</li>
<li>[Bugfix] DeepSeek V3.2 MTP metadata &amp; CUDA graph issues (@xiaohajiayou #26779)</li>
<li>[Bugfix] Python 3.10 compatibility for <code>Self</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27918)</li>
<li>[Core][TPU] Support TPU Data Parallalism (@wenxindongwork #27365)</li>
<li>[BugFix] Fix mixed penalties batch with async scheduling (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27910)</li>
<li>Adds anthropic /v1/messages endpoint to openai api_server (@bbartels #27882)</li>
<li>[KV offload] Offloading connector async scheduling support (@KevinCheung2259 #27648)</li>
<li>[CI/Build] Fix flaky test_transcription_validation.py::test_basic_audio_gemma (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> #27924)</li>
<li>[Bugfix] Fix Qwen Omni audio inference (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27920)</li>
<li>Performance fix MistralTokenizer: cache special ids and tokens (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #27925)</li>
<li>[V1] [Hybrid] Mamba1 Automatic Prefix Caching (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> #26377)</li>
<li>[Misc] Provide Siglip2 chat template (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27939)</li>
<li>[Bugfix][llm]: Abort orphaned requests when llm.chat() batch fails (@Flink-ddd #27420)</li>
<li>[BugFix][LoRA] use adapter_id instead of id field of lora_request (@biswapanda #27728)</li>
<li>[Frontend] Align finish_reason when tool is called with OpenAI (@n0gu-furiosa #25054)</li>
<li>[Hybrid] Pass kernel block size to builders (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> #27753)</li>
<li>[Bugfix] Padded Eagle Specdec with Chunked Prefill (@Flechman #26263)</li>
<li>[XPU]Refine Dockerfile.xpu, avoid oneccl dependency issue (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #27964)</li>
<li>Add ORCA endpoint load metrics support (@efimki #24905)</li>
<li>[CI/Build] Remove the flaky gpt-oss lora test (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27966)</li>
<li>[Model] Add PaddleOCR-VL Model Support  (@zhang-prog #27758)</li>
<li>Early exit for MoE LoRA kernels (@gnovack #27131)</li>
<li>[Bugfix] Skip gs:// model paths for speculator detection (<a class="user-mention notranslate" href="https://github.com/pwschuurman">@pwschuurman</a> #27846)</li>
<li>[BUG] Make 'binary' default option for saving torch compile artifacts when using standalone_compile (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> #27616)</li>
<li>[CI/Testing] Add basic single node dual batch overlap test (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27235)</li>
<li>[Spec Decode] Integrate Suffix Decoding from Arctic Inference (@aurickq #25784)</li>
<li>[Feature][Benchmarks] Support <code>inf</code> burstiness (@sducouedic #26941)</li>
<li>[Bugfix][Qwen][Multimodal] Move Qwen2_5_vl sdpa to custom op and reenable compile (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> #27764)</li>
<li>[Bugfix] change FlashMLA reorder_batch_threshold (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27777)</li>
<li>[Docs] add runai_streamer_sharded to LoadConfig (@andyxning #27937)</li>
<li>Add TP parameter to attention tests (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27683)</li>
<li>[Bugfix][plugin] fla crash on plugin (<a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> #27322)</li>
<li>[Bugfix] Fix MoE Routing Simulation (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #28002)</li>
<li>Remove the tpu docker image nightly build. (@QiliangCui #27997)</li>
<li>[Bugfix][ROCm] Fix ViT rotary embeddings for torch.compile compatibility on ROCm (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #27748)</li>
<li>[LoRA] Lora shrink swizzle (@li2haipeng #27694)</li>
<li>[Refactor] Lazy import tool_parser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27974)</li>
<li>[NIXL][XPU] Pin NIXL version to 0.7.0 (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> #27849)</li>
<li>[Metrics] Enable sleep state metric outside of dev mode (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #27867)</li>
<li>[Bug] Batch invariant: Fix flash attn MLA <code>RuntimeError: scheduler_metadata must have shape (metadata_size)</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27884)</li>
<li>[CPU]Improve dynamic 4bit moe performance (@xiangze-arm #27240)</li>
<li>[CI/Build] Update LM Eval Version in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27944)</li>
<li>[KV Connector] Make KVCacheConfig an explicit constructor argument (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #27887)</li>
<li>[Model] fix ernie45 reasoning_parser (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> #27973)</li>
<li>[CI/Build] Fix OpenAI API correctness on AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28022)</li>
<li>[BugFix][Performance] Restore flashinfer autotuning for all scenarios (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27904)</li>
<li>Load tuned fused_moe_lora shrink and expand kernel configs separately (@yugong333 #27435)</li>
<li>Support using Int4PreshuffledTensor after loading (<a class="user-mention notranslate" href="https://github.com/jerryzh168">@jerryzh168</a> #26066)</li>
<li>[Core] Enable StatLogger in LLMEngine (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28020)</li>
<li>[Model][Bugfix] fix pipeline parallelism support for NemotronH (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> #27968)</li>
<li>[Model] add optimal triton fused moe configs for NemotronH MoE (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> #27967)</li>
<li>[Kernels] Isolate modular kernel code from FusedMoEMethodBase subclasses. (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> #27123)</li>
<li>[BugFix] Fix incorrect preallocated sampled_token_ids tensor size (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28025)</li>
<li>[Perf] SM100 - add swap AB optimization to CUTLASS FP8 GEMM (@LyrisZhong #27284)</li>
<li>[PERF] Decouple projections from GDN custom op (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #27512)</li>
<li>[model] Add support for openPangu_Ultra_MoE (@yt0428 #27521)</li>
<li>[PerfFix] Avoid separate thread for MP executor shm spin (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28012)</li>
<li>[AsyncScheduling] Don't schedule past request max_tokens (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27922)</li>
<li>Remove deprecated <code>--rope-scaling</code> and <code>--rope-theta</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28006)</li>
<li>[ROCm][Perf] New design on ROCm AITER MHA backend Implementation (@ganyi1996ppo #25763)</li>
<li>Added disable rule to track files under benchmarks/lib (@nadavkluger #28048)</li>
<li>[Multimodal] Make MediaConnector extensible. (<a class="user-mention notranslate" href="https://github.com/huachenheli">@huachenheli</a> #27759)</li>
<li>[ROCm] gemm_a16w16 upstreaming (@maleksan85 #26969)</li>
<li>Revert "[PERF] Decouple projections from GDN custom op" (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28080)</li>
<li>[Qwen3-Next] MOE configs for A100-SXM4-80GB TP4 TP8 (@toulzx #27740)</li>
<li>[XPU] Add gpt-oss model support for Intel GPU (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #27786)</li>
<li>[CI/Build] Enable some fixed tests in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28078)</li>
<li>[V0 deprecation] Remove VLLM_USE_V1 usage in most modules (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27955)</li>
<li>[Bugfix] Fix encoder-only model support for transformers backend (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28021)</li>
<li>[BugFix] Fix DCP Assert (AssertionError: DCP not support reorder_batch_threshold &gt; 1 now.) (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28100)</li>
<li>[Model, Core] Support Granite Speech &amp; LoRA for STT (@alex-jw-brooks #24455)</li>
<li>[Refactor] Lazy-loaded reasoning_parser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28092)</li>
<li>[Refactor] to simplify and extract the shared logic between chat completion and responses (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27961)</li>
<li>[bugfix] fix wrong <code>dcp_local_seq_lens</code> calc (@pisceskkk #27518)</li>
<li>[Hybrid allocator + kv connector] revert connector test changes related to hybrid allocator (@KuntaiDu #28011)</li>
<li>[Misc] fix import error for DeepSeekR1ReasoningParser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28114)</li>
<li>Fix excessive logging noise by reducing the log level of the MinimaxM2ToolParser import success message (@minatoaquaMK2 #27635)</li>
<li>Bugfix: Cutlass FP8 FusedMoE bad scaling factors (@amirkl94 #27255)</li>
<li>[Graph Partition][Cache] Use inductor partition ops config (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27702)</li>
<li>[XPU] Enable custom routing functions in IPEX for Llama4 (@frost-intel #28004)</li>
<li>add kimi reasoning parser (@MoyanZitto #28128)</li>
<li>[DCP] check return_lse for all layers in dcp (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #27929)</li>
<li>[BugFix] Support EP/DP + EPLB with MTP (@ilmarkov #25311)</li>
<li>Enabling cooperative multi-gpu tests on multi-gpu nodes (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #27986)</li>
<li>[ROCm][MLA] Support block-size &gt; 1 for AITER MLA backend  (@ganyi1996ppo #27224)</li>
<li>[Bugfix] Validate custom logits processor xargs for online serving (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27560)</li>
<li>[misc] add vLLM Beijing Meetup (@jjzhang #28127)</li>
<li>[Kernel] Fuse computation of g and beta for Gated Delta Net (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28095)</li>
<li>[Core] add support for reasoning parser plugins (@walterbm #28075)</li>
<li>[Bugfix] vLLM should check Inductor config for compile cache enablement status (@gmagogsfm #27637)</li>
<li>[FlashInfer] Avoid FlashInfer block_size 16 + head_size 256 on blackwell (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #27994)</li>
<li>[CI]: Add LMCache Unit Tests (@sammshen #27852)</li>
<li>[Feature] Extend batch invariant torch.compile to B200 (@PaulZhang12 #27856)</li>
<li>[Bugfix] Fix Qwen3-Reranker-8B load (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #28117)</li>
<li>[Docs] Clean up README_TUNING.md (<a class="user-mention notranslate" href="https://github.com/windsonsea">@windsonsea</a> #28088)</li>
<li>[Hardware][IBM Z] Optimize s390x Dockerfile (@R3hankhan123 #28023)</li>
<li>[Chore] Remove Nemotron-Nano-VL config copy (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28126)</li>
<li>[Docs] Add guide to debugging vLLM-torch.compile integration (@zou3519 #28094)</li>
<li>[Feature]: Add corrupted request metric to V1 metrics system. (<a class="user-mention notranslate" href="https://github.com/atalhens">@atalhens</a> #27306)</li>
<li>[CI/Build] Update checking logic in cutlass_group_gemm_supported  (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27948)</li>
<li>[CI/Build] Fix <code>test_defaults_with_usage_context</code> in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27926)</li>
<li>[Core][Hybrid allocator + connector 2/n] Unify <code>remove_skipped_blocks</code> by <code>get_last_useful_token</code> (@KuntaiDu #25431)</li>
<li>[Debugging] Add annotation for easier trace analysis (@dayeol #22496)</li>
<li>[PERF] Decouple projections from GDN custom op. Attempt 2 (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28083)</li>
<li>[Bug] Fix cpu disable shared_experts <code>VLLM_DISABLE_SHARED_EXPERTS_STREAM</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28157)</li>
<li>[Bug] Fix env string <code>"0"</code> same to <code>True</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28159)</li>
<li>[Feature] Enable TP + EP <code>shared_experts</code> overlap with router, 3.7% E2E performance improvement (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28164)</li>
<li>[CI Failure] <code>nm-testing/Qwen2-0.5B-Instruct-FP8-SkipQKV</code> was removed from HF. Skip it in tests (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28170)</li>
<li>[Misc] Remove the duplicate code (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28111)</li>
<li>[Chore] Clean up deepseek v2/v3 config copy (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28055)</li>
<li>[Core][MM] Use non-blocking CPU-GPU copy of multimodal data (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28141)</li>
<li>Make the cv2 dependency optional (@cmpute #27780)</li>
<li>[CI] Add compile/test_multimodal_compile.py to CI (@gmagogsfm #28151)</li>
<li>[flashinfer] fix FI all2all with FI cutlass moe (@mxz297 #28166)</li>
<li>Patch Mistral Tokenizer (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #28146)</li>
<li>Fix hard-coded parameter name in gemma3n.py (@seungduk-yanolja #27946)</li>
<li>[CPU] Enable torch profiling (@aditew01 #28130)</li>
<li>[V0 deprecation]clean up is_v1_supported_oracle (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28116)</li>
<li>[Bugfix][Kernel] fix merge attn states when both prefix and suffix are empty (@courage17340 #28181)</li>
<li>[Frontend] OpenAI Responses API supports Tool/Function calling - non-harmony  (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #26874)</li>
<li>[CPU]Improve cpu fused moe perf (@xiangze-arm #27244)</li>
<li>Disable nm-testing models with issues in CI (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28206)</li>
<li>[Docs] Switch to directory style URLs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28058)</li>
<li>[Kernel][Model] Tune fused_moe Triton configs for MiniMax-M2 on H100 (@minatoaquaMK2 #28200)</li>
<li>[Doc] Add Arm CPUs are on the list of supported targets in vLLM (@milpuz01 #26018)</li>
<li>[HARDWARE][CPU] Add Option for Disabling Binding to Specific CPU Cores (@StanHatko #27953)</li>
<li>[Frontend] Fix logging format when enable response logging (@esmeetu #28049)</li>
<li>CODEOWNERS: Add myself as reviewer on security docs (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> #28216)</li>
<li>[Structured outputs] Upgrade llguidance to 1.3.0 (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #28039)</li>
<li>Add llama 4 scaling support (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #28145)</li>
<li>[Chore] eliminate duplicated and unconditional object serialization in anthropic messages api (@vicoooo26 #27792)</li>
<li>[ROCm] triton fp8 kernel (@maleksan85 #27058)</li>
<li>[Doc]: Make extraInit containers fully configurable in helm chart (@HanFa #27497)</li>
<li>[Test] Add non-MoE DP test coverage (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28235)</li>
<li>[BugFix] Fix FusedMoELoRA + ModularKernel Integration (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28237)</li>
<li>Fix failing test for CRadio (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> #27738)</li>
<li>Speed up mm processor kwargs per request by spliting dynamic and static kwargs (@LJH-LBJ #26483)</li>
<li>[Multimodal][torch.compile] Add compilation config field for turning off ViT/MM compile (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> #28242)</li>
<li>[CI/Build] Loosen STT LoRA Translate Check (Flaky Test) (@alex-jw-brooks #28247)</li>
<li>Add runai model streamer e2e test for GCS (@amacaskill #28079)</li>
<li>Fix issues from #28242 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28257)</li>
<li>[amd][gptoss] Perf gain because of block alignment (@smitkadvani #28024)</li>
<li>[Bug] Fix missing token_ids for reasoning parser models in chat completions   #28246 (<a class="user-mention notranslate" href="https://github.com/baonudesifeizhai">@baonudesifeizhai</a> #28256)</li>
<li>[CI] Reduce Blackwell Fusion test runtime by filtering tests and only run all tests in nightly (@Copilot #28074)</li>
<li>[Kernel] LoRA triton kernels support PDL (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27402)</li>
<li>[Perf] Introduce FlattenLogprobs to store logprobs results to reduce GC overhead (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #28171)</li>
<li>[FixBug]Aeala/ShareGPT_Vicuna_unfiltered marked as multimodal benchmark (@princepride #28265)</li>
<li>[CPU]Avoid repeated random sample compile (@xiangze-arm #28260)</li>
<li>[Misc][Model][Refactor] Pass the prefix into Linear layers (@MengqingCao #28259)</li>
<li>[fix] Revert "fixing mm placeholder replacement issue with gemma3" (@khluu #28285)</li>
<li>[Core][MM] Add mechanism to configure multimodal fields which should stay on CPU (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28168)</li>
<li>[Bugfix] Use latency MOE backend as default for Flashinfer and other misc fixes (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> #27439)</li>
<li>[CLI] add --max-tokens to <code>vllm complete</code> (@Iceber #28109)</li>
<li>[Feature] Default <code>ignore_eos</code> True for <code>random</code> dataset (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28227)</li>
<li>[Log] update shm wait time msg (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #28255)</li>
<li>Revert "[PerfFix] Avoid separate thread for MP executor shm spin (#28012)" (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28289)</li>
<li>[README] Add Arm CPUs to the list of supported targets (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #28290)</li>
<li>[doc] add guide about the provided PTX was compiled with an unsupported toolchain (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> #28305)</li>
<li>[Build] Fix release pipeline failing annotation (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> #28272)</li>
<li>[Bugfix] Fix and add tests for GptOss reasoning parser (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #28000)</li>
<li>[Core] Rework handling of async scheduling config (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28250)</li>
<li>[PerfFix] Avoid separate thread for MP executor shm spin (take 2) (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28319)</li>
<li>Update Flashinfer from <code>v0.4.1</code> to <code>v0.5.2</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27952)</li>
<li>[XPU] Enable Expert parallel for MoE models (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #28263)</li>
<li>remove resolve_op_overloads and use splitting_ops directly (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #28081)</li>
<li>[Bugfix][LoRA][Spec Decode] Support LoRA with speculative decoding (@xiaohongchen1991 #21068)</li>
<li>Update gpu.rocm.inc.md to add support for AMD Ryzen AI MAX / AI 300 Series (gfx1151, gfx1150) (@hammmmy #28308)</li>
<li>[Perf][DeepSeek] Add sigmoid+bias fusion to fused_grouped_topk from TRTLLM (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28124)</li>
<li>Bump arctic-inference requirement (@aurickq #28174)</li>
<li>[bugfix] support eagle with lora cudagraph specialization (@gnovack #28318)</li>
<li>[Model] Consolidate Deepseek-MoE implementation with DeepSeek-v2 (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28101)</li>
<li>Refactor CPU/GPU extension targets for CMake build (@ashahba #28026)</li>
<li>[flashinfer][fix] do not check nvcc availability when using pre-downloaded cubins (@mxz297 #27990)</li>
<li>[Attention] Remove max cudagraph size limit of 992 (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27840)</li>
<li><code>reasoning_content</code> -&gt; <code>reasoning</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27752)</li>
<li>[Bugfix] Update device name for H200 detection (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> #28349)</li>
<li>[Bugfix] Spec decode + structured output + spec model max len edge case (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #28298)</li>
<li>[DCP] Support dcp kv_cache interleave size &gt; 1 (@zhangsicheng5 #26696)</li>
<li>Enhance run_cluster.sh for multi-NIC support (@evberrypi #28328)</li>
<li>[Feat] Drop-in Torch CUDA Profiler (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #27841)</li>
<li>Remove setuptools upper bound constraint (&lt;80) (@ColeMurray #28337)</li>
<li>[Bugfix] Fix test fused quant layernorm tests (<a class="user-mention notranslate" href="https://github.com/ElizaWszola">@ElizaWszola</a> #27865)</li>
<li>[Performance][gpt-oss] Revert gpt-oss max cudagraph size to 1024 (@mmangkad #28345)</li>
<li>[chore] Move some wikimedia images to S3 (@khluu #28351)</li>
<li>fix: close issue 28338 by fixed python version (<a class="user-mention notranslate" href="https://github.com/yihong0618">@yihong0618</a> #28339)</li>
<li>[Misc] fix typo and add detailed log (@andyxning #28178)</li>
<li>[ROCm] Add env to enable/disable aiter triton gemm (@sarckk #28321)</li>
<li>[Misc] Add some comments in qwen3-next (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28267)</li>
<li>[CI] Fix flaky <code>test_eagle_correctness</code> test (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28364)</li>
<li>[Core] Simplify async KV output aggregation (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28327)</li>
<li>[Core] Separate out attention metadata building logic from prepare inputs (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #26764)</li>
<li>[BugFix] Fix cu_num_generated_tokens slicing logic in LogprobsLists.slice() method (@usberkeley #28214)</li>
<li>[CI/Build] Temporary fix to LM Eval Small Models (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28324)</li>
<li>[Kernel] Fix fused_gdn_gating (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28343)</li>
<li>[ROCm][Platform] Add RX7900XTX device id in _ROCM_DEVICE_ID_NAME_MAP (@JartX #28279)</li>
<li>[CI] lora/test_mixtral.py : Add additional expected outputs due to flakiness (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28322)</li>
<li>[Hardware][AMD][Model] Add Triton MoE tuning support and optimized configs for Qwen3 omni for MI308X (@sammysun0711 #28373)</li>
<li>[V0 deprecation] Remove no longer used <code>get_metadata_cls</code> (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28370)</li>
<li>Restore PlaMo2 unit test as <code>pfnet/plamo-2-1b</code> now supports <code>transformers &gt;=4.56</code> (@Alnusjaponica #28019)</li>
<li>[Metrics] Refactor LoRA state tracking (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #26801)</li>
<li>[bugfix] fix siglip batch text output error (@piood #28365)</li>
<li>[Fix] optimize visual token mask with caching and multi-token support (@bo-ke #28374)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> to codeowner for ROCm and multi-modal (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #28360)</li>
<li>[Rocm][fused_moe][fp4] view weight to torch.float4_e2m1fn_x2 when running aiter fused moe for fp4 model (@zejunchen-zejun #27474)</li>
<li>[Kernel] Optimization of the mm_k operator. (@caozuoba #28280)</li>
<li>[RFC][ROCm][AITER] Keep all AITER kernels in <code>_aiter_ops</code> class like <code>_custom_ops</code> and <code>_ipex_ops</code> (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #24490)</li>
<li>[V0 Deprecation] Remove unused <code>context_len</code> and <code>seq_len</code> from M-RoPE (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28395)</li>
<li>[Bugfix] Fix persistent_masked_m_silu_mul_quant tests (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28366)</li>
<li>[Performance] Support FP8 flashinfer TRTLLM MOE on Qwen3 and Qwen-3next (@jiahanc #27492)</li>
<li>[Bugfix] Fix llguidance backend, rollback when EOS was encountered (@Flechman #25905)</li>
<li>[FA/Chore] Bump FA version for FP8 two-level accumulation  (<a class="user-mention notranslate" href="https://github.com/jmkuebler">@jmkuebler</a> #27889)</li>
<li>[Bugfix][EPLB] Disabled shared expert overlap when EPLB is enabled (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28377)</li>
<li>[Misc] Add more scoping for improved trace (@frank-wei #28329)</li>
<li>[BugFix] Fix DeepGEMM over-allocating workspace (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28254)</li>
<li>[Frontend][2/n] remove empty content from _parse_tool_calls_from_content (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #28331)</li>
<li>[CI] Fix Plugin Tests Tests (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> #28413)</li>
<li>[ROCm] Add missing gemm_a8w8_blockscale import (@sarckk #28378)</li>
<li>[PERF] Allreduce fusion. Support torch native matching. Tuning of the thresholds (@ilmarkov #24248)</li>
<li>[Perf] Move gc.freeze logic from EngineCoreProc to EngineCore for better coverage (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27896)</li>
<li>[Bugfix] Ensure calculated KV scales are applied in attention. (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> #27232)</li>
<li>[Test] Remove old non-varlen FA2 test (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28420)</li>
<li>[Feature] Refactor batch invariant fp8 DeepGEMM (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27606)</li>
<li>[CI/Test Fix] Fix CP tests on Blackwell (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28404)</li>
<li>[Feature] Add env var <code>VLLM_MOE_USE_DEEP_GEMM</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28422)</li>
<li>Only register rocm_aiter_ops if aiter is found (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28428)</li>
<li>Fix rotary embedding benchmark script (@xyang16 #28323)</li>
<li>[Misc] FlattenLogprobs -&gt; FlatLogprobs (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28335)</li>
<li>[Frontend] Add sagemaker_standards dynamic lora adapter and stateful session management decorators to vLLM OpenAI API server (@zhaozuy #27892)</li>
<li>[Bugfix] Fix Stream Sync for Shared Expert Overlap (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> #28430)</li>
<li>[Doc] Sleep mode documentation  (@iAmir97 #28357)</li>
<li>[BugFix] Avoid calling KV connector layer APIs when metadata is unset (<a class="user-mention notranslate" href="https://github.com/sdavidbd">@sdavidbd</a> #28253)</li>
<li>[Bugfix] Fix max image size for PaddleOCR-VL (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28442)</li>
<li>[EPLB] Refactor balance_packing to use numpy and optimize GPU-CPU transfers in EPLB (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28369)</li>
<li>[Bugfix] fix qwen3-next crash (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28202)</li>
<li>[BugFix] 'DeepseekV2Config' object has no attribute 'use_mla'`  (@faaany #28387)</li>
<li>[Model][Qwen3VL] Slighly speedup <code>fast_pos_embed_interpolate</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28434)</li>
<li>Multi turn benchmark progress bar for synthetic conversation generation (@segevido #28394)</li>
<li>[CI] Add mergify rules for <code>nvidia</code> label (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28417)</li>
<li>[Attention] Refactor CUDA attention backend selection logic (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #24794)</li>
<li>Fix Fused MoE LoRA Triton kernel bug (@chaojun-zhang #28450)</li>
<li>[Model] Pass <code>mm_features</code> directly into <code>get_mrope_input_positions</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28399)</li>
<li>Add request timeout override for multi-turn benchmarks (@segevido #28386)</li>
<li>[Docs] Fix grammar in CPU installation guide (@maryamtahhan #28461)</li>
<li>[Kernels] Split up fused_moe/layer.py, isolate more modular kernel code (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> #28064)</li>
<li>[BugFix] Fix Failing Ruff Check (@jvlunteren #28469)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> to CODEOWNERS for Observability (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #28457)</li>
<li>[BugFix] Fix RuntimeError in PixtralHFAttention on CPU/XPU (@faaany #28444)</li>
<li>[BugFix] Add test_outputs.py to CI pipeline (@usberkeley #28466)</li>
<li>[Doc] Fix typo in serving docs (@the-codeboy #28474)</li>
<li>Remove weight_scale.T special case for SM90 Block FP8 CUTLASS kernel (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28431)</li>
<li>[NIXL] Generalize block-first backend layouts (FlashInfer-like) (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28282)</li>
<li>[Kernel][Perf] fuse QK Norm and RoPE into one cuda kernel for Qwen Model (@izhuhaoran #27165)</li>
<li>[ROCm][Quantization] extend AMD Quark to support mixed-precision quantized model (@xuebwang-amd #24239)</li>
<li>[Quantization] fix attention quantization of gpt_oss model (@xuebwang-amd #27334)</li>
<li>[CI/Build] Refactor Attention backend for test_prefix_prefill from xformers to SDPA (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28424)</li>
<li>Prefer FlashAttention MLA as default over FlashMLA (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27363)</li>
<li>[Kernel] Optimize rms_norm kernel (@xyang16 #27931)</li>
<li>[BugFix] Fix Siglip2Attention on XPU (@faaany #28448)</li>
<li>[Misc] Remove unused attention prefix prefill ops functions (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #26971)</li>
<li>[Perf] Use np.ndarray instead of list[list[int]] to reduce GC overhead (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #28245)</li>
<li>[V0 deprecation] Clean up num_prefill_tokens logic for V0 (@gcanlin #28203)</li>
<li>[Misc] fix typo in DCP comment (@Livinfly #28389)</li>
<li>[LoRA][1/N]Remove LoRA extra vocab (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #28382)</li>
<li>[TPU] Rename path to tpu platform (@kyuyeunk #28452)</li>
<li>[Misc] Cleanup Executor interface (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28441)</li>
<li>Add Zurich vLLM Meetup (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28488)</li>
<li>[Bugfix] Disable shared expert overlap if Marlin MoE is used (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28410)</li>
<li>[Feature] Allow configuring FlashInfer workspace size (@maxyanghu #28269)</li>
<li>Use FLASHINFER MLA backend when testing fp8_kv_scale_compile (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> #28491)</li>
<li>[BugFix] Graceful handling of torch symm mem errors. (@ilmarkov #27671)</li>
<li>[Frontend] Change CompilationMode to a proper Enum (@gmagogsfm #28165)</li>
<li>[Performance] Cache loaded custom logitsprocs to avoid overheads (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28462)</li>
<li>[[V0 deprecation]]Remove VLLM_USE_V1 env (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28204)</li>
<li>[CPU] Refactor CPU attention backend (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27954)</li>
<li><code>VLLM_USE_TRITON_FLASH_ATTN</code> V0 variable deprecation (@AndreasKaratzas #27611)</li>
<li>[Model][Qwen3VL] Simplify <code>get_mrope_input_positions</code> using numpy (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28302)</li>
<li>[Core] Encoder separation for Encode-Prefill-Decode Disaggregation (@fake0fan #25233)</li>
<li>[BugFix] Add fallback path in <code>apply_rotary_pos_emb_flashattn</code> for non-cuda platforms (@faaany #28447)</li>
<li>[Benchmark] Add retry support to fix workload bias in multi-turn benchmark (@ai-jz #28493)</li>
<li>[Core] Cache <code>vllm_is_batch_invariant</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28304)</li>
<li>[CI/Build] Fix crash due to removed VLLM_USE_V1 attribute in EPD (@fake0fan #28521)</li>
<li>[CI] Introduce autorun_on_main feature (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27836)</li>
<li>[BugFix]: --enable-lora with model granite-4.0-micro crash (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> #27733)</li>
<li>[Model] fix glm4_moe_mtp load weights with GLM-4.6 checkpoint. (@wuyaoxuehun #27597)</li>
<li>[XPU]Fix crash due to removed VLLM_USE_V1 attribute (@chaojun-zhang #28520)</li>
<li>[KVConnector] Enable get_block_ids_with_load_errors() in LMCache connector  (@ziruiliu #27978)</li>
<li>add cpu option for p/d in nixl_connector (@ZhengHongming888 #28356)</li>
<li>[ROCm] [Bugfix] Fix <code>fused_qknorm_rope_kernel</code> rocm compatibility (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #28500)</li>
<li>[Bugfix] Fix gpt_oss packed_modules_mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #28536)</li>
<li>[V0 deprecation] Deprecate use_v1 parameter (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28112)</li>
<li>Fix pre-commit (and XPU) on <code>main</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28556)</li>
<li>[Performance][Hopper] Avoid M dim padding to 4x for most cases (due to cuda graphs paddings) (@alexm-redhat #28492)</li>
<li>[Refactor] Remove redundant TP gather/split in split_qkv in QwenVL (@gcanlin #28271)</li>
<li>[Misc] Refactor Attention kv transfer methods into decorator (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27816)</li>
<li>Remove deprecated fields from <code>CompilationConfig</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27593)</li>
<li>[Perf] Refactor cudagraph_support to enable full CUDA graphs for spec decoding with FlashInfer (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #28479)</li>
<li>Implement ARC KV cache eviction policy (@albertoperdomo2 #27039)</li>
<li>[EPLB][ROCm]: support EPBL for ROCm backend (@PerryZhang01 #27731)</li>
<li>[Model] [Config] Correctly identify granite-4.0-micro as non-hybrid model (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> #28563)</li>
<li>[CI] Skip "Multi-Modal Models Test (Extended) 3" test that's broken in current Transformers (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28559)</li>
<li>[KV connector][WIP] KV cache proxy based on LMCache multi-process mode (@ApostaC #27902)</li>
<li>[BugFix] Priority scheduling and spec tokens preemption (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #28558)</li>
<li>[Misc]Fix typo in llm_engine.py (@frank-wei #28584)</li>
<li>[Performance][B200] Fix deepgemm prologue (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27897)</li>
<li>[ROCM] Fix ROCm warnings, environment flag access, and GEMM kernel naming for consistency in <code>_aiter_ops.py</code> (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #28464)</li>
<li>[TPU] Support GCS path in VLLM_TORCH_PROFILER_DIR (@QiliangCui #28487)</li>
<li>[Bugfix] Adjust Marlin CUDA arch selection to 8.0+PTX;9.0+PTX (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28294)</li>
<li>[Core][AMD] Migrate fully transparent sleep mode to ROCm platform (@HollowMan6 #12695)</li>
<li>[MoE][Kernel][Perf] Improve Shared Expert Stream Overlap (@alexm-redhat #28406)</li>
<li>Skip models that cannot currently init on Transformers v5 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28471)</li>
<li>[Docs] Update meetups.md description (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28583)</li>
<li>[ROCm][Bugfix] Revert removing setuptools version restriction (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #28592)</li>
<li>[platform] Move get_cu_count to utils (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27005)</li>
<li>[Bugfix] Fix SM100 gpt-oss regression due to faulty attn sink support (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28561)</li>
<li>[BugFix] Fix <code>mm_encoder_attn_backend</code> arg type checking (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28599)</li>
<li>[Docs] Add some details about what the MoE block needs for the Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28588)</li>
<li>Rename clashing method names for vLLM model protocol (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27583)</li>
<li>[n-gen] DO NOT repeatedly return finished child requests (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #28591)</li>
<li>[Frontend] split append tool output (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #28333)</li>
<li>[Frontend][responsesAPI][1/n] convert responses API tool input to chat completions tool format (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #28231)</li>
<li>[BugFix][ROCm] Fix <code>get_cu_count</code> missing variable error (@ganyi1996ppo #28608)</li>
<li>[XPU] Support Triton path for LoRA operations on XPU   (@faaany #28511)</li>
<li>Support DeepEP for Kimi-k2-thinking through enabling gemm selection for compressed-tensor marlin wna16 (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #28574)</li>
<li>[build][cmake]: Bundle static ACL and torch libgomp for CPU extension builds (@Radu2k #28059)</li>
<li>[ROCm][BugFix] Remove the usage of <code>device_info</code> from aiter (@ganyi1996ppo #28383)</li>
<li>[Bugfix] Prevent crash on empty grammar string (@tjandy98 #28210)</li>
<li>Use official xformers-0.0.33 built for PT 2.9 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #28600)</li>
<li>Add NUMA node validation for CPU thread binding (@usberkeley #28555)</li>
<li>[Bugfix] fix kimi-linear crash (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28445)</li>
<li>[Frontend] supports interleaved thinking (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28531)</li>
<li>Support all interleaved layer types (@sarckk #28485)</li>
<li>Fix: Correctly filter special tokens in benchmark_prefix_caching (@dw2761 #28615)</li>
<li>[BugFix] Fix type error when assign a trition kernel tensor to a torch.nn.Parameter (@liuzijing2014 #28603)</li>
<li>Fix io processor pooling  #28273 (<a class="user-mention notranslate" href="https://github.com/baonudesifeizhai">@baonudesifeizhai</a> #28484)</li>
<li>[XPU] add sym params to IPEXConfig (@zufangzhu #28611)</li>
<li>[Bugfix] Fix FPS value type for Qwen2.5-Omni video processing (@faaany #28630)</li>
<li>[Hardware][PowerPC] Fix fp16 compilation error for Power in cpu attention backend and bump oneDNN version (@Akashcodes732 #28535)</li>
<li>[ROCm][BugFix]Fix <code>get_cu_count</code> in rocm_aiter_fa.py (@ganyi1996ppo #28618)</li>
<li>[CI/Build] Install uv for AMD MI300: Language Models Tests (Hybrid) %N (@amdfaa #28142)</li>
<li>[CI Failure] Fix backend selection for encoder-only models (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #28534)</li>
<li>[BugFix] DeepSeek-OCR: apply NoRepeatNGramLogitsProcessor to greedy path (@YuanpingSong #28617)</li>
<li>Fix <code>get_num_experts</code> when config sets it explicitly to <code>None</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28652)</li>
<li>[Misc] Turn off encoder torch compile by default (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28634)</li>
<li>Rewrite C++ meta funcs to Python (@janeyx99 #28595)</li>
<li>[BugFix] Ensure <code>EngineArgs.create_engine_config</code> is idempotent (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28515)</li>
<li>[TPU] patch TPU wheel build script to resolve metadata issue (<a class="user-mention notranslate" href="https://github.com/jcyang43">@jcyang43</a> #27279)</li>
<li>[Performance][B200] silu_mul_quant: pack scales in int32 (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28358)</li>
<li>[Bugfix] Fix validate model input for decoder models (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> #27099)</li>
<li>[Attention][Bugfix] Fix FA sink support (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28660)</li>
<li>[Perf] Support stream interval for reducing host overhead (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> #27869)</li>
<li>[bugfix] correct local_chunk_len for DCP in reorg_kvcache with long context (@pisceskkk #28526)</li>
<li>[Bugfix] Eliminate tuple inputs to submodules in graph partitioning (@gmagogsfm #28533)</li>
<li>[Bugfix] [CPU] bump torch to 2.9.0 for Darwin to fix segmentation fault (@kebe7jun #27791)</li>
<li>[Misc] Update CODEOWNERS for simon-mo and comaniac (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> #28675)</li>
<li>[CI] Bug: Fix ci entrypoint pooling (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28684)</li>
<li>[KV Connector] Test async mode in scheduler tests (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #28550)</li>
<li>Mirrored test group definitions for AMD (2025-11-11) (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #28573)</li>
<li>[quantization][config] enable override existing quant_config (<a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> #28510)</li>
<li>[ROCm] Bump up the version of amd-smi to 6.4.3 (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28680)</li>
<li>[CPU][Bugfix] Fix Apple Silicon M1 compilation failure (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28681)</li>
<li>[ci][amd] fix basic models extra init test (@bradleyhd #28676)</li>
<li>[Misc] Remove <code>warn_for_unimplemented_methods</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28613)</li>
<li>[XPU][CI]disable lm cache uts (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #28696)</li>
<li>[Misc] Update xformers to 0.33.0.post1 (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28678)</li>
<li>[Misc] add ignore mapper for quark quantization (@haoyangli-amd #28275)</li>
<li>[Bugfix][CI/Test][Spec Decode] Fix illegal memory access in offline_inference/spec_decode.py (Issue  27619) (@rasmith #28432)</li>
<li>[BugFix][CI/Build][ROCM] Fix import error and apply assert in appropriate case in test_struct_output_generate (@rasmith #28311)</li>
<li>use default CCL_ZE_IPC_EXCHANGE (@yma11 #28700)</li>
<li>[Bugfix] fix dots.ocr pp support (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28705)</li>
<li>[BugFix] Fix multi-modal async scheduling race condition (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28706)</li>
<li>Add output token counting to gsm8k eval (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28594)</li>
<li>[Minor] avoid register new custom and just import silly_attn (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #28578)</li>
<li>[Misc] fix comment in test_envs (@xingliu14 #28529)</li>
<li>[feat]: log number of preempted requests (@610lyn #28522)</li>
<li>[Frontend] Added chat-style multimodal support to /classify. (@WorldExplored #27516)</li>
<li>[Model][MM] Extract conv layer as CustomOp (@shen-shanshan #28455)</li>
<li>[DCP] Support Decode Context Parallel (DCP) for GQA with Flashinfer (@gjc0824 #25438)</li>
<li>Fix KV sharing fast prefill with cudagraph enabled (@sarckk #28537)</li>
<li>[BugFix] Fix FA3 IMA with FULL_AND_PIECEWISE and cascade attention (default) (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28702)</li>
<li>[Doc] Fix macOS installation dependency resolution issue (@shahfasal #26721)</li>
<li>[Model] Fix bailing_moe accuracy problem (@zhaozx-cn #28277)</li>
<li>[Bugfix][Nixl] Fix kernel physical&lt;&gt;logical block_size issue  (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28677)</li>
<li>[Config] Clean up SchedulerConfig initialization (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28665)</li>
<li>[Kernels] Enable FlashInfer FP8 Blockscale on SM90 (for TEP DSR1) (@djmmoss #27134)</li>
<li>[Fix] improve aspect ratio in dummy image generation and add common  VLM tests for PaddleOCR-VL (@dongbo910220 #28711)</li>
<li>[Docs] Update the name of <code>Transformers backend</code> -&gt; <code>Transformers modeling backend</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28725)</li>
<li>[CI][CPU] Smoke test for Apple Silicon using GHA MacOS runner (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28688)</li>
<li>[DisaggEverything] Tokens in&lt;&gt;out <code>/generate</code> endpoint (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #24261)</li>
<li>[Attention] Bump FA for removed method (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28429)</li>
<li>Fix typo in comment: existance -&gt; existence (@OthmanMohammad #28737)</li>
<li>Remove audio optional dependency for mistral-common (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #28722)</li>
<li>[kernel] Improve FP8 PTPC on Hopper for larger shapes (@czhu-cohere #28692)</li>
<li>docs(lora_resolvers): clarify multi-resolver order and storage path requirement (@wangchen615 #28153)</li>
<li>LLaMA4 LoRA Adapter Enablement (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> #28602)</li>
<li>[Bugfix] [ROCm] [AITER]: Fix aiter block quant not compatible with torch compile dynamo (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #28716)</li>
<li>[Docs] Enable some more markdown lint rules for the docs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28731)</li>
<li>[Chore] Rename <code>SchedulerConfig.chunked_prefill_enabled</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28735)</li>
<li>[Bugfix] resolve Qwen3-VL GPTQModel quantized model loading failure (@GuanH #28663)</li>
<li>[BugFix] Fix misprint introduced by modular_kernel refactoring. (@halyavin #28728)</li>
<li>[ROCm][Bugfix] Fix compilation errors with fused_qknorm_rope_kernel.cu (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28682)</li>
<li>[CI] Fix macos smoke test uv cache issue (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28736)</li>
<li>[Bugfix] TypeError: 'NoneType' object is not callable (@mostrowskix #27410)</li>
<li>[ROCm][CI/Build] Change install location of uv (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #28741)</li>
<li>Avoid bytecode hook and simplify TorchCompileWrapperWithCustomDipatch (@laithsakka #25110)</li>
<li>[Bugfix] Fix incorrect use of hidden_states for shared_experts due to do_naive_dispatch_combine (@alexm-redhat #28740)</li>
<li>[Bugfix] Fix ChunkedLocalAttention CUDA Graph setting (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #28739)</li>
<li>[Hybrid] [Kernel] Fix chunk scan kernel when BLOCK_SIZE_DSTATE &gt; 128 (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> #28295)</li>
<li>[Log] Save profiler results to file instead of stdout (@rasmith #28144)</li>
<li>[ROCm][CI/Build] Upgrade to ROCm 7.1 and AITER main (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #28753)</li>
<li>[Test] Rework e2e async scheduling tests (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28744)</li>
<li>[Core] Performance: Use list[np.ndarray] instead of list[list[int]] for output tokens for GC optimization (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26368)</li>
<li>[TPU] Fix import error in tpu launch (@QiliangCui #28758)</li>
<li>[Model][Qwen3VL] Use <code>mm_position</code> to compute mrope positions (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28730)</li>
<li>[Bugfix] Build hadacore kernels on &gt;SM90 (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28748)</li>
<li>Revert "[Core] Performance: Use list[np.ndarray] instead of list[list‚Ä¶ (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28773)</li>
<li>Fix IntermediateTensors initialization and add type hints (@OthmanMohammad #28743)</li>
<li>[NIXL] heterogeneous block_size support (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #26759)</li>
<li>[Performance][DeepGEMM] Estimate expected_m (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28694)</li>
<li>[Redo] #26368 (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28771)</li>
<li>[RL] [V1] Remove unused device argument from reset_kv_cache (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28766)</li>
<li>Use narrow over indexing in <code>hadacore_transform</code> to prep for ABI stable (@janeyx99 #28756)</li>
<li>[Kernel][Moe Configs] llama4 maverick fp8 moe config tp8 on mi325 (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28709)</li>
<li>[Misc] Make <code>SchedulerConfig.max_model_len</code> init-only (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28733)</li>
<li>[PERF] Remove TRTLLM Gen attn kernel limitation <code>max_seq_len &lt;=131072</code> (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28755)</li>
<li>[compile] Enable sequence parallelism matching w/o custom ops enabled  (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #27126)</li>
<li>Allow Gemma3 to take image embeddings (@tingtingtangmeta #28483)</li>
<li>[Doc] Fix failing doc build (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28772)</li>
<li>[Model] Fix lmhead init bug of bailing_moe (@hwhaokun #28777)</li>
<li>Add support for Eagle with separate lm-head and embed_tokens layers (@eldarkurtic #28549)</li>
<li>[CI] Fix broken pipeline (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28781)</li>
<li>[Model][Qwen3VL] Cache positional embedding indices  (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28475)</li>
<li>[Doc]: fix typos in various files (@didier-durand #28567)</li>
<li>[BugFix] Fix <code>AssertionError: DCP not support reorder_batch_threshold &gt; 1 now.</code>  (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28751)</li>
<li>Adding a benchmark for batch invariance (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #28161)</li>
<li>[Benchmark] Fix client seed synchronization in multi-turn benchmark (@ai-jz #28512)</li>
<li>[Model] Allow users to control skip reading cache per request. (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #28194)</li>
<li>[V1] Support MP Executor for multi node distributed inference (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #23691)</li>
<li>Fixed gpt-oss _load_weights_other() parameter position bug (@River12 #28715)</li>
<li>[Bugfix] Fix host and port join for ipv6 in bench serve (@scottzh8 #28679)</li>
<li>Fix gpt oss weight loading with EP + bf16 (@ashors1 #28765)</li>
<li>[Doc]: fix typos in various files (@didier-durand #28811)</li>
<li>fix comment typo (@andyxning #28802)</li>
<li>[Model][QwenVL] Optimize <code>Qwen2_5_VisionAttention</code> q,k preparation (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28769)</li>
<li>Feature: Support Relu2 in FusedMoE fp8 cutlass path (@amirkl94 #27261)</li>
<li>[BugFix] Fix async scheduling + chunked prefill + preemption (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28787)</li>
<li>[Performance][Fix] update nvfp4 code to support renorm routing (@jiahanc #28569)</li>
<li>[NIXL][XPU] update install script of NIXL (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> #28778)</li>
<li>[ROCm][Qwen3-32B] Fix AITER MHA accuracy issue cause by #25763 (@sammysun0711 #28670)</li>
<li>[Bugfix][Model] Prevent special token leakage in KimiK2ToolParser streaming mode (@jscaldwell55 #28543)</li>
<li>[Doc] Add llama4 LoRA tag (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #28825)</li>
<li>[CPU][Bugfix] Fix _to_list in CPU model runner (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #28824)</li>
<li>[BugFix] Fix glm4_moe_mtp load weights bug (@wuyaoxuehun #28805)</li>
<li>[Metrics] Fix KV cache usage percent metric multiproc (@jaywonchung #28792)</li>
<li>[XPU] work around for sp, avoid custom op import error (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #28822)</li>
<li>[BugFix] Temporary fix for IMA with MTP = 2 and full-cg (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28315)</li>
<li>[Bugfix][Perf] Revert applying HF processor on text-only inputs for multimodal models  (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28858)</li>
<li>Cast return value to int64_t for cache size (@tiehexue #28814)</li>
<li>[Bugfix] Fix GPT-OSS on AMD after #28603 (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28816)</li>
<li>[Core] Async Scheduling X Spec Decoding Compatibility (@Ronald1995 #24799)</li>
<li>[BugFix] Fix PP performance and PP kv connector output regression  (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28768)</li>
<li>[Quantization] [Eagle] Add complete quantization support to the draft model in Eagle (@shreyas269 #28435)</li>
<li>[Test] Batch Invariant: Rename and organize tests (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27421)</li>
<li>[Model] Add Afmoe architecture implementation (@pranav4501 #28332)</li>
<li>[BugFix] Corner case that could cause out-of-sync with external launcher mode and dp &gt;1 (@bangshengtang #28774)</li>
<li>[Misc] Fix wrong comment in scheduler (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28880)</li>
<li>[Bugfix] Fix Kimi-K2 tool parser concatenated tool calls parsing (@bbartels #28831)</li>
<li>Run macos smoke test workflow on main commit (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28752)</li>
<li>[ROCm][Quantization] add apply_vllm_mapper in quark config for models like gpt-oss (@xuebwang-amd #28638)</li>
<li>[Refactor] Remove Unused Func in Batch Invariant (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28881)</li>
<li>[Bugfix] Fix wrong CLI defaults for dynamic <code>SchedulerConfig</code> fields (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28872)</li>
<li>[Doc]: fix typos in various files (@didier-durand #28863)</li>
<li>[Misc] Remove unnecessary parentheses from log statements (@andyxning #28897)</li>
<li>[CI] Fix async scheduling + spec decoding test flake (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28902)</li>
<li>[MISC] Remove format.sh (@KuntaiDu #28906)</li>
<li>[CI/Build] Replace wikipedia url with local server ones (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28908)</li>
<li>[BugFix] Fix PP/async scheduling with pooling models (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28899)</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25603">#25603</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Renovamen">@Renovamen</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25796">#25796</a></li>
<li><a class="user-mention notranslate" href="https://github.com/patrick-toulme">@patrick-toulme</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25084">#25084</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kingsmad">@kingsmad</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25825">#25825</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yingjun-mou">@yingjun-mou</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25827">#25827</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhoukezi">@zhoukezi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25854">#25854</a></li>
<li><a class="user-mention notranslate" href="https://github.com/leejnau">@leejnau</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25706">#25706</a></li>
<li><a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25513">#25513</a></li>
<li><a class="user-mention notranslate" href="https://github.com/acisseJZhong">@acisseJZhong</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25912">#25912</a></li>
<li><a class="user-mention notranslate" href="https://github.com/a120092009">@a120092009</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25942">#25942</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Anionex">@Anionex</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25354">#25354</a></li>
<li><a class="user-mention notranslate" href="https://github.com/DrStone1971">@DrStone1971</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25843">#25843</a></li>
<li><a class="user-mention notranslate" href="https://github.com/certainly-param">@certainly-param</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25935">#25935</a></li>
<li><a class="user-mention notranslate" href="https://github.com/natoscott">@natoscott</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26007">#26007</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kmaehashi">@kmaehashi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26005">#26005</a></li>
<li><a class="user-mention notranslate" href="https://github.com/leo-pony">@leo-pony</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25470">#25470</a></li>
<li><a class="user-mention notranslate" href="https://github.com/huijjj">@huijjj</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24947">#24947</a></li>
<li><a class="user-mention notranslate" href="https://github.com/levunet">@levunet</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24768">#24768</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Egor-Krivov">@Egor-Krivov</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25668">#25668</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sixiang-google">@sixiang-google</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25992">#25992</a></li>
<li><a class="user-mention notranslate" href="https://github.com/astralord">@astralord</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26027">#26027</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jasl">@jasl</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26098">#26098</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nrghosh">@nrghosh</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26148">#26148</a></li>
<li><a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25974">#25974</a></li>
<li><a class="user-mention notranslate" href="https://github.com/soldni">@soldni</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26054">#26054</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuafng">@yuafng</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26219">#26219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25823">#25823</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jasonlizhengjian">@jasonlizhengjian</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25998">#25998</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elieserr">@elieserr</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26177">#26177</a></li>
<li><a class="user-mention notranslate" href="https://github.com/orangeng">@orangeng</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26266">#26266</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ymoslem">@ymoslem</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26258">#26258</a></li>
<li><a class="user-mention notranslate" href="https://github.com/abhisheksheth28">@abhisheksheth28</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25521">#25521</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seven-mile">@seven-mile</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26231">#26231</a></li>
<li><a class="user-mention notranslate" href="https://github.com/cfRod">@cfRod</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26289">#26289</a></li>
<li><a class="user-mention notranslate" href="https://github.com/atalhens">@atalhens</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26265">#26265</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gholmes829">@gholmes829</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25164">#25164</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25945">#25945</a></li>
<li><a class="user-mention notranslate" href="https://github.com/antrec">@antrec</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26340">#26340</a></li>
<li><a class="user-mention notranslate" href="https://github.com/plliao">@plliao</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26325">#26325</a></li>
<li><a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26113">#26113</a></li>
<li><a class="user-mention notranslate" href="https://github.com/isharif168">@isharif168</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26347">#26347</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Barry-Delaney">@Barry-Delaney</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25931">#25931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/utkarshsharma1">@utkarshsharma1</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26279">#26279</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Aydin-ab">@Aydin-ab</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25283">#25283</a></li>
<li><a class="user-mention notranslate" href="https://github.com/therealnaveenkamal">@therealnaveenkamal</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25103">#25103</a></li>
<li><a class="user-mention notranslate" href="https://github.com/QierLi">@QierLi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24926">#24926</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24486">#24486</a></li>
<li><a class="user-mention notranslate" href="https://github.com/iwzbi">@iwzbi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/16601">#16601</a></li>
<li><a class="user-mention notranslate" href="https://github.com/roikoren755">@roikoren755</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25947">#25947</a></li>
<li><a class="user-mention notranslate" href="https://github.com/luis5tb">@luis5tb</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25593">#25593</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wangxiongts">@wangxiongts</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25550">#25550</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sangho-vision">@sangho-vision</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26563">#26563</a></li>
<li><a class="user-mention notranslate" href="https://github.com/muzian666">@muzian666</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26562">#26562</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HsChen-sys">@HsChen-sys</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22100">#22100</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FENP">@FENP</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26574">#26574</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gjgjos">@gjgjos</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26339">#26339</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andycandy">@andycandy</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26629">#26629</a></li>
<li><a class="user-mention notranslate" href="https://github.com/aitsvet">@aitsvet</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26713">#26713</a></li>
<li><a class="user-mention notranslate" href="https://github.com/cyb70289">@cyb70289</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26698">#26698</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26538">#26538</a></li>
<li><a class="user-mention notranslate" href="https://github.com/n1ck-guo">@n1ck-guo</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24024">#24024</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ryanli">@ryanli</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26758">#26758</a></li>
<li><a class="user-mention notranslate" href="https://github.com/VladOS95-cyber">@VladOS95-cyber</a> first commit is #26726</li>
<li>@zklapow first commit is #26818</li>
<li>@HDCharles first commit is #26820</li>
<li>@Dhruvilbhatt first commit is #26837</li>
<li>@madongfly first commit is #26853</li>
<li>@li2haipeng first commit is #26319</li>
<li>@pdasigi first commit is #26143</li>
<li>@cern1710 first commit is #26637</li>
<li>@inc-jeong first commit is #26225</li>
<li>@bogdanminko first commit is #27008</li>
<li>@mandy-li first commit is #26883</li>
<li>@kimbochen first commit is #26943</li>
<li>@staghado first commit is #26916</li>
<li>@rkarhila-amd first commit is #25586</li>
<li>@hyongtao-code first commit is #27101</li>
<li>@jianyuh first commit is #27159</li>
<li>@uyzhang first commit is #27012</li>
<li>@shivampr first commit is #26268</li>
<li>@helunwencser first commit is #26832</li>
<li>@dagrayvid first commit is #27196</li>
<li>@ExtReMLapin first commit is #27253</li>
<li>@ReinForce-II first commit is #26789</li>
<li>@LiuLi1998 first commit is #22627</li>
<li>@sagiahrac first commit is #27211</li>
<li>@fangpings first commit is #27133</li>
<li>@jonathanc-n first commit is #27372</li>
<li>@bradleyhd first commit is #27124</li>
<li>@Navya1707 first commit is #27156</li>
<li>@piood first commit is #27324</li>
<li>@xxxxyu first commit is #26092</li>
<li>@usberkeley first commit is #27419</li>
<li>@strinczer first commit is #26706</li>
<li>@hjh0119 first commit is #27469</li>
<li>@wpc first commit is #27328</li>
<li>@yeshsurya first commit is #27188</li>
<li>@rogeryoungh first commit is #27535</li>
<li>@dcmaddix first commit is #27291</li>
<li>@tingtingtangmeta first commit is #27538</li>
<li>@minatoaquaMK2 first commit is #27323</li>
<li>@wangln19 first commit is #27565</li>
<li>@junpuf first commit is #27596</li>
<li>@sammshen first commit is #27600</li>
<li>@mpashkovskii first commit is #26886</li>
<li>@KevinCheung2259 first commit is #27670</li>
<li>@sammysun0711 first commit is #27623</li>
<li>@dumb0002 first commit is #24176</li>
<li>@sairampillai first commit is #25775</li>
<li>@FlamingoPg first commit is #27794</li>
<li>@SumanthRH first commit is #27789</li>
<li>@PaulZhang12 first commit is #27660</li>
<li>@jakub-sochacki first commit is #26919</li>
<li>@RobMulla first commit is #27824</li>
<li>@yugong333 first commit is #27818</li>
<li>@ai-jz first commit is #27850</li>
<li>@xiaohajiayou first commit is #26779</li>
<li>@biswapanda first commit is #27728</li>
<li>@efimki first commit is #24905</li>
<li>@zhang-prog first commit is #27758</li>
<li>@xiangze-arm first commit is #27240</li>
<li>@yt0428 first commit is #27521</li>
<li>@ganyi1996ppo first commit is #25763</li>
<li>@nadavkluger first commit is #28048</li>
<li>@toulzx first commit is #27740</li>
<li>@frost-intel first commit is #28004</li>
<li>@jjzhang first commit is #28127</li>
<li>@walterbm first commit is #28075</li>
<li>@dayeol first commit is #22496</li>
<li>@cmpute first commit is #27780</li>
<li>@seungduk-yanolja first commit is #27946</li>
<li>@aditew01 first commit is #28130</li>
<li>@milpuz01 first commit is #26018</li>
<li>@StanHatko first commit is #27953</li>
<li>@vicoooo26 first commit is #27792</li>
<li>@HanFa first commit is #27497</li>
<li>@amacaskill first commit is #28079</li>
<li>@smitkadvani first commit is #28024</li>
<li>@xiaohongchen1991 first commit is #21068</li>
<li>@hammmmy first commit is #28308</li>
<li>@ashahba first commit is #28026</li>
<li>@zhangsicheng5 first commit is #26696</li>
<li>@evberrypi first commit is #28328</li>
<li>@ColeMurray first commit is #28337</li>
<li>@bo-ke first commit is #28374</li>
<li>@caozuoba first commit is #28280</li>
<li>@zhaozuy first commit is #27892</li>
<li>@maryamtahhan first commit is #28461</li>
<li>@the-codeboy first commit is #28474</li>
<li>@xuebwang-amd first commit is #24239</li>
<li>@Livinfly first commit is #28389</li>
<li>@AndreasKaratzas first commit is #27611</li>
<li>@wuyaoxuehun first commit is #27597</li>
<li>@ziruiliu first commit is #27978</li>
<li>@ZhengHongming888 first commit is #28356</li>
<li>@albertoperdomo2 first commit is #27039</li>
<li>@PerryZhang01 first commit is #27731</li>
<li>@Radu2k first commit is #28059</li>
<li>@tjandy98 first commit is #28210</li>
<li>@dw2761 first commit is #28615</li>
<li>@zufangzhu first commit is #28611</li>
<li>@amdfaa first commit is #28142</li>
<li>@YuanpingSong first commit is #28617</li>
<li>@janeyx99 first commit is #28595</li>
<li>@xingliu14 first commit is #28529</li>
<li>@610lyn first commit is #28522</li>
<li>@WorldExplored first commit is #27516</li>
<li>@gjc0824 first commit is #25438</li>
<li>@shahfasal first commit is #26721</li>
<li>@zhaozx-cn first commit is #28277</li>
<li>@OthmanMohammad first commit is #28737</li>
<li>@GuanH first commit is #28663</li>
<li>@halyavin first commit is #28728</li>
<li>@mostrowskix first commit is #27410</li>
<li>@laithsakka first commit is #25110</li>
<li>@hwhaokun first commit is #28777</li>
<li>@River12 first commit is #28715</li>
<li>@scottzh8 first commit is #28679</li>
<li>@ashors1 first commit is #28765</li>
<li>@jscaldwell55 first commit is #28543</li>
<li>@tiehexue first commit is #28814</li>
<li>@Ronald1995 first commit is #24799</li>
<li>@shreyas269 first commit is #28435</li>
<li>@pranav4501 first commit is #28332</li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.11.0...v0.11.1"><tt>v0.11.0...v0.11.1</tt></a></p>

---

## Operating Systems

### [[$] A high-level quality-of-service interface](https://lwn.net/Articles/1051982/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>
Quality-of-service (QoS) mechanisms attempt to prioritize some processes (or
network traffic, disk I/O, etc.) over others in order to meet a system's
performance goals. This is a difficult topic to handle in the world of Linux,
where workloads, hardware, and user expectations vary wildly. Qais Yousef spoke
at the 2025 Linux Plumbers Conference, alongside his collaborators John Stultz,
Steven Rostedt, and Vincent Guittot, about their plans for introducing a
high-level QoS API for Linux in a way that leaves end users in control of its
configuration. The talk focused specifically on a QoS mechanism for the
scheduler, to prioritize access to CPU resources differently for different kinds
of processes.
(<a href="https://lpc.events/event/19/contributions/2089/attachments/1797/3877/Userspace%20Assisted%20Scheduling%20via%20Sched%20QoS.pdf">slides</a>;
<a href="http://www.youtube.com/watch?v=zuOjTaVohm0">video</a>)
</p>

---

### [Firefox 147 released](https://lwn.net/Articles/1053995/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p><a href="https://www.firefox.com/en-US/firefox/147.0/releasenotes/">Version
147.0</a> of the Firefox web browser has been released. Notable
changes in this release include support for the <a href="https://specifications.freedesktop.org/basedir/latest/">XDG Base
Directory specification</a>, enabling <a href="https://support.mozilla.org/en-US/kb/control-personal-device-local-network-permissions-firefox">local
network access restrictions</a> for users with <a href="https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop">enhanced
tracking protection</a> (ETP) set to "Strict", and a fix that improves
Firefox's rendering with GNOME on fractionally scaled
displays. Firefox&#160;147 also includes a number of <a href="https://www.mozilla.org/en-US/security/advisories/mfsa2026-01/">security
fixes</a>, including several sandbox-escape vulnerabilities.</p>

<p></p>

---

### [Security updates for Tuesday](https://lwn.net/Articles/1053988/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Security updates have been issued by <b>AlmaLinux</b> (mariadb10.11, mariadb:10.11, mariadb:10.3, mariadb:10.5, and tar), <b>Debian</b> (net-snmp), <b>Fedora</b> (coturn, NetworkManager-l2tp, openssh, and tuxanci), <b>Mageia</b> (libtasn1), <b>Oracle</b> (buildah, cups, httpd, kernel, libpq, libsoup, libsoup3, mariadb:10.11, mariadb:10.3, openssl, and podman), <b>SUSE</b> (cpp-httplib, ImageMagick, libtasn1, python-cbor2, util-linux, valkey, and wget2), and <b>Ubuntu</b> (google-guest-agent, linux-iot, and python-urllib3).

---

## Systems

### [A data model for Git (and other docs updates)](https://jvns.ca/blog/2026/01/08/a-data-model-for-git/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! This past fall, I decided to take some time to work on Git&rsquo;s
documentation. I&rsquo;ve been thinking about working on open source docs for a long
time &ndash; usually if I think the documentation for something could be improved,
I&rsquo;ll write a blog post or a zine or something. But this time I wondered: could I
instead make a few improvements to the official documentation?</p>
<p>So <a href="https://marieflanagan.com/">Marie</a> and I made a few changes to the Git
documentation!</p>
<h3 id="a-data-model-for-git">a data model for Git</h3>
<p>After a while working on the documentation, we noticed that Git uses the terms
&ldquo;object&rdquo;, &ldquo;reference&rdquo;, or &ldquo;index&rdquo; in its documentation a lot, but that it didn&rsquo;t
have a great explanation of what those terms mean or how they relate to other
core concepts like &ldquo;commit&rdquo; and &ldquo;branch&rdquo;. So we wrote a new &ldquo;data model&rdquo; document!</p>
<p>You can <a href="https://github.com/git/git/blob/master/Documentation/gitdatamodel.adoc">read the data model here for now</a>.
I assume at some point (after the next release?) it&rsquo;ll also be on the <a href="https://git-scm.com">Git website</a>.</p>
<p>I&rsquo;m excited about this because understanding how Git organizes its commit and
branch data has really helped me reason about how Git works over the years,
and I think it&rsquo;s important to have a short (1600 words!) version of the data
model that&rsquo;s accurate.</p>
<p>The &ldquo;accurate&rdquo; part turned out to not be that easy: I knew the basics of how
Git&rsquo;s data model worked, but during the review process I learned some new
details and had to make quite a few changes (for example how merge conflicts are
stored in the staging area).</p>
<h3 id="updates-to-git-push-git-pull-and-more">updates to <code>git push</code>, <code>git pull</code>, and more</h3>
<p>I also worked on updating the introduction to some of Git&rsquo;s core man pages.
I quickly realized that &ldquo;just try to improve it according to my best judgement&rdquo;
was not going to work: why should the maintainers believe me that my version is
better?</p>
<p>I&rsquo;ve seen a problem a lot when discussing open source documentation changes
where 2 expert users of the software argue about whether an explanation
is clear or not (&ldquo;I think X would be a good way to explain it! Well, I think Y
would be better!&rdquo;)</p>
<p>I don&rsquo;t think this is very productive (expert users of a piece of software
are notoriously bad at being able to tell if an explanation will be clear to
non-experts), so I needed to find a way to identify problems with the man
pages that was a little more evidence-based.</p>
<h3 id="getting-test-readers-to-identify-problems">getting test readers to identify problems</h3>
<p>I asked for test readers on Mastodon to read the current version of
documentation and tell me what they find confusing or what questions they have.
About 80 test readers left comments, and I learned so much!</p>
<p>People left a huge amount of great feedback, for example:</p>
<ul>
<li>terminology they didn&rsquo;t understand (what&rsquo;s a pathspec? what does &ldquo;reference&rdquo; mean? does &ldquo;upstream&rdquo; have a specific meaning in Git?)</li>
<li>specific confusing sentences</li>
<li>suggestions of things things to add (&ldquo;I do X all the time, I think it should be included here&rdquo;)</li>
<li>inconsistencies (&ldquo;here it implies X is the default, but elsewhere it implies Y is the default&rdquo;)</li>
</ul>
<p>Most of the test readers had been using Git for at least 5-10 years, which
I think worked well &ndash; if a group of test readers who have been using Git
regularly for 5+ years find a sentence or term impossible to understand, it
makes it easy to argue that the documentation should be updated to make it
clearer.</p>
<p>I thought this &ldquo;get users of the software to comment on the existing
documentation and then fix the problems they find&rdquo; pattern worked really
well and I&rsquo;m excited about potentially trying it again in the future.</p>
<h3 id="the-man-page-changes">the man page changes</h3>
<p>We ended updating these 4 man pages:</p>
<ul>
<li><code>git add</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-add.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-add.adoc">after</a>)</li>
<li><code>git checkout</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-checkout.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-checkout.adoc">after</a>)</li>
<li><code>git push</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-push.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc">after</a>)</li>
<li><code>git pull</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-pull.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-pull.adoc">after</a>)</li>
</ul>
<p>The <code>git push</code> and <code>git pull</code> changes were the most interesting to me: in
addition to updating the intro to those pages, we also ended up writing:</p>
<ul>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/urls-remotes.adoc#upstream-branches">a section describing what the term &ldquo;upstream branch&rdquo; means</a> (which previously wasn&rsquo;t really explained)</li>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc#options">a cleaned-up description of what a &ldquo;push refspec&rdquo; is</a></li>
</ul>
<p>Making those changes really gave me an appreciation for how much work it is
to maintain open source documentation: it&rsquo;s not easy to write things that are
both clear and true, and sometimes we had to make compromises, for example the sentence
&ldquo;<code>git push</code> may fail if you haven‚Äôt set an upstream for the current branch,
depending on what <code>push.default</code> is set to.&rdquo; is a little vague, but the exact
details of what &ldquo;depending&rdquo; means are really complicated and untangling that is
a big project.</p>
<h3 id="on-the-process-for-contributing-to-git">on the process for contributing to Git</h3>
<p>It took me a while to understand Git&rsquo;s development process.
I&rsquo;m not going to try to describe it here (that could be a whole other post!), but a few quick notes:</p>
<ul>
<li>Git has a <a href="https://git-scm.com/community#discord">Discord server</a>
with a &ldquo;my first contribution&rdquo; channel for help with getting started contributing.
I found people to be very welcoming on the Discord.</li>
<li>I used <a href="https://gitgitgadget.github.io/">GitGitGadget</a> to make all of my contributions.
This meant that I could make a GitHub pull request (a workflow I&rsquo;m comfortable
with) and GitGitGadget would convert my PRs into the system the Git developers
use (emails with patches attached). GitGitGadget worked great and I was very
grateful to not have to learn how to send patches by email with Git.</li>
<li>Otherwise I used my normal email client (Fastmail&rsquo;s web interface) to reply
to emails, wrapping my text to 80 character lines since that&rsquo;s the mailing
list norm.</li>
</ul>
<p>I also found the mailing list archives on <a href="https://lore.kernel.org/git/">lore.kernel.org</a>
hard to navigate, so I hacked together <a href="https://github.com/jvns/git-list-viewer">my own git list viewer</a>
to make it easier to read the long mailing list threads.</p>
<p>Many people helped me navigate the contribution process and review the changes:
thanks to Emily Shaffer, Johannes Schindelin (the author of GitGitGadget),
Patrick Steinhardt, Ben Knoble, Junio Hamano, and more.</p>
<p>(I&rsquo;m experimenting with <a href="https://comments.jvns.ca/post/115861337435768520">comments on Mastodon, you can see the comments here</a>)</p>

---

### [Notes on switching to Helix from vim](https://jvns.ca/blog/2025/10/10/notes-on-switching-to-helix-from-vim/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Earlier this summer I was talking to a friend about how much I
<a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">love using fish</a>, and
how I love that I don&rsquo;t have to configure it. They said that they feel the same
way about the <a href="https://helix-editor.com/">helix</a> text editor, and so I decided
to give it a try.</p>
<p>I&rsquo;ve been using it for 3 months now and here are a few notes.</p>
<h3 id="why-helix-language-servers">why helix: language servers</h3>
<p>I think what motivated me to try Helix is that I&rsquo;ve been trying to get a working
language server setup (so I can do things like &ldquo;go to definition&rdquo;) and getting
a setup that feels good in Vim or Neovim just felt like too much work.</p>
<p>After using Vim/Neovim for 20 years, I&rsquo;ve tried both &ldquo;build my own custom
configuration from scratch&rdquo; and &ldquo;use someone else&rsquo;s pre-buld configuration
system&rdquo; and even though I love Vim I was excited about having things just work
without having to work on my configuration at all.</p>
<p>Helix comes with built in language server support, and it feels nice
to be able to do things like &ldquo;rename this symbol&rdquo; in any language.</p>
<h3 id="the-search-is-great">the search is great</h3>
<p>One of my favourite things about Helix is the search! If I&rsquo;m searching all the
files in my repository for a string, it lets me scroll through the potential
matching files and see the full context of the match, like this:</p>
<img src="https://jvns.ca/images/helix-search.png" />
<p>For comparison, here&rsquo;s what the vim ripgrep plugin I&rsquo;ve been using looks like:</p>
<img src="https://jvns.ca/images/vim-ripgrep.png" />
<p>There&rsquo;s no context for what else is around that line.</p>
<h3 id="the-quick-reference-is-nice">the quick reference is nice</h3>
<p>One thing I like about Helix is that when I press <code>g</code>, I get a little help popup
telling me places I can go. I really appreciate this because I don&rsquo;t often use
the &ldquo;go to definition&rdquo; or &ldquo;go to reference&rdquo; feature and I often forget the
keyboard shortcut.</p>
<img src="https://jvns.ca/images/goto.png" width="300px" />
<h3 id="some-vim-helix-translations">some vim -&gt; helix translations</h3>
<ul>
<li>Helix doesn&rsquo;t have marks like <code>ma</code>, <code>'a</code>, instead I&rsquo;ve been using <code>Ctrl+O</code> and
<code>Ctrl+I</code> to go back (or forward) to the last cursor location</li>
<li>I think Helix does have macros, but I&rsquo;ve been using multiple cursors in every
case that I would have previously used a macro. I like multiple cursors a lot
more than writing macros all the time. If I want to batch change something in
the document, my workflow is to press <code>%</code> (to highlight everything), then <code>s</code>
to select (with a regex) the things I want to change, then I can just edit
all of them as needed.</li>
<li>Helix doesn&rsquo;t have neovim-style tabs, instead it has a nice buffer switcher (<code>&lt;space&gt;b</code>)
I can use to switch to the buffer I want. There&rsquo;s a
<a href="https://github.com/helix-editor/helix/pull/7109">pull request here</a> to implement neovim-style tabs.
There&rsquo;s also a setting <code>bufferline=&quot;multiple&quot;</code> which can act a bit like tabs
with <code>gp</code>, <code>gn</code> for prev/next &ldquo;tab&rdquo; and <code>:bc</code> to close a &ldquo;tab&rdquo;.</li>
</ul>
<h3 id="some-helix-annoyances">some helix annoyances</h3>
<p>Here&rsquo;s everything that&rsquo;s annoyed me about Helix so far.</p>
<ul>
<li>I like the way Helix&rsquo;s <code>:reflow</code> works much less than how
vim reflows text with <code>gq</code>. It doesn&rsquo;t work as well with lists. (<a href="https://github.com/helix-editor/helix/issues/3332">github issue</a>)</li>
<li>If I&rsquo;m making a Markdown list, pressing &ldquo;enter&rdquo; at the end of a list item
won&rsquo;t continue the list. There&rsquo;s a <a href="https://github.com/helix-editor/helix/wiki/Recipes#continue-markdown-lists--quotes">partial workaround</a>
for bulleted lists but I don&rsquo;t know one for numbered lists.</li>
<li>No persistent undo yet: in vim I could use an
<a href="https://vimdoc.sourceforge.net/htmldoc/options.html#'undofile'">undofile</a> so
that I could undo changes even after quitting. Helix doesn&rsquo;t have that feature yet.
(<a href="https://github.com/helix-editor/helix/pull/9154">github PR</a>)</li>
<li>Helix doesn&rsquo;t autoreload files after they change on disk, I have to run
<code>:reload-all</code> (<code>:ra&lt;tab&gt;</code>) to manually reload them. Not a big deal.</li>
<li>Sometimes it crashes, maybe every week or so. I think it might be
<a href="https://github.com/helix-editor/helix/issues/12582">this issue</a>.</li>
</ul>
<p>The &ldquo;markdown list&rdquo; and reflowing issues come up a lot for me because I spend
a lot of time editing Markdown lists, but I keep using Helix anyway so I guess
they can&rsquo;t be making me that mad.</p>
<h3 id="switching-was-easier-than-i-thought">switching was easier than I thought</h3>
<p>I was worried that relearning 20 years of Vim muscle memory would be really hard.</p>
<p>It turned out to be easier than I expected, I started using Helix on a
vacation for a little low-stakes coding project I was doing on the side and
after a week or two it didn&rsquo;t feel so disorienting anymore. I think it might be
hard to switch back and forth between Vim and Helix, but I haven&rsquo;t needed to use
Vim recently so I don&rsquo;t know if that&rsquo;ll ever become an issue for me.</p>
<p>The first time I tried Helix I tried to force it to use keybindings that were
more similar to Vim and that did not work for me. Just learning the &ldquo;Helix way&rdquo;
was a lot easier.</p>
<p>There are still some things that throw me off: for example <code>w</code> in vim and <code>w</code> in
Helix don&rsquo;t have the same idea of what a &ldquo;word&rdquo; is (the Helix one includes the
space after the word, the Vim one doesn&rsquo;t).</p>
<h3 id="using-a-terminal-based-text-editor">using a terminal-based text editor</h3>
<p>For many years I&rsquo;d mostly been using a GUI version of vim/neovim, so switching
to actually using an editor in the terminal was a bit of an adjustment.</p>
<p>I ended up deciding on:</p>
<ol>
<li>Every project gets its own terminal window, and all of the tabs in that
window (mostly) have the same working directory</li>
<li>I make my Helix tab the first tab in the terminal window</li>
</ol>
<p>It works pretty well, I might actually like it better than my previous workflow.</p>
<h3 id="my-configuration">my configuration</h3>
<p>I appreciate that my configuration is really simple, compared to my neovim
configuration which is hundreds of lines. It&rsquo;s mostly just 4 keyboard
shortcuts.</p>
<pre><code>theme = &quot;solarized_light&quot;
[editor]
# Sync clipboard with system clipboard
default-yank-register = &quot;+&quot;

[keys.normal]
# I didn't like that Ctrl+C was the default &quot;toggle comments&quot; shortcut
&quot;#&quot; = &quot;toggle_comments&quot;

# I didn't feel like learning a different way
# to go to the beginning/end of a line so
# I remapped ^ and $
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.select]
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.normal.space]
# I write a lot of text so I need to constantly reflow,
# and missed vim's `gq` shortcut
l = &quot;:reflow&quot;
</code></pre>
<p>There&rsquo;s a separate <code>languages.toml</code> configuration where I set some language
preferences, like turning off autoformatting.
For example, here&rsquo;s my Python configuration:</p>
<pre><code>[[language]]
name = &quot;python&quot;
formatter = { command = &quot;black&quot;, args = [&quot;--stdin-filename&quot;, &quot;%{buffer_name}&quot;, &quot;-&quot;] }
language-servers = [&quot;pyright&quot;]
auto-format = false
</code></pre>
<h3 id="we-ll-see-how-it-goes">we&rsquo;ll see how it goes</h3>
<p>Three months is not that long, and it&rsquo;s possible that I&rsquo;ll decide to go back
to Vim at some point. For example, I wrote a <a href="https://jvns.ca/blog/2023/02/28/some-notes-on-using-nix/">post about switching to
nix</a> a while back but
after maybe 8 months I switched back to Homebrew (though I&rsquo;m still using NixOS
to manage one little server, and I&rsquo;m still satisfied with that).</p>

---

### [New zine: The Secret Rules of the Terminal](https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! After many months of writing deep dive blog posts about the terminal, on
Tuesday I released a new zine called &ldquo;The Secret Rules of the Terminal&rdquo;!</p>
<p>You can get it for $12 here:
<a href="https://wizardzines.com/zines/terminal">https://wizardzines.com/zines/terminal</a>, or get
an <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</p>
<p>Here&rsquo;s the cover:</p>
<div align="center">
<a href="https://wizardzines.com/zines/terminal">
  <img src="https://jvns.ca/images/terminal-cover-small.jpg" width="600px" />
  </a>
</div>
<h3 id="the-table-of-contents">the table of contents</h3>
<p>Here&rsquo;s the table of contents:</p>
<a href="https://wizardzines.com/zines/terminal/toc.png">
  <img src="https://jvns.ca/images/terminal-toc-small.png" width="600px" />
</a>
<h3 id="why-the-terminal">why the terminal?</h3>
<p>I&rsquo;ve been using the terminal every day for 20 years but even though I&rsquo;m very
confident in the terminal, I&rsquo;ve always had a bit of an uneasy feeling about it.
Usually things work fine, but sometimes something goes wrong and it just feels
like investigating it is impossible, or at least like it would open up a huge
can of worms.</p>
<p>So I started trying to write down a list of weird problems I&rsquo;ve run into in terminal and I realized
that the terminal has a lot of tiny inconsistencies like:</p>
<ul>
<li>sometimes you can use the arrow keys to move around, but sometimes pressing the arrow keys just prints <code>^[[D</code></li>
<li>sometimes you can use the mouse to select text, but sometimes you can&rsquo;t</li>
<li>sometimes your commands get saved to a history when you run them, and sometimes they don&rsquo;t</li>
<li>some shells let you use the up arrow to see the previous command, and some don&rsquo;t</li>
</ul>
<p>If you use the terminal daily for 10 or 20 years, even if you don&rsquo;t understand
exactly <em>why</em> these things happen, you&rsquo;ll probably build an intuition for them.</p>
<p>But having an intuition for them isn&rsquo;t the same as understanding why they
happen. When writing this zine I actually had to do a lot of work to figure out
exactly what was <em>happening</em> in the terminal to be able to talk about how to
reason about it.</p>
<h3 id="the-rules-aren-t-written-down-anywhere">the rules aren&rsquo;t written down anywhere</h3>
<p>It turns out that the &ldquo;rules&rdquo; for how the terminal works (how do
you edit a command you type in? how do you quit a program? how do you fix your
colours?) are extremely hard to fully understand, because &ldquo;the terminal&rdquo; is actually
made of many different pieces of software (your terminal emulator, your
operating system, your shell, the core utilities like <code>grep</code>, and every other random
terminal program you&rsquo;ve installed) which are written by different people with different
ideas about how things should work.</p>
<p>So I wanted to write something that would explain:</p>
<ul>
<li>how the 4 pieces of the terminal (your shell, terminal emulator, programs, and TTY driver) fit together to make everything work</li>
<li>some of the core conventions for how you can expect things in your terminal to work</li>
<li>lots of tips and tricks for how to use terminal programs</li>
</ul>
<h3 id="this-zine-explains-the-most-useful-parts-of-terminal-internals">this zine explains the most useful parts of terminal internals</h3>
<p>Terminal internals are a mess. A lot of it is just the way it is because
someone made a decision in the 80s and now it&rsquo;s impossible to change, and
honestly I don&rsquo;t think learning everything about terminal internals is worth
it.</p>
<p>But some parts are not that hard to understand and can really make your
experience in the terminal better, like:</p>
<ul>
<li>if you understand what <strong>your shell</strong> is responsible for, you can configure your shell (or use a different one!) to access your history more easily, get great tab completion, and so much more</li>
<li>if you understand <strong>escape codes</strong>, it&rsquo;s much less scary when <code>cat</code>ing a binary to stdout messes up your terminal, you can just type <code>reset</code> and move on</li>
<li>if you understand how <strong>colour</strong> works, you can get rid of bad colour contrast in your terminal so you can actually read the text</li>
</ul>
<h3 id="i-learned-a-surprising-amount-writing-this-zine">I learned a surprising amount writing this zine</h3>
<p>When I wrote <a href="https://wizardzines.com/zines/git">How Git Works</a>, I thought I
knew how Git worked, and I was right. But the terminal is different. Even
though I feel totally confident in the terminal and even though I&rsquo;ve used it
every day for 20 years, I had a lot of misunderstandings about how the terminal
works and (unless you&rsquo;re the author of <code>tmux</code> or something) I think there&rsquo;s a
good chance you do too.</p>
<p>A few things I learned that are actually useful to me:</p>
<ul>
<li>I understand the structure of the terminal better and so I feel more
confident debugging weird terminal stuff that happens to me (I was even able
to suggest a <a href="https://github.com/fish-shell/fish-shell/issues/10834">small improvement</a> to fish!). Identifying exactly which piece of software is causing a weird thing to happen in my terminal still isn&rsquo;t <em>easy</em> but I&rsquo;m a lot better at it now.</li>
<li>you can write a shell script to <a href="https://jvns.ca/til/vim-osc52/">copy to your clipboard over SSH</a></li>
<li>how <code>reset</code> works under the hood (it does the equivalent of <code>stty sane; sleep 1; tput reset</code>) ‚Äì basically I learned that I don&rsquo;t ever need to worry about
remembering <code>stty sane</code> or <code>tput reset</code> and I can just run <code>reset</code> instead</li>
<li>how to look at the invisible escape codes that a program is printing out (run <code>unbuffer program &gt; out; less out</code>)</li>
<li>why the builtin REPLs on my Mac like <code>sqlite3</code> are so annoying to use (they use <code>libedit</code> instead of <code>readline</code>)</li>
</ul>
<h3 id="blog-posts-i-wrote-along-the-way">blog posts I wrote along the way</h3>
<p>As usual these days I wrote a bunch of blog posts about various side quests:</p>
<ul>
<li><a href="https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/">How to add a directory to your PATH</a></li>
<li><a href="https://jvns.ca/blog/2024/11/26/terminal-rules/">&ldquo;rules&rdquo; that terminal problems follow</a></li>
<li><a href="https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/">why pipes sometimes get &ldquo;stuck&rdquo;: buffering</a></li>
<li><a href="https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/">some terminal frustrations</a></li>
<li><a href="https://jvns.ca/blog/2024/10/31/ascii-control-characters/">ASCII control characters in my terminal</a> on &ldquo;what&rsquo;s the deal with Ctrl+A, Ctrl+B, Ctrl+C, etc?&rdquo;</li>
<li><a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a></li>
<li><a href="https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/">what&rsquo;s involved in getting a &ldquo;modern&rdquo; terminal setup?</a></li>
<li><a href="https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/">reasons to use your shell&rsquo;s job control</a></li>
<li><a href="https://jvns.ca/blog/2025/03/07/escape-code-standards/">standards for ANSI escape codes</a>, which is really me trying to figure out if I think the <code>terminfo</code> database is serving us well today</li>
</ul>
<h3 id="people-who-helped-with-this-zine">people who helped with this zine</h3>
<p>A long time ago I used to write zines mostly by myself but with every project I get more
and more help. I met with <a href="https://marieflanagan.com">Marie Claire LeBlanc Flanagan</a> every weekday from September to June to work
on this one.</p>
<p>The cover is by Vladimir Ka≈°ikoviƒá,
Lesley Trites did copy editing,
Simon Tatham (who wrote <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a>) did technical review, our
Operations Manager Lee did the transcription as well as a million other
things, and <a href="https://github.com/doy">Jesse Luehrs</a> (who is one of the very few
people I know who actually understands the terminal&rsquo;s cursed inner workings)
had so many incredibly helpful conversations with me about what is going on in
the terminal.</p>
<h3 id="get-the-zine">get the zine</h3>
<p>Here are some links to get the zine again:</p>
<ul>
<li>get <a href="https://wizardzines.com/zines/terminal">The Secret Rules of the Terminal</a></li>
<li>get a <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</li>
</ul>
<p>As always, you can get either a PDF version to print at home or a print version
shipped to your house. The only caveat is print orders will ship in <strong>August</strong> &ndash; I
need to wait for orders to come in to get an idea of how many I should print
before sending it to the printer.</p>

---

### [Using `make` to compile C programs (for non-C-programmers)](https://jvns.ca/blog/2025/06/10/how-to-compile-a-c-program/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I have never been a C programmer but every so often I need to compile a C/C++
program from source. This has been kind of a struggle for me: for a
long time, my approach was basically &ldquo;install the dependencies, run <code>make</code>, if
it doesn&rsquo;t work, either try to find a binary someone has compiled or give up&rdquo;.</p>
<p>&ldquo;Hope someone else has compiled it&rdquo; worked pretty well when I was running Linux
but since I&rsquo;ve been using a Mac for the last couple of years I&rsquo;ve been running
into more situations where I have to actually compile programs myself.</p>
<p>So let&rsquo;s talk about what you might have to do to compile a C program! I&rsquo;ll use
a couple of examples of specific C programs I&rsquo;ve compiled and talk about a few
things that can go wrong. Here are three programs we&rsquo;ll be talking about
compiling:</p>
<ul>
<li><a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a></li>
<li><a href="https://www.sqlite.org/download.html">sqlite</a></li>
<li><a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> (a pager you can run to quickly open files from a search with <code>rg -n THING | qf</code>)</li>
</ul>
<h3 id="step-1-install-a-c-compiler">step 1: install a C compiler</h3>
<p>This is pretty simple: on an Ubuntu system if I don&rsquo;t already have a C compiler I&rsquo;ll install one with:</p>
<pre><code>sudo apt-get install build-essential
</code></pre>
<p>This installs <code>gcc</code>, <code>g++</code>, and <code>make</code>. The situation on a Mac is more
confusing but it&rsquo;s something like &ldquo;install xcode command line tools&rdquo;.</p>
<h3 id="step-2-install-the-program-s-dependencies">step 2: install the program&rsquo;s dependencies</h3>
<p>Unlike some newer programming languages, C doesn&rsquo;t have a dependency manager.
So if a program has any dependencies, you need to hunt them down yourself.
Thankfully because of this, C programmers usually keep their dependencies very
minimal and often the dependencies will be available in whatever package manager you&rsquo;re using.</p>
<p>There&rsquo;s almost always a section explaining how to get the dependencies in the
README, for example in <a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a>&rsquo;s README, it
says:</p>
<blockquote>
<p>To compile PaperJam, you need the headers for the libqpdf and libpaper libraries (usually available as libqpdf-dev and libpaper-dev packages).</p>
</blockquote>
<blockquote>
<p>You may need <code>a2x</code> (found in <a href="http://www.methods.co.nz/asciidoc/a2x.1.html">AsciiDoc</a>) for building manual pages.</p>
</blockquote>
<p>So on a Debian-based system you can install the dependencies like this.</p>
<pre><code>sudo apt install -y libqpdf-dev libpaper-dev
</code></pre>
<p>If a README gives a name for a package (like <code>libqpdf-dev</code>), I&rsquo;d basically
always assume that they mean &ldquo;in a Debian-based Linux distro&rdquo;: if you&rsquo;re on a
Mac <code>brew install libqpdf-dev</code> will not work. I still have not 100% gotten
the hang of developing on a Mac yet so I don&rsquo;t have many tips there yet. I
guess in this case it would be <code>brew install qpdf</code> if you&rsquo;re using Homebrew.</p>
<h3 id="step-3-run-configure-if-needed">step 3: run <code>./configure</code> (if needed)</h3>
<p>Some C programs come with a <code>Makefile</code> and some instead come with a script called
<code>./configure</code>. For example, if you download <a href="https://www.sqlite.org/download.html">sqlite&rsquo;s source code</a>, it has a <code>./configure</code> script in
it instead of a Makefile.</p>
<p>My understanding of this <code>./configure</code> script is:</p>
<ol>
<li>You run it, it prints out a lot of somewhat inscrutable output, and then it
either generates a <code>Makefile</code> or fails because you&rsquo;re missing some
dependency</li>
<li>The <code>./configure</code> script is part of a system called
<a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">autotools</a>
that I have never needed to learn anything about beyond &ldquo;run it to generate
a <code>Makefile</code>&rdquo;.</li>
</ol>
<p>I think there might be some options you can pass to get the <code>./configure</code>
script to produce a different <code>Makefile</code> but I have never done that.</p>
<h3 id="step-4-run-make">step 4: run <code>make</code></h3>
<p>The next step is to run <code>make</code> to try to build a program. Some notes about
<code>make</code>:</p>
<ul>
<li>Sometimes you can run <code>make -j8</code> to parallelize the build and make it go
faster</li>
<li>It usually prints out a million compiler warnings when compiling the program.
I always just ignore them. I didn&rsquo;t write the software! The compiler warnings
are not my problem.</li>
</ul>
<h3 id="compiler-errors-are-often-dependency-problems">compiler errors are often dependency problems</h3>
<p>Here&rsquo;s an error I got while compiling <code>paperjam</code> on my Mac:</p>
<pre><code>/opt/homebrew/Cellar/qpdf/12.0.0/include/qpdf/InputSource.hh:85:19: error: function definition does not declare parameters
   85 |     qpdf_offset_t last_offset{0};
      |                   ^
</code></pre>
<p>Over the years I&rsquo;ve learned it&rsquo;s usually best not to overthink problems like
this: if it&rsquo;s talking about <code>qpdf</code>, there&rsquo;s a good change it just means that
I&rsquo;ve done something wrong with how I&rsquo;m including the <code>qpdf</code> dependency.</p>
<p>Now let&rsquo;s talk about some ways to get the <code>qpdf</code> dependency included in the right way.</p>
<h3 id="the-world-s-shortest-introduction-to-the-compiler-and-linker">the world&rsquo;s shortest introduction to the compiler and linker</h3>
<p>Before we talk about how to fix dependency problems: building C programs is split into 2
steps:</p>
<ol>
<li><strong>Compiling</strong> the code into <strong>object files</strong> (with <code>gcc</code> or <code>clang</code>)</li>
<li><strong>Linking</strong> those object files into a final binary (with <code>ld</code>)</li>
</ol>
<p>It&rsquo;s important to know this when building a C program because sometimes you
need to pass the right flags to the compiler and linker to tell them where to
find the dependencies for the program you&rsquo;re compiling.</p>
<h3 id="make-uses-environment-variables-to-configure-the-compiler-and-linker"><code>make</code> uses environment variables to configure the compiler and linker</h3>
<p>If I run <code>make</code> on my Mac to install <code>paperjam</code>, I get this error:</p>
<pre><code>c++ -o paperjam paperjam.o pdf-tools.o parse.o cmds.o pdf.o -lqpdf -lpaper
ld: library 'qpdf' not found
</code></pre>
<p>This is not because <code>qpdf</code> is not installed on my system (it actually is!). But
the compiler and linker don&rsquo;t know how to <em>find</em> the <code>qpdf</code> library. To fix this, we need to:</p>
<ul>
<li>pass <code>&quot;-I/opt/homebrew/include&quot;</code> to the compiler (to tell it where to find the header files)</li>
<li>pass <code>&quot;-L/opt/homebrew/lib -liconv&quot;</code> to the linker (to tell it where to find library files and to link in <code>iconv</code>)</li>
</ul>
<p>And we can get <code>make</code> to pass those extra parameters to the compiler and linker using environment variables!
To see how this works: inside <code>paperjam</code>&rsquo;s Makefile you can see a bunch of environment variables, like <code>LDLIBS</code> here:</p>
<pre><code>paperjam: $(OBJS)
	$(LD) -o $@ $^ $(LDLIBS)
</code></pre>
<p>Everything you put into the <code>LDLIBS</code> environment variable gets passed to the
linker (<code>ld</code>) as a command line argument.</p>
<h3 id="secret-environment-variable-cppflags">secret environment variable: <code>CPPFLAGS</code></h3>
<p><code>Makefiles</code> sometimes define their own environment variables that they pass to
the compiler/linker, but <code>make</code> also has a bunch of &ldquo;implicit&rdquo; environment
variables which it will automatically pass to the C compiler and linker. There&rsquo;s a <a href="https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html#index-CFLAGS0">full list of implicit environment variables here</a>,
but one of them is <code>CPPFLAGS</code>, which gets automatically passed to the C compiler.</p>
<p>(technically it would be more normal to use <code>CXXFLAGS</code> for this, but this
particular <code>Makefile</code> hardcodes <code>CXXFLAGS</code> so setting <code>CPPFLAGS</code> was the only
way I could find to set the compiler flags without editing the <code>Makefile</code>)</p>
<small>
As an aside: it took me a long time to realize how closely tied to C/C++ `make` is -- I used
to think that `make` was just a general build system (and of course you can use it for
anything!) but it has a lot of affordances for building C/C++ programs that it
doesn't have for building any other kind of program.
</small>
<h3 id="two-ways-to-pass-environment-variables-to-make">two ways to pass environment variables to <code>make</code></h3>
<p>I learned thanks to <a href="https://www.owlfolio.org/">@zwol</a> that there are actually two ways to pass environment variables to <code>make</code>:</p>
<ol>
<li><code>CXXFLAGS=xyz make</code> (the usual way)</li>
<li><code>make CXXFLAGS=xyz</code></li>
</ol>
<p>The difference between them is that <code>make CXXFLAGS=xyz</code> will override the
value of <code>CXXFLAGS</code> set in the <code>Makefile</code> but <code>CXXFLAGS=xyz make</code> won&rsquo;t.</p>
<p>I&rsquo;m not sure which way is the norm but I&rsquo;m going to use the first way in this
post.</p>
<h3 id="how-to-use-cppflags-and-ldlibs-to-fix-this-compiler-error">how to use <code>CPPFLAGS</code> and <code>LDLIBS</code> to fix this compiler error</h3>
<p>Now that we&rsquo;ve talked about how <code>CPPFLAGS</code> and <code>LDLIBS</code> get passed to the
compiler and linker, here&rsquo;s the final incantation that I used to get the
program to build successfully!</p>
<pre><code>CPPFLAGS=&quot;-I/opt/homebrew/include&quot; LDLIBS=&quot;-L/opt/homebrew/lib -liconv&quot; make paperjam
</code></pre>
<p>This passes <code>-I/opt/homebrew/include</code> to the compiler and <code>-L/opt/homebrew/lib -liconv</code> to the linker.</p>
<p>Also I don&rsquo;t want to pretend that I &ldquo;magically&rdquo; knew that those were the right
arguments to pass, figuring them out involved a bunch of confused Googling that I
skipped over in this post. I will say that:</p>
<ul>
<li>the <code>-I</code> compiler flag tells the compiler which directory to find header files in, like <code>/opt/homebrew/include/qpdf/QPDF.hh</code></li>
<li>the <code>-L</code> linker flag tells the linker which directory to find libraries in, like <code>/opt/homebrew/lib/libqpdf.a</code></li>
<li>the <code>-l</code> linker flag tells the linker which libraries to link in, like <code>-liconv</code> means &ldquo;link in the <code>iconv</code> library&rdquo;, or <code>-lm</code> means &ldquo;link <code>math</code>&rdquo;</li>
</ul>
<h3 id="tip-how-to-just-build-1-specific-file-make-filename">tip: how to just build 1 specific file: <code>make $FILENAME</code></h3>
<p>Yesterday I discovered this cool tool called
<a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> which you can use to quickly
open files from the output of <code>ripgrep</code>.</p>
<p><code>qf</code> is in a big directory of various tools, but I only wanted to compile <code>qf</code>.
So I just compiled <code>qf</code>, like this:</p>
<pre><code>make qf
</code></pre>
<p>Basically if you know (or can guess) the output filename of the file you&rsquo;re
trying to build, you can tell <code>make</code> to just build that file by running <code>make $FILENAME</code></p>
<h3 id="tip-you-don-t-need-a-makefile">tip: you don&rsquo;t need a Makefile</h3>
<p>I sometimes write 5-line C programs with no dependencies, and I just learned
that if I have a file called <code>blah.c</code>, I can just compile it like this without creating a <code>Makefile</code>:</p>
<pre><code>make blah
</code></pre>
<p>It gets automaticaly expanded to <code>cc -o blah blah.c</code>, which saves a bit of
typing. I have no idea if I&rsquo;m going to remember this (I might just keep typing
<code>gcc -o blah blah.c</code> anyway) but it seems like a fun trick.</p>
<h3 id="tip-look-at-how-other-packaging-systems-built-the-same-c-program">tip: look at how other packaging systems built the same C program</h3>
<p>If you&rsquo;re having trouble building a C program, maybe other people had problems building it
too! Every Linux distribution has build files for every package that they
build, so even if you can&rsquo;t install packages from that distribution directly,
maybe you can get tips from that Linux distro for how to build the package.
Realizing this (thanks to my friend Dave) was a huge ah-ha moment for me.</p>
<p>For example, <a href="https://github.com/NixOS/nixpkgs/blob/405624e81a9b65378328accb0a11c3e5369e651c/pkgs/by-name/pa/paperjam/package.nix#L35">this line from the nix package for <code>paperjam</code></a> says:</p>
<pre><code>  env.NIX_LDFLAGS = lib.optionalString stdenv.hostPlatform.isDarwin &quot;-liconv&quot;;
</code></pre>
<p>This is basically saying &ldquo;pass the linker flag <code>-liconv</code> to build this on a
Mac&rdquo;, so that&rsquo;s a clue we could use to build it.</p>
<p>That same file also says <code>  env.NIX_CFLAGS_COMPILE = &quot;-DPOINTERHOLDER_TRANSITION=1&quot;;</code>. I&rsquo;m not sure what this means, but when I try
to build the <code>paperjam</code> package I do get an error about something called a
<code>PointerHolder</code>, so I guess that&rsquo;s somehow related to the &ldquo;PointerHolder
transition&rdquo;.</p>
<h3 id="step-5-installing-the-binary">step 5: installing the binary</h3>
<p>Once you&rsquo;ve managed to compile the program, probably you want to install it somewhere!
Some <code>Makefile</code>s have an <code>install</code> target that let you install the tool on your
system with <code>make install</code>. I&rsquo;m always a bit scared of this (where is it going
to put the files? what if I want to uninstall them later?), so if I&rsquo;m compiling
a pretty simple program I&rsquo;ll often just manually copy the binary to install it
instead, like this:</p>
<pre><code>cp qf ~/bin
</code></pre>
<h3 id="step-6-maybe-make-your-own-package">step 6: maybe make your own package!</h3>
<p>Once I figured out how to do all of this, I realized that I could use my new
<code>make</code> knowledge to contribute a <code>paperjam</code> package to Homebrew! Then I could
just <code>brew install paperjam</code> on future systems.</p>
<p>The good thing is that even if the details of how all of the different
packaging systems, they fundamentally all use C compilers and linkers.</p>
<h3 id="it-can-be-useful-to-understand-a-little-about-c-even-if-you-re-not-a-c-programmer">it can be useful to understand a little about C even if you&rsquo;re not a C programmer</h3>
<p>I think all of this is an interesting example of how it can useful to
understand some basics of how C programs work (like &ldquo;they have header files&rdquo;)
even if you&rsquo;re never planning to write a nontrivial C program if your life.</p>
<p>It feels good to have some ability to compile C/C++ programs myself, even
though I&rsquo;m still not totally confident about all of the compiler and linker
flags and I still plan to never learn anything about how autotools works other
than &ldquo;you run <code>./configure</code> to generate the <code>Makefile</code>&rdquo;.</p>
<p>Two things I left out of this post:</p>
<ul>
<li><code>LD_LIBRARY_PATH / DYLD_LIBRARY_PATH</code> (which you use to tell the dynamic
linker at runtime where to find dynamically linked files) because I can&rsquo;t
remember the last time I ran into an <code>LD_LIBRARY_PATH</code> issue and couldn&rsquo;t
find an example.</li>
<li><code>pkg-config</code>, which I think is important but I don&rsquo;t understand yet</li>
</ul>

---

### [Standards for ANSI escape codes](https://jvns.ca/blog/2025/03/07/escape-code-standards/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Today I want to talk about ANSI escape codes.</p>
<p>For a long time I was vaguely aware of ANSI escape codes (&ldquo;that&rsquo;s how you make
text red in the terminal and stuff&rdquo;) but I had no real understanding of where they were
supposed to be defined or whether or not there were standards for them. I just
had a kind of vague &ldquo;there be dragons&rdquo; feeling around them. While learning
about the terminal this year, I&rsquo;ve learned that:</p>
<ol>
<li>ANSI escape codes are responsible for a lot of usability improvements
in the terminal (did you know there&rsquo;s a way to copy to your system clipboard
when SSHed into a remote machine?? It&rsquo;s an escape code called <a href="https://jvns.ca/til/vim-osc52/">OSC 52</a>!)</li>
<li>They aren&rsquo;t completely standardized, and because of that they don&rsquo;t always
work reliably. And because they&rsquo;re also invisible, it&rsquo;s extremely
frustrating to troubleshoot escape code issues.</li>
</ol>
<p>So I wanted to put together a list for myself of some standards that exist
around escape codes, because I want to know if they <em>have</em> to feel unreliable
and frustrating, or if there&rsquo;s a future where we could all rely on them with
more confidence.</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#what-s-an-escape-code">what&rsquo;s an escape code?</a></li>
<li><a href="https://jvns.ca/atom.xml#ecma-48">ECMA-48</a></li>
<li><a href="https://jvns.ca/atom.xml#xterm-control-sequences">xterm control sequences</a></li>
<li><a href="https://jvns.ca/atom.xml#terminfo">terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#should-programs-use-terminfo">should programs use terminfo?</a></li>
<li><a href="https://jvns.ca/atom.xml#is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</a></li>
<li><a href="https://jvns.ca/atom.xml#some-reasons-to-use-terminfo">some reasons to use terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#some-more-documents-standards">some more documents/standards</a></li>
<li><a href="https://jvns.ca/atom.xml#why-i-think-this-is-interesting">why I think this is interesting</a></li>
</ul>
<h3 id="what-s-an-escape-code">what&rsquo;s an escape code?</h3>
<p>Have you ever pressed the left arrow key in your terminal and seen <code>^[[D</code>?
That&rsquo;s an escape code! It&rsquo;s called an &ldquo;escape code&rdquo; because the first character
is the &ldquo;escape&rdquo; character, which is usually written as <code>ESC</code>, <code>\x1b</code>, <code>\E</code>,
<code>\033</code>, or <code>^[</code>.</p>
<p>Escape codes are how your terminal emulator communicates various kinds of
information (colours, mouse movement, etc) with programs running in the
terminal. There are two kind of escape codes:</p>
<ol>
<li><strong>input codes</strong> which your terminal emulator sends for keypresses or mouse
movements that don&rsquo;t fit into Unicode. For example &ldquo;left arrow key&rdquo; is
<code>ESC[D</code>, &ldquo;Ctrl+left arrow&rdquo; might be <code>ESC[1;5D</code>, and clicking the mouse might
be something like <code>ESC[M :3</code>.</li>
<li><strong>output codes</strong> which programs can print out to colour text, move the
cursor around, clear the screen, hide the cursor, copy text to the
clipboard, enable mouse reporting, set the window title, etc.</li>
</ol>
<p>Now let&rsquo;s talk about standards!</p>
<h3 id="ecma-48">ECMA-48</h3>
<p>The first standard I found relating to escape codes was
<a href="https://ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA-48</a>,
which was originally published in 1976.</p>
<p>ECMA-48 does two things:</p>
<ol>
<li>Define some general <em>formats</em> for escape codes (like &ldquo;CSI&rdquo; codes, which are
<code>ESC[</code> + something and &ldquo;OSC&rdquo; codes, which are <code>ESC]</code> + something)</li>
<li>Define some specific escape codes, like how &ldquo;move the cursor to the left&rdquo; is
<code>ESC[D</code>, or &ldquo;turn text red&rdquo; is  <code>ESC[31m</code>. In the spec, the &ldquo;cursor left&rdquo;
one is called <code>CURSOR LEFT</code> and the one for changing colours is called
<code>SELECT GRAPHIC RENDITION</code>.</li>
</ol>
<p>The formats are extensible, so there&rsquo;s room for others to define more escape
codes in the future. Lots of escape codes that are popular today aren&rsquo;t defined
in ECMA-48: for example it&rsquo;s pretty common for terminal applications (like vim,
htop, or tmux) to support using the mouse, but ECMA-48 doesn&rsquo;t define escape
codes for the mouse.</p>
<h3 id="xterm-control-sequences">xterm control sequences</h3>
<p>There are a bunch of escape codes that aren&rsquo;t defined in ECMA-48, for example:</p>
<ul>
<li>enabling mouse reporting (where did you click in your terminal?)</li>
<li>bracketed paste (did you paste that text or type it in?)</li>
<li>OSC 52 (which terminal applications can use to copy text to your system clipboard)</li>
</ul>
<p>I believe (correct me if I&rsquo;m wrong!) that these and some others came from
xterm, are documented in <a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html">XTerm Control Sequences</a>, and have
been widely implemented by other terminal emulators.</p>
<p>This list of &ldquo;what xterm supports&rdquo; is not a standard exactly, but xterm is
extremely influential and so it seems like an important document.</p>
<h3 id="terminfo">terminfo</h3>
<p>In the 80s (and to some extent today, but my understanding is that it was MUCH
more dramatic in the 80s) there was a huge amount of variation in what escape
codes terminals actually supported.</p>
<p>To deal with this, there&rsquo;s a database of escape codes for various terminals
called &ldquo;terminfo&rdquo;.</p>
<p>It looks like the standard for terminfo is called <a href="https://publications.opengroup.org/c243-1">X/Open Curses</a>, though you need to create
an account to view that standard for some reason. It defines the database format as well
as a C library interface (&ldquo;curses&rdquo;) for accessing the database.</p>
<p>For example you can run this bash snippet to see every possible escape code for
&ldquo;clear screen&rdquo; for all of the different terminals your system knows about:</p>
<pre><code>for term in $(toe -a | awk '{print $1}')
do
  echo $term
  infocmp -1 -T &quot;$term&quot; 2&gt;/dev/null | grep 'clear=' | sed 's/clear=//g;s/,//g'
done
</code></pre>
<p>On my system (and probably every system I&rsquo;ve ever used?), the terminfo database is managed by ncurses.</p>
<h3 id="should-programs-use-terminfo">should programs use terminfo?</h3>
<p>I think it&rsquo;s interesting that there are two main approaches that applications
take to handling ANSI escape codes:</p>
<ol>
<li>Use the terminfo database to figure out which escape codes to use, depending
on what&rsquo;s in the <code>TERM</code> environment variable. Fish does this, for example.</li>
<li>Identify a &ldquo;single common set&rdquo; of escape codes which works in &ldquo;enough&rdquo;
terminal emulators and just hardcode those.</li>
</ol>
<p>Some examples of programs/libraries that take approach #2 (&ldquo;don&rsquo;t use terminfo&rdquo;) include:</p>
<ul>
<li><a href="https://github.com/mawww/kakoune/commit/c12699d2e9c2806d6ed184032078d0b84a3370bb">kakoune</a></li>
<li><a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/165258d2f3ae594b50f16c7b50ffb06627476269/src/prompt_toolkit/input/ansi_escape_sequences.py#L5-L8">python-prompt-toolkit</a></li>
<li><a href="https://github.com/antirez/linenoise">linenoise</a></li>
<li><a href="https://github.com/rockorager/libvaxis">libvaxis</a></li>
<li><a href="https://github.com/chalk/chalk">chalk</a></li>
</ul>
<p>I got curious about why folks might be moving away from terminfo and I found
this very interesting and extremely detailed
<a href="https://twoot.site/@bean/113056942625234032">rant about terminfo from one of the fish maintainers</a>, which argues that:</p>
<blockquote>
<p>[the terminfo authors] have done a lot of work that, at the time, was
extremely important and helpful. My point is that it no longer is.</p>
</blockquote>
<p>I&rsquo;m not going to do it justice so I&rsquo;m not going to summarize it, I think it&rsquo;s
worth reading.</p>
<h3 id="is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</h3>
<p>I was just talking about the idea that you can use a &ldquo;common set&rdquo; of escape
codes that will work for most people. But what is that set? Is there any agreement?</p>
<p>I really do not know the answer to this at all, but from doing some reading it
seems like it&rsquo;s some combination of:</p>
<ul>
<li>The codes that the VT100 supported (though some aren&rsquo;t relevant on modern terminals)</li>
<li>what&rsquo;s in ECMA-48 (which I think also has some things that are no longer relevant)</li>
<li>What xterm supports (though I&rsquo;d guess that not everything in there is actually widely supported enough)</li>
</ul>
<p>and maybe ultimately &ldquo;identify the terminal emulators you think your users are
going to use most frequently and test in those&rdquo;, the same way web developers do
when deciding which CSS features are okay to use</p>
<p>I don&rsquo;t think there are any resources like <a href="https://caniuse.com/">Can I use&hellip;?</a> or
<a href="https://web-platform-dx.github.io/web-features/">Baseline</a> for the terminal
though. (in theory terminfo is supposed to be the &ldquo;caniuse&rdquo; for the terminal
but it seems like it often takes 10+ years to add new terminal features when
people invent them which makes it very limited)</p>
<h3 id="some-reasons-to-use-terminfo">some reasons to use terminfo</h3>
<p>I also asked on Mastodon why people found terminfo valuable in 2025 and got a
few reasons that made sense to me:</p>
<ul>
<li>some people expect to be able to use the <code>TERM</code> environment variable to
control how programs behave (for example with <code>TERM=dumb</code>), and there&rsquo;s
no standard for how that should work in a post-terminfo world</li>
<li>even though there&rsquo;s <em>less</em> variation between terminal emulators than
there was in the 80s, there&rsquo;s far from zero variation: there are graphical
terminals, the Linux framebuffer console, the situation you&rsquo;re in when
connecting to a server via its serial console, Emacs shell mode, and probably
more that I&rsquo;m missing</li>
<li>there is no one standard for what the &ldquo;single common set&rdquo; of escape codes
is, and sometimes programs use escape codes which aren&rsquo;t actually widely
supported enough</li>
</ul>
<h3 id="terminfo-user-agent-detection">terminfo &amp; user agent detection</h3>
<p>The way that ncurses uses the <code>TERM</code> environment variable to decide which
escape codes to use reminds me of how webservers used to sometimes use the
browser user agent to decide which version of a website to serve.</p>
<p>It also seems like it&rsquo;s had some of the same results &ndash; the way iTerm2 reports
itself as being &ldquo;xterm-256color&rdquo; feels similar to how Safari&rsquo;s user agent is
&ldquo;Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7_4) AppleWebKit/605.1.15 (KHTML,
like Gecko) Version/18.3 Safari/605.1.15&rdquo;. In both cases the terminal emulator
/ browser ends up changing its user agent to get around user agent detection
that isn&rsquo;t working well.</p>
<p>On the web we ended up deciding that user agent detection was not a good
practice and to instead focus on standardization so we can serve the same
HTML/CSS to all browsers. I don&rsquo;t know if the same approach is the future in
the terminal though &ndash; I think the terminal landscape today is much more
fragmented than the web ever was as well as being much less well funded.</p>
<h3 id="some-more-documents-standards">some more documents/standards</h3>
<p>A few more documents and standards related to escape codes, in no particular order:</p>
<ul>
<li>the <a href="https://man7.org/linux/man-pages/man4/console_codes.4.html">Linux console_codes man page</a> documents
escape codes that Linux supports</li>
<li>how the <a href="https://vt100.net/docs/vt100-ug/chapter3.html">VT 100</a> handles escape codes &amp; control sequences</li>
<li>the <a href="https://sw.kovidgoyal.net/kitty/keyboard-protocol/">kitty keyboard protocol</a></li>
<li><a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda">OSC 8</a> for links in the terminal (and notes on <a href="https://github.com/Alhadis/OSC8-Adoption?tab=readme-ov-file">adoption</a>)</li>
<li>A <a href="https://github.com/tmux/tmux/blob/882fb4d295deb3e4b803eb444915763305114e4f/tools/ansicode.txt">summary of ANSI standards from tmux</a></li>
<li>this <a href="https://iterm2.com/feature-reporting/">terminal features reporting specification from iTerm</a></li>
<li>sixel graphics</li>
</ul>
<h3 id="why-i-think-this-is-interesting">why I think this is interesting</h3>
<p>I sometimes see people saying that the unix terminal is &ldquo;outdated&rdquo;, and since I
love the terminal so much I&rsquo;m always curious about what incremental changes
might make it feel less &ldquo;outdated&rdquo;.</p>
<p>Maybe if we had a clearer standards landscape (like we do on the web!) it would
be easier for terminal emulator developers to build new features and for
authors of terminal applications to more confidently adopt those features so
that we can all benefit from them and have a richer experience in the terminal.</p>
<p>Obviously standardizing ANSI escape codes is not easy (ECMA-48 was first
published almost 50 years ago and we&rsquo;re still not there!). I don&rsquo;t even know
what all of the challenges are. But the situation with HTML/CSS/JS used to be
extremely bad too and now it&rsquo;s MUCH better, so maybe there&rsquo;s hope.</p>

---

### [How to add a directory to your PATH](https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I was talking to a friend about how to add a directory to your PATH today. It&rsquo;s
something that feels &ldquo;obvious&rdquo; to me since I&rsquo;ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn&rsquo;t find something that explained all of the steps &ndash; a lot of them just
said &ldquo;add this to <code>~/.bashrc</code>&rdquo;, but what if you&rsquo;re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?</p>
<p>So I wanted to try to write down some more complete directions and mention some
of the gotchas I&rsquo;ve run into over the years.</p>
<p>Here&rsquo;s a table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-1-what-shell-are-you-using">step 1: what shell are you using?</a></li>
<li><a href="https://jvns.ca/atom.xml#step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-4-edit-your-shell-config">step 4: edit your shell config</a></li>
<li><a href="https://jvns.ca/atom.xml#step-5-restart-your-shell">step 5: restart your shell</a></li>
<li>problems:
<ul>
<li><a href="https://jvns.ca/atom.xml#problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate PATH entries making it harder to debug</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your PATH</a></li>
</ul>
</li>
<li>notes:
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-source">a note on source</a></li>
<li><a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">a note on fish_add_path</a></li>
</ul>
</li>
</ul>
<h3 id="step-1-what-shell-are-you-using">step 1: what shell are you using?</h3>
<p>If you&rsquo;re not sure what shell you&rsquo;re using, here&rsquo;s a way to find out. Run this:</p>
<pre><code>ps -p $$ -o pid,comm=
</code></pre>
<ul>
<li>if you&rsquo;re using <strong>bash</strong>, it&rsquo;ll print out <code>97295 bash</code></li>
<li>if you&rsquo;re using <strong>zsh</strong>, it&rsquo;ll print out <code>97295 zsh</code></li>
<li>if you&rsquo;re using <strong>fish</strong>, it&rsquo;ll print out an error like &ldquo;In fish, please use
$fish_pid&rdquo; (<code>$$</code> isn&rsquo;t valid syntax in fish, but in any case the error
message tells you that you&rsquo;re using fish, which you probably already knew)</li>
</ul>
<p>Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I&rsquo;ll only cover bash, zsh, and fish in these directions.</p>
<h3 id="step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</h3>
<ul>
<li>in zsh, it&rsquo;s probably <code>~/.zshrc</code></li>
<li>in bash, it might be <code>~/.bashrc</code>, but it&rsquo;s complicated, see the note in the next section</li>
<li>in fish, it&rsquo;s probably <code>~/.config/fish/config.fish</code> (you can run <code>echo $__fish_config_dir</code> if you want to be 100% sure)</li>
</ul>
<h3 id="a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</h3>
<p>Bash has three possible config files: <code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.profile</code>.</p>
<p>If you&rsquo;re not sure which one your system is set up to use, I&rsquo;d recommend
testing this way:</p>
<ol>
<li>add <code>echo hi there</code> to your <code>~/.bashrc</code></li>
<li>Restart your terminal</li>
<li>If you see &ldquo;hi there&rdquo;, that means <code>~/.bashrc</code> is being used! Hooray!</li>
<li>Otherwise remove it and try the same thing with <code>~/.bash_profile</code></li>
<li>You can also try <code>~/.profile</code> if the first two options don&rsquo;t work.</li>
</ol>
<p>(there are a lot of <a href="https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html">elaborate flow charts</a> out there that explain how bash
decides which config file to use but IMO it&rsquo;s not worth it to internalize them
and just testing is the fastest way to be sure)</p>
<h3 id="step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</h3>
<p>Let&rsquo;s say that you&rsquo;re trying to install and run a program called <code>http-server</code>
and it doesn&rsquo;t work, like this:</p>
<pre><code>$ npm install -g http-server
$ http-server
bash: http-server: command not found
</code></pre>
<p>How do you find what directory <code>http-server</code> is in? Honestly in general this is
not that easy &ndash; often the answer is something like &ldquo;it depends on how npm is
configured&rdquo;. A few ideas:</p>
<ul>
<li>Often when setting up a new installer (like <code>cargo</code>, <code>npm</code>, <code>homebrew</code>, etc),
when you first set it up it&rsquo;ll print out some directions about how to update
your PATH. So if you&rsquo;re paying attention you can get the directions then.</li>
<li>Sometimes installers will automatically update your shell&rsquo;s config file
to update your <code>PATH</code> for you</li>
<li>Sometimes just Googling &ldquo;where does npm install things?&rdquo; will turn up the
answer</li>
<li>Some tools have a subcommand that tells you where they&rsquo;re configured to
install things, like:
<ul>
<li>Node/npm: <code>npm config get prefix</code> (then append <code>/bin/</code>)</li>
<li>Go: <code>go env GOPATH</code> (then append <code>/bin/</code>)</li>
<li>asdf: <code>asdf info | grep ASDF_DIR</code> (then append <code>/bin/</code> and <code>/shims/</code>)</li>
</ul>
</li>
</ul>
<h3 id="step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</h3>
<p>Once you&rsquo;ve found a directory you think might be the right one, make sure it&rsquo;s
actually correct! For example, I found out that on my machine, <code>http-server</code> is
in <code>~/.npm-global/bin</code>. I can make sure that it&rsquo;s the right directory by trying to
run the program <code>http-server</code> in that directory like this:</p>
<pre><code>$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
</code></pre>
<p>It worked! Now that you know what directory you need to add to your <code>PATH</code>,
let&rsquo;s move to the next step!</p>
<h3 id="step-4-edit-your-shell-config">step 4: edit your shell config</h3>
<p>Now we have the 2 critical pieces of information we need:</p>
<ol>
<li>Which directory you&rsquo;re trying to add to your PATH (like  <code>~/.npm-global/bin/</code>)</li>
<li>Where your shell&rsquo;s config is (like <code>~/.bashrc</code>, <code>~/.zshrc</code>, or <code>~/.config/fish/config.fish</code>)</li>
</ol>
<p>Now what you need to add depends on your shell:</p>
<p><strong>bash instructions:</strong></p>
<p>Open your shell&rsquo;s config file, and add a line like this:</p>
<pre><code>export PATH=$PATH:~/.npm-global/bin/
</code></pre>
<p>(obviously replace <code>~/.npm-global/bin</code> with the actual directory you&rsquo;re trying to add)</p>
<p><strong>zsh instructions:</strong></p>
<p>You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:</p>
<pre><code>path=(
  $path
  ~/.npm-global/bin
)
</code></pre>
<p><strong>fish instructions:</strong></p>
<p>In fish, the syntax is different:</p>
<pre><code>set PATH $PATH ~/.npm-global/bin
</code></pre>
<p>(in fish you can also use <code>fish_add_path</code>, some notes on that <a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">further down</a>)</p>
<h3 id="step-5-restart-your-shell">step 5: restart your shell</h3>
<p>Now, an extremely important step: updating your shell&rsquo;s config won&rsquo;t take
effect if you don&rsquo;t restart it!</p>
<p>Two ways to do this:</p>
<ol>
<li>open a new terminal (or terminal tab), and maybe close the old one so you don&rsquo;t get confused</li>
<li>Run <code>bash</code> to start a new shell (or <code>zsh</code> if you&rsquo;re using zsh, or <code>fish</code> if you&rsquo;re using fish)</li>
</ol>
<p>I&rsquo;ve found that both of these usually work fine.</p>
<p>And you should be done! Try running the program you were trying to run and
hopefully it works now.</p>
<p>If not, here are a couple of problems that you might run into:</p>
<h3 id="problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</h3>
<p>If the wrong <strong>version</strong> of a program is running, you might need to add the
directory to the <em>beginning</em> of your PATH instead of the end.</p>
<p>For example, on my system I have two versions of <code>python3</code> installed, which I
can see by running <code>which -a</code>:</p>
<pre><code>$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
</code></pre>
<p>The one your shell will use is the <strong>first one listed</strong>.</p>
<p>If you want to use the Homebrew version, you need to add that directory
(<code>/opt/homebrew/bin</code>) to the <strong>beginning</strong> of your PATH instead, by putting this in
your shell&rsquo;s config file (it&rsquo;s <code>/opt/homebrew/bin/:$PATH</code> instead of the usual <code>$PATH:/opt/homebrew/bin/</code>)</p>
<pre><code>export PATH=/opt/homebrew/bin/:$PATH
</code></pre>
<p>or in fish:</p>
<pre><code>set PATH ~/.cargo/bin $PATH
</code></pre>
<h3 id="problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</h3>
<p>All of these directions only work if you&rsquo;re running the program <strong>from your
shell</strong>. If you&rsquo;re running the program from an IDE, from a GUI, in a cron job,
or some other way, you&rsquo;ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.</p>
<p><strong>in a cron job</strong></p>
<p>Some options:</p>
<ul>
<li>use the full path to the program you&rsquo;re running, like <code>/home/bork/bin/my-program</code></li>
<li>put the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:&hellip;.). You can get the full PATH you&rsquo;re
using in your shell by running <code>echo &quot;PATH=$PATH&quot;</code>.</li>
</ul>
<p>I&rsquo;m honestly not sure how to handle it in an IDE/GUI because I haven&rsquo;t run into
that in a long time, will add directions here if someone points me in the right
direction.</p>
<h3 id="problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate <code>PATH</code> entries making it harder to debug</h3>
<p>If you edit your path and start a new shell by running <code>bash</code> (or <code>zsh</code>, or
<code>fish</code>), you&rsquo;ll often end up with duplicate <code>PATH</code> entries, because the shell
keeps adding new things to your <code>PATH</code> every time you start your shell.</p>
<p>Personally I don&rsquo;t think I&rsquo;ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what&rsquo;s going on with your <code>PATH</code> if you&rsquo;re trying to understand its contents.</p>
<p>Some ways you could deal with this:</p>
<ol>
<li>If you&rsquo;re debugging your <code>PATH</code>, open a new terminal to do it in so you get
a &ldquo;fresh&rdquo; state. This should avoid the duplication.</li>
<li>Deduplicate your <code>PATH</code> at the end of your shell&rsquo;s config  (for example in
zsh apparently you can do this with <code>typeset -U path</code>)</li>
<li>Check that the directory isn&rsquo;t already in your <code>PATH</code> when adding it (for
example in fish I believe you can do this with <code>fish_add_path --path /some/directory</code>)</li>
</ol>
<p>How to deduplicate your <code>PATH</code> is shell-specific and there isn&rsquo;t always a
built in way to do it so you&rsquo;ll need to look up how to accomplish it in your
shell.</p>
<h3 id="problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your <code>PATH</code></h3>
<p>Here&rsquo;s a situation that&rsquo;s easy to get into in bash or zsh:</p>
<ol>
<li>Run a command (it fails)</li>
<li>Update your <code>PATH</code></li>
<li>Run <code>bash</code> to reload your config</li>
<li>Press the up arrow a couple of times to rerun the failed command (or open a new terminal)</li>
<li>The failed command isn&rsquo;t in your history! Why not?</li>
</ol>
<p>This happens because in bash, by default, history is not saved until you exit
the shell.</p>
<p>Some options for fixing this:</p>
<ul>
<li>Instead of running <code>bash</code> to reload your config, run <code>source ~/.bashrc</code> (or
<code>source ~/.zshrc</code> in zsh). This will reload the config inside your current
session.</li>
<li>Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you&rsquo;re
using bash or zsh, the history options in zsh are a bit complicated and I&rsquo;m
not exactly sure what the best way is)</li>
</ul>
<h3 id="a-note-on-source">a note on <code>source</code></h3>
<p>When you install <code>cargo</code> (Rust&rsquo;s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don&rsquo;t mention a specific
directory at all.</p>
<pre><code>This is usually done by running one of the following (note the leading DOT):

. &quot;$HOME/.cargo/env&quot;        	# For sh/bash/zsh/ash/dash/pdksh
source &quot;$HOME/.cargo/env.fish&quot;  # For fish
</code></pre>
<p>The idea is that you add that line to your shell&rsquo;s config, and their script
automatically sets up your <code>PATH</code> (and potentially other things) for you.</p>
<p>This is pretty common (for example <a href="https://github.com/Homebrew/install/blob/deacfa6a6e62e5f4002baf9e1fac7a96e9aa5d41/install.sh#L1072-L1087">Homebrew</a> suggests you eval <code>brew shellenv</code>), and there are
two ways to approach this:</p>
<ol>
<li>Just do what the tool suggests (like adding <code>. &quot;$HOME/.cargo/env&quot;</code> to your shell&rsquo;s config)</li>
<li>Figure out which directories the script they&rsquo;re telling you to run would add
to your PATH, and then add those manually. Here&rsquo;s how I&rsquo;d do that:
<ul>
<li>Run <code>. &quot;$HOME/.cargo/env&quot;</code> in my shell (or the fish version if using fish)</li>
<li>Run <code>echo &quot;$PATH&quot; | tr ':' '\n' | grep cargo</code> to figure out which directories it added</li>
<li>See that it says <code>/Users/bork/.cargo/bin</code> and shorten that to <code>~/.cargo/bin</code></li>
<li>Add the directory <code>~/.cargo/bin</code> to PATH (with the directions in this post)</li>
</ul>
</li>
</ol>
<p>I don&rsquo;t think there&rsquo;s anything wrong with doing what the tool suggests (it
might be the &ldquo;best way&rdquo;!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I&rsquo;m changing.</p>
<h3 id="a-note-on-fish-add-path">a note on <code>fish_add_path</code></h3>
<p>fish has a handy function called <code>fish_add_path</code> that you can run to add a directory to your <code>PATH</code> like this:</p>
<pre><code>fish_add_path /some/directory
</code></pre>
<p>This is cool (it&rsquo;s such a simple command!) but I&rsquo;ve stopped using it for a couple of reasons:</p>
<ol>
<li>Sometimes <code>fish_add_path</code> will update the <code>PATH</code> for every session in the
future (with a &ldquo;universal variable&rdquo;) and sometimes it will update the <code>PATH</code>
just for the current session and it&rsquo;s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.</li>
<li>If you ever need to <em>remove</em> the directory from your <code>PATH</code> a few weeks or
months later because maybe you made a mistake, it&rsquo;s kind of hard to do
(there are <a href="https://github.com/fish-shell/fish-shell/issues/8604">instructions in this comments of this github issue though</a>).</li>
</ol>
<h3 id="that-s-all">that&rsquo;s all</h3>
<p>Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!</p>

---

### [Some terminal frustrations](https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>A few weeks ago I ran a terminal survey (you can <a href="https://jvns.ca/terminal-survey/results-bsky.html">read the results here</a>) and at the end I asked:</p>
<blockquote>
<p>What‚Äôs the most frustrating thing about using the terminal for you?</p>
</blockquote>
<p>1600 people answered, and I decided to spend a few days categorizing all the
responses. Along the way I learned that classifying qualitative data is not
easy but I gave it my best shot. I ended up building a custom
<a href="https://github.com/jvns/classificator">tool</a> to make it faster to categorize
everything.</p>
<p>As with all of my surveys the methodology isn&rsquo;t particularly scientific. I just
posted the survey to Mastodon and Twitter, ran it for a couple of days, and got
answers from whoever happened to see it and felt like responding.</p>
<p>Here are the top categories of frustrations!</p>
<p>I think it&rsquo;s worth keeping in mind while reading these comments that</p>
<ul>
<li>40% of people answering this survey have been using the terminal for <strong>21+ years</strong></li>
<li>95% of people answering the survey have been using the terminal for at least 4 years</li>
</ul>
<p>These comments aren&rsquo;t coming from total beginners.</p>
<p>Here are the categories of frustrations! The number in brackets is the number
of people with that frustration. I&rsquo;m mostly writing this up for myself because
I&rsquo;m trying to write a zine about the terminal and I wanted to get a sense for
what people are having trouble with.</p>
<h3 id="remembering-syntax-115">remembering syntax (115)</h3>
<p>People talked about struggles remembering:</p>
<ul>
<li>the syntax for CLI tools like awk, jq, sed, etc</li>
<li>the syntax for redirects</li>
<li>keyboard shortcuts for tmux, text editing, etc</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>There are just so many little &ldquo;trivia&rdquo; details to remember for full
functionality. Even after all these years I&rsquo;ll sometimes forget where it&rsquo;s 2
or 1 for stderr, or forget which is which for <code>&gt;</code> and <code>&gt;&gt;</code>.</p>
</blockquote>
<h3 id="switching-terminals-is-hard-91">switching terminals is hard (91)</h3>
<p>People talked about struggling with switching systems (for example home/work
computer or when SSHing) and running into:</p>
<ul>
<li>OS differences in keyboard shortcuts (like Linux vs Mac)</li>
<li>systems which don&rsquo;t have their preferred text editor (&ldquo;no vim&rdquo; or &ldquo;only vim&rdquo;)</li>
<li>different versions of the same command (like Mac OS grep vs GNU grep)</li>
<li>no tab completion</li>
<li>a shell they aren&rsquo;t used to (&ldquo;the subtle differences between zsh and bash&rdquo;)</li>
</ul>
<p>as well as differences inside the same system like pagers being not consistent
with each other (git diff pagers, other pagers).</p>
<p>One example comment:</p>
<blockquote>
<p>I got used to fish and vi mode which are not available when I ssh into
servers, containers.</p>
</blockquote>
<h3 id="color-85">color (85)</h3>
<p>Lots of problems with color, like:</p>
<ul>
<li>programs setting colors that are unreadable with a light background color</li>
<li>finding a colorscheme they like (and getting it to work consistently across different apps)</li>
<li>color not working inside several layers of SSH/tmux/etc</li>
<li>not liking the defaults</li>
<li>not wanting color at all and struggling to turn it off</li>
</ul>
<p>This comment felt relatable to me:</p>
<blockquote>
<p>Getting my terminal theme configured in a reasonable way between the terminal
emulator and fish (I did this years ago and remember it being tedious and
fiddly and now feel like I&rsquo;m locked into my current theme because it works
and I dread touching any of that configuration ever again).</p>
</blockquote>
<h3 id="keyboard-shortcuts-84">keyboard shortcuts (84)</h3>
<p>Half of the comments on keyboard shortcuts were about how on Linux/Windows, the
keyboard shortcut to copy/paste in the terminal is different from in the rest
of the OS.</p>
<p>Some other issues with keyboard shortcuts other than copy/paste:</p>
<ul>
<li>using <code>Ctrl-W</code> in a browser-based terminal and closing the window</li>
<li>the terminal only supports a limited set of keyboard shortcuts (no
<code>Ctrl-Shift-</code>, no <code>Super</code>, no <code>Hyper</code>, lots of <code>ctrl-</code> shortcuts aren&rsquo;t
possible like <code>Ctrl-,</code>)</li>
<li>the OS stopping you from using a terminal keyboard shortcut (like by default
Mac OS uses <code>Ctrl+left arrow</code> for something else)</li>
<li>issues using emacs in the terminal</li>
<li>backspace not working (2)</li>
</ul>
<h3 id="other-copy-and-paste-issues-75">other copy and paste issues (75)</h3>
<p>Aside from &ldquo;the keyboard shortcut for copy and paste is different&rdquo;, there were
a lot of OTHER issues with copy and paste, like:</p>
<ul>
<li>copying over SSH</li>
<li>how tmux and the terminal emulator both do copy/paste in different ways</li>
<li>dealing with many different clipboards (system clipboard, vim clipboard, the
&ldquo;middle click&rdquo; clipboard on Linux, tmux&rsquo;s clipboard, etc) and potentially
synchronizing them</li>
<li>random spaces added when copying from the terminal</li>
<li>pasting multiline commands which automatically get run in a terrifying way</li>
<li>wanting a way to copy text without using the mouse</li>
</ul>
<h3 id="discoverability-55">discoverability (55)</h3>
<p>There were lots of comments about this, which all came down to the same basic
complaint &ndash; it&rsquo;s hard to discover useful tools or features! This comment kind of
summed it all up:</p>
<blockquote>
<p>How difficult it is to learn independently. Most of what I know is an
assorted collection of stuff I&rsquo;ve been told by random people over the years.</p>
</blockquote>
<h3 id="steep-learning-curve-44">steep learning curve (44)</h3>
<p>A lot of comments about it generally having a steep learning curve. A couple of
example comments:</p>
<blockquote>
<p>After 15 years of using it, I‚Äôm not much faster than using it than I was 5 or
maybe even 10 years ago.</p>
</blockquote>
<p>and</p>
<blockquote>
<p>That I know I could make my life easier by learning more about the shortcuts
and commands and configuring the terminal but I don&rsquo;t spend the time because it
feels overwhelming.</p>
</blockquote>
<h3 id="history-42">history  (42)</h3>
<p>Some issues with shell history:</p>
<ul>
<li>history not being shared between terminal tabs (16)</li>
<li>limits that are too short (4)</li>
<li>history not being restored when terminal tabs are restored</li>
<li>losing history because the terminal crashed</li>
<li>not knowing how to search history</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>It wasted a lot of time until I figured it out and still annoys me that
&ldquo;history&rdquo; on zsh has such a small buffer;  I have to type &ldquo;history 0&rdquo; to get
any useful length of history.</p>
</blockquote>
<h3 id="bad-documentation-37">bad documentation (37)</h3>
<p>People talked about:</p>
<ul>
<li>documentation being generally opaque</li>
<li>lack of examples in man pages</li>
<li>programs which don&rsquo;t have man pages</li>
</ul>
<p>Here&rsquo;s a representative comment:</p>
<blockquote>
<p>Finding good examples and docs. Man pages often not enough, have to wade
through stack overflow</p>
</blockquote>
<h3 id="scrollback-36">scrollback (36)</h3>
<p>A few issues with scrollback:</p>
<ul>
<li>programs printing out too much data making you lose scrollback history</li>
<li>resizing the terminal messes up the scrollback</li>
<li>lack of timestamps</li>
<li>GUI programs that you start in the background printing stuff out that gets in
the way of other programs&rsquo; outputs</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>When resizing the terminal (in particular: making it narrower) leads to
broken rewrapping of the scrollback content because the commands formatted
their output based on the terminal window width.</p>
</blockquote>
<h3 id="it-feels-outdated-33">&ldquo;it feels outdated&rdquo; (33)</h3>
<p>Lots of comments about how the terminal feels hampered by legacy decisions and
how users often end up needing to learn implementation details that feel very
esoteric. One example comment:</p>
<blockquote>
<p>Most of the legacy cruft, it would be great to have a green field
implementation of the CLI interface.</p>
</blockquote>
<h3 id="shell-scripting-32">shell scripting (32)</h3>
<p>Lots of complaints about POSIX shell scripting. There&rsquo;s a general feeling that
shell scripting is difficult but also that switching to a different less
standard scripting language (fish, nushell, etc) brings its own problems.</p>
<blockquote>
<p>Shell scripting. My tolerance to ditch a shell script and go to a scripting
language is pretty low. It‚Äôs just too messy and powerful. Screwing up can be
costly so I don‚Äôt even bother.</p>
</blockquote>
<h3 id="more-issues">more issues</h3>
<p>Some more issues that were mentioned at least 10 times:</p>
<ul>
<li>(31) inconsistent command line arguments: is it -h or help or &ndash;help?</li>
<li>(24) keeping dotfiles in sync across different systems</li>
<li>(23) performance (e.g. &ldquo;my shell takes too long to start&rdquo;)</li>
<li>(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)</li>
<li>(17) generally feeling scared/uneasy (&ldquo;The debilitating fear that I‚Äôm going
to do some mysterious Bad Thing with a command and I will have absolutely no
idea how to fix or undo it or even really figure out what happened&rdquo;)</li>
<li>(16) terminfo issues (&ldquo;Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.&rdquo;)</li>
<li>(16) lack of image support (sixel etc)</li>
<li>(15) SSH issues (like having to start over when you lose the SSH connection)</li>
<li>(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)</li>
<li>(15) typos &amp; slow typing</li>
<li>(13) the terminal getting messed up for various reasons (pressing <code>Ctrl-S</code>, <code>cat</code>ing a binary, etc)</li>
<li>(12) quoting/escaping in the shell</li>
<li>(11) various Windows/PowerShell issues</li>
</ul>
<h3 id="n-a-122">n/a (122)</h3>
<p>There were also 122 answers to the effect of &ldquo;nothing really&rdquo; or &ldquo;only that I
can&rsquo;t do EVERYTHING in the terminal&rdquo;</p>
<p>One example comment:</p>
<blockquote>
<p>Think I&rsquo;ve found work arounds for most/all frustrations</p>
</blockquote>
<h3 id="that-s-all">that&rsquo;s all!</h3>
<p>I&rsquo;m not going to make a lot of commentary on these results, but here are a
couple of categories that feel related to me:</p>
<ul>
<li>remembering syntax &amp; history (often the thing you need to remember is something you&rsquo;ve run before!)</li>
<li>discoverability &amp; the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)</li>
<li>&ldquo;switching systems is hard&rdquo; &amp; &ldquo;it feels outdated&rdquo; (tools that haven&rsquo;t really
changed in 30 or 40 years have many problems but they do tend to be always
<em>there</em> no matter what system you&rsquo;re on, which is very useful and makes them
hard to stop using)</li>
</ul>
<p>Trying to categorize all these results in a reasonable way really gave me an
appreciation for social science researchers&rsquo; skills.</p>

---

### [What's involved in getting a "modern" terminal setup?](https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Recently I ran a terminal survey and I asked people what frustrated
them. One person commented:</p>
<blockquote>
<p>There are so many pieces to having a modern terminal experience. I wish it
all came out of the box.</p>
</blockquote>
<p>My immediate reaction was &ldquo;oh, getting a modern terminal experience isn&rsquo;t that
hard, you just need to&hellip;.&rdquo;, but the more I thought about it, the longer the
&ldquo;you just need to&hellip;&rdquo; list got, and I kept thinking about more and more
caveats.</p>
<p>So I thought I would write down some notes about what it means to me personally
to have a &ldquo;modern&rdquo; terminal experience and what I think can make it hard for
people to get there.</p>
<h3 id="what-is-a-modern-terminal-experience">what is a &ldquo;modern terminal experience&rdquo;?</h3>
<p>Here are a few things that are important to me, with which part of the system
is responsible for them:</p>
<ul>
<li><strong>multiline support for copy and paste</strong>: if you paste 3 commands in your shell, it should not immediately run them all! That&rsquo;s scary! (<strong>shell</strong>, <strong>terminal emulator</strong>)</li>
<li><strong>infinite shell history</strong>: if I run a command in my shell, it should be saved forever, not deleted after 500 history entries or whatever. Also I want commands to be saved to the history immediately when I run them, not only when I exit the shell session (<strong>shell</strong>)</li>
<li><strong>a useful prompt</strong>: I can&rsquo;t live without having my <strong>current directory</strong> and <strong>current git branch</strong> in my prompt (<strong>shell</strong>)</li>
<li><strong>24-bit colour</strong>: this is important to me because I find it MUCH easier to theme neovim with 24-bit colour support than in a terminal with only 256 colours (<strong>terminal emulator</strong>)</li>
<li><strong>clipboard integration</strong> between vim and my operating system so that when I copy in Firefox, I can just press <code>p</code> in vim to paste (<strong>text editor</strong>, maybe the OS/terminal emulator too)</li>
<li><strong>good autocomplete</strong>: for example commands like git should have command-specific autocomplete (<strong>shell</strong>)</li>
<li><strong>having colours in <code>ls</code></strong> (<strong>shell config</strong>)</li>
<li><strong>a terminal theme I like</strong>: I spend a lot of time in my terminal, I want it to look nice and I want its theme to match my terminal editor&rsquo;s theme. (<strong>terminal emulator</strong>, <strong>text editor</strong>)</li>
<li><strong>automatic terminal fixing</strong>: If a programs prints out some weird escape
codes that mess up my terminal, I want that to automatically get reset so
that my terminal doesn&rsquo;t get messed up (<strong>shell</strong>)</li>
<li><strong>keybindings</strong>: I want <code>Ctrl+left arrow</code> to work (<strong>shell</strong> or <strong>application</strong>)</li>
<li><strong>being able to use the scroll wheel in programs like <code>less</code></strong>: (<strong>terminal emulator</strong> and <strong>applications</strong>)</li>
</ul>
<p>There are a million other terminal conveniences out there and different people
value different things, but those are the ones that I would be really unhappy
without.</p>
<h3 id="how-i-achieve-a-modern-experience">how I achieve a &ldquo;modern experience&rdquo;</h3>
<p>My basic approach is:</p>
<ol>
<li>use the <code>fish</code> shell. Mostly don&rsquo;t configure it, except to:
<ul>
<li>set the <code>EDITOR</code> environment variable to my favourite terminal editor</li>
<li>alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>use any terminal emulator with 24-bit colour support. In the past I&rsquo;ve used
GNOME Terminal, Terminator, and iTerm, but I&rsquo;m not picky about this. I don&rsquo;t really
configure it other than to choose a font.</li>
<li>use <code>neovim</code>, with a configuration that I&rsquo;ve been very slowly building over the last 9 years or so (the last time I deleted my vim config and started from scratch was 9 years ago)</li>
<li>use the <a href="https://github.com/chriskempson/base16">base16 framework</a> to theme everything</li>
</ol>
<p>A few things that affect my approach:</p>
<ul>
<li>I don&rsquo;t spend a lot of time SSHed into other machines</li>
<li>I&rsquo;d rather use the mouse a little than come up with keyboard-based ways to do everything</li>
<li>I work on a lot of small projects, not one big project</li>
</ul>
<h3 id="some-out-of-the-box-options-for-a-modern-experience">some &ldquo;out of the box&rdquo; options for a &ldquo;modern&rdquo; experience</h3>
<p>What if you want a nice experience, but don&rsquo;t want to spend a lot of time on
configuration? Figuring out how to configure vim in a way that I was satisfied
with really did take me like ten years, which is a long time!</p>
<p>My best ideas for how to get a reasonable terminal experience with minimal
config are:</p>
<ul>
<li>shell: either <code>fish</code> or <code>zsh</code> with <a href="https://ohmyz.sh/">oh-my-zsh</a></li>
<li>terminal emulator: almost anything with 24-bit colour support, for example all of these are popular:
<ul>
<li>linux: GNOME Terminal, Konsole, Terminator, xfce4-terminal</li>
<li>mac: iTerm (Terminal.app doesn&rsquo;t have 256-colour support)</li>
<li>cross-platform: kitty, alacritty, wezterm, or ghostty</li>
</ul>
</li>
<li>shell config:
<ul>
<li>set the <code>EDITOR</code> environment variable to your favourite terminal text
editor</li>
<li>maybe alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>text editor: this is a tough one, maybe <a href="https://micro-editor.github.io/">micro</a> or <a href="https://helix-editor.com/">helix</a>? I haven&rsquo;t used
either of them seriously but they both seem like very cool projects and I
think it&rsquo;s amazing that you can just use all the usual GUI editor commands
(<code>Ctrl-C</code> to copy, <code>Ctrl-V</code> to paste, <code>Ctrl-A</code> to select all) in micro and
they do what you&rsquo;d expect. I would probably try switching to helix except
that retraining my vim muscle memory seems way too hard. Also helix doesn&rsquo;t
have a GUI or plugin system yet.</li>
</ul>
<p>Personally I <strong>wouldn&rsquo;t</strong> use xterm, rxvt, or Terminal.app as a terminal emulator,
because I&rsquo;ve found in the past that they&rsquo;re missing core features (like 24-bit
colour in Terminal.app&rsquo;s case) that make the terminal harder to use for me.</p>
<p>I don&rsquo;t want to pretend that getting a &ldquo;modern&rdquo; terminal experience is easier
than it is though &ndash; I think there are two issues that make it hard. Let&rsquo;s talk
about them!</p>
<h3 id="issue-1-with-getting-to-a-modern-experience-the-shell">issue 1 with getting to a &ldquo;modern&rdquo; experience: the shell</h3>
<p>bash and zsh are by far the two most popular shells, and neither of them
provide a default experience that I would be happy using out of the box, for
example:</p>
<ul>
<li>you need to customize your prompt</li>
<li>they don&rsquo;t come with git completions by default, you have to set them up</li>
<li>by default, bash only stores 500 (!) lines of history and (at least on Mac OS)
zsh is only configured to store 2000 lines, which is still not a lot</li>
<li>I find bash&rsquo;s tab completion very frustrating, if there&rsquo;s more than
one match then you can&rsquo;t tab through them</li>
</ul>
<p>And even though <a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">I love fish</a>, the fact
that it isn&rsquo;t POSIX does make it hard for a lot of folks to make the switch.</p>
<p>Of course it&rsquo;s totally possible to learn how to customize your prompt in bash
or whatever, and it doesn&rsquo;t even need to be that complicated (in bash I&rsquo;d
probably start with something like <code>export PS1='[\u@\h \W$(__git_ps1 &quot; (%s)&quot;)]\$ '</code>, or maybe use <a href="https://starship.rs/">starship</a>).
But each of these &ldquo;not complicated&rdquo; things really does add up and it&rsquo;s
especially tough if you need to keep your config in sync across several
systems.</p>
<p>An extremely popular solution to getting a &ldquo;modern&rdquo; shell experience is
<a href="https://ohmyz.sh/">oh-my-zsh</a>. It seems like a great project and I know a lot
of people use it very happily, but I&rsquo;ve struggled with configuration systems
like that in the past &ndash; it looks like right now the base oh-my-zsh adds about
3000 lines of config, and often I find that having an extra configuration
system makes it harder to debug what&rsquo;s happening when things go wrong. I
personally have a tendency to use the system to add a lot of extra plugins,
make my system slow, get frustrated that it&rsquo;s slow, and then delete it
completely and write a new config from scratch.</p>
<h3 id="issue-2-with-getting-to-a-modern-experience-the-text-editor">issue 2 with getting to a &ldquo;modern&rdquo; experience: the text editor</h3>
<p>In the terminal survey I ran recently, the most popular terminal text editors
by far were <code>vim</code>, <code>emacs</code>, and <code>nano</code>.</p>
<p>I think the main options for terminal text editors are:</p>
<ul>
<li>use vim or emacs and configure it to your liking, you can probably have any
feature you want if you put in the work</li>
<li>use nano and accept that you&rsquo;re going to have a pretty limited experience
(for example I don&rsquo;t think you can select text with the mouse and then &ldquo;cut&rdquo;
it in nano)</li>
<li>use <code>micro</code> or <code>helix</code> which seem to offer a pretty good out-of-the-box
experience, potentially occasionally run into issues with using a less
mainstream text editor</li>
<li>just avoid using a terminal text editor as much as possible, maybe use VSCode, use
VSCode&rsquo;s terminal for all your terminal needs, and mostly never edit files in
the terminal. Or I know a lot of people use <code>code</code> as their <code>EDITOR</code> in the terminal.</li>
</ul>
<h3 id="issue-3-individual-applications">issue 3: individual applications</h3>
<p>The last issue is that sometimes individual programs that I use are kind of
annoying. For example on my Mac OS machine, <code>/usr/bin/sqlite3</code> doesn&rsquo;t support
the <code>Ctrl+Left Arrow</code> keyboard shortcut. Fixing this to get a reasonable
terminal experience in SQLite was a little complicated, I had to:</p>
<ul>
<li>realize why this is happening (Mac OS won&rsquo;t ship GNU tools, and &ldquo;Ctrl-Left arrow&rdquo; support comes from GNU readline)</li>
<li>find a workaround (install sqlite from homebrew, which does have readline support)</li>
<li>adjust my environment (put Homebrew&rsquo;s sqlite3 in my PATH)</li>
</ul>
<p>I find that debugging application-specific issues like this is really not easy
and often it doesn&rsquo;t feel &ldquo;worth it&rdquo; &ndash; often I&rsquo;ll end up just dealing with
various minor inconveniences because I don&rsquo;t want to spend hours investigating
them. The only reason I was even able to figure this one out at all is that
I&rsquo;ve been spending a huge amount of time thinking about the terminal recently.</p>
<p>A big part of having a &ldquo;modern&rdquo; experience using terminal programs is just
using newer terminal programs, for example I can&rsquo;t be bothered to learn a
keyboard shortcut to sort the columns in <code>top</code>, but in <code>htop</code>  I can just click
on a column heading with my mouse to sort it. So I use htop instead! But discovering new more &ldquo;modern&rdquo; command line tools isn&rsquo;t easy (though
I made <a href="https://jvns.ca/blog/2022/04/12/a-list-of-new-ish--command-line-tools/">a list here</a>),
finding ones that I actually like using in practice takes time, and if you&rsquo;re
SSHed into another machine, they won&rsquo;t always be there.</p>
<h3 id="everything-affects-everything-else">everything affects everything else</h3>
<p>Something I find tricky about configuring my terminal to make everything &ldquo;nice&rdquo;
is that changing one seemingly small thing about my workflow can really affect
everything else. For example right now I don&rsquo;t use tmux. But if I needed to use
tmux again (for example because I was doing a lot of work SSHed into another
machine), I&rsquo;d need to think about a few things, like:</p>
<ul>
<li>if I wanted tmux&rsquo;s copy to synchronize with my system clipboard over
SSH, I&rsquo;d need to make sure that my terminal emulator has <a href="https://old.reddit.com/r/vim/comments/k1ydpn/a_guide_on_how_to_copy_text_from_anywhere/">OSC 52 support</a></li>
<li>if I wanted to use iTerm&rsquo;s tmux integration (which makes tmux tabs into iTerm
tabs), I&rsquo;d need to change how I configure colours &ndash; right now I set them
with a <a href="https://github.com/chriskempson/base16-shell/blob/588691ba71b47e75793ed9edfcfaa058326a6f41/scripts/base16-solarized-light.sh">shell script</a> that I run when my shell starts, but that means the
colours get lost when restoring a tmux session.</li>
</ul>
<p>and probably more things I haven&rsquo;t thought of. &ldquo;Using tmux means that I have to
change how I manage my colours&rdquo; sounds unlikely, but that really did happen to
me and I decided &ldquo;well, I don&rsquo;t want to change how I manage colours right now,
so I guess I&rsquo;m not using that feature!&rdquo;.</p>
<p>It&rsquo;s also hard to remember which features I&rsquo;m relying on &ndash; for example maybe
my current terminal <em>does</em> have OSC 52 support and because copying from tmux over SSH
has always Just Worked I don&rsquo;t even realize that that&rsquo;s something I need, and
then it mysteriously stops working when I switch terminals.</p>
<h3 id="change-things-slowly">change things slowly</h3>
<p>Personally even though I think my setup is not <em>that</em> complicated, it&rsquo;s taken
me 20 years to get to this point! Because terminal config changes are so likely
to have unexpected and hard-to-understand consequences, I&rsquo;ve found that if I
change a lot of terminal configuration all at once it makes it much harder to
understand what went wrong if there&rsquo;s a problem, which can be really
disorienting.</p>
<p>So I usually prefer to make pretty small changes, and accept that changes can
might take me a REALLY long time to get used to. For example I switched from
using <code>ls</code> to <a href="https://github.com/eza-community/eza">eza</a> a year or two ago and
while I like it (because <code>eza -l</code> prints human-readable file sizes by default)
I&rsquo;m still not quite sure about it. But also sometimes it&rsquo;s worth it to make a
big change, like I made the switch to fish (from bash) 10 years ago and I&rsquo;m
very happy I did.</p>
<h3 id="getting-a-modern-terminal-is-not-that-easy">getting a &ldquo;modern&rdquo; terminal is not that easy</h3>
<p>Trying to explain how &ldquo;easy&rdquo; it is to configure your terminal really just made
me think that it&rsquo;s kind of hard and that I still sometimes get confused.</p>
<p>I&rsquo;ve found that there&rsquo;s never one perfect way to configure things in the
terminal that will be compatible with every single other thing. I just need to
try stuff, figure out some kind of locally stable state that works for me, and
accept that if I start using a new tool it might disrupt the system and I might
need to rethink things.</p>

---

### ["Rules" that terminal programs follow](https://jvns.ca/blog/2024/11/26/terminal-rules/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Recently I&rsquo;ve been thinking about how everything that happens in the terminal
is some combination of:</p>
<ol>
<li>Your <strong>operating system</strong>&rsquo;s job</li>
<li>Your <strong>shell</strong>&rsquo;s job</li>
<li>Your <strong>terminal emulator</strong>&rsquo;s job</li>
<li>The job of <strong>whatever program you happen to be running</strong> (like <code>top</code> or <code>vim</code> or <code>cat</code>)</li>
</ol>
<p>The first three (your operating system, shell, and terminal emulator) are all kind of
known quantities &ndash; if you&rsquo;re using bash in GNOME Terminal on Linux, you can
more or less reason about how how all of those things interact, and some of
their behaviour is standardized by POSIX.</p>
<p>But the fourth one (&ldquo;whatever program you happen to be running&rdquo;) feels like it
could do ANYTHING. How are you supposed to know how a program is going to
behave?</p>
<p>This post is kind of long so here&rsquo;s a quick table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#programs-behave-surprisingly-consistently">programs behave surprisingly consistently</a></li>
<li><a href="https://jvns.ca/atom.xml#these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</a></li>
<li><a href="https://jvns.ca/atom.xml#it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: <code>Ctrl-W</code> should delete the last word</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</a></li>
<li><a href="https://jvns.ca/atom.xml#these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</a></li>
</ul>
<h3 id="programs-behave-surprisingly-consistently">programs behave surprisingly consistently</h3>
<p>As far as I know, there are no real standards for how programs in the terminal
should behave &ndash; the closest things I know of are:</p>
<ul>
<li>POSIX, which mostly dictates how your terminal emulator / OS / shell should
work together. I think it does specify a few things about how core utilities like
<code>cp</code> should work but AFAIK it doesn&rsquo;t have anything to say about how for
example <code>htop</code> should behave.</li>
<li>these <a href="https://clig.dev/">command line interface guidelines</a></li>
</ul>
<p>But even though there are no standards, in my experience programs in the
terminal behave in a pretty consistent way. So I wanted to write down a list of
&ldquo;rules&rdquo; that in my experience programs mostly follow.</p>
<h3 id="these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</h3>
<p>My goal here isn&rsquo;t to convince authors of terminal programs that they <em>should</em>
follow any of these rules. There are lots of exceptions to these and often
there&rsquo;s a good reason for those exceptions.</p>
<p>But it&rsquo;s very useful for me to know what behaviour to expect from a random new
terminal program that I&rsquo;m using. Instead of &ldquo;uh, programs could do literally
anything&rdquo;, it&rsquo;s &ldquo;ok, here are the basic rules I expect, and then I can keep a
short mental list of exceptions&rdquo;.</p>
<p>So I&rsquo;m just writing down what I&rsquo;ve observed about how programs behave in my 20
years of using the terminal, why I think they behave that way, and some
examples of cases where that rule is &ldquo;broken&rdquo;.</p>
<h3 id="it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</h3>
<p>There are a bunch of common conventions that I think are pretty clearly the
program&rsquo;s responsibility to implement, like:</p>
<ul>
<li>config files should go in <code>~/.BLAHrc</code> or <code>~/.config/BLAH/FILE</code> or <code>/etc/BLAH/</code> or something</li>
<li><code>--help</code> should print help text</li>
<li>programs should print &ldquo;regular&rdquo; output to stdout and errors to stderr</li>
</ul>
<p>But in this post I&rsquo;m going to focus on things that it&rsquo;s not 100% obvious are
the program&rsquo;s responsibility. For example it feels to me like a &ldquo;law of nature&rdquo;
that pressing <code>Ctrl-D</code> should quit a REPL, but programs often
need to explicitly implement support for it &ndash; even though <code>cat</code> doesn&rsquo;t need
to implement <code>Ctrl-D</code> support, <code>ipython</code> <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">does</a>. (more about that in &ldquo;rule 3&rdquo; below)</p>
<p>Understanding which things are the program&rsquo;s responsibility makes it much less
surprising when different programs&rsquo; implementations are slightly different.</p>
<h3 id="rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></h3>
<p>The main reason for this rule is that noninteractive programs will quit by
default on <code>Ctrl-C</code> if they don&rsquo;t set up a <code>SIGINT</code> signal handler, so this is
kind of a &ldquo;you should act like the default&rdquo; rule.</p>
<p>Something that trips a lot of people up is that this doesn&rsquo;t apply to
<strong>interactive</strong> programs like <code>python3</code> or <code>bc</code> or <code>less</code>. This is because in
an interactive program, <code>Ctrl-C</code> has a different job &ndash; if the program is
running an operation (like for example a search in <code>less</code> or some Python code
in <code>python3</code>), then <code>Ctrl-C</code> will interrupt that operation but not stop the
program.</p>
<p>As an example of how this works in an interactive program: here&rsquo;s the code <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/vi.py#L2225">in prompt-toolkit</a> (the library that iPython uses for handling input)
that aborts a search when you press <code>Ctrl-C</code>.</p>
<h3 id="rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></h3>
<p>TUI programs (like <code>less</code> or <code>htop</code>) will usually quit when you press <code>q</code>.</p>
<p>This rule doesn&rsquo;t apply to any program where pressing <code>q</code> to quit wouldn&rsquo;t make
sense, like <code>tmux</code> or text editors.</p>
<h3 id="rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</h3>
<p>REPLs (like <code>python3</code> or <code>ed</code>) will usually quit when you press <code>Ctrl-D</code> on an
empty line. This rule is similar to the <code>Ctrl-C</code> rule &ndash; the reason for this is
that by default if you&rsquo;re running a program (like <code>cat</code>) in &ldquo;cooked mode&rdquo;, then
the operating system will return an <code>EOF</code> when you press <code>Ctrl-D</code> on an empty
line.</p>
<p>Most of the REPLs I use (sqlite3, python3, fish, bash, etc) don&rsquo;t actually use
cooked mode, but they all implement this keyboard shortcut anyway to mimic the
default behaviour.</p>
<p>For example, here&rsquo;s <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">the code in prompt-toolkit</a>
that quits when you press Ctrl-D, and here&rsquo;s <a href="https://github.com/bminor/bash/blob/6794b5478f660256a1023712b5fc169196ed0a22/lib/readline/readline.c#L658-L672">the same code in readline</a>.</p>
<p>I actually thought that this one was a &ldquo;Law of Terminal Physics&rdquo; until very
recently because I&rsquo;ve basically never seen it broken, but you can see that it&rsquo;s
just something that each individual input library has to implement in the links
above.</p>
<p>Someone pointed out that the Erlang REPL does not quit when you press <code>Ctrl-D</code>,
so I guess not every REPL follows this &ldquo;rule&rdquo;.</p>
<h3 id="rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</h3>
<p>Terminal programs rarely use colours other than the base 16 ANSI colours. This
is because if you specify colours with a hex code, it&rsquo;s very likely to clash
with some users&rsquo; background colour. For example if I print out some text as
<code>#EEEEEE</code>, it would be almost invisible on a white background, though it would
look fine on a dark background.</p>
<p>But if you stick to the default 16 base colours, you have a much better chance
that the user has configured those colours in their terminal emulator so that
they work reasonably well with their background color. Another reason to stick
to the default base 16 colours is that it makes less assumptions about what
colours the terminal emulator supports.</p>
<p>The only programs I usually see breaking this &ldquo;rule&rdquo; are text editors, for
example Helix by default will use a purple background which is not a default
ANSI colour. It seems fine for Helix to break this rule since Helix isn&rsquo;t a
&ldquo;core&rdquo; program and I assume any Helix user who doesn&rsquo;t like that colorscheme
will just change the theme.</p>
<h3 id="rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</h3>
<p>Almost every program I use supports <code>readline</code> keybindings if it would make
sense to do so. For example, here are a bunch of different programs and a link
to where they define <code>Ctrl-E</code> to go to the end of the line:</p>
<ul>
<li>ipython (<a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/emacs.py#L72">Ctrl-E defined here</a>)</li>
<li>atuin (<a href="https://github.com/atuinsh/atuin/blob/a67cfc82fe0dc907a01f07a0fd625701e062a33b/crates/atuin/src/command/client/search/interactive.rs#L407">Ctrl-E defined here</a>)</li>
<li>fzf (<a href="https://github.com/junegunn/fzf/blob/bb55045596d6d08445f3c6d320c3ec2b457462d1/src/terminal.go#L611">Ctrl-E defined here</a>)</li>
<li>zsh (<a href="https://github.com/zsh-users/zsh/blob/86d5f24a3d28541f242eb3807379301ea976de87/Src/Zle/zle_bindings.c#L94">Ctrl-E defined here</a>)</li>
<li>fish (<a href="https://github.com/fish-shell/fish-shell/blob/99fa8aaaa7956178973150a03ce4954ab17a197b/share/functions/fish_default_key_bindings.fish#L43">Ctrl-E defined here</a>)</li>
<li>tmux&rsquo;s command prompt (<a href="https://github.com/tmux/tmux/blob/ae8f2208c98e3c2d6e3fe4cad2281dce8fd11f31/key-bindings.c#L490">Ctrl-E defined here</a>)</li>
</ul>
<p>None of those programs actually uses <code>readline</code> directly, they just sort of
mimic emacs/readline keybindings. They don&rsquo;t always mimic them <em>exactly</em>: for
example atuin seems to use <code>Ctrl-A</code> as a prefix, so <code>Ctrl-A</code> doesn&rsquo;t go to the
beginning of the line.</p>
<p>Also all of these programs seem to implement their own internal cut and paste
buffers so you can delete a line with <code>Ctrl-U</code> and then paste it with <code>Ctrl-Y</code>.</p>
<p>The exceptions to this are:</p>
<ul>
<li>some programs (like <code>git</code>, <code>cat</code>, and <code>nc</code>) don&rsquo;t have any line editing support at all (except for backspace, <code>Ctrl-W</code>, and <code>Ctrl-U</code>)</li>
<li>as usual text editors are an exception, every text editor has its own
approach to editing text</li>
</ul>
<p>I wrote more about this &ldquo;what keybindings does a program support?&rdquo; question in
<a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a>.</p>
<h3 id="rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: Ctrl-W should delete the last word</h3>
<p>I&rsquo;ve never seen a program (other than a text editor) where <code>Ctrl-W</code> <em>doesn&rsquo;t</em>
delete the last word. This is similar to the <code>Ctrl-C</code> rule &ndash; by default if a
program is in &ldquo;cooked mode&rdquo;, the OS will delete the last word if you press
<code>Ctrl-W</code>, and delete the whole line if you press <code>Ctrl-U</code>. So usually programs
will imitate that behaviour.</p>
<p>I can&rsquo;t think of any exceptions to this other than text editors but if there
are I&rsquo;d love to hear about them!</p>
<h3 id="rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</h3>
<p>Most programs will disable colours when writing to a pipe. For example:</p>
<ul>
<li><code>rg blah</code> will highlight all occurrences of <code>blah</code> in the output, but if the
output is to a pipe or a file, it&rsquo;ll turn off the highlighting.</li>
<li><code>ls --color=auto</code> will use colour when writing to a terminal, but not when
writing to a pipe</li>
</ul>
<p>Both of those programs will also format their output differently when writing
to the terminal: <code>ls</code> will organize files into columns, and ripgrep will group
matches with headings.</p>
<p>If you want to force the program to use colour (for example because you want to
look at the colour), you can use <code>unbuffer</code> to force the program&rsquo;s output to be
a tty like this:</p>
<pre><code>unbuffer rg blah |  less -R
</code></pre>
<p>I&rsquo;m sure that there are some programs that &ldquo;break&rdquo; this rule but I can&rsquo;t think
of any examples right now. Some programs have an <code>--color</code> flag that you can
use to force colour to be on, in the example above you could also do <code>rg --color=always | less -R</code>.</p>
<h3 id="rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</h3>
<p>Usually if you pass <code>-</code> to a program instead of a filename, it&rsquo;ll read from
stdin or write to stdout (whichever is appropriate). For example, if you want
to format the Python code that&rsquo;s on your clipboard with <code>black</code> and then copy
it, you could run:</p>
<pre><code>pbpaste | black - | pbcopy
</code></pre>
<p>(<code>pbpaste</code> is a Mac program, you can do something similar on Linux with <code>xclip</code>)</p>
<p>My impression is that most programs implement this if it would make sense and I
can&rsquo;t think of any exceptions right now, but I&rsquo;m sure there are many
exceptions.</p>
<h3 id="these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</h3>
<p>These rules took me a long time for me to learn because I had to:</p>
<ol>
<li>learn that the rule applied anywhere at all (&quot;<code>Ctrl-C</code> will exit programs&quot;)</li>
<li>notice some exceptions (&ldquo;okay, <code>Ctrl-C</code> will exit <code>find</code> but not <code>less</code>&rdquo;)</li>
<li>subconsciously figure out what the pattern is (&quot;<code>Ctrl-C</code> will generally quit
noninteractive programs, but in interactive programs it might interrupt the
current operation instead of quitting the program&quot;)</li>
<li>eventually maybe formulate it into an explicit rule that I know</li>
</ol>
<p>A lot of my understanding of the terminal is honestly still in the
&ldquo;subconscious pattern recognition&rdquo; stage. The only reason I&rsquo;ve been taking the
time to make things explicit at all is because I&rsquo;ve been trying to explain how
it works to others. Hopefully writing down these &ldquo;rules&rdquo; explicitly will make
learning some of this stuff a little bit faster for others.</p>

---

### [Why pipes sometimes get "stuck": buffering](https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Here&rsquo;s a niche terminal problem that has bothered me for years but that I never
really understood until a few weeks ago. Let&rsquo;s say you&rsquo;re running this command
to watch for some specific output in a log file:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>If log lines are being added to the file relatively slowly, the result I&rsquo;d see
is&hellip; nothing! It doesn&rsquo;t matter if there were matches in the log file or not,
there just wouldn&rsquo;t be any output.</p>
<p>I internalized this as &ldquo;uh, I guess pipes just get stuck sometimes and don&rsquo;t
show me the output, that&rsquo;s weird&rdquo;, and I&rsquo;d handle it by just
running <code>grep thing1 /some/log/file | grep thing2</code> instead, which would work.</p>
<p>So as I&rsquo;ve been doing a terminal deep dive over the last few months I was
really excited to finally learn exactly why this happens.</p>
<h3 id="why-this-happens-buffering">why this happens: buffering</h3>
<p>The reason why &ldquo;pipes get stuck&rdquo; sometimes is that it&rsquo;s VERY common for
programs to buffer their output before writing it to a pipe or file. So the
pipe is working fine, the problem is that the program never even wrote the data
to the pipe!</p>
<p>This is for performance reasons: writing all output immediately as soon as you
can uses more system calls, so it&rsquo;s more efficient to save up data until you
have 8KB or so of data to write (or until the program exits) and THEN write it
to the pipe.</p>
<p>In this example:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>the problem is that <code>grep thing1</code> is saving up all of its matches until it has
8KB of data to write, which might literally never happen.</p>
<h3 id="programs-don-t-buffer-when-writing-to-a-terminal">programs don&rsquo;t buffer when writing to a terminal</h3>
<p>Part of why I found this so disorienting is that <code>tail -f file | grep thing</code>
will work totally fine, but then when you add the second <code>grep</code>, it stops
working!! The reason for this is that the way <code>grep</code> handles buffering depends
on whether it&rsquo;s writing to a terminal or not.</p>
<p>Here&rsquo;s how <code>grep</code> (and many other programs) decides to buffer its output:</p>
<ul>
<li>Check if stdout is a terminal or not using the <code>isatty</code> function
<ul>
<li>If it&rsquo;s a terminal, use line buffering (print every line immediately as soon as you have it)</li>
<li>Otherwise, use &ldquo;block buffering&rdquo; &ndash; only print data if you have at least 8KB or so of data to print</li>
</ul>
</li>
</ul>
<p>So if <code>grep</code> is writing directly to your terminal then you&rsquo;ll see the line as
soon as it&rsquo;s printed, but if it&rsquo;s writing to a pipe, you won&rsquo;t.</p>
<p>Of course the buffer size isn&rsquo;t always 8KB for every program, it depends on the implementation. For <code>grep</code> the buffering is handled by libc, and libc&rsquo;s buffer size is
defined in the <code>BUFSIZ</code> variable. <a href="https://github.com/bminor/glibc/blob/c69e8cccaff8f2d89cee43202623b33e6ef5d24a/libio/stdio.h#L100">Here&rsquo;s where that&rsquo;s defined in glibc</a>.</p>
<p>(as an aside: &ldquo;programs do not use 8KB output buffers when writing to a
terminal&rdquo; isn&rsquo;t, like, a law of terminal physics, a program COULD use an 8KB
buffer when writing output to a terminal if it wanted, it would just be
extremely weird if it did that, I can&rsquo;t think of any program that behaves that
way)</p>
<h3 id="commands-that-buffer-commands-that-don-t">commands that buffer &amp; commands that don&rsquo;t</h3>
<p>One annoying thing about this buffering behaviour is that you kind of need to
remember which commands buffer their output when writing to a pipe.</p>
<p>Some commands that <strong>don&rsquo;t</strong> buffer their output:</p>
<ul>
<li>tail</li>
<li>cat</li>
<li>tee</li>
</ul>
<p>I think almost everything else will buffer output, especially if it&rsquo;s a command
where you&rsquo;re likely to be using it for batch processing. Here&rsquo;s a list of some
common commands that buffer their output when writing to a pipe, along with the
flag that disables block buffering.</p>
<ul>
<li>grep (<code>--line-buffered</code>)</li>
<li>sed (<code>-u</code>)</li>
<li>awk (there&rsquo;s a <code>fflush()</code> function)</li>
<li>tcpdump (<code>-l</code>)</li>
<li>jq (<code>-u</code>)</li>
<li>tr (<code>-u</code>)</li>
<li>cut (can&rsquo;t disable buffering)</li>
</ul>
<p>Those are all the ones I can think of, lots of unix commands (like <code>sort</code>) may
or may not buffer their output but it doesn&rsquo;t matter because <code>sort</code> can&rsquo;t do
anything until it finishes receiving input anyway.</p>
<p>Also I did my best to test both the Mac OS and GNU versions of these but there
are a lot of variations and I might have made some mistakes.</p>
<h3 id="programming-languages-where-the-default-print-statement-buffers">programming languages where the default &ldquo;print&rdquo; statement buffers</h3>
<p>Also, here are a few programming language where the default print statement
will buffer output when writing to a pipe, and some ways to disable buffering
if you want:</p>
<ul>
<li>C (disable with <code>setvbuf</code>)</li>
<li>Python (disable with <code>python -u</code>, or <code>PYTHONUNBUFFERED=1</code>, or <code>sys.stdout.reconfigure(line_buffering=False)</code>, or <code>print(x, flush=True)</code>)</li>
<li>Ruby (disable with <code>STDOUT.sync = true</code>)</li>
<li>Perl (disable with <code>$| = 1</code>)</li>
</ul>
<p>I assume that these languages are designed this way so that the default print
function will be fast when you&rsquo;re doing batch processing.</p>
<p>Also whether output is buffered or not might depend on how you print, for
example in C++ <code>cout &lt;&lt; &quot;hello\n&quot;</code> buffers when writing to a pipe but <code>cout &lt;&lt; &quot;hello&quot; &lt;&lt; endl</code> will flush its output.</p>
<h3 id="when-you-press-ctrl-c-on-a-pipe-the-contents-of-the-buffer-are-lost">when you press <code>Ctrl-C</code> on a pipe, the contents of the buffer are lost</h3>
<p>Let&rsquo;s say you&rsquo;re running this command as a hacky way to watch for DNS requests
to <code>example.com</code>, and you forgot to pass <code>-l</code> to tcpdump:</p>
<pre><code>sudo tcpdump -ni any port 53 | grep example.com
</code></pre>
<p>When you press <code>Ctrl-C</code>, what happens? In a magical perfect world, what I would
<em>want</em> to happen is for <code>tcpdump</code> to flush its buffer, <code>grep</code> would search for
<code>example.com</code>, and I would see all the output I missed.</p>
<p>But in the real world, what happens is that all the programs get killed and the
output in <code>tcpdump</code>&rsquo;s buffer is lost.</p>
<p>I think this problem is probably unavoidable &ndash; I spent a little time with
<code>strace</code> to see how this works and <code>grep</code> receives the <code>SIGINT</code> before
<code>tcpdump</code> anyway so even if <code>tcpdump</code> tried to flush its buffer <code>grep</code> would
already be dead.</p>
<small>
<p>After a little more investigation, there is a workaround: if you find
<code>tcpdump</code>&rsquo;s PID and <code>kill -TERM $PID</code>, then tcpdump will flush the buffer so
you can see the output. That&rsquo;s kind of a pain but I tested it and it seems to
work.</p>
</small>
<h3 id="redirecting-to-a-file-also-buffers">redirecting to a file also buffers</h3>
<p>It&rsquo;s not just pipes, this will also buffer:</p>
<pre><code>sudo tcpdump -ni any port 53 &gt; output.txt
</code></pre>
<p>Redirecting to a file doesn&rsquo;t have the same &ldquo;<code>Ctrl-C</code> will totally destroy the
contents of the buffer&rdquo; problem though &ndash; in my experience it usually behaves
more like you&rsquo;d want, where the contents of the buffer get written to the file
before the program exits. I&rsquo;m not 100% sure whether this is something you can
always rely on or not.</p>
<h3 id="a-bunch-of-potential-ways-to-avoid-buffering">a bunch of potential ways to avoid buffering</h3>
<p>Okay, let&rsquo;s talk solutions. Let&rsquo;s say you&rsquo;ve run this command:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>I asked people on Mastodon how they would solve this in practice and there were
5 basic approaches. Here they are:</p>
<h4 id="solution-1-run-a-program-that-finishes-quickly">solution 1: run a program that finishes quickly</h4>
<p>Historically my solution to this has been to just avoid the &ldquo;command writing to
pipe slowly&rdquo; situation completely and instead run a program that will finish quickly
like this:</p>
<pre><code>cat /some/log/file | grep thing1 | grep thing2 | tail
</code></pre>
<p>This doesn&rsquo;t do the same thing as the original command but it does mean that
you get to avoid thinking about these weird buffering issues.</p>
<p>(you could also do <code>grep thing1 /some/log/file</code> but I often prefer to use an
&ldquo;unnecessary&rdquo; <code>cat</code>)</p>
<h4 id="solution-2-remember-the-line-buffer-flag-to-grep">solution 2: remember the &ldquo;line buffer&rdquo; flag to grep</h4>
<p>You could remember that grep has a flag to avoid buffering and pass it like this:</p>
<pre><code>tail -f /some/log/file | grep --line-buffered thing1 | grep thing2
</code></pre>
<h4 id="solution-3-use-awk">solution 3: use awk</h4>
<p>Some people said that if they&rsquo;re specifically dealing with a multiple greps
situation, they&rsquo;ll rewrite it to use a single <code>awk</code> instead, like this:</p>
<pre><code>tail -f /some/log/file |  awk '/thing1/ &amp;&amp; /thing2/'
</code></pre>
<p>Or you would write a more complicated <code>grep</code>, like this:</p>
<pre><code>tail -f /some/log/file |  grep -E 'thing1.*thing2'
</code></pre>
<p>(<code>awk</code> also buffers, so for this to work you&rsquo;ll want <code>awk</code> to be the last command in the pipeline)</p>
<h4 id="solution-4-use-stdbuf">solution 4: use <code>stdbuf</code></h4>
<p><code>stdbuf</code> uses LD_PRELOAD to turn off libc&rsquo;s buffering, and you can use it to turn off output buffering like this:</p>
<pre><code>tail -f /some/log/file | stdbuf -o0 grep thing1 | grep thing2
</code></pre>
<p>Like any <code>LD_PRELOAD</code> solution it&rsquo;s a bit unreliable &ndash; it doesn&rsquo;t work on
static binaries, I think won&rsquo;t work if the program isn&rsquo;t using libc&rsquo;s
buffering, and doesn&rsquo;t always work on Mac OS. Harry Marr has a really nice <a href="https://hmarr.com/blog/how-stdbuf-works/">How stdbuf works</a> post.</p>
<h4 id="solution-5-use-unbuffer">solution 5: use <code>unbuffer</code></h4>
<p><code>unbuffer program</code> will force the program&rsquo;s output to be a TTY, which means
that it&rsquo;ll behave the way it normally would on a TTY (less buffering, colour
output, etc). You could use it in this example like this:</p>
<pre><code>tail -f /some/log/file | unbuffer grep thing1 | grep thing2
</code></pre>
<p>Unlike <code>stdbuf</code> it will always work, though it might have unwanted side
effects, for example <code>grep thing1</code>&rsquo;s will also colour matches.</p>
<p>If you want to install unbuffer, it&rsquo;s in the <code>expect</code> package.</p>
<h3 id="that-s-all-the-solutions-i-know-about">that&rsquo;s all the solutions I know about!</h3>
<p>It&rsquo;s a bit hard for me to say which one is &ldquo;best&rdquo;, I think personally I&rsquo;m
mostly likely to use <code>unbuffer</code> because I know it&rsquo;s always going to work.</p>
<p>If I learn about more solutions I&rsquo;ll try to add them to this post.</p>
<h3 id="i-m-not-really-sure-how-often-this-comes-up">I&rsquo;m not really sure how often this comes up</h3>
<p>I think it&rsquo;s not very common for me to have a program that slowly trickles data
into a pipe like this, normally if I&rsquo;m using a pipe a bunch of data gets
written very quickly, processed by everything in the pipeline, and then
everything exits. The only examples I can come up with right now are:</p>
<ul>
<li>tcpdump</li>
<li><code>tail -f</code></li>
<li>watching log files in a different way like with <code>kubectl logs</code></li>
<li>the output of a slow computation</li>
</ul>
<h3 id="what-if-there-were-an-environment-variable-to-disable-buffering">what if there were an environment variable to disable buffering?</h3>
<p>I think it would be cool if there were a standard environment variable to turn
off buffering, like <code>PYTHONUNBUFFERED</code> in Python. I got this idea from a
<a href="https://blog.plover.com/Unix/stdio-buffering.html">couple</a> of <a href="https://blog.plover.com/Unix/stdio-buffering-2.html">blog posts</a> by Mark Dominus
in 2018. Maybe <code>NO_BUFFER</code> like <a href="https://no-color.org/">NO_COLOR</a>?</p>
<p>The design seems tricky to get right; Mark points out that NETBSD has <a href="https://man.netbsd.org/setbuf.3">environment variables called <code>STDBUF</code>, <code>STDBUF1</code>, etc</a> which gives you a
ton of control over buffering but I imagine most developers don&rsquo;t want to
implement many different environment variables to handle a relatively minor
edge case.</p>
<p>I&rsquo;m also curious about whether there are any programs that just automatically
flush their output buffers after some period of time (like 1 second). It feels
like it would be nice in theory but I can&rsquo;t think of any program that does that
so I imagine there are some downsides.</p>
<h3 id="stuff-i-left-out">stuff I left out</h3>
<p>Some things I didn&rsquo;t talk about in this post since these posts have been
getting pretty long recently and seriously does anyone REALLY want to read 3000
words about buffering?</p>
<ul>
<li>the difference between line buffering and having totally unbuffered output</li>
<li>how buffering to stderr is different from buffering to stdout</li>
<li>this post is only about buffering that happens <strong>inside the program</strong>, your
operating system&rsquo;s TTY driver also does a little bit of buffering sometimes</li>
<li>other reasons you might need to flush your output other than &ldquo;you&rsquo;re writing
to a pipe&rdquo;</li>
</ul>

---

### [2025 Recap: so many projects](https://fasterthanli.me/articles/2025-recap)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I‚Äôve been working on so many projects in 2025, I thought it was important for me
to make a recap, if only just to clear my head.</p>

<p>There are many, many, many things to go through and we don‚Äôt have a sponsor
today, so I‚Äôm gonna start right away with facet!</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#facet" id="facet"><h2>facet</h2></a>
<p>facet is a project that I started working on in March of this year ‚Äî that‚Äôs
right, it‚Äôs only been ten months, yet it feels like an eternity.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#in-the-beginning" id="in-the-beginning"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#the-first-golden-age" id="the-first-golden-age"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#disappointment" id="disappointment"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#refocus" id="refocus"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#volte-face" id="volte-face"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#just-in-time-for-the-new-year" id="just-in-time-for-the-new-year"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#dodeca" id="dodeca"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#rapace" id="rapace"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#tracey" id="tracey"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#picante" id="picante"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#pikru" id="pikru"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#aasvg-rs" id="aasvg-rs"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#facet-keeps-growing" id="facet-keeps-growing"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#fs-kitty" id="fs-kitty"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#vixen" id="vixen"></a>






























































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Introducing arborium, a tree-sitter distribution](https://fasterthanli.me/articles/introducing-arborium)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>About two weeks ago I entered a discussion with the docs.rs team about,
basically, why we have to look at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages but they're all white on black. It's sad.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-no-colors@2x~eb49252c206ebe40.jxl" title="" width="1083" /></p><p>When we could be looking at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages. this time it's colored.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-yes-colors@2x~708d0f07e1265747.jxl" title="" width="1083" /></p><p>And of course, as always, there are reasons why things are the way they are.
In an effort to understand those reasons, I opened a GitHub issue which resulted
in a <a href="https://github.com/rust-lang/docs.rs/issues/3040">short but productive</a> discussion.</p>

<p>I walked away discouraged, and then decided to, reasons be damned, attack this
problem from three different angles.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#background" id="background"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#problems" id="problems"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#solutions" id="solutions"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-1-just-include-this-script" id="angle-1-just-include-this-script"></a>

























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-2-it-goes-in-the-rustdoc-hole" id="angle-2-it-goes-in-the-rustdoc-hole"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#angle-3-only-in-the-backend" id="angle-3-only-in-the-backend"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#post-mortem" id="post-mortem"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Does Dioxus spark joy?](https://fasterthanli.me/articles/does-dioxus-spark-joy)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <div class="dialog right">
<div class="dialog-head" title="Amos says:">
  <source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Amos" height="42" src="https://cdn.fasterthanli.me/content/img/reimena/amos-neutral~55a3477398fe0cb1.jxl" width="42" />
</div>
<div class="dialog-text markup-container">
<p>Note: this article is adapted from a presentation I gave at a Rust Paris Meetup ‚Äî that‚Äôs why
it sounds a little different than usual. Enjoy!</p>

</div>
</div><p>Good evening! Tonight, I will attempt to answer the question: Does
<a href="https://github.com/dioxuslabs/dioxus">Dioxus</a> spark joy? Or at the very least,
whimsy.</p>

<p>What‚Äôs Dioxus, you ask? It is first and foremost a name that is quote: ‚Äúlegally
not inspired by any Pok√©mon‚Äù.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="The deoxys pokemon
" class="" height="449" src="https://cdn.fasterthanli.me/content/articles/does-dioxus-spark-joy/deoxys@2x~d76eb1e3eeda1b65.jxl" title="" width="422" /></p><p>Even if the author concedes <a href="https://news.ycombinator.com/item?id=39853385">in a Hacker News comment</a> that the ‚ÄúDeoxys‚Äù Pok√©mon
is, I quote: ‚Äúawesome‚Äù.</p>











<a class="anchor" href="https://fasterthanli.me/index.xml#a-short-and-mostly-wrong-history-of-web-apps" id="a-short-and-mostly-wrong-history-of-web-apps"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#a-practical-example" id="a-practical-example"></a>































<a class="anchor" href="https://fasterthanli.me/index.xml#love-hate" id="love-hate"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#does-dioxus-spark-joy" id="does-dioxus-spark-joy"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#afterword" id="afterword"></a>

---

### [Engineering a Rust optimization quiz](https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are several Rust quizzes online, including one that‚Äôs literally called the
‚ÄúUnfair Rust Quiz‚Äù at <a href="https://this.quiz.is.fckn.gay/">https:&#x2f;&#x2f;this.quiz.is.fckn.gay&#x2f;</a>, but when I was given the
opportunity to record an episode of the <a href="https://sdr-podcast.com/">Self-Directed Research
podcast</a> live on the main stage of <a href="https://eurorust.eu/2025/">EuroRust
2025</a>, I thought I‚Äôd come up with something special.</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Question Misc 6 of the unfair Rust quiz, about drop order. " height="714" src="https://cdn.fasterthanli.me/content/articles/engineering-a-rust-optimization-quiz/unfair-rust-quiz@2x~478a86d3446281d7.jxl" title="The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side. " width="795" /><figcaption><div class="markup-container figure-caption"><p>The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side.</p>
</div></figcaption>
</figure>



<a class="anchor" href="https://fasterthanli.me/index.xml#coming-up-with-the-questions" id="coming-up-with-the-questions"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#nih-syndrome-ppt" id="nih-syndrome-ppt"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#server-side-shenanigans-and-room-codes" id="server-side-shenanigans-and-room-codes"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#deploying-the-beast" id="deploying-the-beast"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#d-2-starting-fights-at-a-paris-meetup" id="d-2-starting-fights-at-a-paris-meetup"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#d-1-panic-mode-and-missing-explanations" id="d-1-panic-mode-and-missing-explanations"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#day-of-github-oauth-and-swipe-gestures" id="day-of-github-oauth-and-swipe-gestures"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#showtime" id="showtime"></a>

---

### [Making our own spectrogram](https://fasterthanli.me/articles/making-our-own-spectrogram)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>A couple months ago <a href="https://fasterthanli.me/articles/the-science-of-loudness">I made a loudness meter</a>
and went way too in-depth into how humans have measured loudness over time.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the fasterthanlime audio meter, with RMS, sample peak, true peak, and various loudness metrics.
" class="" height="512" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/fasterthanlime-audio-meter@2x~d5e6b54e3ade21f5.jxl" title="" width="800" /></p><p>Today we‚Äôre looking at a spectrogram visualization I made, which is a lot more entertaining!</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="" class="" height="632" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/spectrogram-sonata-op25-3@2x~c22af608785d58b2.jxl" title="" width="1100" /></p><p>We‚Äôre going to talk about how to extract frequencies from sound waves, but also
how my spectrogram app is assembled from different Rust crates, how it
handles audio and graphics threads, how it draws the spectrogram etc.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#the-humble-sine-wave" id="the-humble-sine-wave"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#approximating-a-square-wave" id="approximating-a-square-wave"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#a-real-world-sample" id="a-real-world-sample"></a>







<a class="anchor" href="https://fasterthanli.me/index.xml#chunking-and-windowing" id="chunking-and-windowing"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#the-gabor-limit" id="the-gabor-limit"></a>





















<a class="anchor" href="https://fasterthanli.me/index.xml#interpolation" id="interpolation"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#color-mapping" id="color-mapping"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#frequency-mapping" id="frequency-mapping"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#audio-input" id="audio-input"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#drawing-the-ui" id="drawing-the-ui"></a>



















































<a class="anchor" href="https://fasterthanli.me/index.xml#updating-the-texture" id="updating-the-texture"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#profiling-my-program" id="profiling-my-program"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#having-fun" id="having-fun"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#closing-words" id="closing-words"></a>

---

### [crates.io phishing attempt](https://fasterthanli.me/articles/crates-io-phishing-attempt)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>Earlier this week, an <a href="https://fasterthanli.me/articles/color-npm-package-compromised">npm supply chain attack</a>.</p>

<p>It‚Äôs turn for <a href="https://crates.io">crates.io</a>, the main public repository for <a href="https://rust-lang.org">Rust</a>
crates (packages).</p>

<p>The phishing e-mail looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="A phishing e-mail: Important: Breach notification regarding crates.io  Hi, BurntSushi! We recently discovered that an unauthorized actor had compromised the crates.io infrastructure and accessed a limited amount of user information. The attacker's access was revoked, and we are currently reviewing our security posture. We are currently drafting a blog post to outline the timeline and the steps we took to mitigate this. In the meantime, we strongly suggest you to rotate your login info by signing in here to our internal SSO, which is a temporary fix to ensure that the attacker cannot modify any packages published by you. " height="653" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/phishing-email~052f360399d29116.jxl" title="" width="708" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;burntsushi.net&#x2f;post&#x2f;3lynehptw6c2n">Andrew Gallant on BlueSky
</a></cite></figcaption>
</figure><p>And it leads to a GitHub login page that looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A fake GitHub sign-in page. " height="1378" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/github-phish~e78524d35ede5efb.jxl" title="" width="1322" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;github.com&#x2f;rust-lang&#x2f;crates.io&#x2f;discussions&#x2f;11889#discussion-8886064">Barre on GitHub
</a></cite></figcaption>
</figure><p>Several maintainers received it ‚Äî the issue is being discussed <a href="https://github.com/rust-lang/crates.io/discussions/11889">on GitHub</a>.</p>

<p>The <a href="https://www.rust-lang.org/governance/teams/dev-tools#team-crates-io">crates.io team</a> has acknowledged
the attack and said they‚Äôd see if they can do something about it.</p>

---

### [color npm package compromised](https://fasterthanli.me/articles/color-npm-package-compromised)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>On September 8 2025, around 13:00 UTC, someone compromised <a href="https://www.npmjs.com/~qix">Josh Junon‚Äôs npm
account (qix)</a> and started publishing backdoored
versions of his package.</p>

<p>Someone noticed and let Josh know:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Hey. Your npm account seems to have been compromised. 1 hour ago it started posting packages with backdoors to all your popular packages. " height="177" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/charlie-noticed@2x~4a9e74f87760a4af.jxl" title="" width="595" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;charlieeriksen.bsky.social&#x2f;post&#x2f;3lydffcyulc2n">Charlie Eriksen on BlueSky
</a></cite></figcaption>
</figure><p>Josh confirmed he‚Äôd gotten pwned by a fake 2FA (two-factor authentication) reset e-mail:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Yep, I've been pwned. 2FA reset email, looked very legitimate.  Only NPM affected. I've sent an email off to @npmjs.bsky.social  to see if I can get access again.  Sorry everyone, I should have paid more attention. Not like me; have had a stressful week. Will work to get this cleaned up. " height="396" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/josh-fake-2fa@2x~ca37f72d582a4442.jxl" title="" width="592" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;bad-at-computer.bsky.social&#x2f;post&#x2f;3lydioq5swk2y">Josh Junon on BlueSky
</a></cite></figcaption>
</figure><p>The phishing e-mail came from <code>npmsj.help</code> (registered 3 days prior) and claimed
users had to reset their 2FA:</p>







<a class="anchor" href="https://fasterthanli.me/index.xml#the-payload" id="the-payload"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#current-situation" id="current-situation"></a>

---

### [The science of loudness](https://fasterthanli.me/articles/the-science-of-loudness)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>My watch has a ‚ÄúNoise‚Äù app: it shows <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">B</mi></mrow></math>, for decibels.</p>

<p><video alt="A video of my Apple Watch showing me how loud the sound coming from my speakers is.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~d96a722c49fdda5d.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~e53858e76f36614d.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~fb169411c4d8653e.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>My amp has a volume knob, which also shows decibels, although.. negative ones, this time.</p>

<p><video alt="A video of me adjusting my volume on my Cambridge AXR100 amplifier. The
volume goes from -61 to -32 decibels in that video.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~46153fb1ad60f230.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~a8af11cbe6d90598.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~9a7388f3f768f948.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>And finally, my video editing software has a ton of meters ‚Äî which are all in decibel or
decibel-adjacent units.</p>

<p><video alt="A screenshot of DaVinci Resolve, showing various meters: we have Bus 1,
Control Room with TP, Loudness, YouTube (LUFS), then Loudness History
with Integrated, Momentary and Short Term. In the Mixer, each track has
its meter.
" class="" controls="controls" height="1127" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x.thumb" preload="none" title="" width="2002"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~9cc29f2809d1202a.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~da316f2ab0e0aeb5.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~e50f9e76cf48090c.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>How do all these decibels fit together?</p>






<a class="anchor" href="https://fasterthanli.me/index.xml#what-even-is-sound" id="what-even-is-sound"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#under-pressure" id="under-pressure"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#signal-processing" id="signal-processing"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#root-mean-square" id="root-mean-square"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#sample-peak-true-peak" id="sample-peak-true-peak"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#the-loudness-wars" id="the-loudness-wars"></a>











































































<a class="anchor" href="https://fasterthanli.me/index.xml#a-weighting" id="a-weighting"></a>

---

### [Summer fasterthanlime update](https://fasterthanli.me/articles/summer-fasterthanlime-update)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are news!</p>

<div class="tip markup-container">
<div class="tip-header bear-mark">
<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Cool bear" class="bear" height="48" src="https://cdn.fasterthanli.me/content/img/reimena/coolbear-idea~85823bd96ffd0bb6.jxl" width="48" />
Cool Bear's hot tip
</div>
<p>TL;DR: If you‚Äôre a patron or sponsor, check your <a href="https://fasterthanli.me/profile">Profile</a> page to
get detailed explainers of every perk. You‚Äôll need to log in. Duh.</p>

</div><p>Here are all the changes I‚Äôm implementing, summarized as a table:</p>

<div class="responsive-table"><table><thead><td>Before</td><td>After</td></thead><tr><td>üìö Articles remain exclusive for <strong>6 months</strong></td><td>Early access (<strong>couple weeks</strong>) for Silver tier</td></tr><tr><td>üéûÔ∏è No early access for video</td><td><strong>Video early access</strong> on Patreon and website</td></tr></table></div>

<a class="anchor" href="https://fasterthanli.me/index.xml#looking-back" id="looking-back"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#a-discord-server" id="a-discord-server"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#early-access-revamp" id="early-access-revamp"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#dual-rss-feeds" id="dual-rss-feeds"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#bye-ko-fi" id="bye-ko-fi"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#more-casual-posting" id="more-casual-posting"></a>




<a class="anchor" href="https://fasterthanli.me/index.xml#what-about-content-that-was-still-exclusive" id="what-about-content-that-was-still-exclusive"></a>

---

### [All color is best-effort](https://fasterthanli.me/articles/all-color-is-best-effort)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I do not come to you with answers today, but rather some observations and a lot of questions.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#the-weird-glitch" id="the-weird-glitch"><h2>The weird glitch</h2></a>
<p>Recently I was editing some video and I noticed this:</p>

<p>



<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/dvr-circles@2x~3c031f1f42693336.jxl" title="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." width="1966" /></p>

<p>Not what the finger is pointing at ‚Äî the dots.</p>

<p>Here are the separate layers this image is made up of: the background is a stock image
I‚Äôve licensed from Envato Elements:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A picture of a canyon, darker than you‚Äôd expect." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/canyon-background@2x~dcfb5771209ddbd5.jxl" title="A picture of a canyon, darker than you‚Äôd expect." width="1966" /></p>

<p>Because I use it as a background image, I‚Äôve cranked down the exposition in the Color tab:</p>













































































<a class="anchor" href="https://fasterthanli.me/index.xml#playing-with-color-spaces" id="playing-with-color-spaces"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#cie-chromaticity-diagram" id="cie-chromaticity-diagram"></a>


























































<a class="anchor" href="https://fasterthanli.me/index.xml#our-first-transfer-function" id="our-first-transfer-function"></a>






















































<a class="anchor" href="https://fasterthanli.me/index.xml#parade-scope" id="parade-scope"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#more-transfer-functions" id="more-transfer-functions"></a>































































































<a class="anchor" href="https://fasterthanli.me/index.xml#how-white-is-your-white" id="how-white-is-your-white"></a>









































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

## Tech News

### [More assorted notes on Liquid Glass (2025)](https://morrick.me/archives/10068)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/5ttvyj/more_assorted_notes_on_liquid_glass_2025">Comments</a></p>

---

### [AI Coding Assistants Are Getting Worse](https://spectrum.ieee.org/ai-coding-degrades)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/6skrep/ai_coding_assistants_are_getting_worse">Comments</a></p>

---

### [Anthropic invests $1.5 million in the Python Software Foundation and open source security](https://pyfound.blogspot.com/2025/12/anthropic-invests-in-python.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/mb07nn/anthropic_invests_1_5_million_python">Comments</a></p>

---

### [Signal creator Moxie Marlinspike wants to do for AI what he did for messaging](https://arstechnica.com/security/2026/01/signal-creator-moxie-marlinspike-wants-to-do-for-ai-what-he-did-for-messaging/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/3bhiap/signal_creator_moxie_marlinspike_wants">Comments</a></p>

---

### [TruffleRuby 33 is Released](https://truffleruby.dev/blog/truffleruby-33-is-released)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p>TruffleRuby kicks off the year with a new <a href="https://truffleruby.dev" rel="ugc">website</a>, a new <a href="https://github.com/truffleruby/truffleruby/releases/tag/graal-33.0.0" rel="ugc">release</a>, and a <a href="https://truffleruby.dev/blog/truffleruby-33-is-released" rel="ugc">blog post</a> to go with it!</p>
<p>Many changes:</p>
<ul>
<li>New versioning</li>
<li>Thread-safe Hash</li>
<li>No system dependencies anymore</li>
<li>Installs in 2 seconds</li>
<li>Development is now fully in the open</li>
</ul>
<p><a href="https://lobste.rs/s/pwj9ue/truffleruby_33_is_released">Comments</a></p>

---

### [Your CLI's completion should know what options you've already typed](https://hackers.pub/@hongminhee/2026/optique-context-aware-cli-completion)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/5se1tq/your_cli_s_completion_should_know_what">Comments</a></p>

---

### [Date is out, Temporal is in](https://piccalil.li/blog/date-is-out-and-temporal-is-in/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/ui99tz/date_is_out_temporal_is">Comments</a></p>

---

### [HiTeX Press: A spam factory for AI-generated books](https://laurent.le-brun.eu/blog/hitex-a-spam-factory-for-ai-generated-books)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/qgfhc3/hitex_press_spam_factory_for_ai_generated">Comments</a></p>

---

### [Bevy 0.18](https://bevy.org/news/bevy-0-18/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/k5yvju/bevy_0_18">Comments</a></p>

---


---

## üìö Â¶Ç‰Ωï‰ΩøÁî®

1. ÊµèËßàÊÑüÂÖ¥Ë∂£ÁöÑÊ†áÈ¢ò
2. ÈòÖËØªAIÁîüÊàêÁöÑÊëòË¶ÅÂø´ÈÄü‰∫ÜËß£ÂÜÖÂÆπ
3. ÁÇπÂáªÈìæÊé•Ê∑±ÂÖ•ÈòÖËØª
4. Êúâ‰ª∑ÂÄºÁöÑÂÜÖÂÆπÂèØ‰ª•Êï¥ÁêÜÂà∞ÂØπÂ∫îÁöÑ‰∏ªÈ¢òÁõÆÂΩï

## üîß ÈÖçÁΩÆ

‰øÆÊîπ `config/sources.yaml` ÂèØ‰ª•:
- Ê∑ªÂä†/Âà†Èô§RSSËÆ¢ÈòÖÊ∫ê
- Ë∞ÉÊï¥HackerNewsÊúÄÂ∞èÂàÜÊï∞ÈòàÂÄº
- ÈÖçÁΩÆÂÜÖÂÆπËøáÊª§ÂÖ≥ÈîÆËØç

*Êú¨ÊñáÊ°£Áî± [daily-digestËÑöÊú¨](../scripts/generate_digest.py) Ëá™Âä®ÁîüÊàê*