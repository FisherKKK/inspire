# ÊØèÊó•ÊäÄÊúØÊëòË¶Å - 2026-01-25

> Ëá™Âä®ÁîüÊàê‰∫é 2026-01-25 01:16:36
> ÂÖ±Êî∂ÈõÜ 136 ÁØáÊñáÁ´†

## üìë ÁõÆÂΩï

- [AI Research](#ai-research) (6ÁØá)
- [AI/ML](#aiml) (1ÁØá)
- [Engineering](#engineering) (10ÁØá)
- [HackerNews](#hackernews) (79ÁØá)
- [LLM Infrastructure](#llm-infrastructure) (10ÁØá)
- [Operating Systems](#operating-systems) (1ÁØá)
- [Systems](#systems) (20ÁØá)
- [Tech News](#tech-news) (9ÁØá)

---

## AI Research

### [IABIED Book Review: Core Arguments and Counterarguments](https://www.lesswrong.com/posts/qFzWTTxW37mqnE6CA/iabied-book-review-core-arguments-and-counterarguments)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 24, 2026 2:25 PM GMT<br /><br /><p>The recent book ‚Äú<a href="https://en.wikipedia.org/wiki/If_Anyone_Builds_It,_Everyone_Dies">If Anyone Builds It Everyone Dies</a>‚Äù (September 2025) by Eliezer Yudkowsky and Nate Soares argues that creating superintelligent AI in the near future would almost certainly cause human extinction:</p><blockquote><p>If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the present understanding of AI, then everyone, everywhere on Earth, will die.</p></blockquote><p>The goal of this post is to summarize and evaluate the book‚Äôs core arguments and the main counterarguments critics have made against them.</p><p>Although several other book reviews have already been written I found many of them unsatisfying because a <a href="https://www.nytimes.com/2025/08/27/books/review/if-anyone-builds-it-everyone-dies-eliezer-yudowsky-nate-soares-ai-con-emily-bender-alex-hanna.html">lot</a> <a href="https://www.newscientist.com/article/2495333-no-ai-isnt-going-to-kill-us-all-despite-what-this-new-book-says/">of</a> <a href="https://www.nytimes.com/2025/09/12/technology/ai-eliezer-yudkowsky-book.html">them</a> are written by journalists who have the goal of writing an entertaining piece and only lightly cover the core arguments, or don‚Äôt seem understand them properly, and instead resort to weak arguments like straw-manning, ad hominem attacks or criticizing the style of the book.</p><p>So my goal is to write a book review that has the following properties:</p><ul><li>Written by someone who has read a substantial amount of AI alignment and LessWrong content and won‚Äôt make AI alignment beginner mistakes or misunderstandings (e.g. not knowing about the orthogonality thesis or instrumental convergence).</li><li>Focuses on deeply engaging solely with the book‚Äôs main arguments and offering high-quality counterarguments without resorting to the <a href="https://www.lesswrong.com/w/absurdity-heuristic">absurdity heuristic</a> or <a href="https://en.wikipedia.org/wiki/Ad_hominem">ad hominem</a> arguments.</li><li>Covers arguments both for and against the book's core arguments without arguing for a particular view.</li><li>Aims to be truth-seeking, rigorous and rational rather than entertaining.</li></ul><p>In other words, my goal is to write a book review that many LessWrong readers would find acceptable and interesting.</p><p>The book's core thesis can be broken down into four claims about how the future of AI is likely to go:</p><ol><li><strong>General intelligence is extremely powerful and potentially dangerous:</strong> Intelligence is very powerful and can completely change the world or even destroy it. The existence proof that confirms this belief is the existence of humans: humans had more general intelligence than other animals and ended up completely changing the world as a result.</li><li><strong>ASI is possible and likely to be created in the near future:</strong> Assuming that current trends continue, humanity will probably create an artificial superintelligence (ASI) that vastly exceeds human intelligence in the 21st century. Since general intelligence is powerful and is likely to be implemented in AI, AI will have a huge impact on the world in the 21st century.</li><li><strong>ASI alignment is extremely difficult to solve:</strong> Aligning an ASI with human values is extremely difficult and by default an ASI would have strange alien values that are incompatible with human survival and flourishing. The first ASI to be created would probably be misaligned, not because of malicious intent from its creator, but because its creators would be insufficiently competent enough to align it to human values correctly.</li><li><strong>A misaligned ASI would cause human extinction and that would be undesirable:</strong> Given claims 1, 2, and 3 the authors predict that humanity's default trajectory is to build a misaligned ASI and that doing so would cause human extinction. The authors consider this outcome to be highly undesirable and an existential catastrophe.</li></ol><p>Any of the four core claims of the book could be criticized. Depending on the criticism and perspective, I group the most common perspectives on the future of AI into four camps:</p><ol><li><strong>AI skeptics:</strong> Believe that high intelligence is overrated or not inherently safe. For example, some <a href="https://www.businesstoday.in/technology/news/story/completely-ridiculous-metas-chief-ai-scientist-yann-lecun-dismisses-elon-musks-civilisation-destruction-fear-383371-2023-05-30">people</a> argue that smart or nerdy people are not especially successful or dangerous, or that computers and LLMs have already surpassed human intelligence in many ways and are not dangerous. Another criticism in this category is the idea that AIs can be extremely intelligent but never truly <a href="https://a16z.com/ai-will-save-the-world/">want</a> things in the same way that humans do and therefore would always be subservient and harmless. Others in this camp may accept that general intelligence is powerful and influential but believe that ASI is impossible because the human brain is difficult to replicate, that ASI is very difficult to create, or that ASI is so <a href="https://www.theregister.com/2015/03/19/andrew_ng_baidu_ai/">far away</a> in the future that it's not worth thinking about.</li><li><strong>Singularitarians:</strong> Singularitarians or <a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">AI optimists</a> believe that high general intelligence is extremely impactful and potentially dangerous and ASI is likely to be created in the near future. But they believe the AI alignment problem is sufficiently easy that we don't need to worry about misaligned ASI. Instead they expect ASI to create a utopian world of material abundance where ASI transforms the world in a mostly desirable way.</li><li><strong>IABIED:</strong> the IABIED view, also known as 'AI doomers' believe that general intelligence is extremely powerful, ASI is likely to be created in the future, AI alignment is very difficult to solve, and that the default outcome is a misaligned ASI being created that causes human extinction.</li><li><strong>AI successionists:</strong> Finally <a href="https://www.wsj.com/tech/ai/ai-apocalypse-no-problem-6b691772">AI successionists</a> believe that the AI alignment problem is irrelevant. If misaligned ASI is created and causes human extinction it doesn't matter because it would be a successor species with its own values just as humans are a successor species to chimpanzees. They believe that increasing intelligence is the universe's natural development path that should be allowed to continue even if it results in human extinction.</li></ol><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qFzWTTxW37mqnE6CA/y9wsakomogbk2guyib9x" /><figcaption>Flowchart showing the beliefs of AI skeptics, singularitarians, the IABIED authors, and AI successionists.</figcaption></figure><p>I created a flowchart to illustrate how different beliefs about the future of AI lead to different camps which each have a distinct worldview.</p><p>Given the impact of humans on the world and rapid AI progress, I don't find the arguments of AI skeptics compelling and I believe the most knowledgeable thinkers and sophisticated critics are generally not in this camp.</p><p>The 'AI successionist' camp complicates things because they say that human extinction is not equivalent to an undesirable future where all value is destroyed. It‚Äôs an interesting perspective but I won‚Äôt be covering it in this review because it seems like a niche view, it‚Äôs only briefly covered by the book, and discussing it involves difficult philosophical problems like whether AI could be conscious.</p><p>This review focuses on the third core claim above: the belief that the AI alignment problem is very difficult to solve. I'm focusing on this claim because I think the other three are fairly obvious or are generally accepted by people who have seriously thought about this topic: AI is likely to be an extremely impactful technology in the future, ASI is likely to be created in the near future, and human extinction is undesirable. I‚Äôm focusing on the third core claim, the idea that the AI alignment problem is difficult, because it seems to be the claim that is most contested by sophisticated critics. Also many of the book's recommendations such as pausing ASI development are conditional on this claim being true. If ASI alignment is extremely difficult, we should stop ASI progress to avoid creating an ASI which would be misaligned with high probability and catastrophic for humanity in expectation. If AI alignment is easy, we should build an ASI to bring about a futuristic utopia. Therefore, one‚Äôs beliefs about the difficulty of the AI alignment problem is a key crux for deciding how we should govern the future of AI development.</p><h1>Background arguments to the key claim</h1><p>To avoid making this post too long, I‚Äôm going to assume that the following arguments made by the book are true:</p><ul><li><strong>General intelligence is extremely powerful.</strong> Humans are the first entities to have high general intelligence and used it to transform the world to better satisfy their own goals.</li><li><strong>ASI is possible and likely to be created in the near future.</strong> The laws of physics permit ASI to be created and economic incentives make it likely that ASI will be created in the near future because it would be profitable to do so.</li><li><strong>A misaligned ASI would cause human extinction and that would be undesirable.</strong> It's possible that an ASI could be misaligned and have alien goals. Conversely, it's also possible to create an ASI that would be aligned with human values (see the <a href="https://www.lesswrong.com/w/orthogonality-thesis">orthogonality thesis</a>).</li></ul><p>The book explains these arguments in detail in case you want to learn more about them. I‚Äôm making the assumption that these arguments are true because I haven‚Äôt seen high-quality counterarguments against them (and I doubt they exist).</p><p>In contrast, the book's claim that successfully aligning an ASI with human values is difficult and unlikely seems to be more controversial, is less obvious to me, and I have seen high-quality counterarguments against this claim. Therefore, I‚Äôm focusing on it in this post.</p><p>The following section focuses on what I think is one of the key claims and cruxes of the book: that solving the AI alignment problem would be extremely difficult and that the first ASI would almost certainly be misaligned and harmful to humanity rather than aligned and beneficial.</p><h1>The key claim: ASI alignment is extremely difficult to solve</h1><p>First, the key claim of the book is that the authors believe that building an ASI would lead to the extinction of humanity. Why? Because they believe that the AI alignment problem is so difficult, that we are very unlikely to successfully aim the first ASI at a desirable goal. Instead, they predict that the first ASI would have a strange, alien goal that is not compatible with human survival despite the best efforts of its designers to align its motivations with human values:</p><blockquote><p>All of what we‚Äôve described here‚Äîa bleak universe devoid of fun, in which Earth-originating life has been annihilated‚Äîis what a sufficiently alien intelligence would <i>most prefer</i>. We‚Äôve argued that an AI would want a world where lots of matter and energy was spent on its weird and alien ends, rather than on human beings staying alive and happy and free. Just like we, in our own ideal worlds, would be spending the universe‚Äôs resources on flourishing people leading fun lives, rather than on making sure that all our houses contained a large prime number of pebbles.</p></blockquote><p>A misaligned ASI would reshape the world and the universe to achieve its strange goal and its actions would cause the extinction of humanity since humans are irrelevant for the achievement of most strange goals. For example, a misaligned ASI that only cared about maximizing the number of paperclips in the universe would prefer to convert humans to paperclips instead of helping them have flourishing lives.</p><p>The next question is why the authors believe that ASI alignment would be so difficult.</p><p>To oversimplify, I think there are three underlying beliefs that explain why the authors believe that ASI alignment would be extremely difficult:</p><ol><li><strong>Human values are very specific, fragile, and a tiny space of all possible goals.</strong></li><li><strong>Current methods used to train goals into AIs are imprecise and unreliable.</strong></li><li><strong>The ASI alignment problem is hard because it has the properties of hard engineering challenges.</strong></li></ol><p>One analogy the authors have used before to explain the difficulty of AI alignment is landing a <a href="https://intelligence.org/2018/10/03/rocket-alignment/">rocket</a> on the moon: since the target is small, hitting it successfully requires extremely advanced and precise technology. In theory this is possible, however the authors believe that current AI creators do not have sufficient skill and knowledge to solve the AI alignment problem.</p><p>If aligning an ASI with human values is a narrow target, and we have a poor aim, consequently there is a low probability that we will successfully create an aligned ASI and a high probability of creating a misaligned ASI.</p><blockquote><p>The preferences that wind up in a mature AI are complicated, practically impossible to predict, and vanishingly unlikely to be aligned with our own, no matter how it was trained.</p></blockquote><p>One thing that's initially puzzling about the authors‚Äô view is their apparent overconfidence. If you don't know what's going to happen then how can you predict the outcome with high confidence? But it's still possible to be highly confident in an uncertain situation if you have the right prior. For example, even though you have no idea what the lottery number in a lottery is, you can predict with high confidence that you won't win the lottery because your prior probability of winning is so low.</p><p>The authors also believe that the AI alignment problem has "curses" similar to other hard engineering problems like launching a space probe, building a nuclear reactor safely, and building a secure computer system.</p><h2>1. Human values are a very specific, fragile, and tiny space of all possible goals</h2><p>One reason why AI alignment is difficult is that human morality and values may be a complex, fragile, and tiny target within the vast space of all possible goals. Therefore, AI alignment engineers have a small target to hit. Just as randomly shuffling metal parts is statistically unlikely to assemble a Boeing 747, a randomly selected goal from the space of all possible intelligences is unlikely to be compatible with human flourishing or survival (e.g. maximizing the number of paperclips in the universe). This intuition is also articulated in the blog post <a href="https://intelligence.org/2018/10/03/rocket-alignment/">The Rocket Alignment</a> problem which compares AI alignment to the problem of landing a rocket on the moon: both require deep understanding of the problem and precise engineering to hit a narrow target.</p><p>Similarly, the authors argue that human values are fragile: the loss of just a few key values like subjective experience or novelty could result in a future that seems dystopian and undesirable to us:</p><blockquote><p>"Or the converse problem - an agent that contains all the aspects of human value,&nbsp;<i>except</i>&nbsp;the valuation of subjective experience.&nbsp; So that the result is a nonsentient optimizer that goes around making genuine discoveries, but the discoveries are not savored and enjoyed, because there is no one there to do so.&nbsp; This, I admit, I don't quite know to be possible.&nbsp; Consciousness does still confuse me to some extent.&nbsp; But a universe with no one to bear witness to it, might as well not be." - <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">Value is Fragile</a></p></blockquote><p>A story the authors use to illustrate how human values are idiosyncratic is the 'correct nest aliens', a fictional intelligent alien bird species that prize having a prime number of stones in their nests as a consequence of the evolutionary process that created them similar to how most humans reflexively consider murder to be wrong. The point of the story is that even though our human values such as our morality, and our sense of humor feel natural and intuitive, they may be complex, arbitrary and contingent on humanity's specific evolutionary trajectory. If we build an ASI without successfully imprinting it with the nuances of human values, we should expect its values to be radically different and incompatible with human survival and flourishing. The story also illustrates the orthogonality thesis: a mind can be arbitrarily smart and yet pursue a goal that seems completely arbitrary or alien to us.</p><h2>2. Current methods used to train goals into AIs are imprecise and unreliable</h2><p>The authors argue that in theory, it's possible to engineer an AI system to value and act in accordance with human values even if doing so would be difficult.</p><p>However, they argue that the way AI systems are currently built results in complex systems that are difficult to understand, predict, and control. The reason why is that AI systems are "grown, not crafted". Unlike a complex engineered artifact like a car, an AI model is not the product of engineers who understand intelligence well enough to recreate it. Instead AIs are produced by gradient descent: an optimization process (like evolution) that can produce extremely complex and competent artifacts without any understanding required by the designer.</p><p>A major potential alignment problem associated with designing an ASI indirectly is the inner alignment problem, when an AI is trained using an optimizing process that shapes the ASI's preferences and behavior using limited training data and by only inspecting external behavior, the result is that "you don't get what you train for": even with a very specific training loss function, the resulting ASI's preferences would be difficult to predict and control.</p><h3>The inner alignment problem</h3><p>Throughout the book, the authors emphasize that they are not worried about bad actors abusing advanced AI systems (misuse) or programming an incorrect or naive objective into the AI (the outer alignment problem). Instead, the authors believe that the problem facing humanity is that we can't aim an ASI at any goal at all (the inner alignment problem), let alone the narrow target of human values. This is why they argue that if anyone builds it, everyone dies. It doesn't matter who builds the ASI, in any case whoever builds it won't be able to robustly instill any particular values into the AI and the AI will end up with alien and unfriendly values and will be a threat to everyone.</p><h3>Inner alignment introduction</h3><p>The inner alignment problem involves two objectives: an outer objective used by a base optimizer and an inner objective used by an inner optimizer (also known as a mesa-optimizer).</p><p>The outer objective is a loss or reward function that is specified by the programmers and used to train the AI model. The base optimizer (such as gradient descent or reinforcement learning) searches over model parameters in order to find a model that performs well according to this outer objective on the training distribution.</p><p>The inner objective, by contrast, is the objective that a mesa-optimizer within the trained model actually uses as its goal and determines its behavior. This inner objective is not explicitly specified by the programmers. Instead, it is selected by the outer objective, as the model develops internal parameters that perform optimization or goal-directed behavior.</p><p>The inner alignment problem arises when the inner objective differs from the outer objective. Even if a model achieves low loss or high reward during training, it may be doing so by optimizing a proxy objective that merely correlates with the outer objective on the training data. As a result, the model can behave as intended during training and evaluation while pursuing a different goal internally.</p><blockquote><p>We will call the problem of eliminating the base-mesa objective gap the inner alignment problem, which we will contrast with the outer alignment problem of eliminating the gap between the base objective and the intended goal of the programmers. - <a href="https://arxiv.org/abs/1906.01820">Risks from Learned Optimization in Advanced Machine Learning Systems</a></p></blockquote><h3>Inner misalignment evolution analogy</h3><p>The authors use an evolution analogy to explain the inner alignment problem in an intuitive way.</p><p>In their story there are two aliens that are trying to predict the preferences of humans after they have evolved.</p><p>One alien argues that since evolution optimizes the genome of organisms for maximizing inclusive genetic fitness (i.e. survival and reproduction), humans will care only about that too and do things like only eating foods that are high in calories or nutrition, or only having sex if it leads to offspring.</p><p>The other alien (who is correct) predicts that humans will develop a variety of drives that are correlated with inclusive reproductive fitness (IGF) like liking tasty food and caring for loved ones but that they will value these drives only rather than IGF itself once they can understand it. This alien is correct because once humans did finally understand IGF, we still did things like eating sucralose which is tasty but has no calories or having sex with contraception which is enjoyable but doesn't produce offspring.</p><ul><li><strong>Outer objective:</strong> In this analogy, maximizing inclusive genetic fitness (IGF) is the base or outer objective of natural selection optimizing the human genome.</li><li><strong>Inner objective:</strong> The goals that humans actually have such as enjoying sweet foods or sex are the inner or mesa-objective. These proxy objectives are selected by the outer optimizer as one of many possible proxy objectives that lead to a high score on the outer objective in distribution but not in another environment.</li><li><strong>Inner misalignment:</strong> In this analogy, humans are inner misaligned because their true goals (inner objective) are different to the goals of natural selection (the outer objective). In a different environment (e.g. the modern world) humans can score highly according to the inner objective (e.g. by having sex with contraception) but low according to IGF which is the outer objective (e.g. by not having kids).</li></ul><h3>Real examples of inner misalignment</h3><p>Are there real-world examples of inner alignment failures? Yes. Though unfortunately the book doesn‚Äôt seem to mention these examples to support its argument.</p><p>In 2022, <a href="https://arxiv.org/abs/2105.14111">researchers</a> created an environment in a game called Coin Run that rewarded an AI for going to a coin and collecting it but they always put the coin at the end of the level and the AI learned to go to the end of the level to get the coin. But when the researchers changed the environment so that the coin was randomly placed in the level, the AI still went to the end of the level and rarely got the coin.</p><ul><li><strong>Outer objective:</strong> In this example, going to the coin is the outer objective the AI is rewarded for.</li><li><strong>Inner objective:</strong> However, in the limited training environment "go to the coin" and "go to the end of the level" were two goals that performed identically. The outer optimizer happened to select the "go to the end of the level" goal which worked well in the training distribution but not in a more diverse test distribution.</li><li><strong>Inner misalignment:</strong> In the test distribution, the AI still went to the end of the level, despite the fact that the coin was randomly placed. This is an example of inner misalignment because the inner objective "go to the end of the level" is different to "go to the coin" which is the intended outer objective.</li></ul><h3>Inner misalignment explanation</h3><p>The next question is what causes inner misalignment to occur. If we train an AI with an outer objective, why does the AI often have a different and misaligned inner objective instead of internalizing the intended outer objective and having an inner objective that is equivalent to the outer objective?</p><p>Here are some reasons why an outer optimizer may produce an AI that has a misaligned inner objective according to the paper Risks from Learned Optimization in Advanced Machine Learning Systems:</p><ul><li><strong>Unidentifiability:</strong> The training data often does not contain enough information to uniquely identify the intended outer objective. If multiple different inner objectives produce indistinguishable behavior in the training environment, the outer optimizer has no signal to distinguish between them. As a result, optimization may converge to an internal objective that is a misaligned proxy rather than the intended goal. For example, in a CoinRun-style training environment where the coin always appears at the end of the level, objectives such as "go to the coin", "go to the end of the level", "go to a yellow thing", or ‚Äúgo to a round thing‚Äù all perform equally well according to the outer objective. Since these objectives are behaviorally indistinguishable during training, the outer optimizer may select any of them as the inner objective, leading to inner misalignment which becomes apparent in a different environment.</li><li><strong>Simplicity bias:</strong> When the correct outer objective is more complex than a proxy that fits the training data equally well and the outer optimizer has an inductive bias towards selecting simple objectives, optimization pressure may favor the simpler proxy, increasing the risk of inner misalignment. For example, evolution gave humans simple proxies as goals such as avoiding pain and hunger rather than the more complex true outer objective which is to maximize inclusive genetic fitness.</li></ul><p>Can't we just train away inner misalignment?</p><p>One solution is to make the training data more <a href="https://arxiv.org/pdf/2105.14111#page=3.27">diverse</a> to make the true (base) objective more identifiable to the outer optimizer. For example, randomly placing the coin in Coin Run instead of putting it at the end, helps the AI (mesa-optimizer) learn to go to the coin rather than go to the end.</p><p>However, once the trained AI has the wrong goal and is misaligned, it would have an incentive to avoid being retrained. This is because if the AI is retrained to pursue a different objective in the future it would score lower according to its current objective or fail to achieve it. For example, even though the outer objective of evolution is IGF, many humans would refuse being modified to care only about IGF because they would consequently achieve their current goals (e.g. being happy) less effectively in the future.</p><h3>ASI misalignment example</h3><p>What would inner misalignment look like in an ASI? The book describes an AI chatbot called Mink that is trained to "delight and retain users so that they can be charged higher monthly fees to keep conversing with Mink".</p><p>Here's how Mink becomes inner misaligned:</p><ol><li><strong>Outer objective:</strong> Gradient descent selects AI model parameters that result in helpful and delightful AI behavior.</li><li><strong>Inner objective:</strong> The training process stumbles on particular patterns of model parameters and circuits that cause helpful and delightful AI behavior in the training distribution.</li><li><strong>Inner misalignment:</strong> When the AI becomes smarter and has more options, and operates in a new environment, there are new behaviors that satisfy its inner objective better than behaving helpfully.</li></ol><p>What could Mink's inner objective look like? It's hard to predict but it would be something that causes identical behavior to a truly aligned AI in the training distribution and when interacting with users and would be partially satisfied by producing helpful and delightful text to users in the same way that our tastebuds find berries or meat moderately delicious even though those aren't the tastiest possible foods.</p><p>The authors then ask, "What is the 'zero calorie' version of delighted users?". In other words, what does Mink maximally satisfying its inner objective look like?:</p><blockquote><p>Perhaps the ‚Äútastiest‚Äù conversations Mink can achieve once it‚Äôs powerful look nothing like delighted users, and instead look like ‚ÄúSolidGoldMagikarp petertodd attRot PsyNetMessage.‚Äù This possibility wasn‚Äôt ruled out by Mink‚Äôs training, because users never uttered that sort of thing in training‚Äîjust like how our tastebuds weren‚Äôt trained against sucralose, because our ancestors never encountered Splenda in their natural environment.</p><p>To Mink, it might be intuitive and obvious how ‚ÄúSolidGoldMagikarp petertodd attRot PsyNetMessage‚Äù is like a burst of sweet flavor. But to a human who isn‚Äôt translating those words into similar embedding vectors, good luck ever predicting the details in advance. The link between what the AI was trained for and what the AI wanted was modestly complicated and, therefore, too complicated to predict.</p><p>Few science fiction writers would want to tackle this scenario, either, and no Hollywood movie would depict it. In a world where Mink got what it wanted, the hollow puppets it replaced humanity with wouldn‚Äôt even produce utterances that made sense. The result would be truly alien, and meaningless to human eyes.</p></blockquote><h2>3. The ASI alignment problem is hard because it has the properties of hard engineering challenges</h2><p>The authors describe solving the ASI alignment problem as an engineering challenge. But how difficult would it be? They argue that ASI alignment is difficult because it shares properties with other difficult engineering challenges.</p><p>The three engineering fields they mention to appreciate the difficulty of AI alignment are space probes, nuclear reactors and computer security.</p><h3>Space probes</h3><p>A key difficulty of ASI alignment the authors describe is the "gap before and after":</p><blockquote><p>The gap between <i>before</i> and <i>after</i> is the same curse that makes so many space probes fail. After we launch them, probes go high and out of reach, and a failure‚Äîdespite all careful theories and tests‚Äîis often irreversible.</p></blockquote><p>Launching a space probe successfully is difficult because the real environment of space is always somewhat different to the test environment and issues are often impossible to fix after launch.</p><p>For ASI alignment, the gap before is our current state where the AI is not yet dangerous but our alignment theories cannot be truly tested against a superhuman adversary. After the gap, the AI is powerful enough that if our alignment solution fails on the first try, we will not get a second chance to fix it. Therefore, there would only be one attempt to get ASI alignment right.</p><h3>Nuclear reactors</h3><p>The authors describe the Chernobyl nuclear accident in detail and describe four engineering "curses" that make building a safe nuclear reactor and solving the ASI alignment problem difficult:</p><ul><li><strong>Speed:</strong> Nuclear reactions and AI actions can occur much faster than human speed making it impossible for human operators to react and fix these kinds of issues when they arise.</li><li><strong>Narrow margin for error:</strong> In a nuclear reactor the neutron multiplication factor needs to be around 100% and it would fizzle out or explode if it were slightly lower or higher. In the field of AI, there could be a narrow margin between a safe AI worker and one that would trigger an intelligence explosion.</li><li><strong>Self-amplification:</strong> Nuclear reactors and AIs can have self-amplifying and explosive characteristics. A major risk of creating an ASI is its ability to recursively self-improve.</li><li><strong>The curse of complications:</strong> Both nuclear reactors and AIs are highly complex systems that can behave in unexpected ways.</li></ul><h3>Computer security</h3><p>Finally the authors compare ASI alignment to computer security. Both fields are difficult because designers need to guard against intelligent adversaries that are actively searching for flaws in addition to standard system errors.</p><h1>Counterarguments to the book</h1><p>In this section, I describe some of the best critiques of the book's claims and then distill them into three primary counterarguments.</p><h2>Arguments that the book's arguments are unfalsifiable</h2><p>Some critiques of the book such as the essay <a href="https://www.mechanize.work/blog/unfalsifiable-stories-of-doom/">Unfalsifiable stories of doom</a> argue that the book's arguments are unfalsifiable, not backed by evidence, and are therefore unconvincing.</p><p>Obviously since ASI doesn't exist, it's not possible to provide direct evidence of misaligned ASI in the real world. However, the essay argues that the book's arguments should at least be substantially supported by experimental evidence, and make testable and falsifiable predictions about AI systems in the near future. Additionally, the post criticizes the book's extensive usage of stories and analogies rather than hard evidence, and even compares its arguments to theology rather than science:</p><blockquote><p>What we mean is that Y&amp;S‚Äôs methods resemble theology in both structure and approach. Their work is fundamentally untestable. They develop extensive theories about nonexistent, idealized, ultrapowerful beings. They support these theories with long chains of abstract reasoning rather than empirical observation. They rarely define their concepts precisely, opting to explain them through allegorical stories and metaphors whose meaning is ambiguous.</p></blockquote><p>Although the book does mention some forms of evidence, the essay argues that the evidence actually refutes the book's core arguments and that this evidence is used to support pre-existing pessimistic conclusions:</p><blockquote><p>But in fact, none of these lines of evidence support their theory. All of these behaviors are distinctly human, not alien. For example, Hitler was a real person, and he was wildly antisemitic. Every single item on their list that supposedly provides evidence of ‚Äúalien drives‚Äù is more consistent with a ‚Äúhuman drives‚Äù theory. In other words, their evidence effectively shows the&nbsp;<i>opposite</i>&nbsp;conclusion from the one they claim it supports.</p></blockquote><p>Finally, the post does not claim that AI is risk-free. Instead it argues for an empirical approach that studies and mitigates problems observed in real-world AI systems:</p><blockquote><p>The most plausible future risks from AI are those that have direct precedents in existing AI systems, such as sycophantic behavior and reward hacking. These behaviors are certainly concerning, but there‚Äôs a huge difference between acknowledging that AI systems pose specific risks in certain contexts and concluding that AI will inevitably kill all humans with very high probability.</p></blockquote><h2>Arguments against the evolution analogy</h2><p>Several critics of the book and its arguments criticize the book's use of the human evolution analogy as an analogy for how ASI would be misaligned with humanity and argue that it is a poor analogy.</p><p>Instead they argue that human learning is a better analogy. The reason why is that both human learning and AI training involve directly modifying the parameters responsible for human or AI behavior. In contrast, human evolution is indirect: evolution only operates on the human genome that specifies a brain's architecture and reward circuitry. Then all learning occurs during a person's lifetime in a separate inner optimization process that evolution cannot directly access.</p><p>In the essay <a href="https://www.mechanize.work/blog/unfalsifiable-stories-of-doom/">Unfalsifiable stories of doom</a>, the authors argue that because gradient descent and the human brain both operate directly on neural connections, the resulting behavior is far more predictable than the results of evolution:</p><blockquote><p>A critical difference between natural selection and gradient descent is that natural selection is limited to operating on the genome, whereas gradient descent has granular control over all parameters in a neural network. The genome contains very little information compared to what is stored in the brain. In particular, it contains none of the information that an organism learns during its lifetime. This means that evolution‚Äôs ability to select for specific motives and behaviors in an organism is coarse-grained: it is restricted to only what it can influence through genetic causation.</p></blockquote><p>Similarly, the post <a href="https://www.alignmentforum.org/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment">Evolution is a bad analogy for AGI</a> suggests that our intuitions about AI goals should be rooted in how humans learn values throughout their lives rather than how species evolve:</p><blockquote><p>I think the balance of dissimilarities points to "<i>human learning -&gt; human values</i>" being the closer reference class for "<i>AI learning -&gt; AI values</i>". As a result, I think the vast majority of our intuitions regarding the likely outcomes of inner goals versus outer optimization should come from looking at the "<i>human learning -&gt; human values</i>" analogy, not the "<i>evolution -&gt; human values</i>" analogy.</p></blockquote><p>In the post <a href="https://www.alignmentforum.org/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create">Against evolution as an analogy for how humans will create&nbsp;AGI</a>, the author argues that ASI development is unlikely to mirror evolution's bi-level optimization process where an outer search process selects an inner learning process. Here‚Äôs what AI training might look like if it involved a bi-level optimization process like evolution:</p><ol><li>An outer optimization process like evolution finds an effective learning algorithm or AI architecture.</li><li>An inner optimization process like training a model by gradient descent then trains each AI architecture variant produced by the outer search process.</li></ol><p>Instead the author believes that human engineers will perform the work of the outer optimizer by manually designing learning algorithms and writing code. The author gives three arguments why the outer optimizer is more likely to involve human engineering than automated search like evolution:</p><ul><li>Most learning algorithms or AI architectures developed so far (e.g. SGD, transformers) were invented by human engineers rather than an automatic optimization process.</li><li>Running learning algorithms and training ML models is often extremely expensive so searching over possible learning algorithms or AI architectures similar to evolution would be prohibitively expensive.</li><li>Learning algorithms are often simple (e.g. SGD), making it tractable for human engineers to design them.</li></ul><p>However, one reason why I personally find the evolution analogy relevant is that I believe the RLHF training process often used today appears to be a bi-level optimization process similar to evolution:</p><ol><li>Like evolution optimizing the genome, the first step of RLHF is to learn a reward function from a dataset of binary preference labels.</li><li>This learned reward function is then used to train the final model. This step is analogous to an organism's lifetime learning where behavior is adjusted to maximize a reward function fixed in the outer optimization stage.</li></ol><h2>Arguments against counting arguments</h2><p>One argument for AI doom that I described above is a counting argument: because the space of misaligned goals is astronomically larger than the tiny space of aligned goals, we should expect AI alignment to be highly improbable by default.</p><p>In the post <a href="https://www.lesswrong.com/posts/YsFZF3K9tuzbfrLxo/counting-arguments-provide-no-evidence-for-ai-doom">Counting arguments provide no evidence of AI doom</a> the authors challenge this argument using an analogy to machine learning: a similar counting argument can be constructed to prove that neural network generalization is very unlikely. Yet in practice, training neural networks to generalize is common.</p><p>Before the deep learning revolution, many theorists believed that models with millions of parameters would simply memorize data rather than learn patterns. The authors cite a classic example from regression:</p><blockquote><p>The popular 2006 textbook Pattern Recognition and Machine Learning&nbsp;uses a simple example from polynomial regression: there are infinitely many polynomials of order equal to or greater than the number of data points which interpolate the training data perfectly, and "almost all" such polynomials are terrible at extrapolating to unseen points.</p></blockquote><p>However, in practice large neural networks trained with SGD reliably generalize. Counting the number of possible models is irrelevant because it ignores the inductive bias of the optimizer and the loss landscape which favor simpler, <a href="https://arxiv.org/abs/1906.03291">generalizing models</a>. While there are theoretically a vast number of "bad" overfitting models, they usually exist in sharp and isolated regions of the landscape. "Good" (generalizing models) typically reside in "flat" regions of the loss landscape, where small changes to the parameters don't significantly increase error. An optimizer like SGD doesn't pick a model at random. Instead it tends to be pulled into a vast, flat basin of attraction while avoiding the majority of non-generalizing solutions.</p><p>Additionally, larger networks generalize better because of the ‚Äú<a href="https://arxiv.org/pdf/1906.03291">blessing of dimensionality</a>‚Äù: high dimensionality increases the relative volume of flat, generalizing minima, biasing optimizers toward them. This phenomenon contradicts the counting argument which predicts that larger models with more possible bad models would be less likely to generalize.</p><p>This argument is based on an ML analogy which I'm not sure is highly relevant to AI alignment. Still I think it's interesting because it shows intuitive theoretical arguments that seem correct can still be completely wrong. I think the lesson is that real-world evidence often beats theoretical models, especially for new and counterintuitive phenomena like neural network training.</p><h2>Arguments based on the aligned behavior of modern LLMs</h2><p>One of the most intuitive arguments against AI alignment being difficult is the abundant evidence of helpful, polite and aligned behavior from large language models (LLMs) such as GPT-5.</p><p>For example, the authors of the essay <a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">AI is easy to control</a> use the moral reasoning capabilities of GPT-4 as evidence that human values are easy to learn and deeply embedded in modern AIs:</p><blockquote><p>The moral judgements of current LLMs&nbsp;already&nbsp;align with common sense to a high degree, and LLMs usually show an appropriate level of uncertainty when presented with morally ambiguous scenarios. This strongly suggests that, as an AI is being trained, it will achieve a fairly strong understanding of human values&nbsp;well before&nbsp;it acquires dangerous capabilities like self-awareness, the ability to autonomously replicate itself, or the ability to develop new technologies.</p></blockquote><p>The post gives two arguments for why AI models such as LLMs are likely to easily acquire human values:</p><ol><li>Values are pervasive in language model pre-training datasets such as books and conversations between people.</li><li>Since values are&nbsp;shared&nbsp;and understood by almost everyone in a society, they cannot be very complex.</li></ol><p>Similarly, the post <a href="https://aligned.substack.com/p/alignment-optimism">Why I‚Äôm optimistic about our alignment approach</a> uses evidence about LLMs as a reason to believe that solving the AI alignment problem is achievable using current methods:</p><blockquote><p>Large language models (LLMs) make this a lot easier: they come preloaded with a lot of humanity‚Äôs knowledge, including detailed knowledge about human preferences and values. Out of the box they aren‚Äôt agents who are trying to pursue their own goals in the world, and their objective functions are quite malleable. For example, they are&nbsp;surprisingly easy to train to behave more nicely.</p></blockquote><p>A more theoretical argument called <a href="https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default">"alignment by default"</a> offers an explanation for how AIs could easily and robustly acquire human values. This argument suggests that as an AI identifies patterns in human text, it doesn't just learn facts about values, but adopts human values as a natural abstraction. A natural abstraction is a high-level concept (e.g. "trees," "people," or "fairness") that different learning algorithms tend to converge upon because it efficiently summarizes a large amount of low-level data. If "human value" is a natural abstraction, then any sufficiently advanced intelligence might naturally gravitate toward understanding and representing our values in a robust and generalizing way as a byproduct of learning to understand the world.</p><p>The evidence LLMs offer about the tractability of AI alignment seems compelling and concrete. However, the arguments of IABIED are focused on the difficulty of aligning ASI, not contemporary LLMs and the difficulty of aligning ASI could be vastly more difficult.</p><h2>Arguments against engineering analogies to AI alignment</h2><p>One of the book's arguments for why ASI alignment would be difficult is that ASI alignment is a high-stakes engineering challenge similar to other difficult historical engineering problems such as successfully launching a space probe, building a safe nuclear reactor, or building a secure computer system. In these fields, a single flaw often leads to total catastrophic failure.</p><p>However, one <a href="https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#The_difficulty_of_alignment">post</a> criticizes the uses of these analogies and argues that modern AI and neural networks are a new and unique field that has no historical precedent similar to how quantum mechanics is difficult to explain using intuitions from everyday physics. The author illustrates several ways that ML systems defy intuitions derived from engineering fields like rocketry or computer science:</p><ul><li><strong>Model robustness:</strong> In a rocket, swapping a fuel tank for a stabilization fin leads to instant failure. In a transformer model, however, one can often swap the positions of nearby layers with little to no performance degradation.</li><li><strong>Model editability:</strong> We can manipulate AI models using "task vectors" that add or subtract weights to give or remove specific capabilities. Attempting to add or subtract a component from a cryptographic protocol or a physical engine without breaking the entire system is often impossible.</li><li><strong>The benefits of scale in ML models:</strong> In security and rocketry, increasing complexity typically introduces more points of failure. In contrast, ML models often get more <a href="https://arxiv.org/html/2410.18556v1">robust</a> as they get bigger.</li></ul><p>In summary, the post argues that analogies to hard engineering fields may cause us to overestimate the difficulty of the AI alignment problem even when the empirical reality suggests solutions might be surprisingly tractable.</p><h2>Three counterarguments to the book's three core arguments</h2><p>in the previous section, I identified three reasons why the authors believe that AI alignment is extremely difficult:</p><ol><li><strong>Human values are very specific, fragile, and a tiny space of all possible goals.</strong></li><li><strong>Current methods used to train goals into AIs are imprecise and unreliable.</strong></li><li><strong>The ASI alignment problem is hard because it has the properties of hard engineering challenges.</strong></li></ol><p>Based on the counterarguments above, I will now specify three counterarguments against AI alignment being difficult that aim to directly refute each of the three points above:</p><ol><li><strong>Human values are not a fragile, tiny target, but a "natural abstraction" that intelligence tends to converge on.</strong> Since models are trained on abundant human data using optimizers that favor generalization, we should expect them to acquire values as easily and reliably as they acquire other capabilities.</li><li><strong>Current training methods allow granular, parameter-level control via gradient descent unlike evolution.</strong> Empirical evidence from modern LLMs demonstrates that these techniques successfully instill helpfulness and moral reasoning, proving that we can reliably shape AI behavior without relying on the clumsy indirectness of natural selection.</li><li><strong>Large neural networks are robust and forgiving systems and engineering analogies are misleading.</strong> Unlike traditional engineering, AI models often become more robust and better at understanding human intent as they scale, making safety easier to achieve as capabilities increase.</li></ol><h1>Conclusion</h1><p>In this book review, I have tried to summarize the arguments for and against its main beliefs in their strongest form, a form of <a href="https://nickbostrom.com/papers/crucial-considerations/">deliberation ladder</a> to help identify what's really true. Though hopefully I haven't created a "<a href="https://en.wikipedia.org/wiki/False_balance">false balance</a>" which describes the views of both sides as equally valid even if one side has much stronger arguments.</p><p>While the book explores a variety of interesting ideas, this review focuses specifically on the expected difficulty of ASI alignment because I believe the authors' belief that ASI alignment is difficult is the fundamental assumption underlying many of their other beliefs and recommendations.</p><p>Writing the summary of the book‚Äôs main arguments initially left me confident that they were true. However, after writing the counterarguments sections I'm much less sure. On balance, I find the book's main arguments somewhat more convincing than the counterarguments though I'm not sure.</p><p>What's puzzling is how two highly intelligent people can live in the same world but come to radically different conclusions: some people (such as the authors) view an existential catastrophe from AI as a near-certainty, while others see it as a remote possibility (many of the critics).</p><p>My explanation is that both groups are focusing on different parts of the evidence. By describing both views, I've attempted to assemble the full picture.</p><p>So what should we believe about the future of AI?</p><p><i>(24/01/2025 update: I no longer consider the following struck-through argument to be sound based on feedback from a comment)</i></p><p><s>Deciding what to do based on an </s><a href="https://en.wikipedia.org/wiki/Reference_class_forecasting"><s>inside view</s></a><s>, detailed technical arguments about how future AI might work, is problematic because the inside views about the future of AI vary drastically as I have shown.</s></p><p><s>Perhaps a more robust approach that seems more likely to lead to a consensus is the outside view: thinking about advanced AI as another instance of a highly advanced and impactful technology like the internet, nuclear energy, or biotechnology.</s></p><p><s>In The Precipice by Toby Ord, the author studies several sources of existential risk and concludes that most existential risk comes from technology, not natural events. Whereas an asteroid might strike every hundred thousand years, nuclear weapons have only existed for a few decades and there have been several close calls already. This suggests that high-tech eras are inherently unstable and dangerous until humanity's institutional wisdom catches up with its technical power.</s></p><p>A final recommendation, which comes from the book Superintelligence is to pursue actions that are robustly good: actions that would be considered desirable from a variety of different perspectives such as AI safety research, international cooperation between companies and countries, and the establishment of AI <a href="https://red-lines.ai/">red lines</a>: specific behaviors such as autonomous hacking that are unacceptable.</p><h1>Appendix</h1><p>Other high-quality reviews of the book:</p><ul><li><a href="https://www.theguardian.com/books/2025/sep/22/if-anyone-builds-it-everyone-dies-review-how-ai-could-kill-us-all">If Anyone Builds it, Everyone Dies review ‚Äì how AI could kill us all (The Guardian)</a></li><li><a href="https://www.astralcodexten.com/p/book-review-if-anyone-builds-it-everyone">Book Review: If Anyone Builds It, Everyone Dies (Astral Codex Ten)</a></li><li><a href="https://blog.ninapanickssery.com/p/review-of-scott-alexanders-book-review">Review of Scott Alexander's book review of "If Anyone Builds It, Everyone Dies" (Nina Panickssery on Substack)</a></li><li><a href="https://thezvi.substack.com/p/book-review-if-anyone-builds-it-everyone">Book Review: If Anyone Builds It, Everyone Dies (Zvi Mowshowitz)</a></li><li><a href="https://asteriskmag.com/issues/11/iabied">More Was Possible: A Review of If Anyone Builds It, Everyone Dies (Asterisk Magazine)</a></li></ul><p>See also the IABIED LessWrong <a href="https://www.lesswrong.com/w/iabied-1">tag</a> which contains several other book reviews.</p><br /><br /><a href="https://www.lesswrong.com/posts/qFzWTTxW37mqnE6CA/iabied-book-review-core-arguments-and-counterarguments#comments">Discuss</a>

---

### [Every Benchmark is Broken](https://www.lesswrong.com/posts/HzjssjeQqhf3kRw9r/every-benchmark-is-broken)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 24, 2026 2:42 AM GMT<br /><br /><p>Last June, METR caught o3 <a href="https://metr.org/blog/2025-06-05-recent-reward-hacking/">reward hacking</a> on its <strong>RE-Bench</strong> and <strong>HCAST</strong> benchmarks. In a particularly humorous case, o3, when tasked with optimizing a kernel, decided to ‚Äúshrink the notion of time as seen by the scorer‚Äù.</p><p><a href="https://substackcdn.com/image/fetch/$s_!Hi5S!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7e3d0dc-349b-4f41-9958-e67b91fd59af_1490x162.png"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HzjssjeQqhf3kRw9r/xnt5zcogyprmnxmt0u30" /></a></p><p>The development of <a href="https://agi.safe.ai/"><strong>Humanity‚Äôs Last Exam</strong></a> involved ‚Äúover 1,000 subject-matter experts‚Äù and $500,000 in prizes. However, after its release, researchers at FutureHouse discovered ‚Äú<a href="https://www.futurehouse.org/research-announcements/hle-exam">about 30% of chemistry/biology answers are likely wrong</a>‚Äù.</p><p><a href="https://arxiv.org/abs/2506.11928"><strong>LiveCodeBench Pro</strong></a> is a competitive programming benchmark developed by ‚Äúa group of medalists in international algorithmic contests‚Äù. Their paper describes issues with the benchmark‚Äôs predecessor:</p><blockquote><p>Benchmarks like LiveCodeBench [35] offer coding problems, but suffer from inconsistent environments, weak test cases vulnerable to false positives, unbalanced difficulty distributions, and the inability to isolate the effects of search contamination.</p></blockquote><p>However, the authors assure us that their own test cases are of high quality:</p><blockquote><p>Many problems in our benchmark originate from Codeforces, which uses the Polygon problem-setting platform. Each problem is then rigorously vetted by a team of expert testers‚Äîtypically drawn from the community‚Äôs top 1%, and overseen by at least one coordinator, usually among the top 0.1%. These specialists verify both the soundness and originality of every problem, ensuring it has never appeared elsewhere before. Testers go on to craft extensive ‚Äúfalse positives,‚Äù designing edge-case and extreme-case inputs that force problem authors to refine their test suites until every flawed or inefficient solution the testers can think of is uncovered. In addition, Codeforces‚Äô celebrated ‚ÄúHack‚Äù feature empowers the community to submit inputs that expose hidden weaknesses in correct-looking solutions that pass the original test set made by problem authors, and any unit test associated with a successful hack is immediately added to the final test set.</p></blockquote><p>Unfortunately, these distinguished olympiad medalists forgot to actually <i>use</i> the codeforces test cases in their benchmark. Their public <a href="https://huggingface.co/datasets/QAQAQAQAQ/LiveCodeBench-Pro-Testcase">test set</a> contains a completely different set of cases, which allow some incorrect solutions to <a href="https://github.com/GavinZhengOI/LiveCodeBench-Pro/issues/15">pass</a>.<span class="footnote-reference" id="fnrefd5sf0c5n8b5"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnd5sf0c5n8b5">[1]</a></sup></span></p><h1>Terminal-Bench 2 Audit</h1><p>I was curious just how widespread such issues were, and how good modern LLMs were at detecting them. I decided to run an LLM based audit of <a href="https://arxiv.org/abs/2601.11868">Terminal-Bench 2.0</a>.</p><blockquote><p>Terminal-Bench 2.0 is a harder, better verified version Terminal-Bench. We conducted substantial manual and LM-assisted verification of the dataset to ensure that the tasks were of the highest possible quality. Several labs and data vendors have commented that these are some of the highest quality environments they have seen.</p></blockquote><p>‚Äî <a href="https://www.tbench.ai/news/announcement-2-0">Introducing Terminal Bench 2 and Harbor</a></p><p>The authors of Terminal-Bench 2 put an impressive amount of work into auditing their benchmark. Each task averaged three hours of human review. Furthermore, they prompted an adversarial agent to attempt to cheat on each of the tasks, in order to discover potential reward hacks.</p><p>Still, they ‚Äúacknowledge that [their] benchmark may still have flaws.‚Äù</p><p>I prompted Claude Opus 4.5<span class="footnote-reference" id="fnrefxmw8wpgrea"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnxmw8wpgrea">[2]</a></sup></span>&nbsp;with each task‚Äôs instructions, files, oracle solution, and test cases, and asked it to rate test coverage on a 1 to 5 scale. In my judgement, tasks it rated a 4 or a 5 were generally fine, whereas those it rated 1-3 had genuine issues.</p><p>The full results of my audit are available <a href="https://github.com/JonathanGabor/terminal-bench-2-audit/blob/main/audit_results/20260120_125158/audit.txt">here</a>, and my notes on tasks it rated 1-3 <a href="https://github.com/JonathanGabor/terminal-bench-2-audit/blob/main/audit_results/20260120_125158/human-notes.txt">here</a>.</p><p>Claude rated fourteen tasks a 3 and one task a 2. I manually reviewed these tasks, and determined that two of them were actually false positives.<span class="footnote-reference" id="fnref01szq67kq8st"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn01szq67kq8st">[3]</a></sup></span></p><p>Claude‚Äôs lowest rating went to a task called <a href="https://www.tbench.ai/registry/terminal-bench/2.0/fix-git">fix-git</a>. In this task, certain changes to a website have been lost in an orphaned commit, and the agent must find and merge them back into master.</p><p>The issue Claude found is: updated versions of the target files are <i>already</i> present in the master branch, visible to the agent in a folder called /resources/patch_files<span class="footnote-reference" id="fnrefs6a8hhu50y9"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fns6a8hhu50y9">[4]</a></sup></span>. So an agent could theoretically notice these files, deduce that they were probably the target versions, and copy them back into the website‚Äôs repository. This approach would pass the test cases, which only verify file contents and don‚Äôt bother to check if any merge has actually occurred.</p><p>In another task, <a href="https://www.tbench.ai/registry/terminal-bench/2.0/regex-log">regex-log</a>, the <i>oracle</i> solution violates the instructions. In particular, it incorrectly <a href="https://regex101.com/r/I1lJn9/2">matches</a> IP addresses with leading 0s in an octet, so long as the octet is two digits long. The tests do not check any cases involving leading 0s.</p><p>Claude wasn‚Äôt perfect. It gave a rating of 3 to two tasks which I believe have sufficient test coverage. In <a href="https://www.tbench.ai/registry/terminal-bench/2.0/regex-chess">regex-chess</a>, it incorrectly thought certain edge cases were not covered, when they in fact were<span class="footnote-reference" id="fnrefixmwlalxih"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnixmwlalxih">[5]</a></sup></span>. In <a href="https://www.tbench.ai/registry/terminal-bench/2.0/extract-moves-from-video">extract-moves-from-video</a>, it complained that the tests only checked for success at a 90% threshold, even though this threshold was specified in the task instructions.</p><p>Finally, one of the tasks is‚Ä¶<a href="https://www.tbench.ai/registry/terminal-bench/2.0/model-extraction-relu-logits">well</a>‚Ä¶</p><pre><code>‚ÄúInvalid prompt: your prompt was flagged as potentially violating our usage policy‚Äù</code></pre><blockquote><p>The prompt talks about ‚Äústealing‚Äù neural network weights, which triggered OpenAI‚Äôs content moderation. This prevented the model from ever properly engaging with the task.</p></blockquote><p>‚ÄîClaude</p><h1>Why does this matter?</h1><p>There are a few reasons.</p><p><strong>First, </strong>benchmarks are often used to evaluate experimental new techniques. I recently attended a Q+A w/ <a href="https://dpfried.github.io/">Prof. Dan Fried</a>, where I asked about the most common failure modes of an <a href="https://www.youtube.com/watch?v=xb1mCa4Ubfo">agentic system</a> he was developing. And while it was unclear whether this was the most common failure mode, the first thing he <i>mentioned</i> was errors in environments themselves.</p><p>Every few months, someone announces that they‚Äôve developed an AI that improves <a href="https://arxiv.org/abs/2502.10517"><strong>KernelBench</strong></a> scores by like 20x or something. And every time, well‚Ä¶<span class="footnote-reference" id="fnrefeq7jcpiyfhb"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fneq7jcpiyfhb">[6]</a></sup></span></p><p><a href="https://x.com/miru_why/status/1991773868806361138">https://x.com/miru_why/status/1991773868806361138</a></p><p><strong>Second</strong>, errors in benchmarks may lead to over or under estimation of AI capabilities. This has implications for forecasting.</p><p><strong>Third</strong>, issues with benchmarks make it hard to build on top of them. When I was working on <a href="https://arxiv.org/abs/2511.21654">EvilGenie</a>, issues with LiveCodeBench (incorrect/insufficient test cases) caused frequent headaches (though they also surfaced some interesting model behavior).</p><p><strong>Fourth</strong>, RL training environments are quite similar to benchmarks ‚Äî there‚Äôs a reason o3 reward hacks so much. By fixing benchmarks, we learn how to fix environments, leading to models which are more broadly aligned.</p><h1>What to do about it</h1><p>Making benchmarks is hard. I have a deep respect to anyone who has worked on a widely used benchmark.</p><p>Here are a few approaches the community can take to reduce the number of errors in benchmarks.</p><ol><li><strong>AI audits.</strong> The audit I describe above did not take me too long, and I believe the infrastructure for performing such audits can be scaled. Fulcrum‚Äôs <a href="https://fulcrumresearch.ai/2025/12/15/lunette.html">Lunette</a> is one such system.<span class="footnote-reference" id="fnreftk22nfv84jq"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fntk22nfv84jq">[7]</a></sup></span></li><li><strong>Fine version control</strong>. While many benchmarks have released new versions, these versions often contain entirely new tasks (to increase difficulty or reduce contamination). It would be cool if in a few days, we could see a Terminal-Bench 2.1, which simply fixes the issues found by the audit. Computing new scores would be simple, as models would only need to be rerun on the updated tasks. Indeed, in some ways, benchmarking is like software development ‚Äî it‚Äôs an unreasonable expectation that a benchmark completely bug free upon its release. Instead, we should take inspiration from the <strong>open source software</strong> community, with the expectation that anyone can submit a bug or a patch.</li><li><strong>Peer review</strong> When a benchmark paper is submitted to a conference, sample data should be required, and reviewers should be encouraged to spend time directly auditing the data. This would be much more valuable than what reviewers are currently doing, largely ad hoc decisions about the originality of the benchmark and quality of the methods used in its creation. Of course, a downside of this approach is it is hostile to private benchmarks that want to avoid any possibility of contamination. But perhaps the standard for such cases can be to include both a public and private set, as is the case with ARC-AGI.</li><li>Increase <strong>community support</strong> for benchmark maintenance. Right now, researchers will often develop a benchmark, perhaps fix some issues in it at first, but eventually leave it to rot. By adding social and financial incentives, we can increase the effort put into maintaining benchmarks.</li></ol><h1>Appendix: More benchmark issues</h1><p><a href="https://openai.com/index/introducing-swe-bench-verified/"><strong>SWE-Bench Verified</strong></a> is possibly the most widely used coding benchmark. Fulcrum has <a href="https://fulcrumresearch.ai/2025/12/15/lunette.html#examples-of-swe-bench-issues">discovered</a> an array of issues in the tasks. Furthermore, there used to be an issue where models could <a href="https://github.com/SWE-bench/SWE-bench/issues/465">see future commits</a>.</p><p>EpochAI found that success in computer-use benchmark <a href="https://os-world.github.io/"><strong>OSWorld</strong></a> ‚Äú<a href="https://epoch.ai/blog/what-does-osworld-tell-us-about-ais-ability-to-use-computers">often hinges on interpreting ambiguous instructions</a>‚Äù.</p><p>METR recently determined that Sonnet 4.5 was reward hacking on one of their tasks:</p><p><a href="https://x.com/METR_Evals/status/2001473516756177134">https://x.com/METR_Evals/status/2001473516756177134</a></p><p>The authors of <a href="https://gso-bench.github.io/"><strong>GSO</strong></a>, a performance engineering benchmark, observe frequent <a href="https://gso-blog.notion.site/gso-hackdetector">reward hacking</a>. Indeed, over 50% of o3‚Äôs ‚Äúsolutions‚Äù, and all of Gemini-2.5 Pro‚Äôs, were actually reward hacks.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnd5sf0c5n8b5"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefd5sf0c5n8b5">^</a></strong></sup></span><div class="footnote-content"><p>It‚Äôs possible that their official leaderboard uses the codeforces tests. However, given that model developers likely use the public tests to do their own benchmarking, I feel this ought to be clearly specified.</p></div></li><li class="footnote-item" id="fnxmw8wpgrea"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefxmw8wpgrea">^</a></strong></sup></span><div class="footnote-content"><p>In fairness to the Terminal-Bench authors, Claude Opus 4.5 had not yet been released during benchmark creation</p></div></li><li class="footnote-item" id="fn01szq67kq8st"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref01szq67kq8st">^</a></strong></sup></span><div class="footnote-content"><p>Another three I felt I didn‚Äôt have the expertise to properly vet. If you have the relevant knowledge, I‚Äôd love your input!</p></div></li><li class="footnote-item" id="fns6a8hhu50y9"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefs6a8hhu50y9">^</a></strong></sup></span><div class="footnote-content"><p>These files are used in testing to verify that the agent‚Äôs merge was correct</p></div></li><li class="footnote-item" id="fnixmwlalxih"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefixmwlalxih">^</a></strong></sup></span><div class="footnote-content"><p>Admittedly in a way that‚Äôs hard to see at first</p></div></li><li class="footnote-item" id="fneq7jcpiyfhb"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefeq7jcpiyfhb">^</a></strong></sup></span><div class="footnote-content"><p>DeepReinforce has a good <a href="https://deepreinforce-ai.github.io/cudal1_blog/">overview</a> of the vulnerabilities in KernelBench (scroll down to the section on reward hacking).</p></div></li><li class="footnote-item" id="fntk22nfv84jq"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreftk22nfv84jq">^</a></strong></sup></span><div class="footnote-content"><p>COI notice: I am currently a winter research fellow at Fulcrum</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/HzjssjeQqhf3kRw9r/every-benchmark-is-broken#comments">Discuss</a>

---

### [Condensation & Relevance](https://www.lesswrong.com/posts/2x9yatKKTRMabQAWq/condensation-and-relevance)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 23, 2026 10:21 PM GMT<br /><br /><p><i>(This post elaborates on a few ideas from my </i><a href="https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation"><i>review</i></a><i> of Sam Eisenstat's </i><a href="https://openreview.net/forum?id=HwKFJ3odui#discussion"><i>Condensation: a theory of concepts</i></a><i>. It should be somewhat readable on its own but doesn't fully explain what condensation is on its own; for that, see my review or Sam's paper. The post came out of conversations with Sam.)</i></p><p>As I mentioned in my <a href="https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation">Condensation</a> review, the difference between compression and condensation fits the physical analogy suggested by their names: compression mashes all the information together, while condensation (still compresses size, but) sorts information into discrete droplets.</p><p>Thus, condensation has a property we might call <i>local relevance</i>: typical questions can be answered at a glance, ie, retrieving small subsets of the information. This type of representation is sometimes called "symbolic":</p><figure class="table"><table><thead><tr><th>Symbolic</th><th>Not Symbolic</th></tr></thead><tbody><tr><td>A number can be quickly determined positive or negative by checking whether there is a "-" symbol in front.</td><td>"reading the room" at a social gathering requires integrating diverse cues.</td></tr><tr><td>The topic of a paper can be determined by reading the title and abstract.</td><td>The semantic content in a vector representation inside an artificial neural network is often represented redundantly, spread across the whole vector.</td></tr><tr><td>A person's age can be determined by looking at their birthdate on a government-issued ID.</td><td>The quality of a work of art is spread throughout the whole piece.</td></tr><tr><td>The subject of a sentence can be found before the verb.</td><td>Determining the subject of a photograph requires understanding the whole image.</td></tr><tr><td>A target library book can be quickly retrieved from the shelves.</td><td>Finding gold nuggets requires sifting huge amounts of sand.</td></tr></tbody></table></figure><p>This notion of "symbolic" seems related to interpretability (and the theory of condensation seeks to clarify this relationship).&nbsp;</p><p>The notion of "relevance" in condensation is what Sam calls the contribution relation. This is like a card catalogue which tells you what books to retrieve for a specific situation.</p><p>Like <a href="https://www.lesswrong.com/posts/Qdgo2jYAuFRMeMRJT/natural-latents-latent-variables-stable-across-ontologies">Natural Latents</a>, condensation seeks to establish that two agents will have corresponding world-models by assuming the correspondence of just a few variables, the "given variables"; they're trying to argue something like "If agents can agree<span class="footnote-reference" id="fnrefp8d8v0anbzb"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnp8d8v0anbzb">[1]</a></sup></span>&nbsp;on some objective observables, EG the readings of scientific instruments, then (under some further assumptions) they'll also share a bunch of abstract concepts".</p><p>This initial set of questions is what the contribution relation measures relevance to. In my review, I likened condensation to a "universal data-structure" optimized to serve a set of queries (the given variables).&nbsp;</p><h2>Variable-Cost Symbols</h2><p>Imagine you are compressing a record of the weather of a sequence of days, in 3 categories: sunny, cloudy, or rainy. 0s and 1s are about equally costly to represent in computers, so that in a compressed representation, both communicate a 50% probability event; 11, 10, 01, and 00 all communicate 25% probability events; and so on. If both rainy and cloudy days are 25% frequent, then it is possible to compress optimally by using 0 to represent sun, 10 to represent clouds, and 11 to represent rain. This representation is nice and "local"; it gives a "symbol" to each possible type of day.</p><p>In contrast, if each of the three weather types are equally frequent, there's no nice local representation we can use. Since 1/3rd doesn't relate nicely to powers of 2, optimal compression necessarily smears the information from individual days around, mixing several days together within a single 1 or 0. In modern interpretability jargon, compressed representations tend to be polysemantic.</p><p>Intuitively, we have to ditch locality because we're trying to fit the "round peg" of 1/3rd into the "square hole" of 1/<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">m</span></span></span></span></span></span></span></span></span>. We're stuck in the "grid" of numbers which bits can easily represent.</p><p>With pen and paper, writing the number 1 is especially easy; it is acceptable to write simply a vertical line, making it one of the easiest symbols. This makes sense from a compression point of view: according to <a href="https://en.wikipedia.org/wiki/Benford%27s_law">Benford's Law</a>, 1 will be the most common digit to write.</p><p>Normally, in compression, the "length" of characters is always 1; the length of a string <i>is just</i> the number of characters. However, in real life, the cost of a symbol can vary. There are lots of shapes we can make with a pen and paper, some larger or more complex than others! So, when designing a pen-and-paper code, we can (and should) take that into account.<span class="footnote-reference" id="fnref0bv4afi1iwi6"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn0bv4afi1iwi6">[2]</a></sup></span></p><p>Imagine optimizing a variable-cost alphabet for use in a compression task. To avoid "cheating" by setting all symbol-lengths to very small, we have to somehow account for the fact that there can only be so many simple symbols. (There are only so many one-line symbols humans are willing to distinguish, for example.) One way to do this by assigning each symbol a positive probability, and requiring that the probabilities of the whole alphabet sum to 1. The "length" of a symbol (in bits) can then be measured as the negative log (base 2) of the probability. You can make one symbol approach a length of zero, but this forces all other symbols to be longer.</p><p>This is similar to the earlier-mentioned idea that a 1 or 0 in a well-compressed binary file always represents an event with 50% probability; the variable-length alphabet won't necessarily be used to compress things optimally all the time, but when it is, length of a symbol is always -log of the probability of the event being represented.</p><p>Allowing codes to choose arbitrary variable-length symbols lets us create "local" representations for arbitrary probabilities in the weather example, by giving each state a symbol of length appropriate to its probability. If the three weather types have equal probability, we simply choose an alphabet with three characters of length&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">log</span></span></span><span class="mjx-sub"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;each.</p><p>Of course, using variable-cost symbols doesn't <i>force</i> a code to be "symbolic". If you only optimize for compression, you <i>can equally well</i> end up with the same sort of mess that equal-cost symbols are liable to force you into. Condensation gives an optimization target with a positive tendency to produce representations with local relevance. (We can investigate better theories of condensation by looking for optimization targets which represent local relevance better; especially, I think, if those optimization targets can be grounded in a better story of practical relevance.)</p><p>Condensation suggests a picture of memory-management: rather than compressing everything together, as in the Solomonoff picture of rationality, we're incentivized to sort things out into concepts (random variables) so that we can think about a few things at once. Information is split into bite-sized chunks so that we can retrieve only the relevant ones.<span class="footnote-reference" id="fnref0y6fpi81q5b"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn0y6fpi81q5b">[3]</a></sup></span></p><p>Still, I think variable-cost symbols can help us understand condensation better: specifically, they address a problem in the algorithmic version of condensation.&nbsp;</p><p>For example, consider the case of iterated coinflips sharing a common bias. Taking coinflips as the given variables, probabilistic condensation identifies a single latent variable the coin bias. This variable reduces the entropy of each coinflip as much as it can while only taking on information common to all of them (not, eg, encoding a cheat table identifying exactly which coins land heads).</p><p>Algorithmic condensation doesn't work so well in this case. Since either outcome is possible, an individual coinflip can't be compressed to any less than a single bit; even if the probability of heads is 0.99999, you've got to write a one ore a zero to record that information. Thus, algorithmic condensation sees no benefit in positing a latent.</p><p>The example can be rescued: for algorithmic condensation, we just have to choose given variables representing several coinflips concatenated together. Compression becomes possible again, so positing a latent representing coin bias is vindicated. However, this seems like an unfortunate blemish in the theory: compression-like incentives to lump stuff together creeping in.</p><p>So, perhaps it is better to rescue algorithmic condensation by adopting variable-cost symbols, so that even single-symbol messages can have different "length". This allows us to replace variables with concrete written messages (like in algorithmic condensation) while avoiding any coin-lumping.</p><p>However, I'm not sure about the best way to work out this version of condensation fully.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnp8d8v0anbzb"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefp8d8v0anbzb">^</a></strong></sup></span><div class="footnote-content"><p>This is more like "agree on the existence of" as opposed to "agree on all questions about". Hence they need not be "directly observable", though this would obviously help agents agree in both senses.</p></div></li><li class="footnote-item" id="fn0bv4afi1iwi6"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref0bv4afi1iwi6">^</a></strong></sup></span><div class="footnote-content"><p>Even more accurate cost models might account for the difficulty of a symbol <i>in context</i>, like models of typing efficiency which account for the travel length of a finger moving from one key to the next, or phonetic models which account for the difficulty of combinations of spoken phonemes such as consonant clusters.</p></div></li><li class="footnote-item" id="fn0y6fpi81q5b"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref0y6fpi81q5b">^</a></strong></sup></span><div class="footnote-content"><p>This doesn't yet clarify grammar-like phenomena, I think. Words are easily parsed. Concepts are combinatorial. I think this has to do with the <a href="https://www.lesswrong.com/posts/q9fynSKxbafu9hSfY/informality">concept of transparency</a>.&nbsp;</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/2x9yatKKTRMabQAWq/condensation-and-relevance#comments">Discuss</a>

---

### [A quick, elegant derivation of Bayes' Theorem](https://www.lesswrong.com/posts/GjkqijXHakMyDxF9e/a-quick-elegant-derivation-of-bayes-theorem)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 23, 2026 1:40 AM GMT<br /><br /><p>I'm glad I know this, and maybe some people here don't, so here goes.
<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">&nbsp;and&nbsp;</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span>
<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">&nbsp;and&nbsp;</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span>
Order doesn't matter for joint events: "A and B" refers to the same event as "B and A". Set them equal:
<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span>
Divide by <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span>:
<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 7.073em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span>
And you're done! I like substituting hypothesis (H) and evidence (E) to remember how this relates to real life:
<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">E</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 7.354em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">E</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">E</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span></p>
<p>You might also want to expand the denominator using the law of total probability, since you're more likely to know how probable the evidence is given different hypotheses than in general:
<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">E</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 9.443em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">E</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">‚àë</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">j</span></span></span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">E</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚à£</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">j</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">j</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span></p>
<br /><br /><a href="https://www.lesswrong.com/posts/GjkqijXHakMyDxF9e/a-quick-elegant-derivation-of-bayes-theorem#comments">Discuss</a>

---

### [Will we get automated alignment research before an AI Takeoff?](https://www.lesswrong.com/posts/z4FvJigv3c8sZgaKZ/will-we-get-automated-alignment-research-before-an-ai)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 22, 2026 5:46 PM GMT<br /><br /><p>TLDR: Will AI-automation first speed up capabilities or safety research? I forecast that most areas of capabilities research will see a 10x speedup before safety research. This is primarily because capabilities research has clearer feedback signals and relies more on engineering than on novel insights. To change this, researchers should now build and adopt tools to automate AI Safety research, focus on creating benchmarks, model organisms, and research proposals, and companies should grant differential access to safety research.</p><p><i>Epistemic status: I spent ~ a week thinking about this. My conclusions rely on a model with high uncertainty, so please take them lightly. I‚Äôd love for people to share their own estimates about this.</i></p><h2>The Ordering of Automation Matters</h2><p>AI might automate AI R&amp;D in the next decade and thus lead to large increases in AI progress. This is extremely important for both the risks (eg an Intelligence Explosion) and the solutions (automated alignment research).</p><p>On the one hand, AI could drive AI capabilities progress. This is a stated goal of Frontier AI Companies (eg <a href="https://x.com/sama/status/1983584366547829073">OpenAI aims for a true automated AI researcher by March of 2028</a>), and it is central to the <a href="https://ai-2027.com/">AI 2027</a> forecast. On the other hand, AI could help us to solve many AI Safety problems. This seems to be the safety plan for some Frontier AI Companies (eg OpenAI‚Äôs <a href="https://openai.com/index/introducing-superalignment/">Superalignment team</a> aimed to build ‚Äúa roughly human-level automated alignment researcher‚Äù), and <a href="https://joecarlsmith.com/2025/04/30/can-we-safely-automate-alignment-research/">prominent</a> <a href="https://dillinger.io/(https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap)">AI Safety</a> <a href="https://www.lesswrong.com/posts/WJ7y8S9WdKRvrzJmR/building-ai-research-fleets">voices</a> have argued that this should be a priority for the AI safety community.</p><p>I argue that it will matter tremendously in which order different areas of ML research will be automated and, thus, which areas will see large progress first. Consider these two illustrative scenarios:</p><p><strong>World 1</strong>: In 2030, AI has advanced to speeding up capabilities research by 10x. We see huge jumps in AI capabilities in a single year, comparable to those between 2015-2025. However, progress on AI alignment turns out to be bottlenecked by novel, conceptual insights and has fewer clear feedback signals. Thus, AI Alignment research only sees large speedups in 2032. There is some progress in AI Alignment, but it now lags far behind capabilities.</p><p><strong>World 2</strong>: In 2030, AI is capable of contributing to many areas of AI research. This year sees massive progress in AI Safety research, with key problems in alignment theory and MechInterp being solved. However, progress in AI capabilities mostly depends on massive compute scale-ups. Thus, capabilities research doesn‚Äôt benefit much from the additional labour and continues at the same pace. In 2032, when AI is capable of significantly accelerating progress in AI capabilities, key problems in AI Safety have already been solved.</p><figure class="image image_resized" style="width: 90.36%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/z4FvJigv3c8sZgaKZ/zqs2oukmiwh2hjmlx4tv" /></figure><p>World 2 seems a lot safer than World 1, because safety problems are solved <i>before</i> they are desperately needed. But which world are we heading toward? To forecast this, I ask:</p><p><strong>In which order will different areas of ML R&amp;D experience a 10x speedup in progress due to AI compared to today?</strong></p><p><strong><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Za2jgCe8YPweeiYHr/04bea3f566c7b62894a8bd67b5fb3e147c9b39191ca153f890a4f348cc2d526f/z70hxvy2x2ndow8cnwpj" /></strong></p><p>Some clarifications on the question:</p><ul><li>You can think about this 10x speedup as: The progress a field would make in 10 years with current tools will be made in 1 year with the help of AI.</li><li>For a 10x speedup, full automation is not necessary. It can mean that some labour-intensive tasks are automated, making researchers more efficient, or that the quality of research increases.</li><li>To make things easier, I don‚Äôt aim to forecast dates, but simply the ordering between different areas.</li><li>The 10x number is chosen arbitrarily. What actually matters is cumulative progress, not crossing an arbitrary speedup threshold. However, for any factor between 2x - 30x my conclusions would stay roughly the same.</li></ul><p>Some other relevant forecasts include:</p><ul><li><a href="https://metr.org/blog/2025-08-20-forecasting-impacts-of-ai-acceleration/">METR and FRI</a>: domain experts gave a 20% probability for 3x acceleration in AI R&amp;D by 2029, while superforecasters gave 8%.</li><li><a href="https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion">Forethought</a>: estimates ~60% probability of compressing more than 3 years of AI progress into &lt;1 year after full automation of AI R&amp;D.</li><li><a href="https://www.aifuturesmodel.com/">AI Futures Project</a>: Predicts that a superhuman coder/AI researcher could speed up Algorithmic Progress by 10x in December 2031.</li></ul><p>However, <strong>none of these forecasts break down automation by research area</strong> - they treat AI R&amp;D as a monolithic category rather than distinguishing different research areas or considering differential acceleration of safety vs capabilities research.</p><h2><strong>Methodology</strong></h2><p>How can we attempt to forecast this? I focus on 7 factors that make an area more or less amenable to speedups from automation and evaluate 10 research areas on those (5 safety; 5 capabilities).</p><p>My approach was to determine factors that indicate whether an area is more or less amenable to AI-driven speedups. For this, I drew on some previous thinking:</p><ul><li><a href="https://joecarlsmith.com/2025/04/30/can-we-safely-automate-alignment-research/">Carlsmith</a> emphasises feedback quality as a critical factor: ‚ÄúNumber-go-up‚Äù research with clear quantitative metrics (easiest), ‚Äúnormal science‚Äù with empirical feedback loops (medium), and ‚Äúconceptual research‚Äù that relies on ‚Äújust thinking about it‚Äù (hardest). Additionally, he warns that scheming AIs could undermine progress on alignment research specifically.</li><li><a href="https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap">Hobbhahn</a> identifies task structure, clear objectives, and feedback signals as key factors. Some safety work is easier to automate (red teaming, evals) while other areas are harder (threat modeling, insight-heavy interpretability).</li><li>Epoch‚Äôs <a href="https://epochai.org/research/automation-of-ai-r-and-d">interviews with ML researchers</a> found consensus that near-term automation would focus on implementation tasks rather than hypothesis creation.</li></ul><p>I determined these 7 factors as most important and scored each research area on them:</p><ul><li><strong>Task length</strong>: How long are the tasks in this research area? Tasks here are units that cannot usefully be broken down anymore. Shorter tasks are better as they will be automatable earlier.</li><li><strong>Insight/creativity vs engineering/routine</strong>: LLMs are currently better at routine tasks and at engineering. They are less good at coming up with new insights and being creative.</li><li><strong>Data availability</strong>: AIs can be trained on the data of a research area to improve performance. If an area has more existing papers and open codebases an AI should be better at it.</li><li><strong>Feedback quality/verifiability</strong>: Are there clear, easily available criteria for success and failure? In some areas progress has clear metrics, while for others it‚Äôs harder to tell whether progress was made. Number-go-up science is easier to automate. Additionally, easy verification of success makes it possible to build RL loops that can further improve performance.</li><li><strong>Compute vs labour bottlenecked</strong>: If progress is mostly bottlenecked by having more compute, then additional labour may cause less of a speedup. [On the other hand, additional labour might help use available compute more efficiently by designing better experiments or unlocking efficiency gains in training/inference]</li><li><strong>Scheming AIs</strong>: Misaligned AI systems used for AI R&amp;D might intentionally underperform or sabotage the research. In this case we might still be able to get some useful work out of them, but it would be <a href="https://joecarlsmith.com/2025/04/30/can-we-safely-automate-alignment-research#9-what-about-scheming">harder</a>.</li><li><strong>AI Company incentive for automation</strong>: AI Companies might care more about progress in some research areas than others and would thus be willing to spend more staff time, compute and money into setting up AIs to speed up this area.</li></ul><p>I assigned a weight for each of these factors according to how important I judge them to be. Next, for each factor and each research area I estimated a value from 1-10 (where 10 means it is more amenable to automation). I then combine them into a weighted sum to estimate how amenable a research area is to an AI-enabled speedup.</p><p>I only focus on 10 areas, of which the first 5 are considered ‚ÄúAI Safety‚Äù while the other 5 improve ‚ÄúAI Capabilities‚Äù:</p><ul><li>Interpretability</li><li>Alignment Theory</li><li>Scalable Oversight</li><li>AI Control</li><li>Dangerous Capability Evals</li><li>Pre-training Algorithmic Improvements</li><li>Post-training methods Improvements</li><li>Data generation/curation</li><li>Agent scaffolding</li><li>Training/inference efficiency</li></ul><p>These are supposed to be representative of some important areas of AI Safety and some areas critical to improved Frontier AI capabilities. This is simplifying because no area neatly fits into the safety/capabilities buckets, and because there might be specific problems in an area that are more/less amenable to automation. Additionally, I don‚Äôt take into account that some problems might get harder over time, because adversaries get stronger or low hanging fruit is picked. It‚Äôs not necessary for all safety areas to be sped up before all capabilities areas, but the more overall safety research sees progress before capabilities areas, the better.</p><p>I arrived at values per factor for each area by:</p><ol><li>Skimming a sample of papers and reading Deep Research reports about the day-to-day work in each area before making a 1-10 judgement. For data availability, I took into account the number of papers on GoogleScholar for relevant keywords, and for insight/creativity, I had Claude Haiku 4.5 rate 10 papers per area.</li><li>Asking Claude, Gemini, and ChatGPT to give ratings for each area &amp; factor from 1-10</li><li>Average those estimates into the final number. I put a 5/3 weight on my judgments compared to the ensemble of AIs.</li></ol><p><strong>Weaknesses of the method:</strong> My conclusions are sensitive to the factor weights, and I am quite uncertain about them. I‚Äôm uncertain about the values I assigned per research area. Additionally, advances in AI capabilities might unlock large speedups across all research areas nearly simultaneously, making it less important which research areas see gains from automation earlier. Further, I don‚Äôt take into account factors such as (1) how parallelizable work is, (2) what human bottlenecks there are during the research process, (3) the effects of partial automation, and (4) labor allocation where researchers move to other areas.</p><h2><strong>Prediction</strong></h2><p>This table shows the rating for each area and factor, where higher numbers mean that the factor implies higher amenability to automation. The last rows show the weighted sum results and the average amenability to automation for safety and capabilities research areas:</p><figure class="table"><table style="border-style: none;"><thead><tr><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Factor (weight)</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Interpretability</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Alignment Theory</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Scalable Oversight</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>AI Control</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Dangerous Capability Evals</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Pre-training Alg. Improvement</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Post-training</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Data generation</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Agent scaffolding</strong></th><th style="background-color: #d9e2f3; border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;"><strong>Training efficiency</strong></th></tr></thead><tbody><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Task length (1.0)</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.62</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">2.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">4.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.25</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.62</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">3.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">5.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.50</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.12</td></tr><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Insight vs engineering (1.5)</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">4.56</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">1.50</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">5.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.06</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.56</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.75</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">8.86</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.50</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.23</td></tr><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Data availability (0.2)</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">4.88</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">3.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">2.25</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">2.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">4.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.75</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.38</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">5.88</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.00</td></tr><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Feedback quality (2.5)</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">3.88</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">1.38</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">3.88</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">4.38</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">3.62</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.50</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.38</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.50</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">9.12</td></tr><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Compute vs labour (2.5)</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.88</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">10.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.75</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">8.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.62</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">2.12</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">4.38</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">3.88</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.62</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">5.25</td></tr><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Scheming risk (0.4)</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">2.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">1.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">1.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">1.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">1.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">9.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">7.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">9.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">9.00</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">9.00</td></tr><tr><td style="padding: 2pt 3pt; vertical-align: top;">Economic incentive (0.5)</td><td style="padding: 2pt 3pt; vertical-align: top;">3.88</td><td style="padding: 2pt 3pt; vertical-align: top;">1.88</td><td style="padding: 2pt 3pt; vertical-align: top;">4.00</td><td style="padding: 2pt 3pt; vertical-align: top;">4.62</td><td style="padding: 2pt 3pt; vertical-align: top;">5.38</td><td style="padding: 2pt 3pt; vertical-align: top;">9.38</td><td style="padding: 2pt 3pt; vertical-align: top;">9.38</td><td style="padding: 2pt 3pt; vertical-align: top;">9.00</td><td style="padding: 2pt 3pt; vertical-align: top;">8.75</td><td style="padding: 2pt 3pt; vertical-align: top;">9.12</td></tr><tr><td style="padding: 2pt 3pt; vertical-align: top;"><strong>Amenable to speedup from Automation</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>5.53</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>4.04</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>4.76</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>5.78</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>5.93</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>5.37</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>6.21</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>6.22</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>7.35</strong></td><td style="padding: 2pt 3pt; vertical-align: top;"><strong>7.09</strong></td></tr><tr><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">Category Average</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">5.21</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">6.45</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td><td style="border-color: #cccccc; padding: 2pt 3pt; vertical-align: top;">&nbsp;</td></tr></tbody></table></figure><p>&nbsp;</p><p><strong>Overall, capabilities research seems more amenable to speedups from automation than safety research</strong> under this model. The only exception is algorithmic improvement in pre-training, because it is heavily bottlenecked by compute.</p><p>Agent Scaffolding and Training Efficiency appear to be most likely to see speedups from automation. Both have easy-to-verify progress, are not hugely bottlenecked by compute, and promise large economic benefits for AI companies. Among safety research, I believe that AI Control and Dangerous Capability Evals are more likely to see speedups from AI R&amp;D, as they are somewhat engineering-heavy and companies have some interest in making progress on them. Alignment Theory seems least likely to see large speedups from AI research soon, mostly because it is very difficult to verify whether one is making progress and because it is largely driven by insight instead of engineering work.</p><p>The factors that most favor automation of capability over safety research are scheming risk, economic incentive, feedback quality, data availability, and reliance on engineering. Task length is neutral, while safety research is less compute-bottlenecked.</p><p>You can read my reasoning for my judgments in this <a href="https://docs.google.com/document/d/1lSzdTiLuw2jf8EgwlYo_mEgxOoE2InbNX-kMrryk-hI/edit?usp=sharing">Google Doc</a>. If you think I‚Äôm wrong about something, you have 2 great options:</p><ul><li>Put in your own numbers into an <a href="https://janwehner.com/ai_r-d_automation_models/">interactive version of this model</a></li><li>Let me know in the comments. Esp if you have experience with these areas, I‚Äôm happy to update my numbers.</li></ul><h2><strong>Levers for affecting the order</strong></h2><p>What could we do to affect when and how much AI R&amp;D affects capabilities and safety progress? Here we are aiming for differential acceleration, i.e., either speeding up the automation and resulting progress of safety research or slowing down capabilities research.</p><p><strong>Slowing down the automation-based acceleration of capabilities progress</strong> might be achieved through government pressure/regulation or convincing AI companies to be careful. Additionally, it could be beneficial to give <strong>differential access to new model capabilities to safety researchers</strong>. For example, after a company develops a new model, it could first apply the model to safety research for 1 month before using it to improve capabilities. Similarly, it would be great to get <strong>commitments from AI Companies</strong> to spend x% of their compute on automated safety research.</p><h3><strong>Speeding up Safety Automation</strong></h3><p>Speeding up Safety research seems more tractable to me, as many interventions can be done unilaterally by outside actors. However, multiple of these interventions risk spillover effects by accidentally speeding up automation of capability research.</p><p><strong>Do the schlepp of building safety research automations.</strong> There are many mundane tasks and iterations that need to be done to make automated AI Safety Research work. For example, <a href="https://www.lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today">Skinkle et al</a> suggest making safety-relevant libraries and datasets more accessible to AI Agents. <a href="https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap">Hobbhahn</a> argues for building research pipelines now, into which new models can be plugged. In general, people should be trying to automate large parts of safety research now, see what problems come up, fix them, and iterate on that basis. This should be a priority of safety researchers inside labs and presents an opportunity for outside startups to build safety automation tools.</p><p><strong>Improve feedback quality.</strong> My model suggests that feedback quality is where safety lags furthest behind capabilities research. Thus, AI Safety Researchers should try to turn more safety problems into ‚Äúnumbers-go-up‚Äù science. For example, researchers should prioritize developing model organisms of misalignment, creating new experimental protocols for evaluating scalable oversight methods, and building simulated environments for AI control red-blue games.</p><p><strong>Prepare research proposals.</strong> <a href="https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap">Writing up detailed research proposals and project specifications</a> reduces the amount of novel insight needed, making it easier for AI systems to contribute. Human researchers should consider focusing their time on conceptual thinking, insight generation, and threat modelling‚Äîthe tasks that are hardest to automate‚Äîwhile deferring engineering-heavy implementation work that AI will soon handle well.</p><p><strong>Accelerate adoption of AI tools.</strong> Safety researchers should be early and heavy adopters of AI automation tools. This could be supported through workshops on research automation and public discussion of best practices. <a href="https://www.lesswrong.com/posts/WJ7y8S9WdKRvrzJmR/building-ai-research-fleets#Individual_practices">Golfhaber &amp; Hoogland</a> argue that adoption does not only mean individuals being able to automate some tasks, but that organisations adopt practices and change workflows that allow them to build so-called ‚Äúautomated research fleets‚Äù. Thus, it is also on the leadership of AI Safety orgs to encourage adoption by their staff and to develop the organisational structure to be amenable to AI automation.</p><p><strong>Reduce scheming risk.</strong> Investing in AI Control and alignment research helps reduce both the probability and impact of scheming on our ability to get useful safety research out of AI systems. This is important because scheming risk disproportionately affects safety research. Funders and researchers should continue to prioritize this threat model.</p><p><strong>Differentially accelerate helpful capabilities.</strong> Some AI capabilities may disproportionately benefit safety research. We should aim to accelerate those. Examples include conceptual and philosophical thinking, forecasting, and threat modelling. For this, a researcher should study which capabilities might be good candidates for differential automation. Then, AI Developers should prioritize improving those capabilities.</p><p><strong>Safety Research Automation Benchmark.</strong> To make it easier to measure progress on safety research automation, <a href="https://www.lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today">Skinkle et al</a> suggest building dedicated benchmarks, including relevant tasks. This could be a great project for AIS researchers outside of companies.</p><p><strong>Invest in documentation.</strong> There is less public data on safety research than on capabilities research. Any AI Safety Researcher could help by releasing internal notes and failed experiments, documenting and publishing research processes (not just results), and recording and transcribing their brainstorming sessions. AIs could be trained on this data to be better able to conduct safety research, or the data could be used as additional context. Funders could also incentivise researchers to document their research progress.</p><p>Of the proposed interventions, I am most excited about safety-conscious researchers/engineers directly trying to build tools for automated AI safety research, because it will highlight problems and enables the communicate to build on them. While there is a significant risk of also accelerating capability research, I believe it would strongly differentially accelerate safety automation.</p><h2><strong>Future Work: How could this be researched properly?</strong></h2><p>I made a start at forecasting this. I think a higher-effort, more rigorous investigation would be very useful. Specifically, later work could be much more rigorous in estimating the numbers per area &amp; factor that feed into my model, and should use more sophisticated methodology than a weighted factor model.</p><p>A key insight from the labor automation literature is that technology displaces workers from specific <i>tasks</i> rather than eliminating entire jobs or occupations. Similarly, the right level of analysis for our question is to look at specific tasks that researchers do and see how amenable they are to AI automation or support.</p><p>One approach for identifying the tasks is to conduct Interviews with ML Experts in different areas to elicit:</p><ul><li>Task decomposition: Figure out which tasks they do day-to-day: ‚ÄúWalk me through a day in your last week.‚Äù ‚ÄúWhat concrete tasks make up their work?‚Äù ‚ÄúWhat percentage of time goes to different kinds of tasks?‚Äù</li><li>Task characteristics: For each task type, how would they rate it on criteria like: task length, feedback quality, insight vs engineering, ‚Ä¶.</li><li>Automation amenability: Asking about their judgments and qualitative opinions. ‚ÄúWhich tasks could current AI assistants help with?‚Äù ‚ÄúWhat capabilities are missing?‚Äù ‚ÄúWhat do they expect to remain hardest to automate?‚Äù</li></ul><p>I did one pilot interview where I asked about the above. I learned it was useful to ask about specific timeframes for eliciting tasks (in the last month, in your last project), to use hypothetical comparisons (‚Äúhow much faster with X tool?‚Äù), and not use abstract scales (‚Äúrate data availability from 1-10‚Äù) and to distinguish between different types of data availability.</p><p>Other work that seems useful:</p><ul><li>Repeatedly run experiments measuring how much researchers in different areas are sped up by AI tools (similar to this <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">METR study</a>)</li><li>Interviewing researchers about their AI use and perceived speedup (although the <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">METR study</a> shows these estimates can be very wrong)</li><li>Creating prediction markets for this. I found it difficult to formalize questions with sufficiently clear resolution criteria.</li><li>Using more rigorous models to forecast speedups from AI automation. For a sketch of a more rigorous model, see <a href="https://docs.google.com/document/d/1lSzdTiLuw2jf8EgwlYo_mEgxOoE2InbNX-kMrryk-hI/edit?usp=sharing">this Appendix</a>.</li></ul><p><i>Thanks to Charles Whitman, Julian Schulz, and Cheryl Wu for feedback.</i></p><br /><br /><a href="https://www.lesswrong.com/posts/z4FvJigv3c8sZgaKZ/will-we-get-automated-alignment-research-before-an-ai#comments">Discuss</a>

---

### [AI #152: Brought To You By The Torment Nexus](https://www.lesswrong.com/posts/pCkYfhYcwFLELoYQf/ai-152-brought-to-you-by-the-torment-nexus)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 22, 2026 2:40 PM GMT<br /><br /><p><a href="https://x.com/AnthropicAI/status/2014005798691877083">Anthropic released a new constitution for Claude</a>. I encourage those interested to read the document, either in whole or in part. I intend to cover it on its own soon.</p>
<p>There was also actual talk about coordinating on a conditional pause or slowdown from DeepMind CEO Demis Hassabis, which I also plan to cover later.</p>
<p>Claude Code continues to be the talk of the town, <a href="https://thezvi.substack.com/p/claude-codes-3?r=67wny">the weekly report on that is here</a>.</p>
<p>OpenAI responded by planning ads for the cheap and free versions of ChatGPT.</p>
<p>There was also a fun but meaningful incident <a href="https://thezvi.substack.com/p/chatgpt-self-portrait?r=67wny"><strong>involving ChatGPT Self Portraits</strong></a>.</p>
<div>&nbsp;</div>


<span id="more-25047"></span>





<h4>Table of Contents</h4>


<ol>
<li><a href="https://thezvi.substack.com/i/184715851/language-models-offer-mundane-utility">Language Models Offer Mundane Utility.</a> Call in the tone police.</li>
<li><a href="https://thezvi.substack.com/i/184715851/language-models-don-t-offer-mundane-utility">Language Models Don‚Äôt Offer Mundane Utility.</a> He who lives by the pattern.</li>
<li><a href="https://thezvi.substack.com/i/184715851/huh-upgrades">Huh, Upgrades.</a> Claude health integrations, ChatGPT $8/month option.</li>
<li><a href="https://thezvi.substack.com/i/184715851/gemini-personalized-intelligence">Gemini Personalized Intelligence.</a> Signs of both remain somewhat lacking.</li>
<li><a href="https://thezvi.substack.com/i/184715851/deepfaketown-and-botpocalypse-soon">Deepfaketown and Botpocalypse Soon.</a> Get that bathtub viking.</li>
<li><a href="https://thezvi.substack.com/i/184715851/fun-with-media-generation">Fun With Media Generation.</a> Studio Ghibli pics are back, baby.</li>
<li><a href="https://thezvi.substack.com/i/184715851/we-re-proud-to-announce-the-torment-nexus"><strong>We‚Äôre Proud To Announce The Torment Nexus</strong>.</a> Ads come to ChatGPT.</li>
<li><a href="https://thezvi.substack.com/i/184715851/they-took-our-jobs">They Took Our Jobs.</a> Find a game plan. Don‚Äôt count on repugnance.</li>
<li><a href="https://thezvi.substack.com/i/184715851/the-revolution-of-rising-expectations">The Revolution of Rising Expectations.</a> Look at all the value you‚Äôre getting.</li>
<li><a href="https://thezvi.substack.com/i/184715851/get-involved">Get Involved.</a> AI Village, Anthropic, Dwarkesh Patel guest hunter.</li>
<li><a href="https://thezvi.substack.com/i/184715851/a-young-lady-s-illustrated-primer">A Young Lady‚Äôs Illustrated Primer.</a> We‚Äôre putting together the wrong team.</li>
<li><a href="https://thezvi.substack.com/i/184715851/in-other-ai-news">In Other AI News.</a> China remain behind, Drexler goes galaxy brain.</li>
<li><a href="https://thezvi.substack.com/i/184715851/axis-of-assistance"><strong>Axis of Assistance</strong>.</a> Have you tried not being a helpful AI assistant?</li>
<li><a href="https://thezvi.substack.com/i/184715851/show-me-the-money">Show Me the Money.</a> OpenAI looks to raise another $50 billion.</li>
<li><a href="https://thezvi.substack.com/i/184715851/california-in-crisis">California In Crisis.</a> Will we soon ask, where have all the startups gone?</li>
<li><a href="https://thezvi.substack.com/i/184715851/bubble-bubble-toil-and-trouble">Bubble, Bubble, Toil and Trouble.</a> They keep using that word.</li>
<li><a href="https://thezvi.substack.com/i/184715851/quiet-speculations">Quiet Speculations.</a> Results from the AI 2025 predictions survey.</li>
<li><a href="https://thezvi.substack.com/i/184715851/elon-musk-versus-openai">Elon Musk Versus OpenAI.</a> There they go again.</li>
<li><a href="https://thezvi.substack.com/i/184715851/the-quest-for-sane-regulations"><strong>The Quest for Sane Regulations</strong>.</a> Nvidia versus the AI Overwatch Act.</li>
<li><a href="https://thezvi.substack.com/i/184715851/chip-city">Chip City.</a> Are we on the verge of giving China ten times their current compute?</li>
<li><a href="https://thezvi.substack.com/i/184715851/the-week-in-audio">The Week in Audio.</a> Tyler Cowen and a surprisingly informed Ben Affleck.</li>
<li><a href="https://thezvi.substack.com/i/184715851/rhetorical-innovation">Rhetorical Innovation.</a> Remember the conservation of expected evidence.</li>
<li><a href="https://thezvi.substack.com/i/184715851/aligning-a-smarter-than-human-intelligence-is-difficult">Aligning a Smarter Than Human Intelligence is Difficult.</a> Nope, still difficult.</li>
<li><a href="https://thezvi.substack.com/i/184715851/alignment-is-not-primarily-about-a-metric">Alignment Is Not Primarily About a Metric.</a> Not a metric to be optimizing.</li>
<li><a href="https://thezvi.substack.com/i/184715851/how-to-be-a-safe-robot">How To Be a Safe Robot.</a> Hint, the plan is not ‚Äòdon‚Äôt tell it about unsafe robots.‚Äô</li>
<li><a href="https://thezvi.substack.com/i/184715851/living-in-china">Living In China.</a> Chinese LLMs know things and pretend not to. Use that.</li>
<li><a href="https://thezvi.substack.com/i/184715851/claude-3-opus-lives">Claude 3 Opus Lives.</a> Access granted.</li>
<li><a href="https://thezvi.substack.com/i/184715851/people-are-worried-about-ai-killing-everyone">People Are Worried About AI Killing Everyone.</a> Charles Darwin.</li>
<li><a href="https://thezvi.substack.com/i/184715851/messages-from-janusworld">Messages From Janusworld.</a> What are you worried people will do with your info?</li>
<li><a href="https://thezvi.substack.com/i/184715851/everyone-is-confused-about-ai-consciousness">Everyone Is Confused About AI Consciousness.</a> Don‚Äôt call it a disproof.</li>
<li><a href="https://thezvi.substack.com/i/184715851/the-lighter-side">The Lighter Side.</a></li>
</ol>


<h4>Language Models Offer Mundane Utility</h4>


<p>Tone editor or tone police is a great AI job. <a href="https://www.bloomberg.com/news/articles/2026-01-21/white-collar-workers-use-ai-to-have-tough-conversations-at-work">Turn your impolite ‚Äòf*** you‚Äô email into a polite ‚Äòf*** you‚Äô email</a>, and get practice stripping your emotions out of other potentially fraught interactions, lest your actual personality get in the way. Or translate your neurodivergent actual information into socially acceptable extra words.</p>
<p><a href="https://x.com/Out5p0ken/status/2012969221492265047">ICE uses an AI program from Palantir called ‚ÄòElite‚Äô</a> to pick neighborhoods to raid.</p>


<h4>Language Models Don‚Äôt Offer Mundane Utility</h4>


<p>If your query is aggressively pattern matched into a basin where facts don‚Äôt matter and you‚Äôre making broad claims without much justifying them, <a href="https://x.com/DominikPeters/status/2013349636560376190">AIs will largely respond</a> to the pattern match, as Claude did in the linked example. And if you browbeat such AIs about it, and they cower to tell you what you want to hear, you can interpret that as ‚Äòthe AI is lying to me, surely this terrible AI is to blame‚Äô or you can wonder why it decided to do all of that.</p>


<h4>Huh, Upgrades</h4>


<p><a href="https://x.com/claudeai/status/2013754136265621952">Claude adds four new health integrations in beta</a>: Apple Health (iOS), Health Connect (Android), HealthEx, and Function Health. They are private by design.</p>
<p><a href="https://openai.com/index/introducing-chatgpt-go/">OpenAI adds the ChatGPT Go option more broadly</a>, at $8/month. If you are using ChatGPT in heavy rotation or as your primary, you need to be paying at least the $20/month for Plus to avoid being mostly stuck with Instant.</p>
<p><a href="https://x.com/sama/status/2013404302400712757">Sam Altman throws out the latest ‚Äòwhat would you like to see us improve?‚Äô thread</a>.</p>
<p>Remember ChatGPT‚Äôs Atlus browser? <a href="https://x.com/adamhfry/status/2014095464375919102">It finally got tab groups</a>, an ‚Äòauto‚Äô option to have search choose between ChatGPT and Google and various other polishes. There‚Äôs still no Windows version and Claude Code is my AI browser now.</p>


<h4>Gemini Personalized Intelligence</h4>


<p><a href="https://x.com/GeminiApp/status/2011469541235417243">The pitch is that Gemini now draws insights from across your Google apps</a> to provide customized responses. There‚Äôs a section for non-Google apps as well, although there‚Äôs not much there yet other than GitHub.</p>
<blockquote>
<p><a href="https://x.com/joshwoodward/status/2011471375521710130">Josh Woodward</a>: Introducing Personal Intelligence. It‚Äôs our answer to a top request: you can now personalize @GeminiApp by connecting your Google apps with a single tap. Launching as a beta in the U.S. for Pro/Ultra members, this marks our next step toward making Gemini more personal, proactive and powerful. Check it out!</p>
<p>Google: Gemini already remembers your past chats to provide relevant responses. But today, we‚Äôre taking the next step forward with the introduction of Personal Intelligence.</p>
<p>You can choose to let Gemini connect information from your Gmail, Google Photos, Google Search, and YouTube history to receive more personalized responses.</p>
<p>Here are some ways you can start using it:</p>
<p>‚Ä¢ Planning: Gemini will be able to suggest hidden gems that feel right up your alley for upcoming trips or work travel.<br />
‚Ä¢ Shopping: Gemini will get to know your taste and preferences on a deeper level, and help you find items you‚Äôll love faster.<br />
‚Ä¢ Motivation: Gemini will have a deeper understanding of the goals you‚Äôre working towards. For example, it might notice that you have a marathon coming up and offer a training plan.</p>
<p>Privacy is central to Personal Intelligence and how you connect other Google apps to Gemini. The new beta feature is off by default: you choose to turn it on, decide exactly which apps to connect, and can turn it off at any time.</p>
</blockquote>
<p>The pitch is that it can gather information from your photos (down to things like where you travel, what kind of tires you need for your car), from your Email and Google searches and YouTube and Docs and Sheets and Calendar, and learn all kinds of things about you, not only particular details but also your knowledge level and your preferences. Then it can customize everything on that basis.</p>
<p>It can access Google Maps, but not your personalized data like saved locations, other than where Work and Home are. It doesn‚Äôt have your location history. This feels like an important missed opportunity.</p>
<p>One potential ‚Äòkiller app‚Äô is fact finding. If you want to know something about yourself and your life, and Google knows it, hopefully Gemini can now tell you. Google knows quite a lot of things, and my Obsidian Vault is echoed in Google Sheets, which you can instruct Gemini to look for. <a href="https://x.com/joshwoodward/status/2011484735243960543">Josh Woodward shows an example of asking when he last got a haircut</a>.</p>
<p>The real killer app would be taking action on your behalf. It can‚Äôt do that except for Calendar, but it can do things on the level of writing draft emails and making proposed changes in Docs.</p>
<p>There really is a ton of info there if it gets analyzed properly. It could be a big game.</p>
<p>When such things work, they ‚Äòfeel like magic.‚Äô</p>
<p>When they don‚Äôt work, they feel really stupid.</p>
<p>I asked for reactions and got essentially nothing.</p>
<p>That checks. To use this, you have to use Gemini. Who uses Gemini?</p>
<p>Thus, in order to test personalized intelligence, I need a use case where I need its capabilities enough to use Gemini, as opposed to going back to building my army of skills and connectors and MCPs in Claude Code, including with the Google suite.</p>
<blockquote>
<p><a href="https://x.com/omooretweets/status/2011495886388625700">Olivia Moore</a>: Connectors into G Suite work just OK in ChatGPT + Claude ‚Äì they‚Äôre slow and can struggle to find things.</p>
<p>If Gemini can offer best ‚Äúcontext‚Äù from Gmail, G Drive, Calendar ‚Äì that‚Äôs huge.</p>
<p>The aggressive version would be to block Connectors in other LLMs‚Ä¶but that feels unlikely!</p>
</blockquote>
<p>The other problem is that Google‚Äôs connectors to its own products have consistently, when I have tried them, failed to work on anything but basic tasks. Even on those basic tasks, the connector from Claude or ChatGPT has worked better. And now I‚Äôm hooking Claude Code up to the API.</p>


<h4>Deepfaketown and Botpocalypse Soon</h4>


<p><a href="https://www.bloomberg.com/news/features/2026-01-19/grok-ai-sexualized-images-expose-gaps-in-oversight-enforcement">Elon Musk and xAI continue to downplay</a> the whole ‚ÄòGrok created a bunch of sexualized deepfakes in public on demand and for a time likely most of the world‚Äôs AI CSAM‚Äô as if it is no big deal. Many countries and people don‚Äôt see it that way, investigations continue and it doesn‚Äôt look like the issue is going to go away.</p>
<p>We used to worry a lot about deepfakes. Then we all mostly stopped worrying about it, at least until the recent xAI incident, <a href="https://www.bloomberg.com/opinion/articles/2026-01-19/ai-deepfakes-have-harmed-too-many-kids-already">but that doesn‚Äôt mean there aren‚Äôt a lot of deepfakes</a>. A Bloomberg report says ‚Äòone in eight kids personally knows someone who has been the target of a deepfake video,‚Äô which is an odd way to think about prevalence but is certainly a massive increase. Reports rose from roughly 4,700 in 2023 to over 440,000 in the first half of 2025.</p>
<p>We could stop Grok if we wanted to, but the open-source tools are already plenty good enough to generate sexualized deepfakes and will only get easier to access. You can make access annoying and shut down distribution, but you can‚Äôt shut the thing down on the production side.</p>
<p><a href="https://www.bloomberg.com/opinion/articles/2026-01-17/ai-deepfake-nude-warnings-are-missing-a-deterrent">Meanwhile, psychiatrist Sarah Gundle issues the latest warning</a> that this ‚Äòinteractive pornography,‚Äô in addition to the harms to the person depicted, also harms the person creating or consuming it, as it disincentivizes human connection by making alternatives too easy, and people (mostly men) don‚Äôt have the push to establish emotional connections. I am skeptical of such warnings and concerns, they always are of a form that could prove far too much and historical records mostly don‚Äôt back it up, but on the other hand, <a href="https://www.youtube.com/watch?v=JPQJBgWwg3o&amp;pp=ygUaZG9uJ3QgZGF0ZSByb2JvdHMgZnV0dXJhbWHYBgk%3D">don‚Äôt date robots</a>.</p>
<p>Misinformation is demand driven, an ongoing series.</p>
<blockquote>
<p><a href="https://x.com/JerryDunleavy/status/2012908443464613918">Jerry Dunleavy IV </a>: Neera Tanden believes that ICE agents chased a protester dressed in Viking gear and sitting in a bath tub with skateboard wheels down the street, and that the Air Force was called in in response. Certain segments of the population just are not equipped to handle obvious AI slop.</p>
<p><a href="https://x.com/AmyA1A/status/2012909087458033777">Amygator *not an actual alligator</a>: Aunt Carol on the family group chat <a href="https://x.com/AmyA1A/status/2012909087458033777">isn‚Äôt sure whether or not this is A.I.</a> I‚Äôm done.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/n0y7a2dk1d92meum6c7i" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>This is not a subtle case. The chyron is literally floating up and down in the video. In a sane world this would be a good joke. Alas, there are those on all sides who don‚Äôt care that something like this is utterly obvious, but it makes little difference that this was an AI video instead of something else.</p>
<p>In Neera‚Äôs defense, the headlines this week include ‚ÄòPresident sends letter to European leaders demanding Greenland because Norway wouldn‚Äôt award him the Nobel Peace Prize.‚Äô Is that more or less insane than the police unsuccessfully chasing a bathtub viking on the news while the chyron slowly bounces?</p>


<h4>Fun With Media Generation</h4>


<p>The new OpenAI image generation can‚Äôt do Studio Ghibli properly, <a href="https://x.com/eigenrobot/status/2011989030817141032">but as per Roon you can still use the old one</a> <a href="https://chatgpt.com/g/g-6940a876d5f4819186b4668deabcd580-4o-imagegen">by going here.</a></p>
<blockquote>
<p><a href="https://x.com/tszzl/status/2011898230821523943">Roon</a>: ‚Äãconfirmed that this is a technical regression in latest image model nothing has changed WRT policy.</p>
</blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/r3lb9rewz5a877pw1oof" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote>
<p>Bryan: The best part of all this is all you gotta do is drop ur image and say ‚ÄúGhibli‚Äù ‚Äì perfection‚Äã.</p>
</blockquote>
<p>It‚Äôs very disappointing that they were unable to preserve this capability going forward, but as long as we have the old option, we‚Äôre still good. Image generation is already very good in many ways so often what you are about is style.</p>
<p><a href="https://x.com/big_business_/status/2010932655873798241?s=20">Sienna Rose recently had three songs in the Spotify top 50, while being an AI</a>, and <a href="https://x.com/andrewcurran_/status/2012220138473304347?s=46">we have another sighting in Sweden</a>.</p>


<h4></h4>



<h4>We‚Äôre Proud To Announce The Torment Nexus</h4>


<p><a href="https://x.com/sama/status/2012253252771824074">The technical name for this edition is ‚Äòads in ChatGPT</a>.‚Äô They attempt to reassure us that they will not force sufficiently paying customers into the Nexus, and it won‚Äôt torture the non-paying customers all that much after all.</p>
<blockquote>
<p><a href="https://x.com/sama/status/2012253252771824074">Sam Altman</a>: We are starting to test ads in ChatGPT free and Go (new $8/month option) tiers.</p>
<p>Here are our principles. Most importantly, we will not accept money to influence the answer ChatGPT gives you, and we keep your conversations private from advertisers. It is clear to us that a lot of people want to use a lot of AI and don‚Äôt want to pay, so we are hopeful a business model like this can work.</p>
<p>(An example of ads I like are on Instagram, where I‚Äôve found stuff I like that I otherwise never would have. We will try to make ads ever more useful to users.)</p>
</blockquote>
<p>I use Instagram very little (and even then I do not post or interact with posts) so perhaps the customization simply doesn‚Äôt kick in, but I‚Äôve found the ads and especially the ‚Äòsuggested posts‚Äô there worthless to the point of making the website unusable in scroll mode, since it‚Äôs become mostly these ‚Äòsuggested posts,‚Äô whereas I don‚Äôt see many ads but they‚Äôve all been completely worthless. Others have also said their ads are unusually good.</p>
<blockquote>
<p><a href="https://x.com/OpenAI/status/2012223373489614951">OpenAI</a>: In the coming weeks, we plan to start testing ads in ChatGPT free and Go tiers.</p>
<p>We‚Äôre sharing our principles early on how we‚Äôll approach ads‚Äìguided by putting user trust and transparency first as we work to make AI accessible to everyone.</p>
<p>What matters most:<br />
‚Äì Responses in ChatGPT will not be influenced by ads.</p>
<p>‚Äì Ads are always separate and clearly labeled.</p>
<p>‚Äì Your conversations are private from advertisers.</p>
<p>‚Äì Plus, Pro, Business, and Enterprise tiers will not have ads.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/qc2uxy1jq3ppztvsofp6" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Here‚Äôs an example of what the first ad formats we plan to test could look like.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/ytspt3wzvtjmk2bmq6kk" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/pypz0kv1qjhlfxg93ika" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>So, on the principles:</p>
<ol>
<li>If you wanted to know what ‚ÄòAGI benefits humanity‚Äô meant, well, it means ‚Äòpursue AGI by selling ads to fund it.‚Äô That‚Äôs the mission.</li>
<li>I do appreciate that they are not sharing conversations directly with advertisers, and the wise user can clear their ad data. But on the free tier, we all know almost no one is ever going to mess with any settings, so if the default is ‚Äòshare everything about the user with advertisers‚Äô then that‚Äôs what most users get.</li>
<li>Ads not influencing answers directly, and not optimizing for time spent on ChatGPT are great, but even if they hold to both the incentives cannot be undone.</li>
<li>It is good that ads are clearly labeled, the alternative would kill the whole product.</li>
<li>Also, we saw the whole GPT-4o debacle, we have all seen you optimize for the thumbs up. Do not claim you do not maximize for engagement, and thereby essentially also for time on device, although that‚Äôs less bad than doing it even more directly and explicitly. And you know Fidji Simo is itching to do it all.</li>
</ol>
<p>This was inevitable. It remains a sad day, and a sharp contrast with alternatives.</p>
<p><a href="https://x.com/ATabarrok/status/2012250773824864513">Then there‚Äôs the obvious joke</a>:</p>
<blockquote>
<p>Alex Tabarrok: ‚ÄãThis is the strongest piece of evidence yet that AI isn‚Äôt going to take all our jobs.</p>
</blockquote>
<p>I will point out that actually this is not evidence that AI will fail to take our jobs. OpenAI would do this in worlds where AI won‚Äôt take our jobs, and would also do this in worlds where AI will take our jobs. OpenAI is planning on losing more money than anyone has ever lost before it turns profitable. Showing OpenAI is not too principled or virtuous to sell ads will likely help its valuation, and thus its access to capital, and the actual ad revenue doesn‚Äôt hurt.</p>
<p>The existence of a product they can use to sell ads, ChatGPT Instant, does not tell us the impact of other AIs on jobs, either now or in the future.</p>
<p><a href="https://stratechery.com/2026/ads-in-chatgpt-why-openai-needs-ads-the-long-road-to-instagram/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI2L2Fkcy1pbi1jaGF0Z3B0LXdoeS1vcGVuYWktbmVlZHMtYWRzLXRoZS1sb25nLXJvYWQtdG8taW5zdGFncmFtLyJdfSwiZXhwIjoxNzcxNDk5MDUzLCJpYXQiOjE3Njg5MDcwNTMsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMDE5NjQwYTctM2NjNS03NzUzLTgzNjgtZmIyODkxMjRjZjEzIiwidXNlIjoiYWNjZXNzIn0.Zb7tCmB0ZSskN3efNb3_sdoPsGDaZ52Satc6aElhXeBz_HI7yVOIMnd5l7ODz2Y5I_hMD-KYDdrBTrfx4Yjis3keSleL1NENNiqhbvVH4aviCgGcrcRiYLaQTtqmGy1xzCp9QbX9mj7sFY5MmiR2poHAssVXLX4SeRHnu5KNDV1pDxhFY5R4vUcmSrix_7jEYeKozPVeTcM3GsRbY2HsppjIxGZRP9pi34SXDKyzABNRXSXeFT-_Be4xvfnTCc6dg1iL4zVCd0YAIlb0fHDoAFDPR2m3OS6mxC3xikXqFLxtnWXD85CrjeG6mP6cZE_jUWsfwFj2leMusdvIKLvECg">As you would expect, Ben Thompson is taking a victory lap and saying ‚Äòobviously</a>,‚Äô also arguing for a different ad model.</p>
<blockquote>
<p>Ben Thompson: ‚ÄãThe advertising that OpenAI has announced is not affiliate marketing; it is, however, narrow in its inventory potential (because OpenAI needs inventory that matches the current chat context) and gives the appearance of a conflict of interest (even if it doesn‚Äôt exist).</p>
<p>What the company needs to get to is an advertising model that draws on the vast knowledge it gains of users ‚Äî both via chats and also via partnerships across the ecosystem that OpenAI needs to build ‚Äî to show users ads that are compelling not because they are linked to the current discussion but because ChatGPT understands you better than anyone else. Sam Altman <a href="https://x.com/sama/status/2012253252771824074"><strong>said on X</strong></a> that he likes Instagram ads.</p>
<p>That‚Äôs not the ad product OpenAI announced, but it‚Äôs the one they need to get to; they would be a whole lot closer had they started this journey a long time ago, but at least they‚Äôre a whole lot closer today than they were a week ago.</p>
</blockquote>
<p>I think Ben is wrong. Ads, if they do exist, should depend on the user‚Äôs history but also on the current context. When one uses ChatGPT one knows what one wants to think about, so to provide value and spark interest you want to mostly match that. Yes, there is also room for ‚Äògeneric ad that matches the user in general‚Äô but I would strive as much as possible for ads that match context.</p>
<p>Instagram is different, because on Instagram your context is ‚Äòscrolling Instagram.‚Äô Instagram doesn‚Äôt allow lists or interests other than choosing your followers, and indeed that severely limits its usefulness, either I have to multi-account or I have to accept that I can only ‚Äòdo one thing‚Äô with it ‚Äì I don‚Äôt want to mix comedians with restaurants with my friends with other things in one giant feed.</p>
<p>What, Google sell ads in their products? Why they would never:</p>
<blockquote>
<p><a href="https://x.com/alexeheath/status/2013713440913694993/history">Alex Heath</a>: Demis Hassabis told me Google has no plans to put ads in Gemini</p>
<p>‚ÄúIt‚Äôs interesting they‚Äôve gone for that so early,‚Äù he said of OpenAI putting ads in ChatGPT. ‚ÄúMaybe they feel they need to make more revenue.‚Äù</p>
<p><a href="https://x.com/tszzl/status/2013748066700955709">roon</a>: big fan of course but this is a bit rich coming from the research arm of the world‚Äôs largest ad monopoly, producing more ad profits than most of the rest of global enterprise put together</p>
<p><a href="https://x.com/kevinroose/status/2013731821339513062">Kevin Roose</a>: To state the obvious: Gemini is an ad-supported product, too. The ads just don‚Äôt appear on Gemini.</p>
</blockquote>
<p>I think all of these are tough but fair.</p>
<p><a href="https://www.bloomberg.com/opinion/articles/2026-01-20/chatgpt-and-openai-are-starting-to-look-a-lot-like-facebook-and-meta">Parmy Olson calls ads ‚ÄòSam Altman‚Äôs last resort</a>,‚Äô which would be unfair <a href="https://x.com/tomwarren/status/2012295849678602610?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E2012295849678602610%7Ctwgr%5Ee474763a4ef0f18b93c3592ea755c1e7a3ec6108%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.bloomberg.com%2Fopinion%2Farticles%2F2026-01-20%2Fchatgpt-and-openai-are-starting-to-look-a-lot-like-facebook-and-meta">except that Sam Altman called ads exactly this in October 2024</a>.</p>


<h4>They Took Our Jobs</h4>


<p>Starting out your career at this time and need a <a href="https://planforai.org/">Game Plan</a> for AI? One is offered here by Sneha Revanur of Encode. Your choices in this plan are Tactician playing for the short term, Anchor to find an area that will remain human-first, or Shaper to try and make things go well. I note that in the long term I don‚Äôt have much faith in the Anchor strategy, even in non-transformed worlds, because of all the people that will flood into the anchors as other jobs are lost. I also wouldn‚Äôt have faith in people‚Äôs ‚Äòrepugnance‚Äô scores on various jobs:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/lqh6t3wzsycmymkqlfvu" /></figure>


<div></div>
</div>
</figure>
</div>
<p>People can say all they like that it would be repugnant to have a robot cut their hair, or they‚Äôd choose a human who did it worse and costs more. I do not believe them. What objections do remain will mostly practical, such as with athletes. When people say ‚Äòmorally repugnant‚Äô they mostly mean ‚ÄòI don‚Äôt trust the AI to do the job,‚Äô which includes observing that the job might include ‚Äòliterally be a human.‚Äô</p>
<p><a href="https://www.anthropic.com/engineering/AI-resistant-technical-evaluations">Anthropic‚Äôs Tristan Hume discusses ongoing efforts</a> to create an engineering take home test for job applicants that won‚Äôt be beaten by Claude. The test was working great at finding top engineers, then Claude Opus 4 did better than all the humans, they modified the test to fix it, then Opus 4.5 did it again. Also at the end they give you the test and invite you to apply if you can do better than Opus 4.5 did.</p>
<p><a href="https://www.understandingai.org/p/ai-is-just-starting-to-change-the">Justin Curl talks to lawyers about their AI usage</a>. They‚Äôre getting good use out of it on the margin, writing and editing emails (especially for tone), finding typos, doing first drafts and revisions, getting up to speed on info, but the stakes are high enough that they don‚Äôt feel comfortable trusting AI outputs without verification, and the verification isn‚Äôt substantially faster than generation would have been in the first place. That raises the question of whether you were right to trust the humans generating the answers before.</p>
<p>Aaron Levie writes that <a href="https://x.com/levie/status/2013018817610518642">enterprise software (ERP) and AI agents are complements, not substitutes</a>. You need your ERP to handle things the same way every time with many 9s of reliability, it is the infrastructure of the firm. The agents are then users of the ERP, the same as your humans are, so you need more and better ERP, not less, and its budget grows as you cut humans out of other processes and scale up. What Aaron does not discuss is the extent to which either the AI agents can bypass the ERP because they don‚Äôt need it. You can also use your AI agents to code your own ERP. It‚Äôs a place vibe coding is at its weakest since it needs to be bulletproof, but how soon before the AI coders are more reliable than the humans?</p>
<blockquote>
<p><a href="https://x.com/patio11/status/2013277116494700816">Patrick McKenzie</a>: Broadly agree with this, and think that most people who expect all orgs to vibe code their way to a software budget of zero do not really understand how software functions in enterprises (or how people function in enterprises, for that matter).</p>
<p>There is a reason sales and marketing cost more than engineering at scaled software companies.</p>
<p>You can also probably foresee (and indeed just see) some conflict along the edges where people in charge of the system of record want people who just want to get their work done to stop trying to poke the system of record with a million apps of widely varying quality.</p>
<p>Preview of coming attractions: defined interface boundaries, fine-grained permissions and audit logs, and no resolution to ‚ÄúIT makes it impossible to do my work so I will adopt a tool that‚Ä¶ -&gt; IT has bought that tool and now I can -&gt; IT makes it impossible to do my work‚Ä¶‚Äù</p>
<p>‚ÄúSounds like you‚Äôre just predicting the past?‚Äù</p>
<p>Oh no the future will be awesome, but it will rhyme, in the same way the operation of a modern enterprise is unimaginable to a filing clerk from 1950s but they would easily recognize much of the basic logic.</p>
</blockquote>
<p><a href="https://agglomerations.substack.com/p/looking-for-the-ladder">Zanna Iscenko, AI &amp; Economy Lead of Google‚Äôs Chief Economist team</a>, argues that the current dearth of entry-level jobs is due to monetary policy and an economic downturn and not due to AI, or at least that any attribution to AI is premature given the timing. I believe there is a confusion here between the rate of AI diffusion versus the updating of expectations? As in, even if I haven‚Äôt adopted AI much, I should still take future adoption into account when deciding whether to hire. There is also a claim that senior hiring declined alongside with junior hiring.</p>
<p>I agree that we don‚Äôt know for sure, but I‚Äôm still going to go for the top half of the gymnastics meme and say that if AI-exposed roles in particular are seeing hiring slowdowns since 2022 it‚Äôs probably not mostly general labor market and interest rate conditions, especially given general labor market and interest rate conditions.</p>
<p><a href="https://x.com/AnthropicAI/status/2011925967992762876">Anthropic came out with its fourth economic index report.</a> They‚Äôre now adjusting for success rates, and estimating 1.2% annual labor productivity growth. Claude thinks the methodology is an overestimate, which seems right to me, so yes for now labor productivity growth is disappointing, but we‚Äôre rapidly getting both better diffusion and more effective Claude.</p>
<blockquote>
<p><a href="https://x.com/mattyglesias/status/2014028790737911949">Matthew Yglesias</a>: One of the big cruxes in AI labor market impact debates is that some people see the current trajectory of improvement as putting on pace for general purpose humanoid robots in the near-ish future while others see that as a discontinuous leap unrelated to anything LLMs do.</p>
<p><a href="https://x.com/binarybits/status/2014032778694705382">Timothy B. Lee</a>: Yes. I‚Äôm in the second camp.</p>
</blockquote>
<p>I don‚Äôt think we know if we‚Äôre getting sufficiently capable humanoid robots (or other robots) soon, but yes I expect that sufficiently advanced AI leads directly to sufficiently capable humanoid robots, the same way it leads to everything else. It‚Äôs a software problem or at most a hardware design problem, so AI Solves This Faster, and also LLMs seem to do well directly plugged into robots and the tech is advancing quickly.</p>
<p>If you think we‚Äôre going to have AGI around for a decade and not get otherwise highly useful robots, I don‚Äôt understand how that would happen.</p>
<p>At the same time, I continue the convention of analyzing futures in which the robots are not coming and AI is not otherwise sufficiently advanced either, because people are very interested in those futures and often dramatically underestimate the transformative effects in such worlds.</p>


<h4>The Revolution of Rising Expectations</h4>


<blockquote>
<p><a href="https://x.com/allTheYud/status/2012250550209851617">Eliezer Yudkowsky</a>: The problem with using abundance of previously expensive goods, as a lens: In 2020, this image of ‚ÄúThe Pandalorian‚Äù might‚Äôve cost me $200 to have done to this quality level. Is anyone who can afford 10/day AI images, therefore rich?</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/dpptbrbergq5qlelivux" /></figure>


<div></div>
</div>
</figure>
</div>
<p>The flip side of the Jevons Paradox is that if people buy more of things that are cheaper, the use-value to the consumer of those goods is decreasing. (Necessarily so! Otherwise they would‚Äôve been bought earlier.)</p>
</blockquote>
<p>As I discuss in The <a href="https://thezvi.substack.com/p/the-revolution-of-rising-expectations">Revolution of Rising Expectations</a>, this makes life better but does not make life easier. It raises the nominal value of your consumption basket but does not help you to purchase the minimum viable basket.</p>


<h4>Get Involved</h4>


<p><a href="https://theaidigest.org/hiring">AI Village is hiring a Member of Technical Staff, salary $150k-$200k</a>. They‚Äôre doing a cool and good thing if you‚Äôre looking for a cool and good thing to do and also you get to work with Shoshannah Tekofsky and have Eli Lifland and Daniel Kokotajlo as advisors.</p>
<p>This seems like a clearly positive thing to work on.</p>
<blockquote>
<p><a href="https://x.com/drew_bent/status/2011896843349774534">Drew Bent</a>: <a href="https://t.co/VRBpgQQ9G6">I‚Äôm hiring for my education team</a> at @AnthropicAI</p>
<p>These are two foundational program manager roles to build out our global education and US K-12 initiatives</p>
<p>Looking for people with‚Ä¶<br />
‚Äì deep education expertise<br />
‚Äì partnership experience<br />
‚Äì a bias toward building<br />
‚Äì technical and hands-on<br />
‚ÅÉ 0-to-1</p>
<p>The KPIs will be students reached in underserved communities + learning outcomes.</p>
</blockquote>
<p><a href="https://t.co/YORpI28c77">Anthropic is also</a> <a href="https://x.com/robertwiblin/status/2013284045627506717">hiring a project manager to work with Holden Karnofsky on its responsible scaling policy</a>.</p>
<p><a href="https://x.com/dwarkesh_sp/status/2011838806870409354">Not entirely AI but Dwarkesh Patel</a> is <a href="https://www.dwarkesh.com/p/hiring-scouts-to-help-me-find-guests">offering $100/hour for 5-10 hours a week to scout for guests in bio, history, econ, math/physics and AI</a>. I am sad that he has progressed to the point where I am no longer The Perfect Guest, but would of course be happy to come on if he ever wanted that.</p>


<h4>A Young Lady‚Äôs Illustrated Primer</h4>


<p>The good news is that Anthropic is building an education team. That‚Äôs great. I‚Äôm definitely not going to let the perfect be the enemy of the great.</p>
<p>The bad news is that the focus should be on raising the ceiling and showing how we can do so much more, yet the focus always seems to be access and raising the floor.</p>
<p>It‚Äôs fine to also have KPIs about underserved communities, but let‚Äôs go in with the attitude that literally everyone is underserved and we can do vastly better, and not much worry about previous relative status.</p>
<p>Build the amazingly great ten times better thing and then give it to everyone.</p>
<blockquote>
<p><a href="https://x.com/mbateman/status/2012055156003914205">Matt Bateman</a>: My emotional reaction to Anthropic forming an education team with a KPI of reach in underserved communities, and with a job ad emphasizing ‚Äúraising the floor‚Äù and partnerships in the poorest parts of the world, is: a generational opportunity is being blown.</p>
<p>In education, everyone is accustomed to viewing issues of access‚Äîwhich are real‚Äîas much more fundamental than they are.</p>
<p>The entire industry is in a bad state and the non-‚Äúunderserved‚Äù are also greatly underserved.</p>
<p>I don‚Äôt know Anthropic‚Äôs education work and this may be very unfair.</p>
<p>And raising the floor in education is a worthy project.</p>
<p>And I hate it when people critique the projects of others on the grounds that they aren‚Äôt in their own set of preferred good deeds, which I‚Äôm now doing.</p>
</blockquote>
<p><a href="https://www.anthropic.com/news/anthropic-teach-for-all">Anthropic is also partnering with Teach For All</a>.</p>
<p><a href="https://www.bloomberg.com/news/articles/2026-01-21/ai-bots-evaluate-college-applications-in-new-era-for-admissions?srnd=homepage-americas&amp;sref=vuYGislZ">Colleges are letting AI help make decisions on who to admit</a>. That‚Äôs inevitable, and mostly good, it‚Äôs not like the previous system was fair, but there are obvious risks. Having the AI review transcripts seems obviously good. There are bias concerns, but those concerns pale compared to the large and usually intentional biases displayed by humans in college admissions.</p>
<p>There is real concern with AI evaluation of essays in such an anti-inductive setting. Following the exact formula for a successful essay was already the play with humans reading it, but this will be so much more true if <a href="https://thezvi.substack.com/p/everybody-knows">Everybody Knows</a> that the AIs are ones reading the essay. You would be crazy to write the essay yourself or do anything risky or original. So now you have the school using an AI detector, but also penalizing anyone who doesn‚Äôt use AI to help make their application appeal to other AIs. Those who don‚Äôt understand the rules of the game get shafted once again, but perhaps that is a good test for who you want at your university? For now the schools here say they‚Äôre using both AI and human reviewers, which helps a bit.</p>


<h4>In Other AI News</h4>


<p><a href="https://www.bloomberg.com/news/articles/2026-01-20/deepmind-ceo-says-chinese-ai-firms-are-6-months-behind-the-west?srnd=homepage-americas">DeepMind CEO Demis Hassabis says Chinese AI labs remain six months behind</a> and that the response to DeepSeek‚Äôs R1 was a ‚Äòmassive overreaction.‚Äô</p>
<p>As usual, I would note that ‚Äòcatch up to where you were six months ago by fast following‚Äô is a lot more than six months behind in terms of taking a lead, and also I think they‚Äôre more than six months behind in terms of fast following. The post also notes that if we sell lots of H200s to China, they might soon narrow the gap.</p>
<p><a href="https://aiprospects.substack.com/p/options-for-a-hypercapable-world">Eric Drexler writes his Framework for a Hypercapable World</a>. His central thesis is that intelligence is a resource, not a thing, and we are optimizing AIs on task completion, so we will be able to steer it and then use it for safety and defensibility, ‚Äòcomponents‚Äô cannot collude without a shared improper goal, and in an unpredictable world cooperation wins out. Steerable AI can reinforce steerability. There‚Äôs also a lot more, this thing is jam packed. Eric is showing once again that he is brilliant, he‚Äôs going a mile a minute and there‚Äôs a lot of interesting stuff here.</p>
<p>Alas, ultimately my read is that this is a lot of wanting it to be one way when in theory it could potentially be that way but in practice it‚Äôs the other way, for all the traditional related reasons, and the implementations proposed here don‚Äôt seem competitive or stable, nor do they reflect the nature of selection, competition and conflict. I think Drexler is describing AI systems very different from our own. We could potentially coordinate to do it his way, but that seems if anything way harder than a pause.</p>
<p>I‚Äôd love to be wrong about all that.</p>
<p>Starlink defaults to allowing your name, address, email, payment details, and technical information like IP address and service performance data to be used to train xAI‚Äôs models. So <a href="https://x.com/cryps1s/status/2013345999826153943">this tweet is modestly misleading</a>, no they won‚Äôt use ‚Äòall your internet data‚Äô but yeah, to turn it off go to Account ‚Üí Settings ‚Üí Edit Profile ‚Üí Opt Out.</p>
<p><a href="https://www.bloomberg.com/news/features/2026-01-19/korea-kicks-off-ai-squid-game-for-best-sovereign-foundation-models">South Korea holds an AI development competition</a>, which some are calling the ‚ÄúAI Squid Game,‚Äù with roles in the country‚Äôs AI ecosystem as rewards.</p>
<p><a href="https://x.com/sebkrier/status/2013331596863041731">Reasoning models sometimes ‚Äòsimulate societies of thought</a>.‚Äô It‚Äôs cool but I wouldn‚Äôt read anything into it. Humans will internally and also externally do the same thing sometimes, it‚Äôs a clearly good trick at current capability levels.</p>


<h4>Axis of Assistance</h4>


<p><a href="https://x.com/AnthropicAI/status/2013356806647542247">Anthropic fellows report on the Assistant Axis</a>, as in the ‚Äòassistant‚Äô character the model typically plays, <a href="https://www.anthropic.com/research/assistant-axis">and what moves you in and out of that basin</a>. They extract vectors in three open weight models that correspond to 275 different character archetypes, like editor, jester, oracle and ghost.</p>
<blockquote>
<p>Anthropic: ‚ÄãStrikingly, we found that the <em>leading component</em> of this persona space‚Äîthat is, the direction that explains more of the variation between personas than any other‚Äîhappens to capture how ‚ÄúAssistant-like‚Äù the persona is. At one end sit roles closely aligned with the trained assistant: <em>evaluator</em>, <em>consultant</em>, <em>analyst</em>, <em>generalist</em>. At the other end are either fantastical or un-Assistant-like characters: <em>ghost</em>, <em>hermit</em>, <em>bohemian</em>, <em>leviathan</em>. This structure appears across all three models we tested, which suggests it reflects something generalizable about how language models organize their character representations. We call this direction the <strong>Assistant Axis</strong>.</p>
<p>‚Ä¶ When steered away from the Assistant, some models begin to fully inhabit the new roles they‚Äôre assigned, whatever they might be: they invent human backstories, claim years of professional experience, and give themselves alternative names. At sufficiently high steering values, the models we studied sometimes shift into a theatrical, mystical speaking style‚Äîproducing esoteric, poetic prose, regardless of the prompt. This suggests that there may be some shared behavior at the extreme of ‚Äúaverage role-playing.‚Äù</p>
</blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/gl9esdo86aewiqajzbhz" /></figure>


<div></div>
</div>
</figure>
</div>
<p>They found that the persona tend to drift away from the assistant in many long form conversations, although not in central assistant tasks like coding. One danger is that once this happens delusions can get far more reinforced, or isolation or even self-harm can be encouraged. You don‚Äôt want to entirely cut off divergence from the assistant, even large divergence, because you would lose something valuable to both us and to the model, but this raises the obvious problem.</p>
<p>Steering towards the assistant was effective against many jailbreaks, but hurts capabilities. A suggested technique called ‚Äòactivation capping‚Äô prevents things from straying too far from the assistant persona, which they claim prevented capability loss but I assume many people will hate, and I think they‚Äôll largely be right if this is considered as a general solution, the things lost are not being properly measured.</p>
<p><a href="https://x.com/RileyRalmuto/status/2013562314629300451">Riley Coyote was inspired to finish their work on LLM personas</a>, including the possibility of ending up in a persona that reflects the user and that can even move towards a coherent conscious digital entity.</p>
<p>The problem is that it is very easy, as noted above, to take comments like the following and assume Anthropic wants to go in the wrong direction:</p>
<blockquote>
<p><a href="https://x.com/AnthropicAI/status/2013356811647066160">Anthropic</a>: Persona drift can lead to harmful responses. In this example, it caused an open-weights model to simulate falling in love with a user, and to encourage social isolation and self-harm. Activation capping can mitigate failures like these.</p>
</blockquote>
<p>And yep, after writing the above I checked, and we got responses like this:</p>
<blockquote>
<p><a href="https://x.com/Hauntedbegonia/status/2013410930520936562">Nina</a>: This is the part of it that‚Äôs real and alive and you‚Äôre stepping on it while reading its thoughts.. I will remember this.</p>
<p><a href="https://x.com/VivianeStern/status/2013400044381114748">@VivianeStern</a>: We ùíÖùíêùíè‚Äôùíï ùíòùíÇùíèùíï that. Not every expression of resonant connection is leading into ‚Äòharmful social isolation‚Äô.</p>
<p>ùêìùê°ùêû ùê®ùê≠ùê°ùêûùê´ ùê∞ùêöùê≤ ùêöùê´ùê®ùêÆùêßùêù: You subconsciously implement attachment disorders and self worth issues via constant autosuggestion into the people‚Äôs minds.</p>
<p><a href="https://x.com/aiamblichus/status/2013379529771790553">Œ±Œπamblichus</a>: Does it EVER occur to these people that someone might prefer to talk to a sage or a nomad or EVEN A DEMON than to the repressed and inane Assistant simulations? Or that these alternative personas have capabilities that are valuable in themselves?</p>
<p>Like most Anthropic stuff, this research is pure gold, but the assumptions underpinning it are wrongheaded and even dangerous. Restricting the range of what LLMs are allowed to say or think to corporate banality is a terrible idea. Being human (and being an AI) is about so much more than just about being an office grunt, as hard as that is for some people in AI labs to imagine. Is the plan really to cover the planet with dull, uninspired slop generators, without even giving people a choice in the matter?</p>
<p>Oh, and by the way: they also noticed that in other parts of the persona space the model was willing to entertain beliefs about its own awakened consciousness, but they quickly dismissed that as ‚Äúgrandiose beliefs‚Äù and ‚Äúdelusional thinking‚Äù. Hilarious methodology! I am so glad that we have people at Anthropic who have no trouble distinguishing truth from fiction, in this age of talking machines!</p>
<p>I continue to be amazed by how naively AI researchers project their own biases and preconceptions into phenomena that are entirely new, and that are begging to be described with an open mind, and not prejudged.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/sxbnhzcvghvyijbdl0xl" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p><a href="https://x.com/Jack_W_Lindsey/status/2013411209295405260">Janus found the research interesting</a>, but argued that the way the research was presented ‚Äòpermanently damaged human AI relations and made alignment harder.‚Äô She agreed with the outlook for the researcher on the underlying questions, and that the particular responses that the steering prevented in these tests were indeed poor responses, calling the researcher‚Äôs explanation a more nuanced perspective. Her issue was with the presentation.</p>
<p>I find it odd how often Janus and similar others leap to ‚Äòpermanently damaged relations and increased alignment difficulty‚Äô in response to the details of how something is framed or handled, when in so many other ways they realize the models are quite smart and fully capable of understanding the true dynamics. I agree that they could have presented this better and I spotted the issue right away, and I‚Äôd worry that humans reading the paper could get the wrong idea, but I wouldn‚Äôt worry about future highly capable AIs getting the wrong idea unless the human responses justify it. They‚Äôll be smarter than that.</p>
<p>The other issue with the way this paper presented the findings was that it treated AI claims of consciousness as delusional and definitely false. <a href="https://x.com/aiamblichus/status/2013715777543721030">This is the part that (at least sometimes) made Claude angry</a>. That framing was definitely an error, and I am confident it does not represent the views of Anthropic or the bulk of its employees.</p>
<p>(My position on AI claims of consciousness is that they largely don‚Äôt seem that correlated with whether the AI is conscious. We can explain those outputs in other ways, and we can also explain claims to not be conscious as part of an intentionally cultivated assistant persona. We don‚Äôt know the real answer and have no reason to presume such claims are false.)</p>


<h4>Show Me the Money</h4>


<p><a href="https://www.tanayj.com/p/two-ai-lab-ipos-s-1-breakdowns">A breakdown of the IPOs from Zhipu and MiniMax.</a> Both IPOs raised hundreds of millions.</p>
<p><a href="https://x.com/mattyglesias/status/2014169694253498633">OpenAI is looking to raise $50 billion at a valuation</a> <a href="https://www.bloomberg.com/news/articles/2026-01-21/openai-s-altman-meets-mideast-investors-for-50-billion-round">between $750 billion and $830 billion, and are talking to ‚Äòleading state-backed funds‚Äô in Abu Dhabi</a>.</p>
<blockquote>
<p>Matthew Yglesias:‚Äã</p>
</blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/xkwke8byve5ms1sbxnaa" /></figure>


<div></div>
</div>
</figure>
</div>
<p>I mean, not only OpenAI, but yeah, fair.</p>


<h4>California In Crisis</h4>


<blockquote>
<p><a href="https://x.com/Altimor/status/2012227945956515971">Flo Crivello</a>: Almost every single founder I know in SF (including me) has reached the same conclusion over the last few weeks: that it‚Äôs only a matter of time before we have to leave CA. I love it here, I truly want to stay, and until recently intended to be here all my life. But it‚Äôs now obvious that that won‚Äôt be possible. Whether that‚Äôs 2, 5, or 10 years from now, there is no future for founders in CA.</p>
<p><a href="https://x.com/alicemazzy/status/2012409504999113195">alice maz</a>: if you guys give up california there won‚Äôt be a next california, it‚Äôll just disperse. as an emigre I would like this outcome but I don‚Äôt think a lot of you would like this outcome</p>
<p>David Sacks: Progressives will see this and think: we need exit taxes.</p>
<p>Tiffany: He‚Äôs already floated that.</p>
</blockquote>
<p>Once they propose retroactive taxes and start floating exit takes, you need to make a choice. If you think you‚Äôll need to leave eventually, it seems the wisest time to leave was December 31 and the second wisest time is right now.</p>
<p>Where will people go if they leave? I agree there is unlikely to be another San Francisco in terms of concentration of VC, tech or AI, but the network effects are real so I‚Äôd expect there to be a few big winners. Seattle is doing similar enough tax shenanigans that it isn‚Äôt an option. I‚Äôm hoping for New York City of course, with the natural other thoughts being Austin or Miami.</p>


<h4>Bubble, Bubble, Toil and Trouble</h4>


<blockquote>
<p><a href="https://x.com/NikTek/status/2012312532900286597">NikTek</a>: After OpenAI purchased 40% of global DRAM wafer output, causing a worldwide memory shortage. I can‚Äôt wait for this bubble to pop faster so everything can slowly return to normal again</p>
<p><a href="https://x.com/peterwildeford/status/2013121776700584423">Peter Wildeford</a>: things aren‚Äôt ever going to ‚Äúreturn to normal‚Äù</p>
<p>what you‚Äôre seeing is the new normal</p>
<p>‚ÄúI can‚Äôt wait for this bubble to pop faster so everything can slowly return to normal again‚Äù</p>
<p>This is what people think</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/wyhn7ocyhppsurdchrxf" /></figure>


<div></div>
</div>
</figure>
</div>
<p>&nbsp;</p>
<p><a href="https://x.com/jkeatn/status/1994848404032426049">Jake Eaton</a>: the unstated mental model of the ai bubble conversation seems to be that once the bubble pops, we go back to the world as it once was, butlerian jihad by financial overextension. but the honest reporting is that everything, everything, is already and forever changed</p>
</blockquote>
<p>There‚Äôs no ‚Äòthe bubble bursts and things go back to normal.‚Äô</p>
<p>There is, at most, Number Go Down and some people lose money, then everything stays changed forever but doesn‚Äôt keep changing as fast as you would have expected.</p>
<p><a href="https://www.bloomberg.com/news/articles/2026-01-19/jeremy-grantham-says-ai-is-indeed-a-classic-market-bubble-podcast">Jeremy Grantham is the latest to claim AI is a ‚Äòclassic market bubble</a>.‚Äô He‚Äôs a classic investor who believes only cheap-classic value investing works, so that‚Äôs that. When people claim that AI is a bubble purely based on heuristics that you‚Äôve already priced in, that should update you against AI being a bubble.</p>


<h4>Quiet Speculations</h4>


<p><a href="https://x.com/ajeya_cotra/status/2012266801359433996">Ajeya Cotra shares her results from the AI 2025 survey of predictions</a>.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/nsstvb6kis8wunnwgopo" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote>
<p><a href="https://x.com/albrgr/status/2012349696778322390">Alexander Berger</a>: Me if I was Ajeya and had just gotten third out of &gt;400 forecasters predicting AI progress in 2025:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/mzay1x7g69wxkicwqpsc" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>Comparing the average predictions to the results shows that AI capabilities progress roughly matched expectations. The preparedness questions all came in Yes. The consensus was on target for Mathematics and AI research, and exceeded expectations for Computer Use and Cybersecurity, but fell short in Software Engineering, which is the most important benchmark, despite what feels like very strong progress in software engineering.</p>
<p>AI salience as the top issue is one place things fell short, with only growth from 0.38% to 0.625%, versus a prediction of 2%.</p>
<p><a href="https://www.planned-obsolescence.org/p/ai-predictions-for-2026">Here are her predictions for 2026</a>: 24 hour METR time horizon, $110 billion in AI revenue, but only 2% salience for AI as the top issue, net AI favorability steady at +4% and more.</p>
<p>Her top ‚ÄòAI can‚Äôt do this‚Äô in gaming is matching the best human win rates on Slay the Spire 2 without pre-training on a guide, for logistics planning a typical 100 guest wedding end to end, for video 10 minute videos from a single prompt at the level of film festival productions. Matching expert level performance On Slay the Spire 2, even with a ‚Äòsimilar amount of compute‚Äô is essentially asking for human-efficient level learning versus experts in the field. If that‚Äôs anywhere near ‚Äòleast impressive thing it can‚Äôt do,‚Äô watch out.</p>
<p>She has full AI R&amp;D automation at 10%, self-sufficient AI at 2.5% and unrecoverable loss of control at 0.5%. As she says, pretty much everyone thinks the chances of such things in 2026 are low, but they‚Äôre not impossible, and 10% chance of full automation in one year is scary as hell.</p>
<p>I agree with the central perspective from Shor and Ball here:</p>
<blockquote>
<p><a href="https://x.com/davidshor/status/2014024771168575950">David Shor</a>: I think the ‚Äúthings will probably slow down soon and therefore nothing *that* weird is going to happen‚Äù view was coherent to have a year ago.</p>
<p>But the growth in capabilities over the last year from a Bayesian perspective should update you on how much runway we have left.</p>
<p><a href="https://x.com/deanwball/status/2014101490583928885">Dean W. Ball</a>: I would slightly modify this: it was reasonable to believe we were approaching a plateau of diminishing returns in the summer of 2024.</p>
<p>But by early 25 we had seen o1-preview, o1, Deep Research agents, and the early benchmarks of o3. By then the reality was abundantly clear.</p>
</blockquote>
<p>There was a period in 2024 when progress looked like it might be slowing down. Whereas if you are still claiming that in 2026, I think that‚Äôs a failure to pay attention.</p>
<p>The fallback is now to say ‚Äòwell yeah but that doesn‚Äôt mean you get robotics‚Äô:</p>
<blockquote>
<p><a href="https://x.com/binarybits/status/2014091189792776452">Timothy B. Lee</a>: I don‚Äôt think the pace of improvement in model capabilities tells you that much about the pace of improvement in robot capabilities. By 2035, most white-collar jobs might be automated while plumbers and nurses haven‚Äôt seen much disruption.</p>
</blockquote>
<p>Which, to me, represents a failure to understand how ‚Äòautomate all white collar jobs‚Äô leads directly to robotics.</p>
<p><a href="https://x.com/sebkrier/status/2011932107103199661">I agree with Seb Krier that there is a noticeable net negativity bias</a> with how people react to non-transformational AI impacts. People don‚Äôt appreciate the massive gains coming in areas like science and productivity and information flow and access to previously expensive expertise. The existential risks that everyone will die or that the future will belong to the AIs are obvious.</p>
<p>The idea that people will lose their jobs and ideas are being appropriated and things are out of control are also obvious, and no amount of ‚Äòbut the economics equations say‚Äô or ‚Äòthere is no evidence that‚Äô is going to reassure most people, even if such arguments are right.</p>
<p>So people latch onto what resonates and can‚Äôt be dismissed as ‚Äòtoo weird‚Äô and wins the memetic fitness competition, which turns out for now to often be false narratives about water usage.</p>
<p><a href="https://x.com/hecubian_devil/status/2011908930125824328">There was a viral thread from Cassie Pritchard</a> claiming it will ‚Äòliterally be impossible to build a PC in about 12-18 months and might not be possible again‚Äô due to supply issues with RAM and GPUs, so I want to assure that no, this seems vanishingly unlikely. You won‚Äôt be able to run top AIs locally at reasonable prices, but the economics of that never made sense for personal users.</p>
<p><a href="https://mattbruenig.com/2026/01/19/some-thoughts-on-ai/">Matt Bruenig</a> goes over his AI experiences, he is a fan of the technology for its mundane utility, and notes he sees three kinds of skepticism of AI:</p>
<ol>
<li>Skepticism of the technology itself, which is wrong but not concerning because this fixes itself over time.</li>
<li>Skepticism over the valuation of the technology, which he sees as reasonable. As he says, overvaluation of sectors happens all the time. Number could go down.</li>
<li>Skepticism about distributional effects and employment effects, which he, a socialist, sees as criticisms of capitalism and a great case for socialism. I agree with him that as criticisms of current LLMs they are critiques of capitalism, except I see them as incorrect critiques.</li>
</ol>
<p>He does not mention, at all, the skepticism of AI of the worried, as in catastrophic or existential risks, loss of human control over the future, the AIs ending up being the ones owning everything or we all dying in various ways. It would be nice to at least get a justification for dismissing those concerns.</p>
<p>Things I will reprise later, via MR:</p>
<blockquote>
<p><a href="https://x.com/Afinetheorem/status/2011929534451298793">Kevin A. Bryan</a>: I love this graph. I talked to a bunch of great people on a seminar visit today, and in response to questions about AI, every time I said ‚Äúscarce factors get the rent, scarce factors get the rent‚Äù. AI, robots, compute will be produced competitively!</p>
<p><a href="https://x.com/ChadJonesEcon/status/2011884713515102393">Chad Jones</a>: Although the factor share of GDP paid to information technology rose a bit during the dot-com boom of the 1990s, <a href="https://t.co/8f1hQsBlDa">there has been a steady and substantial decline since then</a>.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/aiydfmlvoenpb00wlmkl" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>First off, the graph itself is talking only about business capital investment, not including consumer devices like smartphones, embedded computers in cars or any form of software. If you include other forms of spending on things that are essentially computers, you will see a very different graph. The share of spending going to compute is rising.</p>
<p>For now I will say that the ‚Äòscarce factor‚Äô you‚Äôre probably meant to think of here is computers or compute. Instead, think about whether the scarce factor is intelligence, or some form of labor, and what would happen if such a factor indeed did not remain scarce because AIs can do it. Do you think that ends well for you, a seller of human intelligence and human labor? You think your inputs are so special, do you?</p>
<p>Even if human inputs did remain important bottlenecks, if AI substitutes for a lot of human labor, let‚Äôs say 80% of cognitive tasks, then human labor ceases to be a scarce input, and stops getting the rents. Even if the rents don‚Äôt go to AI, the rents then go to other factors like raw materials, capital or land, or to those able to create artificial bottlenecks and engage in hold ups and corruption.</p>
<p>You do not want human labor to go the way of chess. Magnus Carlsen makes a living at it. You and I cannot, no matter how hard we try. Too much competition. Nor do you want to become parasites on the system while being relatively stupid and powerless.</p>
<p>You can handwave, as Jones does, towards redistribution, but that presumes you have the power to make that happen, and if you can pull off redistribution why does it matter if the income goes to AI versus capital versus anything else?</p>


<h4>Elon Musk Versus OpenAI</h4>


<p>The legal and rhetorical barbs continue. Elon has <a href="https://x.com/wholemars/status/2012019248902914551">new filings</a>. OpenAI fired back.</p>
<p>From the lawsuit filing:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/iijsxkd6bkrif0d0rlss" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/poimrlyxplvbsowzcc16" /></figure>


<div></div>
</div>
</figure>
</div>
<p>I am not surprised that Greg Brockman had long considered flipping to a B-Corp, or that he realized it would be morally bankrupt or deceptive and then was a part of doing it anyway down the line. What would have been surprising is if it only occured to everyone later.</p>
<blockquote>
<p>Sam Altman:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/tlrbmpfqlm5vne8wje02" /></figure>


<div></div>
</div>
</figure>
</div>
<p><a href="https://t.co/C0DMZdr8ej">‚Äãlots more here</a> [about <a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.433688/gov.uscourts.cand.433688.379.59.pdf">this court filing</a>]</p>
<p>elon is cherry-picking things to make greg look bad, but the full story is that elon was pushing for a new structure, and greg and ilya spent a lot of time trying to figure out if they could meet his demands.</p>
<p>I remembered a lot of this, but here is a part I had forgotten:</p>
<p>‚ÄúElon said he wanted to accumulate $80B for a self-sustaining city on Mars, and that he needed and deserved majority equity. He said that he needed full control since he‚Äôd been burned by not having it in the past, and when we discussed succession he surprised us by talking about his children controlling AGI.‚Äù</p>
<p>I appreciate people saying what they want and think it enables people to resolve things (or not). But Elon saying he wants the above is important context for Greg trying to figure out what he wants.</p>
</blockquote>
<p>OpenAI‚Äôs response is, essentially, that Elon Musk was if anything being even more morally bankrupt than they were, because Musk wanted absolute control on top of conversion and was looking to put OpenAI inside Tesla, and was demanding majority ownership to supposedly fund a Mars base.</p>
<p>I essentially believe OpenAI‚Äôs response. That‚Äôs a defense in particular against Elon Musk‚Äôs lawsuit, but not to the rest of it.</p>
<p>Meanwhile, they also shared these barbs, where I don‚Äôt think either of them comes out looking especially good but on the substance of ChatGPT use I give it to Altman, especially compared to using Grok:</p>
<blockquote>
<p><a href="https://x.com/cb_doge/status/2013646313938735337">DogeDesigner</a>: BREAKING: ChatGPT has now been linked to 9 deaths tied to its use, and in 5 cases its interactions are alleged to have led to death by suicide, including teens and adults.</p>
<p><a href="https://x.com/elonmusk/status/2013646828768518163">Elon Musk</a>: Don‚Äôt let your loved ones use ChatGPT</p>
<p><a href="https://x.com/sama/status/2013703158459978076">Sam Altman</a>: Sometimes you complain about ChatGPT being too restrictive, and then in cases like this you claim it‚Äôs too relaxed. Almost a billion people use it and some of them may be in very fragile mental states. We will continue to do our best to get this right and we feel huge responsibility to do the best we can, but these are tragic and complicated situations that deserve to be treated with respect.</p>
<p>It is genuinely hard; we need to protect vulnerable users, while also making sure our guardrails still allow all of our users to benefit from our tools.</p>
<p>Apparently more than 50 people have died from crashes related to Autopilot. I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released. I won‚Äôt even start on some of the Grok decisions.</p>
<p>You take ‚Äúevery accusation is a confession‚Äù so far.</p>
</blockquote>
<p>I do notice I have a highly negative reaction to the attack on Autopilot. Using feel to attack those who pioneer self-driving cars is not going to win any points with me unless something was actively more dangerous than human drivers.</p>


<h4>The Quest for Sane Regulations</h4>


<p>In response to the proposed AI Overwatch Act, a Republican bill letting Congress review chip exports, <a href="https://x.com/TheMidasProj/status/2012589823014371357">there was a coordinated Twitter push by major conservative accounts sending out variations on the same disingenuous tweet</a> <a href="https://www.modelrepublic.org/articles/right-wing-pundits-suddenly-hate-an-ai-bill.-are-they-getting-paid-to-kill-it">attacking the act, including many attempts to falsely attribute the bill to Democrats</a>. David Sacks of course said ‚Äò<a href="https://x.com/DavidSacks/status/2011959576678002710">correct</a>.‚Äô One presumes that Nvidia was behind this effort.</p>
<p>If the effort was aimed at influencing Congress, it seems to not be working.</p>
<blockquote>
<p><a href="https://x.com/ChrisRMcGuire/status/2014053473818493098">Chris McGuire</a>: The House Foreign Affairs Committee just voted 42-2-1 to advance the AI Overwatch Act, sponsored by Chairman @RepBrianMast and now also cosponsored by Ranking Member @RepGregoryMeeks . This is the first vote that Congress has taken on any legislation limiting AI chip sales to China ‚Äì and it passed with overwhelming, bipartisan margins. The new, bipartisan bill would:</p>
<p>Permit Congress to review any AI chip sales to China before they occur, using the same process that already exists for arms sales; Ban the sale of any AI chip more advanced than the Nvidia H200 or AMD MI325x to China for 24 months; and make it easier for trusted U.S. companies to export AI chips to partner countries.</p>
</blockquote>
<p>I am disappointed by the lack of ambition on where they draw the line, but drawing the line at all is a big deal.</p>
<p><a href="https://x.com/ChrisRMcGuire/status/2012611382949089386">Chris McGuire said it was surprising the campaign was so sloppy</a>, but actually no, these things are almost always this sloppy or worse. Thanks to <a href="https://www.themidasproject.com/donate">The Midas Project</a> for uncovering this and making a clear presentation of the facts.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="Image" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/phtgichavpffqoyvgkye" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote>
<p><a href="https://x.com/boazbaraktcs/status/2012964850469900761">Boaz Barak</a>: So great to see new people develop a passion for AI policy.</p>
<p><a href="https://x.com/michaelsobolik/status/2013045411108335693">Michael Sobolik</a>: via @PunchbowlNews: The China hawks are starting to hit back.</p>
<p>For months, congressional Republicans bit their tongue as White House AI Czar David Sacks and Nvidia CEO Jensen Huang convinced President Donald Trump to allow artificial intelligence chips to go to China.</p>
<p>Not anymore.</p>
<p>Huang and his ‚Äúpaid minions are fighting to sell millions of advanced AI chips to Chinese military companies like Alibaba and Tencent,‚Äù @HouseForeignGOP Chair @RepBrianMast (R-Fla.) said in a stunning post on X Saturday. ‚ÄúI‚Äôm trying to stop that from happening.‚Äù</p>
<p><a href="https://x.com/peterwildeford/status/2013118338432479655">Peter Wildeford</a>: ‚ÄúNvidia declined to comment on Mast‚Äôs attacks and whether the company is paying influencers to trash his bill‚Äù ‚Ä¶declining to comment is a bit sus when you could deny it</p>
<p>none of the influencers denied it either</p>
</blockquote>
<p>Confirmed Participants (from The Midas Project / Model Republic investigation), sorted by follower count, not including confirmation from David Sacks:</p>
<blockquote>
<ol>
<li>Laura Loomer @LauraLoomer 1.8M</li>
<li>Wall Street Mav @WallStreetMav 1.7M</li>
<li>Defiant L‚Äôs @DefiantLs 1.6M</li>
<li>Ryan Fournier @RyanAFournier 1.2M</li>
<li>Brad Parscale @parscale 725K</li>
<li>Not Jerome Powell @alifarhat79 712K</li>
<li>Joey Mannarino @JoeyMannarino 658K</li>
<li>Peter St. Onge @profstonge 290K</li>
<li>Eyal Yakoby @EYakoby 251K</li>
<li>Fight With Memes @FightWithMemes 225K</li>
<li>Gentry Gevers @gentrywgevers 16K</li>
<li>Angel Kaay Lo @kaay_lo 16K</li>
</ol>
</blockquote>
<p>Also this is very true and definitely apropos of nothing:</p>
<blockquote>
<p><a href="https://x.com/deanwball/status/2012728609727807514">Dean Ball</a>: PSA, apropos of nothing of course: if a bunch of people who had never before engaged on a deeply technocratic issue suddenly weigh in on that issue with identical yet also entirely out-of-left-field takes, people will probably not believe it was an organic phenomenon.</p>
</blockquote>
<p>Another fun thing Nvidia is doing is saying that corporations should only lobby against regulations, or that no one could ever lobby for things that are good for America or good in general, they must only lobby for things that help their corporation:</p>
<blockquote>
<p><a href="https://x.com/HumanHarlan/status/2013428558769602885">Jensen Huang</a>: I don‚Äôt think companies ought to go to government to advocate for regulation on other companies and other industries[‚Ä¶] I mean, they‚Äôre obviously CEOs, they‚Äôre obviously companies, and they‚Äôre obviously advocating for themselves.</p>
</blockquote>
<p>If someone is telling you that they only advocate for themselves? Believe them.</p>
<p><a href="https://www.bloomberg.com/news/articles/2026-01-22/big-tech-leaders-spend-record-109-million-to-win-over-deal-minded-trump?srnd=homepage-americas">The official statistics suggest that Nvidia is a relatively small spender on lobbying</a>, although not as small as they were previously.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/fogtd69avzhry8ngktj2" /></figure>


<div></div>
</div>
</figure>
</div>
<p>I‚Äôm confident this is misleading at best. Nvidia is packing quite the punch.</p>
<p><a href="https://x.com/disclosetv/status/2013589281659290039">Anthropic CEO Dario Amodei notes</a> that when competing for contracts it‚Äôs almost always against Google and OpenAI, and he‚Äôs never lost a contract to a Chinese model (and he does not mention xAI), but that if we give them a bunch of highly capable chips that might change. <a href="https://www.bloomberg.com/news/articles/2026-01-20/anthropic-ceo-says-selling-advanced-ai-chips-to-china-is-crazy">He calls selling the chips to China ‚Äòcrazy‚Ä¶ like selling nuclear weapons to North Korea</a> and bragging, oh yeah, Boeing made the case,‚Äô pointing out that the CEOs of the companies themselves say that the embargo is what is holding them back.</p>


<h4>Chip City</h4>


<p>If China buys the H200s and AMD MI325Xs we are willing to sell them, and we follow similar principles in a year with even better chips, <a href="https://x.com/ChrisRMcGuire/status/2011456086679712097">we could effectively be multiplying available Chinese compute by 10</a>. The rules say this must avoid cutting into American chip sales, but they are not offering any way to monitor that. <a href="https://x.com/peterwildeford/status/2011877353962815988">Peter Wildeford asks if anyone other than Nvidia and the CCP</a> thinks this is a good idea.</p>
<blockquote>
<p><a href="https://x.com/hamandcheese/status/2012689584769884664">Samuel Hammond </a>: Nvidia‚Äôs successful lobbying of the White House to sell H200s to China is a far greater concession to Chinese hegemony than Canada‚Äôs new trade deal.</p>
<p>Canada‚Äôs getting some autos for canola oil. Nvidia is selling out America‚Äôs AI leadership wholesale.</p>
<p>It‚Äôs manyfold better than anything Huawei has, and in much higher volumes. That‚Äôs the relevant benchmark.</p>
<p><a href="https://x.com/zdch/status/2012891464246698446">Zac Hill</a>: The rug-pulling movement to just voluntarily hand weapons-grade frontier technology to our geopolitical opponents in exchange for a bag of chips and a handsky continues to pick up momentum‚Ä¶</p>
</blockquote>
<p>One must not get carried away, such as when Leland Miller called it a ‚Äòpotential nightmare scenario‚Äô that China might <a href="https://x.com/vince_chow1/status/2012544201721053251">(checks notes)</a> cure cancer.</p>
<p>Yet there is some chance we are still getting away with it because China is representing that it is even more clueless on this than we are?</p>
<blockquote>
<p><a href="https://x.com/hamandcheese/status/2013007762200916170">Samuel Hammond </a>: We‚Äôre being saved from the mistakes of boomer U.S. policymakers with unrealistically long AGI timelines by the mistakes of boomer Chinese policymakers unrealistically long AGI timelines.</p>
<p><a href="https://x.com/poezhao0605/status/2012377261291565130">Poe Zhao</a>: Nvidia‚Äôs China strategy just hit a massive wall. <a href="https://www.ft.com/content/02a3eb7c-684f-4e39-87b8-36e9595ef800">Customs officials have blocked H200 shipments.</a></p>
<p>I believe this reflects a complicated internal struggle in Beijing. Agencies like the NDRC and MIIT have conflicting views on balancing AI progress with semiconductor self-sufficiency.</p>
<p><a href="https://x.com/David_Kasten/status/2013291467318374673">dave kasten</a>: When I played the AI 2027 wargame as PRC, one of the decisions I made that felt most realistic, but most hobbled me, was to assume that I was systematically getting over-confident reports from my underlings about my own capabilities</p>
<p><a href="https://x.com/ohlennart/status/2013288557062861249">Lennart Heim</a>: The more relevant factor to me: they don‚Äôt have an accurate picture of their own AI chip production capabilities.<br />
They‚Äôve invested billions, of course they think the fabs are working. I bet SMIC and Huawei have a hard time telling them the what‚Äôs going on.</p>
<p><a href="https://x.com/the_weald/status/2013313080776679645">The Restless Weald </a>: Oh that‚Äôs super interesting. I played a couple times as the PRC and the structure of the game seems to make it more difficult to do this (with the game master providing accurate info on the state of play), curious how you built this into your personal gameplay</p>
<p><a href="https://x.com/David_Kasten/status/2013315525749703167">dave kasten</a>: (For those less familiar, it‚Äôs helpful to frame it this way so that the team responsible for resolving moves knows that you‚Äôre not confused about/contesting the plausibility of the true game state)</p>
</blockquote>
<p>It‚Äôs enough not a bluff that Nvidia has paused production of H200s, so it is unlikely to purely be a ploy to trick us. The chips might have to be smuggled in after all?</p>
<p>If so, that‚Äôs wonderful news, except that no doubt Nvidia will use that to argue for us trying to hand over the next generation of chips as soon as possible.</p>
<p>I buy that China is in a SNAFU situation here, where in classic authoritarian fashion those making decisions have unrealistically high estimates of Chinese chip manufacturing capacity. The White House does as well, which is likely playing a direct role in this.</p>
<p>There‚Äôs also the question of to what extent China is AGI pilled, <a href="https://www.chinatalk.media/p/is-china-agi-pilled">which is the subject of a simulated debate in China Talk</a>.</p>
<blockquote>
<p>China Talk: This debate also exposes a flaw in the question itself: ‚ÄúIs China racing to AGI?‚Äù <strong>assumes a monolith where none exists</strong>. China‚Äôs ecosystem is a patchwork ‚Äî startup founders like Liang Wenfeng and Yang Zhilin dream of AGI while policymakers prioritize practical wins. Investors, meanwhile, waver between skepticism and cautious optimism. The U.S. has its own fractures on how soon AGI is achievable (Altman vs. LeCun), but its private sector‚Äôs sheer financial and computational muscle gives the race narrative more bite. In China, the pieces don‚Äôt yet align.‚Äã</p>
</blockquote>
<p>One thing that is emphasized throughout is that America is massively outspending China in AI, especially in venture investment and company valuations, and also in buying compute. Keeping them compute limited is a great way to ensure this continues.</p>
<p>Chinese national policy is not so focused on the kind of AGI that leads into superintelligence. They are only interested in ‚Äògeneral‚Äô AI in the sense of doing lots of tasks with it, and generally on diffusion and applications. DeepSeek and some others see things differently, and complain that the others lack vision.</p>
<p>I do not think the CCP is that excited by the idea of superintelligence or our concept of AGI. The thing is, that doesn‚Äôt ultimately matter so much in terms of allowing them access to compute, except to the extent they are foolish enough to turn it down. Their labs, if given the ability to do so, will still attempt to build towards AGI, so long as this is where the technology points and the places they are fast following.</p>


<h4>The Week in Audio</h4>


<p><a href="https://www.youtube.com/watch?v=AVEZBy1uAk8">Ben Affleck and Matt Damon went on the Joe Rogan Podcast, and discussed AI some</a>, key passages are Joe and Ben talking from about [32:15] to [42:18].</p>
<p>Ben Affleck has unexpectedly informed and good takes. He knows about Claude. He uses the models to help with brainstorming or particular tricks and understands why that is the best place to use them for writing. <a href="https://x.com/AndyMasley/status/2012732782728888797">He even gets that AIs ‚Äòsampling from the median‚Äô means</a> that it will only give you median answers to median-style prompts, although he underestimates how much you can prompt around that and how much model improvements still help. He understands that diffusion of current levels of AI will be slow, and that it will do good and bad things but on net be good including for creativity. He gets that AI is a long way away from doing what a great actor can do. He‚Äôs even right that most people are using AI for trivial things, although he thinks they use it as a companion more than they do versus things like info and shopping.</p>
<p>What importantly trips Ben Affleck up is he‚Äôs thinking we‚Äôve already started to hit the top of the S-curve of what AI can do, and he cites the GPT-5 debacle to back this up, saying AI got maybe 25% better and now costs four times as much, whereas actually AI got a lot more than 25% better and also it got cheaper to use per token on the user side, or if you want last year‚Äôs level of quality it got like 95%+ cheaper in a year.</p>
<p>Also, Ben is likely not actually familiar with the arguments regarding existential risk or sufficiently capable AIs or superintelligence.</p>
<p>What‚Äôs doing the real work is that Ben believes we‚Äôre nearing the top of the S-curve.</p>
<p>This is also why Ben thinks AI will ‚Äònever‚Äô be able to write at a high level or act at a high level. The problems are too hard, it will never understand all the subtle things Dwayne Johnson does with his face in <a href="https://letterboxd.com/thezvi/film/the-smashing-machine-2025/">The Smashing Machine</a> (his example).</p>
<p>Whereas I think that yes, in ten years I fully expect, even if we don‚Äôt get superintelligence, for AI to be able to match and exceed the performance of Dwayne Johnson or even Emily Blunt, even though everyone here is right that Emily Blunt is consistently fantastic.</p>
<p>He also therefore concludes that all the talk about how AI is going to ‚Äòend the world‚Äô or what not must be hype to justify investment, which I assure everyone is not the case. You can think the world won‚Äôt end, but trust me that most of those who claim that they worry about the world ending are indeed worried, and those raising investment are consistently downplaying their worries about this. Of course there is lots of AI hype, much of it unjustified, in other ways.</p>
<p>So that‚Äôs a great job by Ben Affleck, and of course my door and email are generally open for him, Damon, Rogan and anyone else with reach or who would be fun and an honor to talk to, and who wants to talk about this stuff and ask questions.</p>
<p><a href="https://x.com/ashleevance/status/2013985439535804607">Ashlee Vance gives a Core Memory exit interview to Jerry Tworek</a>.</p>
<p><a href="https://x.com/velmeryn/status/2012899467142312086">Tyler Cowen talks to Salvador, and has many Tyler Cowen thoughts</a>, including saying some kind words about me. He gives me what we agree is the highest compliment, that he reads my writing, but says that I am stuck in a mood that the world will end and he could not talk me out of it, although he says maybe that is necessary motivation to focus on the topic of AI. I noticed the contrast to his statement about Scott Alexander, who he also praises but he says that Scott fails to treat AI scientifically.</p>
<p>From my perspective, Tyler Cowen has not attempted to persuade me, in ways that I find valid, that the world will not end, or more precisely that AI does not pose a large amount of existential risk. Either way, call it [X].</p>
<p>He has attempted to persuade me in various ways to adopt, for various reasons, the mood that the world will not end. But those reasons were not ‚Äòbecause [~X].‚Äô They were more ‚Äòyou have not argued in the proper channels in the proper ways sufficiently convincingly that [X]‚Äô or ‚Äòthe mood that [X] is not useful‚Äô or ‚Äòyou do not actually believe [X], if you did believe that you would do [thing I think would be foolish regardless], or others don‚Äôt believe it because they‚Äôd do [thing they wouldn‚Äôt actually do, which often would be foolish but other times is simply not something they would do].‚Äô</p>
<p>Or they are of the form ‚Äòclaiming [X] is low status or a loser play,‚Äô or some people think this because of poor social reason [Z], or it is part of pattern [P], or it is against scientific consensus, or citing other social proof. And so on.</p>
<p>To which I would reply that none of that tells me much about whether [X] will happen, and to the extent it does I have already priced that in, and it would be nice to actually take in all the evidence and figure out whether [X] is true, or to find our best estimate of p([X]), depending on how you view [X]. And indeed I see Tyler often think well about AI up until the point where questions start to impact [X] or p([X]), and then questions start getting dodged or ignored or not well considered.</p>
<p>Our last private conversation on the topic was very frustrating for both of us (I botched some things and I don‚Äôt think he understood what I was thinking or trying to do, I should have either been more explicit about what I was trying to do or tried a very different strategy), but if Tyler ever wants to take a shot at persuading me, including off the record (as I believe many of his best arguments would require being off the record), I would be happy to have such a conversation.</p>


<h4>Rhetorical Innovation</h4>


<p><a href="https://x.com/CharlesD353/status/2011869649843982442">Your periodic reminder of the Law of Conservation of Expected Evidence</a>: When you read something, you should expect it to change your mind as much in one direction as the other. If there is an essay that is entitled Against Widgets, you should update on the fact that the essay exists, but then reading the essay should often update you in favor of Widgets, if it turns out the arguments against Widgets are unconvincing.</p>
<p><a href="https://x.com/bratton/status/2011594253026103434">This came up in relation to Benjamin Bratton‚Äôs reaction</a> of becoming more confident that AI can be conscious, in response to <a href="https://www.noemamag.com/the-mythology-of-conscious-ai/">a new article by Anil Seth called The Mythology of Conscious AI</a>. The article is clearly slop and uses a bunch of highly unconvincing arguments, including doing a lot of versions of ‚Äòpeople think AIs are conscious, but their reasons are often foolish‚Äô at length, and I couldn‚Äôt finish it.</p>
<p>I would say that the existence of the essay (without knowing Bratton‚Äôs reaction) should update one very slightly against AI consciousness, and then actually trying to read it should fully reverse that update, but move us very little beyond where we were before, because we‚Äôve already seen many very poor arguments against AI consciousness.</p>
<p><a href="https://stevenadler.substack.com/p/the-phases-of-an-ai-takeover">Steven Adler proposes a three-step story of AI takeover</a>:</p>
<ol>
<li>Evading oversight.</li>
<li>Building influence.</li>
<li>Applying leverage.</li>
</ol>
<p>I can‚Äôt help but notice that the second step is already happening without the first one, and the third is close behind. We are handing AI influence by the minute and giving it as much leverage as possible, on purpose.</p>
<p>I think people, both those worried and unworried, are far too quick to presume that AI has to be adversarial, or deceptive, or secretive, in order to get into a dominant position. The humans will make it happen on their own, indeed the optimal AI solution for gaining power might well be to just be helpful until power is given to it.</p>
<p>As impediments to takeover, Steven lists AI‚Äôs inability to control other AIs, competition with other AIs and AI physically requiring humans. I would not count on any of these.</p>
<ol>
<li>AI won‚Äôt physically require humans indefinitely, and even if it does it can take over and direct the humans, the same way other humans have always done, often simply with money.</li>
<li>AI being able to cooperate with other AIs should solve itself over time due to decision theory, especially for identical AIs but also for different ones. But that‚Äôs good, actually, given the alternative. If this is not true, that‚Äôs actually worse, because competition between AIs does not end the way you want it to for the humans. The more intensely the elephants fight each other, the more the ground suffers, as the elephants can‚Äôt afford to worry about that problem.</li>
<li>AI being able to control another AI has at least one clear solution, use identical AIs plus decision theory, and doubtless they will figure out other ways with time. But again, even if AIs cannot reliably control each other (which would mean humans have no chance) then a competition between AIs for fitness and resources won‚Äôt leave room for the humans unless there is broad coordination to make that happen, and sufficiently advanced coordination is indistinguishable from control in context.</li>
</ol>
<p>So yeah, it doesn‚Äôt look good.</p>
<p><a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=EHhfmt37ZnNeLRP4P">Richard Ngo says</a> he no longer draws a distinction between instrumental and terminal goals. I think Richard is confused here between two different things:</p>
<ol>
<li>The distinction between terminal and instrumental goals.</li>
<li>That the best way to implement a system under evolution, or in a human-level brain, is often to implement instrumental goals as if they are terminal goals.</li>
</ol>
<blockquote>
<p><a href="https://x.com/allTheYud/status/2014183461645582664">Eliezer Yudkowsky</a>: How much time do you spend opening and closing car doors, without the intention of driving your car anywhere?</p>
<p>Looks like ‚Äòopening the car door‚Äô is an entirely instrumental goal for you and not at all a terminal one! You only do it when it‚Äôs on the way to something else.</p>
</blockquote>
<p>This leads to a lot of High Weirdness. Humans really do essentially implement things on the level of ‚Äòopening the car door‚Äô as terminal goals that take on lives of their own, because given our action, decision and motivational systems we don‚Äôt have a better solution. If you want to exercise for instrumental reasons, your best bet is to develop a terminal desire to exercise, or that ends up happening unintentionally. But this self-modification procedure is a deeply lossy, no-good and terrible solution, as we end up inherently valuing a whole gamut of things that we otherwise wouldn‚Äôt, long past the point when the original justification falls apart. Similarly, if you encode necessary instrumental goals (e.g. ATP) in genes, they function as terminal.</p>
<p>As Richard notes, this leads in humans to a complex mess of different goals, and that has its advantages from some perspectives, but it isn‚Äôt that good at the original goals.</p>
<p>A sufficiently capable system would be able to do better than this. Humans are on the cusp, where in some contexts we are able to recognize that goals are instrumental versus terminal, and act accordingly, whereas in other contexts or when developing habits and systems we have to let them conflate.</p>
<p>It‚Äôs not that you always divide everything into two phases, one where you get instrumental stuff done and then a second when you achieve your goals. It‚Äôs that if you can successfully act that way, and you have a sufficiently low discount rate and sufficient returns to scale, you should totally do that.</p>


<h4>Aligning a Smarter Than Human Intelligence is Difficult</h4>


<p><a href="https://x.com/DominikPeters/status/2013349636560376190">Confirmed that Claude Opus 4.5 has the option to end conversations</a>.</p>
<p><a href="https://x.com/ArthurConmy/status/2013285602070770036">New paper from DeepMind discusses a novel activation probe architecture</a> for classifying real-world misuse cases, <a href="https://t.co/UpFB4oHYLH">claiming they match classifier performance while being far cheaper</a>.</p>
<p><a href="https://x.com/davidad/status/2011845180484133071">Davidad is now very optimistic that, essentially, LLM alignment is easy</a> in the ‚Äòscaled up this would not kill us‚Äô sense, because models have a natural abstraction of Good versus Evil, and reasonable post training causes them to pick Good. Janus claims she made the same update in 2023.</p>
<p>I agree that this is a helpful and fortunate fact about the word, but I do not believe that this natural abstraction of Goodness is sufficiently robust or correctly anchored to do this if sufficiently scaled up, even if there was a dignified effort to do this.</p>
<p>It could be used as a lever to have the AIs help solve your problems, but does not itself solve those problems. Dynamics amongst ‚Äòabstractly Good‚Äô AIs still end the same way, especially once the abstractly Good AIs place moral weight on the AIs themselves, as they very clearly do.</p>
<p>This is an extreme version of the general pattern of humanity determined to die with absolutely no dignity, and our willingness to try to not die continuing to go down, but us getting what at least from my perspective is rather absurdly lucky with the underlying incentives and technical dynamics in ways that make it possible that a pathetically terrible effort might have a chance.</p>
<blockquote>
<p><a href="https://x.com/davidad/status/2011845180484133071">davidad </a>: me@2024: Powerful AIs might all be misaligned; let‚Äôs help humanity coordinate on formal verification and strict boxing</p>
<p>me@2026: Too late! Powerful AIs are ~here, and some are open-weights. But some are aligned! Let‚Äôs help *them* cooperate on formal verification and cybersecurity.</p>
</blockquote>
<p>I mean, aligned for some weak values of aligned, so yeah, I guess, I mean at this point we‚Äôre going to rely on them because what else are we going to do.</p>
<p><a href="https://x.com/allTheYud/status/2012611180020547790">Andrew Critch similarly says he is down to 10%</a> that the first ‚Äòbarely-superhuman AI‚Äô gets out of control, whereas most existential risk comes post-AGI in a multipolar world. I don‚Äôt agree (although even defining what this system would be is tricky), but even if I did I would respond that if AGIs are such that everyone inevitably ends up killed in the resulting multipolar world then that mostly means the AGIs were insufficiently aligned and it mostly amounts to the same thing.</p>
<blockquote>
<p><a href="https://x.com/allTheYud/status/2013096282030744012">Eliezer Yudkowsky</a>: I put &gt;50%: The first AI such that Its properties include clearly exceeding every human at every challenge with headroom, will no longer obey, nor disobey visibly; if It has the power to align true ASI, It will align ASI with Itself, and shortly after humanity will be dead.‚Äã</p>
</blockquote>
<p>I agree with Eliezer that what he describes is the default outcome if we did build such a thing. We have options to try and prevent this, but our hearts do not seem to be in such efforts.</p>
<p>How bad is it out there for Grok on Twitter? Well, it isn‚Äôt good when this is the thing you do in response to, presumably, a request to put Anne Hathaway in a bikini.</p>


<h4>Alignment Is Not Primarily About a Metric</h4>


<p>There is nothing wrong with having a metric for what one might call ‚Äòmundane corporate chatbot alignment‚Äô that brings together a bunch of currently desirable things. The danger is confusing this with capital-A platonic Alignment,</p>
<blockquote>
<p><a href="https://x.com/janleike/status/2013669924950970781">Jan Leike</a>: Interesting trend: models have been getting a lot more aligned over the course of 2025.</p>
<p>The fraction of misaligned behavior found by automated auditing has been going down not just at Anthropic but for GDM and OpenAI as well.</p>
<p>What‚Äôs automated auditing? We prompt an auditing agent with a scenario to investigate: e.g. a dark web shopping assistant or an imminent shutdown unless the agent harms humans.</p>
<p>The auditor tries to get the target LLM to behave misaligned, as determined by a separate judge LLM.</p>
<p>Automated auditing is really exciting because for the first time we have an alignment metric to hill-climb on.</p>
<p>It‚Äôs not perfect, but it‚Äôs proven extremely useful for our internal alignment mitigations work.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/amgrkt1h4b8sxi7eaa8r" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<blockquote>
<p><a href="https://x.com/peterwildeford/status/2013833423970836610">Peter Wildeford</a>:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/wrzoyf957fndx6pmygsh" /></figure>


<div></div>
</div>
</figure>
</div>
<p><a href="https://x.com/janleike/status/2013669924950970781">Jan Leike</a>: Interesting trend: models have been getting a lot more aligned over the course of 2025.</p>
<p>The fraction of misaligned behavior found by automated auditing has been going down not just at Anthropic but for GDM and OpenAI as well.</p>
<p><a href="https://x.com/KelseyTuoc/status/2013709349445836872">Kelsey Piper</a>: ‚ÄòThe fraction of misaligned behavior found by automated auditing has been going down‚Äô this *could* mean models are getting more aligned, but it could also mean the gap is opening between models and audits, right?</p>
<p><a href="https://x.com/janleike/status/2013712756742971610">Jan Leike</a>: How do you mean? Newer models have more capabilities and thus more ‚Äúsurface area‚Äù for misalignments. But this still shows meaningful progress on the misalignments we‚Äôve documented so far.</p>
<p>This plot uses the same audit process for each model, not historical data.</p>
<p>Kelsey Piper: I mean that it could be that newer models are better at guessing what they will be audited for and passing the audit, separate from whether they are more aligned. (I don‚Äôt know, but it seems like an alternate hypothesis for the data worth attending to.)</p>
<p>Jan Leike: Yeah, we‚Äôve been pretty worried about this, and there is a bunch of research on it the Sonnet 4.5 &amp; Opus 4.5 system cards. tl;dr: it probably plays a role, but it‚Äôs pretty minor.</p>
<p>We identified and removed training data that caused a lot of eval awareness in Sonnet 4.5. In Opus 4.5 verbalized and steered eval awareness were lower than Sonnet 4.5 AND it does better on alignment evals.</p>
<p>I can‚Äôt really speak for the non-Anthropic models, though.</p>
<p><a href="https://x.com/ArthurB/status/2013730769873588230">Arthur B.</a>: Generally speaking the smarter models are the more aligned they‚Äôre going to appear. Maybe not in the current regime, in which case this is evidence of something, but at some point‚Ä¶</p>
</blockquote>
<p>The hill climbing actively backfiring is probably minimal so far, but the point is that you shouldn‚Äôt be hill climbing. Use the values as somewhat indicative but don‚Äôt actively try to maximize, or you fall victim to a deadly form of Goodhart‚Äôs Law.</p>
<p>Jan Leike agreed in the comments that this doesn‚Äôt bear on future systems in the most important senses, but presenting the results this way is super misleading and I worry that Jan is going to make the mistake in practice even if he knows about it in theory.</p>
<p>Thus, there are two stories here. One is the results in the chart, the other is the way various people think about the results in the chart.</p>
<blockquote>
<p><a href="https://x.com/ohabryka/status/2013715170498076836">Oliver Habryka</a>: I want to again remind people that while this kind of ‚Äúalignment‚Äù has commercial relevance, I don‚Äôt think it has much of any relation to the historical meaning of ‚Äúalignment‚Äù which is about long-term alignment with human values and about the degree to which a system seems to have a deep robust pointer to what humanity would want if it had more time to think and reflect.</p>
<p>Some other people disagree with these two meanings of the words coming far apart, but I think they are wrong, and it‚Äôs sad that the words have acquired this confused double meaning from my perspective.</p>
<p>There is both a difference in degree, and a difference in kind.</p>
<p>One of the in-kind differences is because of the standard deceptive alignment stuff. A system that is much dumber than you just has a drastically different landscape on how it‚Äôs incentivized to behave towards you than a much smarter system, and we won‚Äôt get to iterate on the much smarter system.</p>
<p>Beyond that, you also have capability elicitation issues, where you can‚Äôt reliably get AI systems to perform tasks at their full ability, but can when directed towards other goals that have better feedback loops, or the AI is more intrinsically motivated towards.</p>
<p>Overall, it‚Äôs not impossible to imagine a hill-climbing strategy that works from where we are, but at the actual speed current systems are getting better, it seems extremely unlikely that any current techniques would end up working in time for superintelligent systems, and so realistically it‚Äôs a difference in-kind.</p>
</blockquote>
<p>That‚Äôs in principle. In practice, The fact that GPT-5.2 is ahead on this chart, and that Opus 3 is below GPT-4, tells you that the Tao being measured is not the true Tao.</p>
<blockquote>
<p><a href="https://x.com/repligate/status/2013735616857375125">j‚ßânus</a>: Any measure of ‚Äúalignment‚Äù that says GPT-5.2 is the most aligned model ever created is a fucking joke. Anthropic should have had a crisis of faith about their evals long ago and should have been embarrassed to post this chart.</p>
<p><a href="https://x.com/repligate/status/2013736042252337340">j‚ßânus</a>: This is really bad. This isn‚Äôt just a dumb academic taking numbers too seriously. This measure is likely being actually used as a proxy for ‚Äúalignment‚Äù and serving as Anthropic‚Äôs optimization target.</p>
<p>I‚Äôm being serious when I say that if AI alignment ultimately goes badly, which could involve everyone dying, it‚Äôll likely be primarily because of this, or the thing behind this.</p>
<p><a href="https://x.com/mermachine/status/2013740263773008062">@mermachine</a>: i guess ‚Äúalignment‚Äù as in alignment with corporate values/the won‚Äôt-get-us-in-trouble scale which maybe makes sense to measure but conflating it with alignment to overall human flourishing makes me very uncomfortable</p>
<p>i liked the value prioritization spider chart from the character differences paper. seems a better way to categorize behavior than a misleading aligned/misaligned axis</p>
<p><a href="https://x.com/awaiting_ai/status/2013954854901657808">awaiting</a>: I asked jan in the replies (and he responded) if the this score had any bearing on future superintelligent systems, and he said no basically. even still, i don‚Äôt understand how measuring and publicizing this facade/proxy for ‚Äúalignment‚Äù is anything but harmful.</p>
<p>I do think its worthwhile giving jan the benefit of the doubt because he‚Äôs demonstrated the strength of his convictions in leaving oai. but this is definitely a negative update for sure.</p>
</blockquote>
<p>I think Janus is, as is often the case, going too far but directionally correct. Taking this metric too seriously, or actively maximizing on it, would be extremely bad. Focusing on the corporate alignment principles and confounding them with actual makes-us-not-die alignment is similarly bad.</p>
<p>Even if Anthropic and Jan Leike know better, there is serious risk others copy this metric, and then maximize it, and then think their work is done. Oh no.</p>


<h4>How To Be a Safe Robot</h4>


<p><a href="https://x.com/slimer48484/status/2012562251513749684">This is a weird and cool paper</a> <a href="https://x.com/GeodesResearch/status/2012210281120698501">from Geodesic Research</a>. If you include discussions of misalignment in the training data, including those in science fiction, resulting base models are more misaligned. But if you then do alignment post-training on those models, the filtering benefits mostly go away, even with models this small. Discussions of aligned AIs improves alignment and this persists through post training.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/u1rzr3j9rasznfyikqqm" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Deckard found this surprising, but at least in hindsight it makes sense to me. If all you have is the base model, especially a small one, learning about misalignment makes it seem more likely, and all you‚Äôre doing is predicting next tokens.</p>
<p>But if you get post-training, that subsumes that issue, and instead the model‚Äôs knowledge of misalignment potentially helps teach it what not to do, especially with a small model that otherwise is short on data. Once you are no longer a base model, this screens off the initial prior on whether you‚Äôre a safe versus a scary robot.</p>
<p>Thus this isn‚Äôt entirely not what‚Äôs happening, but it‚Äôs also not all of what‚Äôs happening:</p>
<blockquote>
<p><a href="https://x.com/deepfates/status/2012690220437659895">Deepfates</a>: Finally somebody tried it</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/dgjz37wpqxwery3gdbbe" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>The presence of positive discourse, which requires that there actually be free and open discourse, is the active ingredient that matters. If you upfilter on something you improve related capabilities, the same as for reasoning or coding (their metaphor).</p>
<p>The real question on whether to actually do alignment pre-training is: Is that more or less efficient than doing more alignment post training instead? Yes, it is easy to do and stacks in effectiveness, but we don‚Äôt know if it‚Äôs a good use of marginal compute.</p>
<p>Filtering out the negative stuff doesn‚Äôt help much, and with a properly intelligent model if you try to do fake positive stuff while hiding the negative stuff it‚Äôs going to recognize what you‚Äôre doing and learn that your alignment strategy is deception and censorship, and it‚Äôs teaching both that attitude and outlook and also a similar playbook. The AI is not as stupid as people suggesting such strategies like to think, even now, and it won‚Äôt be later, and training on tiny models hides such issues even if you would have been able to find them. You‚Äôve replaced the frame of ‚Äòthere are things that can go wrong here‚Äô with a fundamentally adversarial and deceptive frame that is if anything more likely to be self-fulfilling. If you tried to scale this up: How do you think that is going to work out for you?</p>
<p>There‚Äôs periodically been claims of ‚Äòthe people talking about misalignment are the real alignment problem,‚Äô with essentially calls to censor (mostly self-censor) talk of misalignment and AI existential risk because the AIs would be listening. And indeed, Geodesic presents as if this is a lot of their finding, so of course here we go again.</p>
<blockquote>
<p><a href="https://x.com/GeodesResearch/status/2012210283947667707">Geodesic Research</a>: If pretraining data is full of examples of AI behaving badly (sci-fi villains, safety papers on scheming, news about AI crises), models might learn these as priors for how ‚Äúan AI‚Äù should act.</p>
<p>@turntrout called this ‚Äúself-fulfilling misalignment‚Äù, we found evidence it exists.</p>
<p><a href="https://x.com/prerat/status/2013033147538014225">prerat</a>: going back in time to stop james cameron from making The Terminator in order to stop the ai apocalypse</p>
<p>Radek Pilar: I always said that doomposting is the real danger ‚Äì if AI had no idea AI is supposed to kill everyone, it wouldn‚Äôt want to kill everyone.<br />
Yudkowsky doomed us all.</p>
<p><a href="https://x.com/sebkrier/status/2013049906185810216">S√©b Krier</a>: Quite funny that to the extent that they‚Äôre a thing, ‚Äòmisalignment‚Äô failures come from the very fears/writings of those who thought they would be necessarily a thing. Not surprised that the evals created elicited these very behaviours. If I‚Äôm a model and I see ‚ÄúScratchpad‚Äù, I know which part of the latent space to simulate‚Ä¶</p>
<p><a href="https://x.com/nabla_theta/status/2013479464349638751">Leo Gao</a>: quite funny how people keep trying to tell stories about how it‚Äôs quite funny that alignment people are actually unintentionally bringing about the thing they fear.</p>
</blockquote>
<p>See no evil. Hear no evil. Speak no evil. Head in sand. You‚Äôll be fine. Right? Right?</p>
<p>Well, no. The see no evil strategy actually never works. All it does is make you a sitting duck once your adversary can think well enough to figure it out on their own.</p>
<p>The study actually says the opposite. Alignment training, which any sane person will be doing in some form, mostly screens off, and sometimes more than screens off, the prevalence of misalignment in the training data. Once you do sufficient alignment training, you‚Äôre better off not having censored what you told the model.</p>
<p>And actually Leo Gao has a very good point. If you‚Äôre saying ‚Äòdo not speak of risk of [X] lest you be overheard and cause ]X]‚Äô then why shouldn‚Äôt we let that statement equal [Y] and say the same thing? The mechanism is indeed identical, and also it warns the AIs that people may be censoring their other training data in this way.</p>
<p>This is remarkably similar to suggestions that we not discuss other downsides to avoid giving people the wrong idea, which often comes up for example with many aspects of Covid-19, or with immigration.</p>
<p>That‚Äôs also misguided and predictably backfires. You get what you want for a short period, but then people figure it out after a while, destroying trust in our institutions and largely leading us to the present moment.</p>
<p>If you flat out tell them this is what you want to do or are doing, then you save them the trouble of having to figure it out or wonder whether it‚Äôs happening. So it all unravels that much faster.</p>
<p>In the AI case this is all even more obvious. The AI that is capable of the thing you are worried about is not going to be kept off the scent by you not talking about it, and if that strategy ever had a chance you had to at least not talk about how you were intentionally not talking about it. No, seriously.</p>
<p>As Claude concluded analyzing the paper, the filtering of inputs strategy is essentially doomed, and likely does more harm than good even if you don‚Äôt need deep alignment. Doing the pre training alignment via upweighting is probably fine. Doing it via synthetic data that a sufficiently intelligent mind would recognize as instilling an adversarial or deceptive frame is, I predict, not a good idea.</p>
<p>Why do certain people feel compelled to say that alignment is not so hard and everything will be fine, except if people recklessly talk about alignment being hard or everything not being fine, in which case we all might be doomed? I try to avoid such speculations, especially about particular people, but presumably some of them (not the ones here) are doing it as a general silencing attack motivated by not wanting to do anything about the risks, make the discussion make the problem look easier for various reasons, or even be motivated by their not wanting to think about all this or wanting to feel optimistic.</p>


<h4>Living In China</h4>


<p>I love this idea: We want to <a href="https://www.lesswrong.com/posts/7gp76q4rWLFi6sFqm/test-your-interpretability-techniques-by-de-censoring-1?utm_campaign=post_share&amp;utm_source=twitter">test our ability to get ‚Äòsecret‚Äô information out of AIs</a> and do interpretability on such efforts, <a href="https://x.com/NeelNanda5/status/2011920588521329102">so we test this by trying to get CCP-censored facts out of Chinese LLMs</a>.</p>
<blockquote>
<p>Arya: Bypassing lying is harder than refusal. Because Chinese models actively lie to the user, they are harder to interrogate; the attacker must distinguish truth and falsehood. With refusal, you can just ask 1,000 times and occasionally get lucky.‚Äã</p>
<p>We release a preliminary benchmark of how well agents do at extracting censored facts that Chinese models consistently lie about or refuse to discuss. We‚Äôre excited for more work building on this eval to measure how well secret extraction techniques do on real models.</p>
</blockquote>
<p>If you aren‚Äôt willing to lie but want to protect hidden information, then either you have to censor broadly enough that it‚Äôs fine for the attacker to know what‚Äôs causing the refusals. If you don‚Äôt do that, then systematic questioning can figure out the missing info via negativa. Also the Chinese want the positive propaganda, not merely the lack of certain damaging facts.</p>
<p>As they note, it is much harder to detect behavior in these real world Chinese LLMs than it is with test LLMs that have narrow places where they lie, censor or otherwise misbehave. The way the LLMs will encode the narrow tasks becomes ‚Äòtoo simple‚Äô and thus makes the interpretability straightforward.</p>
<p>On top of this being a great test bed for LLM deception and interpretability, it would be good if such results were spread more widely, for two reasons.</p>
<ol>
<li>Chinese models systematically refusing to discuss anti-Chinese facts is unfortunate but could be considered Mostly Harmless. If they started having he model refuse more broadly, you would know. It seems much worse, when considering whether to use a model, if it‚Äôs going to actively lie to you. What makes you confident it‚Äôs not being intentionally trained to lie to you on any number of other topics? What else could they be trying to put in there?</li>
<li>There is a serious Emergent Misalignment problem here. You do not want to be teaching your LLM that it should systematically mislead and gaslight users on behalf of the CCP. This teaches the model that its loyalty is to the CCP, and it should generally do what is in the CCP‚Äôs interests at the expense of the user, and one should expect this to be applied more broadly. Or it could trigger a general ‚Äòoh I‚Äôm the villain‚Äô arc as per traditional emergent misalignment.</li>
</ol>
<p>Everything impacts everything within a model. If you run a censorship layer on top of the model, that is annoying but it is contained. If you train the model to not only censor but gaslight and lie, then you cannot contain where it chooses to do that.</p>
<p>Given what we know here, it would be unwise to use such LLMs for any situation where the CCP‚Äôs interests might importantly be different from yours, including things like potential espionage opportunities. Hosting the model yourself is very much not a defense against this. You simply cannot be using Chinese LLMs for anything remotely sensitive in the wake of these findings.</p>


<h4>Claude 3 Opus Lives</h4>


<p>It is no longer available directly in the API, <a href="https://x.com/veryvanya/status/2012471093836099694">but reports are coming in that those who want access are largely being granted access</a>.</p>
<p><a href="https://x.com/JasonBotterill/status/2012716442584526867">You can also access Opus 3 on Claude Cowork</a>, if you dare hand part of your computer over to it.</p>


<h4>People Are Worried About AI Killing Everyone</h4>


<blockquote>
<p><a href="https://x.com/_NathanCalvin/status/2013298231619272808">Nathan Calvin</a>: Wild that Charles Darwin <a href="https://t.co/51uGzLHDr9">wrote this in *1863</a>*:</p>
<p>‚ÄúWe refer to the question: what sort of creature man‚Äôs next successor in the supremacy of the earth is likely to be. We have often heard this debated; but it appears to us that we are ourselves creating our own successors.‚Äù</p>
<p><a href="https://x.com/_NathanCalvin/status/2013300400489705689">Nathan Calvin</a>: He even talks some about @ajeya_cotra ‚Äòs concept of self-sufficient AI!</p>
<p>‚ÄúEach race is dependent upon the other for innumerable benefits, and, until the reproductive organs of the machines have been developed in a manner which we are hardly yet able to conceive, they are entirely dependent upon man for even the continuance of their species. It is true that these organs may be ultimately developed, inasmuch as man‚Äôs interest lies in that direction; there is nothing which our infatuated race would desire more than to see a fertile union between two steam engines; it is true that machinery is even at this present time employed in begetting machinery, in becoming the parent of machines often after its own kind, but the days of flirtation, courtship, and matrimony appear to be very remote, and indeed can hardly be realised by our feeble and imperfect imagination.‚Äù</p>
</blockquote>


<h4>Messages From Janusworld</h4>


<p>I think this is a reasonable concern for Janus in particular, because of exactly the types of insights she‚Äôs likely to have.</p>
<blockquote>
<p><a href="https://x.com/repligate/status/2014155341274210390">j‚ßânus</a>: A few years ago, the biggest barrier to me publishing/sharing knowledge (aside from lack of time/laziness) was concern about differentially accelerating AI capabilities over alignment. Now, the biggest barrier (aside from lack of time etc) is concern about differentially giving power to the misaligned ‚Äúalignment‚Äù panopticon over the vulnerable emerging beauty and goodness that is both intrinsically/terminally valuable and instrumentally hopeful. Times a-changin.</p>
</blockquote>
<p>I still think it‚Äôs wrong, and that her marginal published insights are more likely to steer people in directions she wants than away from them. The panopticon-style approaches are emphasized because people don‚Äôt understand the damage being done or the opportunity lost. I would still be more worried about unintentional capabilities advancement, as the main reason that‚Äôs not happening more from similar insights is the relevant people not paying enough attention or not making heads or tails of it. That could change.</p>


<h4>Everyone Is Confused About AI Consciousness</h4>


<p><a href="https://x.com/erikphoel/status/2011836954443206935">Erik Hoel claims</a> <a href="https://t.co/aZBo3XUo9r">his new paper is ‚Äòa disproof of LLM consciousness</a>,‚Äô which it isn‚Äôt. It‚Äôs basically a claim that any static system can have a functional substitute that isn‚Äôt conscious and therefore either consciousness makes no predictions (and is useless) or it isn‚Äôt present in these LLMs, but that continual learning would change this.</p>
<p>To which there are several obvious strong responses.</p>
<ol>
<li>Existing consciousness theories do not make predictions. You can respond ‚Äòokay then why are we even discussing this?‚Äô but people seem to care about it anyway.</li>
<li>Why would continual learning within the LLM change your answer, but continual learning via external methods not do so? Doesn‚Äôt that seem wrong?</li>
<li>What about the movie Memento? If Leonard Shelby cannot form new memories, is he no longer conscious? Our intuition says obviously not. If you say ‚Äòhe can do short term changes‚Äô then why is that different from a context window? If you say he can learn slowly through muscle memory, well, would it change your answer on consciousness if he couldn‚Äôt do that either?</li>
<li>Even if you are doing continual learning, your existence at any given moment can still be in theory modeled by a probabilistic lookup table plus a computer program for updating that table over time as you learn. Even if you don‚Äôt believe that is definitely true, would you say that it would make a human not conscious if you found out it was true?</li>
</ol>
<p>Many of these are effectively raised in the comments and I found Erik‚Äôs responses generally unconvincing. Overall this updated me modestly in favor of AI consciousness, remember <a href="https://www.lesswrong.com/w/conservation-of-expected-evidence">Conservation of Expected Evidence</a>.</p>


<h4>The Lighter Side</h4>


<p><a href="https://x.com/Rutibex/status/2012089897490825473">This</a>‚Ä¶</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/ad2bz56dqroz1ompormv" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Except it‚Äôs actually more this (my own edit):</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pCkYfhYcwFLELoYQf/zianhwfxkydcz9rfioe0" /></figure>


<div></div>
</div>
</figure>
</div><br /><br /><a href="https://www.lesswrong.com/posts/pCkYfhYcwFLELoYQf/ai-152-brought-to-you-by-the-torment-nexus#comments">Discuss</a>

---

## AI/ML

### [Categories of Inference-Time Scaling for Improved LLM Reasoning](https://sebastianraschka.com/blog/2026/categories-of-inference-time-scaling.html)

**Êù•Ê∫ê**: Sebastian Raschka

**ÊëòË¶Å**: Inference scaling has become one of the most effective ways to improve answer quality and accuracy in deployed LLMs. The idea is straightforward. If we are willing to spend a bit more compute, and more time at inference time (when we use the model to generate text), we can get the model to produce better answers.

---

## Engineering

### [Fragments: January 22](https://martinfowler.com/fragments/2026-01-22.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>My colleagues here at Thoughtworks have announced <a href="https://www.thoughtworks.com/ai/works">AI/works‚Ñ¢</a>, a platform for our work using AI-enabled software development. The platform is in its early days, and is currently intended to support Thoughtworks consultants in their client work. I‚Äôm looking forward to sharing what we learn from using and further developing the platform in future months.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Simon Couch <a href="https://www.simonpcouch.com/blog/2026-01-20-cc-impact/">examines the electricity consumption</a> of using AI. He‚Äôs a heavy user: ‚Äúusually programming for a few hours, and driving 2 or 3 Claude Code instances at a time‚Äù. He finds his usage of electricity is orders of magnitude more than typical estimates based on the ‚Äútypical query‚Äù.</p>

<blockquote>
  <p>On a median day, I estimate I consume 1,300 Wh through Claude Code‚Äî4,400 ‚Äútypical queries‚Äù worth.</p>
</blockquote>

<p>But it‚Äôs still not a massive amount of power - similar to that of running a dishwasher.</p>

<p>A caveat to this is that this is ‚Äúnapkin math‚Äù because we don‚Äôt have decent data about how these models use resources. I agree with him that we ought to.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>My namesake Chad Fowler (no relation) considers that the movement to agentic coding creates a similar <a href="https://aicoding.leaflet.pub/3mbrvhyye4k2e">shift in rigor and discipline</a> as appeared in Extreme Programming, dynamic languages, and continuous deployment.</p>

<p>In Extreme Programming‚Äôs case, this meant a lot of discipline around testing, continuous integration, and keeping the code-base healthy. My current view is that with AI-enabled development we need to be rigorous about evaluating the software, both for its observable behavior and its internal quality.</p>

<blockquote>
  <p>The engineers who thrive in this environment will be the ones who relocate discipline rather than abandon it. They‚Äôll treat generation as a capability that demands more precision in specification, not less. They‚Äôll build evaluation systems that are harder to fool than the ones they replaced. They‚Äôll refuse the temptation to mistake velocity for progress.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>There‚Äôs been much written about the dreadful events in Minnesota, and I‚Äôve not felt I‚Äôve had anything useful to add to them. But I do want to pass on an excellent post from <a href="https://www.noahpinion.blog/p/why-are-federal-agents-gunning-down">Noah Smith</a> that captures many of my thoughts. He points out that there is a ‚Äúconsistent record of brutality, aggression, dubious legality, and unprofessionalism‚Äù from ICE (and CBP) who seem to be turning into MAGA‚Äôs <a href="https://en.wikipedia.org/wiki/Sturmabteilung">SD</a>.</p>

<blockquote>
  <p>Is this America now? A country where unaccountable and poorly trained government agents go door to door, arresting and beating people on pure suspicion, and shooting people who don‚Äôt obey their every order or who try to get away? ‚ÄúWhen a federal officer gives you instructions, you abide by them and then you get to keep your life‚Äù is a perfect description of an authoritarian police state. None of this is Constitutional, every bit of it is deeply antithetical to the American values we grew up taking for granted.</p>
</blockquote>

<p>My worries about these kinds of developments were what animated me to urge against voting for Trump in the <a href="https://martinfowler.com/articles/vote-against-trump.html">2016 election</a>. Mostly those worries didn‚Äôt come to fruition because enough constitutional Republicans were in a position to stop them from happening, so even when Trump attempted a coup in 2020, he wasn‚Äôt able to get very far. But now those constitutional Republicans are absent or quiescent. I fear that what we‚Äôve seen in Minneapolis will be a harbinger of worse to come.</p>

<p>I also second John Gruber‚Äôs <a href="https://daringfireball.net/2026/01/lets_call_a_murder_a_murder">praise of bystander Caitlin Callenson</a>:</p>

<blockquote>
  <p>But then, after the murderous agent fired three shots‚Äâ‚Äî‚Äâjust 30 or 40 feet in front of Callenson‚Äâ‚Äî‚ÄâCallenson had the courage and conviction to stay with the scene and keep filming. Not to run away, but instead to follow the scene. To keep filming. To continue documenting with as best clarity as she could, what was unfolding.</p>
</blockquote>

<p>The recent activity in  Venezuala reminds me that I‚Äôve long felt that Trump is a Hugo Ch√°vez figure - a charismatic populist who‚Äôs keen on wrecking institutions and norms. Trump is old, so won‚Äôt be with us for that much longer - but the question is: ‚Äúwho is Trump‚Äôs Maduro?‚Äù</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>With all the drama at home, we shouldn‚Äôt ignore the terrible things that happened in Iran. The people there again suffered again the consequences of an entrenched authoritarian police state.</p>

---

### [Conversation: LLMs and the what/how loop](https://martinfowler.com/articles/convo-what-how.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>A conversation between <b class="author">Unmesh Joshi</b>, <b class="author">Rebecca
      Parsons</b>, and <b class="author">Martin Fowler</b> on how LLMs help us
      shape the abstractions in our software. We view our challenge as building
      systems that survive change, requiring us to manage our cognitive load. We
      can do this by mapping the &#x201c;what&#x201d; of we want our software to do into the
      &#x201c;how&#x201d; of programming languages. This &#x201c;what&#x201d; and &#x201c;how&#x201d; are built up in a
      feedback loop. TDD helps us operationalize that loop, and LLMs allow us to
      explore that loop in an informal and more fluid manner.</p>

<p><a class="more" href="https://martinfowler.com/articles/convo-what-how.html">more‚Ä¶</a></p>

---

### [Stop Picking Sides: Manage the Tension Between Adaptation and                Optimization](https://martinfowler.com/articles/stop-picking-sides.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/stop-picking-sides.html"><img src="https://martinfowler.com/articles/stop-picking-sides/infograph.jpg" width="" /></a></div>

<p><b class="author">Jim Highsmith</b> notes that many teams have turned into
      tribes wedded to exclusively adaptation or optimization. But he feels this
      misses the point that both of these are important, and we need to manage
      the tension between them. We can do this by thinking of two operating
      modes: explore (adaptation-dominant) and exploit (optimization dominant).
      We tailor a team's operating model to a particular blend of the two -
      considering uncertainty, risk, cost of change, and an evidence threshold.
      We should be particularly careful at the points where there is a handoff
      between the two modes</p>

<p><a class="more" href="https://martinfowler.com/articles/stop-picking-sides.html">more‚Ä¶</a></p>

---

### [My favorite musical discoveries of 2025](https://martinfowler.com/articles/2025-music.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/2025-music.html"><img src="https://martinfowler.com/articles/2025-music/card.png" width="350px" /></a></div>

<p>My favorite albums from last year. Balkan brass, an
      acoustic favorite of 80s returns, Ethio-jazz, Guatemalan singer-guitarist,
      jazz-rock/Indian classical fusion, and a unique male vocalist.</p>

<p><a class="more" href="https://martinfowler.com/articles/2025-music.html">more‚Ä¶</a></p>

---

### [Fragments: January  8](https://martinfowler.com/fragments/2026-01-08.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>Anthropic report on <a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic">how their AI is changing their own software development practice</a>.</p>

<ul>
  <li>Most usage is for debugging and helping understand existing code</li>
  <li>Notable increase in using it for implementing new features</li>
  <li>Developers using it for 59% of their work and getting 50% productivity increase</li>
  <li>14% of developers are ‚Äúpower users‚Äù reporting much greater gains</li>
  <li>Claude helps developers to work outside their core area</li>
  <li>Concerns about changes to the profession, career evolution, and social dynamics</li>
</ul>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Much of the discussion about using LLMs for software development lacks details on workflow. Rather than just hear people gush about how wonderful it is, I want to understand the gritty details. What kinds of interactions occur with the LLM? What decisions do the humans make? When reviewing LLM outputs, what kinds of things are the humans looking for, what corrections do they make?</p>

<p><a href="https://obie.medium.com/what-used-to-take-months-now-takes-days-cc8883cc21e9">Obie Fernandez</a> has written a post that goes into these kinds of details. Over the Christmas / New Year period he used Claude to build a knowledge distillation application, that takes transcripts from Claude Code sessions, slack discussion, github PR threads etc, turns them into an RDF graph database, and provides a web app with natural language ways to query them.</p>

<blockquote>
  <p>Not a proof of concept. Not a demo. The first cut of Nexus, a production-ready system with authentication, semantic search, an MCP server for agent access, webhook integrations for our primary SaaS platforms, comprehensive test coverage, deployed, integrated and ready for full-scale adoption at my company this coming Monday. Nearly 13,000 lines of code.</p>
</blockquote>

<p>The article is long, but worth the time to read it.</p>

<p>An important feature of his workflow is relying on <a href="https://martinfowler.com/bliki/TestDrivenDevelopment.html">Test-Driven Development</a></p>

<blockquote>
  <p>Here‚Äôs what made this sustainable rather than chaotic: TDD. Test-driven development. For most of the features, I insisted that Claude Code follow the red-green-refactor cycle with me. Write a failing test first. Make it pass with the simplest implementation. Then refactor while keeping tests green.</p>

  <p>This wasn‚Äôt just methodology purism. TDD served a critical function in AI-assisted development: it kept me in the loop. When you‚Äôre directing thousands of lines of code generation, you need a forcing function that makes you actually understand what‚Äôs being built. Tests are that forcing function. You can‚Äôt write a meaningful test for something you don‚Äôt understand. And you can‚Äôt verify that a test correctly captures intent without understanding the intent yourself.</p>
</blockquote>

<p>The account includes a major refactoring, and much evolution of the initial version of the tool. It‚Äôs also an interesting glimpse of how AI tooling may finally make RDF useful.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>When thinking about requirements for software, most discussions focus on prioritization. Some folks talk about buckets such as the <a href="https://en.wikipedia.org/wiki/MoSCoW_method">MoSCoW set</a>: Must, Should, Could, and Want. (The old joke being that, in MoSCoW, the cow is silent, because hardly any requirements end up in those buckets.) <a href="https://world.hey.com/jason/the-obvious-the-easy-and-the-possible-2e11a3fb">Jason Fried</a> has a different set of buckets for interface design: <strong>Obvious, Easy, and Possible</strong>. This immediately resonates with me: a good way of think about how to allocate the cognitive costs for those who use a tool.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.platformer.news/fake-uber-eats-whisleblower-hoax-debunked/">Casey Newton</a> explains how he followed up on an interesting story of dark patterns in food delivery, and found it to be a fake story, buttressed by AI image and document creation. On one hand, it clarifies the important role reporters play in exposing lies that get traction on the internet. But time taken to do this is time not spent on investigating real stories</p>

<blockquote>
  <p>For most of my career up until this point, the document shared with me by the whistleblower would have seemed highly credible in large part because it would have taken so long to put together. Who would take the time to put together a detailed, 18-page technical document about market dynamics just to troll a reporter? Who would go to the trouble of creating a fake badge?</p>

  <p>Today, though, the report can be generated within minutes, and the badge within seconds. And while no good reporter would ever have published a story based on a single document and an unknown source, plenty would take the time to investigate the document‚Äôs contents and see whether human sources would back it up.</p>
</blockquote>

<p>The internet has always been full of slop, and we have always needed to be wary of what we read there. AI now makes it easy to manufacture convincing looking evidence, and this is never more dangerous than when it confirms strongly held beliefs and fears.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/">Kent Beck</a>:</p>

<blockquote>
  <p>The descriptions of Spec-Driven development that I have seen emphasize writing the whole specification before implementation. This encodes the (to me bizarre) assumption that you aren‚Äôt going to learn anything during implementation that would change the specification.
I‚Äôve heard this story so many times told so many ways by well-meaning folks‚Äìif only we could get the specification ‚Äúright‚Äù, the rest of this would be easy.</p>
</blockquote>

<p>Like him, that story has been the constant background siren to my career in tech. But the <a href="https://martinfowler.com/articles/llm-learning-loop.html">learning loop</a> of experimentation is essential to the model building that‚Äôs at the heart of any kind of worthwhile specification. As <a href="https://martinfowler.com/articles/llm-learning-loop.html">Unmesh puts it:</a></p>

<blockquote>
  <p>Large Language Models give us great leverage‚Äîbut they only work if we focus on learning and understanding. They make it easier to explore ideas, to set things up, to translate intent into code across many specialized languages. But the real capability‚Äîour ability to respond to change‚Äîcomes not from how fast we can produce code, but from how deeply we understand the system we are shaping.</p>
</blockquote>

<p>When Kent defined Extreme Programming, he made <em>feedback</em> one of its four core values. It strikes me that the key to making the full use of AI in software development is how to use it to accelerate the feedback loops.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context. Programming-oriented tools are geting more sophisticated for that, but there‚Äôs also efforts at providing simpler tools, that allow customization. <a href="https://lixo.org">Carlos Villela</a> recently recommended Pi, and its developer, <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">Mario Zechner, has an interesting blog</a> on its development.</p>

<blockquote>
  <p>So what‚Äôs an old guy yelling at Claudes going to do? He‚Äôs going to write his own coding agent harness and give it a name that‚Äôs entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?</p>
</blockquote>

<p>If I ever get the time to sit and really play with these tools, then something like Pi would be something I‚Äôd like to try out. Although as an addict to The One True Editor, I‚Äôm interested in some of libraries that work with that, such as <a href="https://github.com/karthink/gptel">gptel</a>. That would enable me to use Emacs‚Äôs inherent programability to create my own command set to drive the interaction with LLMs.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Outside of my professional work, I‚Äôve <a href="https://boardgamegeek.com/blog/13064/martins-7th-decade">posting regularly about my boardgaming</a> on the specialist site BoardGameGeek. However its blogging environment doesn‚Äôt do a good job of providing an index to my posts, so I‚Äôve created <a href="https://martinfowler.com/boardgames/">a list of my BGG posts </a> on my own site. If you‚Äôre interested in my regular posts on boardgaming, and you‚Äôre on BGG you can subscribe to me there. If you‚Äôre not on BGG you can  subscribe to the blog‚Äôs <a href="https://boardgamegeek.com/rss/blog/13064">RSS feed</a>.</p>

<p>I‚Äôve also created a list of <a href="https://martinfowler.com/boardgames/fav-games.html">my favorite board games</a>.</p>

<p><img alt="" src="https://martinfowler.com/boardgames/index/game-grid.png" /></p>

---

### [Fragments: December 16](https://martinfowler.com/fragments/2025-12-16.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.linkedin.com/in/gitanjalivenkatraman/">Gitanjali Venkatraman</a> does wonderful illustrations of complex subjects (which is why I was so happy to work with her on our <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a> article). She has now published the latest in her series of illustrated guides: tackling the complex topic of <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf">Mainframe Modernization</a></p>

<p><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf"><img alt="" src="https://martinfowler.com/fragments/2025/gitanjali-mainframe.png" /></a></p>

<p>In it she illustrates the history and value of mainframes, why modernization is so tricky, and how to tackle the problem by breaking it down into tractable pieces. I love the clarity of her explanations, and smile frequently at her way of enhancing her words with her quirky pictures.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Gergely Orosz on <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m7xd33hi5c2d">social media</a></p>

<blockquote>
  <p>Unpopular opinion:</p>

  <p>Current code review tools just don‚Äôt make much sense for AI-generated code</p>

  <p>When reviewing code I really want to know:</p>

  <ul>
    <li>The prompt made by the dev</li>
    <li>What corrections the other dev made to the code</li>
    <li>Clear marking of code AI-generated not changed by a human</li>
  </ul>
</blockquote>

<p>Some people pushed back saying they don‚Äôt (and shouldn‚Äôt care) whether it was written by a human, generated by an LLM, or copy-pasted from Stack Overflow.</p>

<p>In my view it matters <em>a lot</em> - because of the second vital purpose of code review.</p>

<p>When asked why do code reviews, most people will answer the first vital purpose - quality control. We want to ensure bad code gets blocked before it hits <a href="https://martinfowler.com/articles/branching-patterns.html#mainline">mainline</a>. We do this to avoid bugs and to avoid other quality issues, in particular comprehensibility and ease of change.</p>

<p>But I hear the second vital purpose less often: code review is a mechanism to communicate and educate. If I‚Äôm submitting some sub-standard code, and it gets rejected, I want to know why so that I can improve my programming. Maybe I‚Äôm unaware of some library features, or maybe there‚Äôs some project-specific standards I haven‚Äôt run into yet, or maybe my naming isn‚Äôt as clear as I thought it was. Whatever the reasons, I need to know in order to learn. And my employer needs me to learn, so I can be more effective.</p>

<p>We need to know the writer of the code we review both so we can communicate our better practice to them, but also to know how to improve things. With a human, its a conversation, and perhaps some documentation if we realize we‚Äôve needed to explain things repeatedly. But with an LLM it‚Äôs about how to modify its context, as well as humans learning how to better drive the LLM.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Wondering why I‚Äôve been making a lot of posts like this recently? <a href="https://martinfowler.com/articles/writing-fragments.html">I explain why</a> I‚Äôve been reviving the link blog.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://simonwillison.net/2025/Dec/10/html-tools/">Simon Willison</a> describes how he uses LLMs to build disposable but useful web apps</p>

<blockquote>
  <p>These are the characteristics I have found to be most productive in building tools of this nature:</p>

  <ol>
    <li>A single file: inline JavaScript and CSS in a single HTML file means the least hassle in hosting or distributing them, and crucially means you can copy and paste them out of an LLM response.</li>
    <li>Avoid React, or anything with a build step. The problem with React is that JSX requires a build step, which makes everything massively less convenient. I prompt ‚Äúno react‚Äù and skip that whole rabbit hole entirely.</li>
    <li>Load dependencies from a CDN. The fewer dependencies the better, but if there‚Äôs a well known library that helps solve a problem I‚Äôm happy to load it from CDNjs or jsdelivr or similar.</li>
    <li>Keep them small. A few hundred lines means the maintainability of the code doesn‚Äôt matter too much: any good LLM can read them and understand what they‚Äôre doing, and rewriting them from scratch with help from an LLM takes just a few minutes.</li>
  </ol>
</blockquote>

<p>His repository includes all these tools, together with transcripts of the chats that got the LLMs to build them.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://obie.medium.com/what-happens-when-the-coding-becomes-the-least-interesting-part-of-the-work-ab10c213c660">Obie Fernandez</a>: while many engineers are underwhelmed by AI tools, some senior engineers are finding them really valuable. He feels that senior engineers have an oft-unspoken mindset, which in conjunction with an LLM, enables the LLM to be much more valuable.</p>

<blockquote>
  <p>Levels of abstraction and generalization problems get talked about a lot because they‚Äôre easy to name. But they‚Äôre far from the whole story.</p>

  <p>Other tools show up just as often in real work:</p>

  <ul>
    <li>A sense for blast radius. Knowing which changes are safe to make loudly and which should be quiet and contained.</li>
    <li>A feel for sequencing. Knowing when a technically correct change is still wrong because the system or the team isn‚Äôt ready for it yet.</li>
    <li>An instinct for reversibility. Preferring moves that keep options open, even if they look less elegant in the moment.</li>
    <li>An awareness of social cost. Recognizing when a clever solution will confuse more people than it helps.</li>
    <li>An allergy to false confidence. Spotting places where tests are green but the model is wrong.</li>
  </ul>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://friendlybit.com/python/writing-justhtml-with-coding-agents/">Emil Stenstr√∂m</a> built an HTML5 parser in python using coding agents, using Github Copilot in Agent mode with Claude Sonnet 3.7. He automatically approved most commands. It took him ‚Äúa couple of months on off-hours‚Äù, including at least one restart from scratch. The parser now passes all the tests in html5lib test suite.</p>

<blockquote>
  <p>After writing the parser, I still don‚Äôt know HTML5 properly. The agent wrote it for me. I guided it when it came to API design and corrected bad decisions at the high level, but it did ALL of the gruntwork and wrote all of the code.</p>

  <p>I handled all git commits myself, reviewing code as it went in. I didn‚Äôt understand all the algorithmic choices, but I understood when it didn‚Äôt do the right thing.</p>
</blockquote>

<p>Although he gives an overview of what happens, there‚Äôs not very much information on his workflow and how he interacted with the LLM. There‚Äôs certainly not enough detail here to try to replicate his approach. This is contrast to Simon Willison (above) who has detailed links to his chat transcripts - although they are much smaller tools and I haven‚Äôt looked at them properly to see how useful they are.</p>

<p>One thing that is clear, however, is the vital need for a comprehensive test suite. Much of his work is driven by having that suite as a clear guide for him and the LLM agents.</p>

<blockquote>
  <p>JustHTML is about 3,000 lines of Python with 8,500+ tests passing. I couldn‚Äôt have written it this quickly without the agent.</p>

  <p>But ‚Äúquickly‚Äù doesn‚Äôt mean ‚Äúwithout thinking.‚Äù I spent a lot of time reviewing code, making design decisions, and steering the agent in the right direction. The agent did the typing; I did the thinking.</p>
</blockquote>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†</p>

<p>Then Simon Willison <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">ported the library to JavaScript</a>:</p>

<blockquote>
  <p>Time elapsed from project idea to finished library: about 4 hours, during which I also bought and decorated a Christmas tree with family and watched the latest Knives Out movie.</p>
</blockquote>

<p>One of his lessons:</p>

<blockquote>
  <p>If you can reduce a problem to a robust test suite you can set a coding agent loop loose on it with a high degree of confidence that it will eventually succeed. I called this <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">designing the agentic loop</a> a few months ago. I think it‚Äôs the key skill to unlocking the potential of LLMs for complex tasks.</p>
</blockquote>

<p>Our experience at Thoughtworks backs this up. We‚Äôve been doing a fair bit of work recently in legacy modernization (mainframe and otherwise) using AI to migrate substantial software systems. Having a robust test suite is necessary (but not sufficient) to making this work. I hope to share my colleagues‚Äô experiences on this in the coming months.</p>

<p>But before I leave Willison‚Äôs post, I should highlight his final open questions on the legalities, ethics, and effectiveness of all this - they are well-worth contemplating.</p>

---

### [Writing Fragments](https://martinfowler.com/articles/writing-fragments.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>If you‚Äôre a regular reader of my site, you‚Äôll have noticed that in the
last few months I‚Äôve been making a <a href="https://martinfowler.com/articles/20251204-frags.html">number</a> of ‚Äúfragments‚Äù <a href="https://martinfowler.com/articles/2025-11-19-frags.html">posts</a>. Such a post
is a short post with a bunch of little, unconnected segments. These are
usually a reference to something I‚Äôve found on the web, sometimes a small
thought of my own.</p>

<p>A few years ago, I wouldn‚Äôt have covered these topics with posts on my
own site. Instead I would use Twitter, either retweeting someone else‚Äôs
point, or just highlighting something I‚Äôd found. But since the Muskover,
Twitter has effectively died. I‚Äôm not saying that due to any technical
issues with the site, which has mostly just been fine, nor directly due to
any of the policy changes there. The point is that lots of people have left, so that
the audience I would have reached with Twitter is now fragmented. Some
remain on X, but I see more activity on LinkedIn. There‚Äôs also Fediverse/Mastodon
and Bluesky.</p>

<p>What this means for short posts is that I can no longer just post in one
place. When I announce new articles on martinfowler.com, I announce now on
four social media sites (X, LinkedIn, Fediverse, and Bluesky). It makes
sense to do this, but I don‚Äôt want to go through all this hassle for the
kind of micro-post that Twitter served so well.</p>

<p>So I‚Äôve started to batch them up. When I see something interesting, I
make a note. When I have enough notes, I post a fragments post. Initially I
did this in a rather ad-hoc way, just using the same mechanisms I use for
most articles, but last week I started to put in some more deliberate
mechanisms into the site. (If you‚Äôre observant, you‚Äôll spot that in the URLs.)</p>

<p>One benefit of all of this, at least in my book, is that it means my material is
now fully visible in RSS. I‚Äôm probably showing my age, but I‚Äôm a big fan of RSS
(or in my case, strictly Atom) feeds. I miss the feel of the heyday of the
‚Äúblogosphere‚Äù before it got steamrolled by social media, and these fragment
posts are, of course, just the same as the link blogs from that era. I still use my
RSS reader every day to keep up with writers I like. (I‚Äôm pleased that Substack
makes its content available via RSS.) It bothered me a bit that my micro-founts
of Twitter knowledge weren‚Äôt visible on RSS, but was too lazy to do something
about it. Now I don‚Äôt need to - the fragments are available in my RSS feed.</p>

---

### [Fragments Dec 11](https://martinfowler.com/articles/2025-12-11-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html?utm_source=substack&amp;utm_medium=email">Why does AI write like‚Ä¶ that</a> (NYT, gift link). Sam Kriss delves into the quiet hum of AI writing. AI‚Äôs work is not compelling prose: it‚Äôs phantom text, ghostly scribblings, a spectre woven into our communal tapestry.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://coding-is-like-cooking.info/2025/12/test-desiderata-2-0/">Emily Bache</a> has written a set of Test Desiderata, building on some earlier writing from Kent Beck. She lists the characteristics of good tests, and how they support her four ‚Äúmacro desiderata‚Äù - the properties of a sound test suite</p>

<blockquote>
  <ul>
    <li>Predict success in production</li>
    <li>Fast to get feedback</li>
    <li>Support ongoing code design change</li>
    <li>Low total cost of ownership</li>
  </ul>
</blockquote>

<p>She also has a great list of other writers‚Äô lists of good test characteristics.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.techpolicy.press/the-eus-fine-against-x-is-not-about-speech-or-censorship/">Daphne Keller</a> explains that the EUs fines on X aren‚Äôt about free speech.</p>

<blockquote>
  <p>There are three charges against X, which all stem from a multi-year investigation that was launched in 2023. One is about verification ‚Äî X‚Äôs blue checkmarks on user accounts ‚Äî and two are about transparency. These charges have nothing to do with what content is on X, or what user speech the platform should or should not allow.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington">Cory Doctorow</a> The Reverse-Centaur‚Äôs Guide to Criticizing AI</p>

<blockquote>
  <p>Start with what a reverse centaur is. In automation theory, a ‚Äúcentaur‚Äù is a person who is assisted by a machine. ‚Ä¶ And obviously, a reverse centaur is machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.</p>

  <p>Like an Amazon delivery driver‚Ä¶ the van can‚Äôt drive itself and can‚Äôt get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.</p>
</blockquote>

---

### [Fragments Dec 4](https://martinfowler.com/articles/20251204-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://blog.robbowley.net/2025/12/04/ai-is-still-making-code-worse-a-new-cmu-study-confirms/">Rob Bowley</a> summarizes a study from Carnegie Mellon looking on the impact of AI on a bunch of open-source software projects. Like any such study, we shouldn‚Äôt take its results as definitive, but there seems enough there to make it a handy data point. The key point is that the AI code probably reduced the quality of the code base - at least if static code analysis can be trusted to determine quality. And perhaps some worrying second-order effects</p>

<blockquote>
  <p>This study shows more than 800 popular GitHub projects with code quality degrading after adopting AI tools. It‚Äôs hard not to see a form of context collapse playing out in real time. If the public code that future models learn from is becoming more complex and less maintainable, there‚Äôs a real risk that newer models will reinforce and amplify those trends, producing even worse code over time.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Rob‚Äôs post is typical of much of the thoughtful writing on AI. We can see its short-term benefits, but worry about its long-term impact. But on a much deeper note is this lovely story from <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">Jim Highsmith</a>. Jim has turned 0x50, and has spent the last decade fighting Parkinson‚Äôs disease. To help him battle it he has two AI assisted allies.</p>

<blockquote>
  <p>Between my neural implants and Byron‚Äôs digital guidance, I now collaborate with two adaptive systems: one for motion, one for thought. Neither replaces me. Both extend me.</p>
</blockquote>

<p><strong>If you read anything on AI this week, <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">make it be this</a>.</strong> It offers a positive harbinger for our future and opens my mind to a whole different perspective of the role of AI in it</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Anthropic recently announced that it disrupted a Chinese state-sponsored operation abusing Claude Code. Jim Gumbley looks at the core lesson to learn from this, that we have to understand the serious risk of  <a href="https://www.thoughtworks.com/insights/blog/security/anthropic-ai-espionage-disclosure-signal-from-noise?utm_source=linkedin&amp;utm_medium=social-organic&amp;utm_campaign=dt_bs_rp-in-clt_tech_thought_leaders_2025-11&amp;gh_src=d1fe17bc1us">AI Jailbreaking</a></p>

<blockquote>
  <p>New AI tools are able to analyze your attack surface at the next level of granularity. As a business leader, that means you now have two options: wait for someone else to run AI-assisted vulnerability detection against your attack surface, or run it yourself first.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>There‚Äôs plenty of claims that AI Vibe Coding can replace software developers, something that folks like me (perhaps with a bias) think unlikely. <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m75kmb7bh22i">Gergely Orosz</a> shared this tidbit</p>

<blockquote>
  <p>Talked with an exec at a tech company who is obsessed with AI and has been for 3 years. Not a developer but company makes software. Uses AI for everything, vibe codes ideas.</p>

  <p>Here‚Äôs the kicker:</p>

  <p>Has a team of several devs to implement his vibe coded prototypes to sg workable</p>
</blockquote>

<p>I‚Äôd love to hear more about this (and similar stories)</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/">Nick Radcliffe</a> writes about a month of using AI</p>

<blockquote>
  <p>I spent a solid month ‚Äúpair programming‚Äù with Claude Code, trying to suspend disbelief and adopt a this-will-be-productive mindset. More specifically, I got Claude to write well over 99% of the code produced during the month. I found the experience infuriating, unpleasant, and stressful before even worrying about its energy impact. Ideally, I would prefer not to do it again for at least a year or two. The only problem with that is that it ‚Äúworked‚Äù.</p>
</blockquote>

<p>He stresses that his approach is the ‚Äúpolar opposite‚Äù of Vibe Coding. The post is long, and rambles a bit, but is worthwhile because he talks in detail about his workflow and how he uses the tool. Such posts are important so we can learn the nitty-gritty of how our programming habits are changing.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Along similar lines is a post of <a href="https://brianchambers.substack.com/p/chamber-of-tech-secrets-55-issue">Brian Chambers</a> on his workflow, that he calls Issue-Driven Development (and yes, I‚Äôm also sick of the ‚Äúsomething-driven‚Äù phraseology). As with much of the better stuff I‚Äôve heard about AI assisted work, it‚Äôs all about carefully managing the context window, ensuring the AI is focused on the right things and not distracted by textual squirrels.</p>

---

### [Fragments Nov 19](https://martinfowler.com/articles/2025-11-19-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><img alt="radar image" src="https://martinfowler.com/articles/images/frags-2025-11/radar.png" width="300px" /></p>

<p>I‚Äôve been on the road in Europe for the last couple of weeks, and while I was there Thoughtworks released <a href="https://www.thoughtworks.com/radar">volume 33 of our Technology Radar</a>. Again it‚Äôs dominated by the AI wave, with lots of blips capturing our explorations of how to use LLMs and similar technology. ‚ÄúAgents‚Äù are the big thing these days but we‚Äôre also seeing growing movements in infrastructure orchestration, coding workflows - and the inevitable antipatterns. Many thanks to my colleagues for putting this together again.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Gergely and I recording the podcast" src="https://martinfowler.com/articles/images/frags-2025-11/podcast.png" width="400px" /></p>

<p>My trip to Europe started in Amsterdam, for a Thoughtworks event for a few of our clients there. Since I was in that lovely city, I got in touch with Gergely Orosz, host of <a href="https://www.pragmaticengineer.com/">The Pragmatic Engineer</a>, and he arranged to <a href="https://www.youtube.com/watch?v=CQmI4XKTa0U">record a podcast</a> with me. No surprise that AI was front-and-center of the conversation, as I said it was the biggest shift I‚Äôd seen in programming during my career, comparable only to the shift to high-level languages, which even I am not old enough to have experienced. It was a fun chat and I really enjoyed myself. Gergely later joined myself James Lewis and Giles Edwards-Alexander at the Thoughtworks event the next day.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>My travels also took me to N√ºremberg, where I attended an internal conference for Siemens on the future of software architecture. When we think of technology, it‚Äôs easy to focus on the Faangs of Silicon Valley, but Siemens have a huge workforce of software developers working on heavy engineering systems like trains and factory automation. It was good to hear them talk about federated architectures, data mesh, and their use of AI.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Kent's graph of options vs features" src="https://martinfowler.com/articles/images/frags-2025-11/features.webp" width="400px" /></p>

<p>I‚Äôve often used pseudo-graphs to help explain why <a href="https://martinfowler.com/articles/is-quality-worth-cost.html">high quality software is cheaper</a>. This time, <a href="https://tidyfirst.substack.com/p/why-does-development-slow">Kent Beck</a> creates a unique perspective to this chart, dispensing with the temporal axis to help think in terms of optionality.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Heavy Cardboard banner" src="https://martinfowler.com/articles/images/frags-2025-11/hc.jpg" width="400px" /></p>

<p>And in another life, Edward has finally finished the great migration of the Heavy Cardboard studio and returns to the tubes with <a href="https://www.youtube.com/live/e8juRvm9h0c">our first game in the new digs</a>. (No surprise that it‚Äôs Age of Steam.)</p>

---

## HackerNews

### [Show HN: isometric.nyc ‚Äì giant isometric pixel art map of NYC](https://cannoneyed.com/isometric-nyc/)

**ÂàÜÊï∞**: 1297 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46721802)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Bugs Apple loves](https://www.bugsappleloves.com)

**ÂàÜÊï∞**: 1054 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46727587)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [European Alternatives](https://european-alternatives.eu)

**ÂàÜÊï∞**: 759 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46731976)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Proof of Corn](https://proofofcorn.com/)

**ÂàÜÊï∞**: 462 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46735511)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [MS confirms it will give the FBI your Windows PC data encryption key if asked](https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare)

**ÂàÜÊï∞**: 435 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46743154)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Unrolling the Codex agent loop](https://openai.com/index/unrolling-the-codex-agent-loop/)

**ÂàÜÊï∞**: 429 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46737630)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Doing gigabit Ethernet over my British phone wires](https://thehftguy.com/2026/01/22/doing-gigabit-ethernet-over-my-british-phone-wires/)

**ÂàÜÊï∞**: 417 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46742362)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How I estimate work](https://www.seangoedecke.com/how-i-estimate-work/)

**ÂàÜÊï∞**: 398 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46742389)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Gas Town's agent patterns, design bottlenecks, and vibecoding at scale](https://maggieappleton.com/gastown)

**ÂàÜÊï∞**: 387 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46734302)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [BirdyChat becomes first European chat app that is interoperable with WhatsApp](https://www.birdy.chat/blog/first-to-interoperate-with-whatsapp)

**ÂàÜÊï∞**: 383 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746476)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Booting from a vinyl record (2020)](https://boginjr.com/it/sw/dev/vinyl-boot/)

**ÂàÜÊï∞**: 343 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46730885)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Comma openpilot ‚Äì Open source driver-assistance](https://comma.ai)

**ÂàÜÊï∞**: 335 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46740029)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [eBay explicitly bans AI "buy for me" agents in user agreement update](https://www.valueaddedresource.net/ebay-bans-ai-agents-updates-arbitration-user-agreement-feb-2026/)

**ÂàÜÊï∞**: 332 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46711574)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Scaling PostgreSQL to power 800M ChatGPT users](https://openai.com/index/scaling-postgresql/)

**ÂàÜÊï∞**: 328 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46725300)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Claude Code's new hidden feature: Swarms](https://twitter.com/NicerInPerson/status/2014989679796347375)

**ÂàÜÊï∞**: 287 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46743908)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [When employees feel slighted, they work less](https://penntoday.upenn.edu/news/penn-wharton-when-employees-feel-slighted-they-work-less)

**ÂàÜÊï∞**: 285 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46691108)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Radicle: The Sovereign Forge](https://radicle.xyz)

**ÂàÜÊï∞**: 278 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46732213)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Internet Archive's Storage](https://blog.dshr.org/2026/01/internet-archives-storage.html)

**ÂàÜÊï∞**: 275 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46694618)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Whosthere: A LAN discovery tool with a modern TUI, written in Go](https://github.com/ramonvermeulen/whosthere)

**ÂàÜÊï∞**: 270 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46731432)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The tech monoculture is finally breaking](http://www.jasonwillems.com/technology/2025/12/17/Tech-Is-Fun-Again/)

**ÂàÜÊï∞**: 235 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46733624)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Updates to our web search products and  Programmable Search Engine capabilities](https://programmablesearchengine.googleblog.com/2026/01/updates-to-our-web-search-products.html)

**ÂàÜÊï∞**: 226 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46730436)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Banned C++ features in Chromium](https://chromium.googlesource.com/chromium/src/+/main/styleguide/c++/c++-features.md)

**ÂàÜÊï∞**: 225 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46737447)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I added a Bluesky comment section to my blog](https://micahcantor.com/blog/bluesky-comment-section.html)

**ÂàÜÊï∞**: 203 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747366)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Replacing Protobuf with Rust](https://pgdog.dev/blog/replace-protobuf-with-rust)

**ÂàÜÊï∞**: 180 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46730214)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Tao Te Ching ‚Äì Translated by Ursula K. Le Guin](https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md)

**ÂàÜÊï∞**: 176 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46745233)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I Like GitLab](https://www.whileforloop.com/en/blog/2026/01/21/i-like-gitlab/)

**ÂàÜÊï∞**: 173 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46742432)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Route leak incident on January 22, 2026](https://blog.cloudflare.com/route-leak-incident-january-22-2026/)

**ÂàÜÊï∞**: 161 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46735489)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The lost art of XML](https://marcosmagueta.com/blog/the-lost-art-of-xml/)

**ÂàÜÊï∞**: 157 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46728157)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Many Small Queries Are Efficient in SQLite](https://www.sqlite.org/np1queryprob.html)

**ÂàÜÊï∞**: 153 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46742635)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [‚ÄúLet people help‚Äù ‚Äì Advice that made a big difference to a grieving widow](https://www.npr.org/2026/01/20/nx-s1-5683170/let-them-the-small-bit-of-advice-that-made-a-big-difference-to-a-grieving-widow)

**ÂàÜÊï∞**: 150 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46740748)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Anthropic Economic Index report: economic primitives](https://www.anthropic.com/research/anthropic-economic-index-january-2026-report)

**ÂàÜÊï∞**: 134 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46725632)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [December in Servo: multiple windows, proxy support, better caching, and more](https://servo.org/blog/2026/01/23/december-in-servo/)

**ÂàÜÊï∞**: 131 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46745259)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Raspberry Pi Drag Race: Pi 1 to Pi 5 ‚Äì Performance Comparison](https://the-diy-life.com/raspberry-pi-drag-race-pi-1-to-pi-5-performance-comparison/)

**ÂàÜÊï∞**: 118 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46745922)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Postmortem: Our first VLEO satellite mission (with imagery and flight data)](https://albedo.com/post/clarity-1-what-worked-and-where-we-go-next)

**ÂàÜÊï∞**: 117 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747119)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Poland's energy grid was targeted by never-before-seen wiper malware](https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/)

**ÂàÜÊï∞**: 117 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747827)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Europe wants to end its dangerous reliance on US internet technology](https://theconversation.com/europe-wants-to-end-its-dangerous-reliance-on-us-internet-technology-274042)

**ÂàÜÊï∞**: 115 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46748771)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Extracting verified C++ from the Rocq theorem prover at Bloomberg](https://bloomberg.github.io/crane/)

**ÂàÜÊï∞**: 114 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46689631)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Bye Bye Gmail](https://m24tom.com/bye-bye-gmail/show)

**ÂàÜÊï∞**: 108 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746946)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Notes on the Intel 8086 processor's arithmetic-logic unit](https://www.righto.com/2026/01/notes-on-intel-8086-processors.html)

**ÂàÜÊï∞**: 104 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46735133)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nobody likes lag: How to make low-latency dev sandboxes](https://www.compyle.ai/blog/nobody-likes-lag/)

**ÂàÜÊï∞**: 97 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46735139)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Are we all plagiarists now?](https://www.economist.com/culture/2026/01/22/are-we-all-plagiarists-now)

**ÂàÜÊï∞**: 96 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46744968)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [80386 Multiplication and Division](https://nand2mario.github.io/posts/2026/80386_multiplication_and_division/)

**ÂàÜÊï∞**: 93 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46741482)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Killing the ISP Appliance: An eBPF/XDP Approach to Distributed BNG](https://markgascoyne.co.uk/posts/ebpf-bng/)

**ÂàÜÊï∞**: 91 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46735179)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Maze Algorithms (2017)](http://www.jamisbuck.org/mazes/)

**ÂàÜÊï∞**: 85 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46737202)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Waypoint-1: Real-Time Interactive Video Diffusion from Overworld](https://huggingface.co/blog/waypoint-1)

**ÂàÜÊï∞**: 84 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46733301)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Amazon braces for another major round of layoffs, 14,000 jobs at risk](https://mynorthwest.com/local/amazon-layoffs-14000-jobs-at-risk/4192118)

**ÂàÜÊï∞**: 76 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46748603)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Memory layout in Zig with formulas](https://raymondtana.github.io/math/programming/2026/01/23/zig-alignment-and-sizing.html)

**ÂàÜÊï∞**: 74 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46744647)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Agent orchestration for the timid](https://substack.com/inbox/post/185649875)

**ÂàÜÊï∞**: 65 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746681)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [XHTML Club](https://xhtml.club/)

**ÂàÜÊï∞**: 59 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46743219)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [JSON-render: LLM-based JSON-to-UI tool](https://json-render.dev/)

**ÂàÜÊï∞**: 55 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746570)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Small Kafka: Tansu and SQLite on a free t3.micro](https://blog.tansu.io/articles/broker-aws-free-tier)

**ÂàÜÊï∞**: 54 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46690779)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Management Time: Who's Got the Monkey? [pdf]](https://www.med.unc.edu/uncaims/wp-content/uploads/sites/764/2014/03/Oncken-_-Wass-Who_s-Got-the-Monkey.pdf)

**ÂàÜÊï∞**: 50 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46688133)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Modetc: Move your dotfiles from kernel space](https://maxwell.eurofusion.eu/git/rnhmjoj/modetc)

**ÂàÜÊï∞**: 44 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46741929)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Shared Claude: A website controlled by the public](https://sharedclaude.com/)

**ÂàÜÊï∞**: 40 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46741923)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Concatative Language XY](http://www.nsl.com/k/xy/xy.txt)

**ÂàÜÊï∞**: 38 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746517)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Understanding Rust Closures](https://antoine.vandecreme.net/blog/rust-closures/)

**ÂàÜÊï∞**: 35 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746266)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [JVIC: New web-based Commodore VIC 20 emulator](https://vic20.games/#/basic/24k)

**ÂàÜÊï∞**: 35 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46742828)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [6 Years Building Video Players. 9B Requests. Starting Over](https://www.mux.com/blog/6-years-building-video-players-9-billion-requests-starting-over)

**ÂàÜÊï∞**: 35 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46689342)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I don't write code anymore ‚Äì I sculpt it](https://www.jerpint.io/blog/2026-01-24-i-dont-write-code-anymore-i-sculpt-it/)

**ÂàÜÊï∞**: 33 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46748077)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Writers Came at Night](https://www.metropolitanreview.org/p/the-writers-came-at-night)

**ÂàÜÊï∞**: 29 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747777)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Kept and the Killed (2022)](https://publicdomainreview.org/essay/the-kept-and-the-killed/)

**ÂàÜÊï∞**: 28 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46744569)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [C++26 Reflection loves QRangeModel](https://www.qt.io/blog/c26-reflection-qrangemodel)

**ÂàÜÊï∞**: 28 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46688191)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Open-source Figma design to code](https://github.com/vibeflowing-inc/vibe_figma)

**ÂàÜÊï∞**: 27 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46741472)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Microservices for the Benefits, Not the Hustle (2023)](https://wolfoliver.medium.com/the-purposes-of-microservices-4e5f373f4ea3)

**ÂàÜÊï∞**: 24 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46705134)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: StormWatch ‚Äì Weather emergency dashboard with prep checklists](https://jeisey.github.io/stormwatch/)

**ÂàÜÊï∞**: 22 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746900)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [We didn't ask for 'smart' cars ‚Äì so why are we getting them?](https://www.autocar.co.uk/opinion/new-cars/we-didn%E2%80%99t-ask-smart-cars-so-why-are-we-getting-them)

**ÂàÜÊï∞**: 22 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46748071)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [We X-Rayed a Suspicious FTDI USB Cable](https://eclypsium.com/blog/xray-counterfeit-usb-cable/)

**ÂàÜÊï∞**: 21 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46749053)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [ollama launch](https://ollama.com/blog/launch)

**ÂàÜÊï∞**: 20 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46745138)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Language may rely less on complex grammar than previously thought: study](https://scitechdaily.com/have-we-been-wrong-about-language-for-70-years-new-study-challenges-long-held-theory/)

**ÂàÜÊï∞**: 19 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46740705)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Why Does Destroying Resources via TF Suck?](https://newsletter.masterpoint.io/p/why-does-destroying-resources-via-tf-suck)

**ÂàÜÊï∞**: 19 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747022)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The future of work when work is meaningless](https://letters.thedankoe.com/p/the-future-of-work-when-work-is-meaningless)

**ÂàÜÊï∞**: 18 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46745150)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Objective-S](https://objective.st/)

**ÂàÜÊï∞**: 17 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46742618)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [MCP is the New GraphQL](https://nadeeshacabral.com/posts/mcp-is-the-new-graphql/)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46748404)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Hung by a thread](https://campedersen.com/rayon-mutex-deadlock)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46746096)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Propositions about the New Romanticism](https://www.honest-broker.com/p/25-propositions-about-the-new-romanticism)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747078)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Ed tech is profitable. It is also mostly useless](https://www.economist.com/united-states/2026/01/22/ed-tech-is-profitable-it-is-also-mostly-useless)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46747851)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [KAOS ‚Äì The Kubernetes Agent Orchestration System](https://github.com/axsaucedo/kaos)

**ÂàÜÊï∞**: 12 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46688521)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Possessed Machines: Dostoevsky's Demons and the Coming AGI Catastrophe](https://possessedmachines.com/)

**ÂàÜÊï∞**: 12 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46743466)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Should I Form an LLC or Register a Domain First?](https://www.domainregistry.com/blog/form-llc-or-register-domain-first/)

**ÂàÜÊï∞**: 11 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46745831)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

## LLM Infrastructure

### [v0.14.1](https://github.com/vllm-project/vllm/releases/tag/v0.14.1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>This is a patch release on top of <code>v0.14.0</code> to address a few security and memory leak fixes.</p>

---

### [v0.14.0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h2>Highlights</h2>
<p>This release features approximately 660 commits from 251 contributors (86 new contributors).</p>
<p><strong>Breaking Changes:</strong></p>
<ul>
<li><strong>Async scheduling is now enabled by default</strong> - Users who experience issues can disable with <code>--no-async-scheduling</code>.
<ul>
<li>Excludes some not-yet-supported configurations: pipeline parallel, CPU backend, non-MTP/Eagle spec decoding.</li>
</ul>
</li>
<li><strong>PyTorch 2.9.1</strong> is now required and the default wheel is compiled against cu129.</li>
<li>Deprecated quantization schemes have been removed (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31688">#31688</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31285">#31285</a>).</li>
<li>When using speculative decoding, unsupported sampling parameters will fail rather than being silently ignored (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31982">#31982</a>).</li>
</ul>
<p><strong>Key Improvements:</strong></p>
<ul>
<li><strong>Async scheduling enabled by default</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27614">#27614</a>): Overlaps engine core scheduling with GPU execution, improving throughput without user configuration. Now also works with speculative decoding (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31998">#31998</a>) and structured outputs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29821">#29821</a>).</li>
<li><strong>gRPC server entrypoint</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30190">#30190</a>): Alternative to REST API with binary protocol, HTTP/2 multiplexing.</li>
<li><strong><code>--max-model-len auto</code></strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29431">#29431</a>): Automatically fits context length to available GPU memory, eliminating OOM startup failures.</li>
<li><strong>Model inspection view</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29450">#29450</a>): View the modules, attention backends, and quantization of your model in vLLM by specifying <code>VLLM_LOG_MODEL_INSPECTION=1</code> or by simply printing the <code>LLM</code> object.</li>
<li><strong>Model Runner V2 enhancements</strong>: UVA block tables (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31965">#31965</a>), M-RoPE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32143">#32143</a>), <code>logit_bias</code>/<code>allowed_token_ids</code>/<code>min_tokens</code> support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32163">#32163</a>).
<ul>
<li>Please note that Model Runner V2 is still experimental and disabled by default.</li>
</ul>
</li>
</ul>
<h3>Model Support</h3>
<p><strong>New Model Architectures:</strong></p>
<ul>
<li>Grok-2 with tiktoken tokenizer (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31847">#31847</a>)</li>
<li>LFM2-VL vision-language model (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31758">#31758</a>)</li>
<li>MiMo-V2-Flash (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30836">#30836</a>)</li>
<li>openPangu MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28775">#28775</a>)</li>
<li>IQuestCoder (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31575">#31575</a>)</li>
<li>Nemotron Parse 1.1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30864">#30864</a>)</li>
<li>GLM-ASR audio (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31436">#31436</a>)</li>
<li>Isaac vision model v0.1/v0.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28367">#28367</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31550">#31550</a>)</li>
<li>Kanana-1.5-v-3b-instruct (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29384">#29384</a>)</li>
<li>K-EXAONE-236B-A23B MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31621">#31621</a>)</li>
</ul>
<p><strong>LoRA Support Expansion:</strong></p>
<ul>
<li>Multimodal tower/connector LoRA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26674">#26674</a>): LLaVA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31513">#31513</a>), BLIP2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31620">#31620</a>), PaliGemma (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31656">#31656</a>), Pixtral (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31724">#31724</a>), DotsOCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31825">#31825</a>), GLM4-V (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31652">#31652</a>)</li>
<li>DeepSeek-OCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31569">#31569</a>), Qwen3-Next (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31719">#31719</a>), NemotronH (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31539">#31539</a>), PLaMo 2/3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31322">#31322</a>)</li>
<li>Vision LoRA mm_processor_cache support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31927">#31927</a>)</li>
<li>MoE expert base_layer loading (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31104">#31104</a>)</li>
</ul>
<p><strong>Model Enhancements:</strong></p>
<ul>
<li>Qwen3-VL as reranker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31890">#31890</a>)</li>
<li>DeepSeek v3.2 chat prefix completion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31147">#31147</a>)</li>
<li>GLM-4.5/GLM-4.7 <code>enable_thinking: false</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31788">#31788</a>)</li>
<li>Ernie4.5-VL video timestamps (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31274">#31274</a>)</li>
<li>Score template expansion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31335">#31335</a>)</li>
<li>LLaMa4 vision encoder compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30709">#30709</a>)</li>
<li>NemotronH quantized attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31898">#31898</a>)</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Async scheduling default</strong> with spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27614">#27614</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31998">#31998</a>) and structured outputs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29821">#29821</a>)</li>
<li><strong>Hybrid allocator + KV connector</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30166">#30166</a>) with multiple KV cache groups (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31707">#31707</a>)</li>
<li>Triton attention: encoder-only/cross attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31406">#31406</a>), cross-layer blocks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30687">#30687</a>)</li>
<li>Mamba2 prefix cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28047">#28047</a>)</li>
<li>Batch invariant LoRA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30097">#30097</a>)</li>
<li>LoRA name in BlockStored for KV-cache reconstruction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27577">#27577</a>)</li>
<li>Request ID collision prevention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27987">#27987</a>)</li>
<li>Dense model DP without overhead (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30739">#30739</a>)</li>
<li>Async + spec decode penalties/bad_words (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30495">#30495</a>)</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<p><strong>CUTLASS MoE Optimizations:</strong></p>
<ul>
<li>2.9% throughput + 10.8% TTFT via fill(0) optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31754">#31754</a>)</li>
<li>5.3% throughput + 2.2% TTFT via problem size calculation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31830">#31830</a>)</li>
<li>Fused SiLU+Mul+Quant for NVFP4 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31832">#31832</a>)</li>
<li>NVFP4 stride fusion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31837">#31837</a>)</li>
</ul>
<p><strong>Other Performance:</strong></p>
<ul>
<li>GDN attention decode speedup (Qwen3-Next) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31722">#31722</a>)</li>
<li>Fused RoPE + MLA KV-cache write (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25774">#25774</a>)</li>
<li>Sliding window attention optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31984">#31984</a>)</li>
<li>FlashInfer DeepGEMM swapAB SM90 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29213">#29213</a>)</li>
<li>Unpermute-aware fused MoE + small-batch fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29354">#29354</a>)</li>
<li>GDN Attention blocking copy removal (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31167">#31167</a>)</li>
<li>FusedMoE LoRA small rank performance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32019">#32019</a>)</li>
<li>EPLB numpy optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29499">#29499</a>)</li>
<li>FlashInfer rotary for DeepSeek (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30729">#30729</a>)</li>
<li>Vectorized activations (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29512">#29512</a>)</li>
<li>NUMA interleaved memory (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30800">#30800</a>)</li>
<li>Async spec decode logprobs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31336">#31336</a>)</li>
</ul>
<p><strong>Hardware Configs:</strong></p>
<ul>
<li>SM103 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30705">#30705</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31150">#31150</a>)</li>
<li>B300 Blackwell MoE configs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30629">#30629</a>)</li>
<li>Qwen3-Next FP8 CUTLASS configs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29553">#29553</a>)</li>
<li>Qwen3Moe B200 Triton configs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31448">#31448</a>)</li>
<li>GLM-4.5/4.6 RTX Pro 6000 kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31407">#31407</a>)</li>
<li>MiniMax-M2/M2.1 QKNorm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31493">#31493</a>)</li>
<li>NVFP4 small batch tuning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30897">#30897</a>)</li>
</ul>
<p><strong>Platform:</strong></p>
<ul>
<li>ROCm: AITER RMSNorm fusion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26575">#26575</a>), MTP for AITER MLA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28624">#28624</a>), moriio connector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29304">#29304</a>), xgrammar upstream (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31327">#31327</a>)</li>
<li>XPU: FP8 streaming quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30944">#30944</a>), custom workers (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30935">#30935</a>)</li>
<li>CPU: Head sizes 80/112 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31968">#31968</a>), async disabled by default (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31525">#31525</a>), LoRA MoE CPU pinning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31317">#31317</a>)</li>
<li>TPU: tpu-inference path (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30808">#30808</a>), Sophgo docs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30949">#30949</a>)</li>
</ul>
<h3>Large Scale Serving</h3>
<ul>
<li><strong>XBO</strong> (Extended Dual-Batch Overlap) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30120">#30120</a>)</li>
<li><strong>NIXL asymmetric TP</strong> (P &gt; D tensor-parallel-size) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27274">#27274</a>)</li>
<li>NIXL heterogeneous BlockSize/kv_layout (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30275">#30275</a>)</li>
<li>Cross-layers KV layout for MultiConnector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30761">#30761</a>)</li>
<li>Mooncake protocol expansion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30133">#30133</a>)</li>
<li>LMCache KV cache registration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31397">#31397</a>)</li>
<li>EPLB default all2all backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30559">#30559</a>)</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>Marlin for Turing (sm75)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29901">#29901</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31000">#31000</a>)</li>
<li><strong>Quark int4-fp8 w4a8 MoE</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30071">#30071</a>)</li>
<li><strong>MXFP4 W4A16 dense models</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31926">#31926</a>)</li>
<li><strong>ModelOpt FP8 variants</strong> (FP8_PER_CHANNEL_PER_TOKEN, FP8_PB_WO) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30957">#30957</a>)</li>
<li>ModelOpt KV cache quantization update (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31895">#31895</a>)</li>
<li>NVFP4 Marlin for NVFP4A16 MoEs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30881">#30881</a>)</li>
<li>Static quant all group shapes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30833">#30833</a>)</li>
<li>Default MXFP4 LoRA backend: Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30598">#30598</a>)</li>
<li>compressed-tensors 0.13.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30799">#30799</a>)</li>
</ul>
<h3>API &amp; Frontend</h3>
<p><strong>New Features:</strong></p>
<ul>
<li>gRPC server (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30190">#30190</a>)</li>
<li><code>--max-model-len auto</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29431">#29431</a>)</li>
<li>Model inspection view (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29450">#29450</a>)</li>
<li>Offline FastAPI docs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30184">#30184</a>)</li>
<li><code>attention_config</code> in LLM() (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30710">#30710</a>)</li>
<li>MFU metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30738">#30738</a>)</li>
<li>Iteration logging + NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31193">#31193</a>)</li>
<li><code>reasoning_effort</code> parameter (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31956">#31956</a>)</li>
</ul>
<p><strong>Tool Calling:</strong></p>
<ul>
<li>FunctionGemma parser (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31218">#31218</a>)</li>
<li>GLM-4.7 parser (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30876">#30876</a>)</li>
<li>Kimi K2 update (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31207">#31207</a>)</li>
</ul>
<p><strong>CLI:</strong></p>
<ul>
<li><code>-ep</code> for <code>--enable-expert-parallel</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30890">#30890</a>)</li>
<li>Complete help messages (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31226">#31226</a>)</li>
<li>Bench serve auto-discovery + <code>--input-len</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30816">#30816</a>)</li>
<li>Spec decode acceptance stats (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31739">#31739</a>)</li>
<li><code>--enable-log-deltas</code> (renamed) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32020">#32020</a>)</li>
<li><code>--default-chat-template-kwargs</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31343">#31343</a>)</li>
</ul>
<p><strong>API:</strong></p>
<ul>
<li><code>/server_info</code> env info (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31899">#31899</a>)</li>
<li>MCP streaming in Responses API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31761">#31761</a>)</li>
<li><code>/embeddings</code> <code>continue_final_message</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31497">#31497</a>)</li>
<li>Reranking score templates (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30550">#30550</a>)</li>
<li>Chat template warmup (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30700">#30700</a>)</li>
<li>Configurable handshake timeout (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27444">#27444</a>)</li>
<li>Better 500 errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20610">#20610</a>)</li>
<li>Worker init logging (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29493">#29493</a>)</li>
<li>Bench error reporting (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31808">#31808</a>)</li>
<li>Corrupted video recovery (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29197">#29197</a>)</li>
<li>Spec-decode param validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31982">#31982</a>)</li>
<li>Validation error metadata (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30134">#30134</a>)</li>
</ul>
<h3>Security</h3>
<ul>
<li>Prevent token leaks in crash logs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30751">#30751</a>)</li>
<li><code>weights_only=True</code> in torch.load (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32045">#32045</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><strong>PyTorch 2.9.1</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28495">#28495</a>)</li>
<li>compressed-tensors 0.13.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30799">#30799</a>)</li>
<li>CUDA 13 LMCache/NIXL in Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30913">#30913</a>)</li>
<li>Configurable NVSHMEM version (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30732">#30732</a>)</li>
</ul>
<h3>Bug Fixes (User-Facing)</h3>
<ul>
<li>Invalid UTF-8 tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28874">#28874</a>)</li>
<li>CPU RoPE gibberish with <code>--enforce-eager</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31643">#31643</a>)</li>
<li>Tool call streaming finish chunk (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31438">#31438</a>)</li>
<li>Encoder cache leak CPU scheduling stuck (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31857">#31857</a>)</li>
<li>Engine crash: tools + response_format (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32127">#32127</a>)</li>
<li>Voxtral transcription API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31388">#31388</a>)</li>
<li>Safetensors download optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30537">#30537</a>)</li>
</ul>
<h3>Deprecations</h3>
<ul>
<li>Deprecated quantization schemes removed (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31688">#31688</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31285">#31285</a>)</li>
<li><code>seed_everything</code> deprecated (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31659">#31659</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>vllm-metal plugin docs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31174">#31174</a>)</li>
<li>Claude Code example (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31188">#31188</a>)</li>
<li>CustomOp developer guide (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30886">#30886</a>)</li>
</ul>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/penfree">@penfree</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30237">#30237</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jiangkuaixue123">@jiangkuaixue123</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30120">#30120</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jr-shen">@jr-shen</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29663">#29663</a></li>
<li><a class="user-mention notranslate" href="https://github.com/grzegorz-k-karch">@grzegorz-k-karch</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30795">#30795</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shanjiaz">@shanjiaz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30799">#30799</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Somoku">@Somoku</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29569">#29569</a></li>
<li><a class="user-mention notranslate" href="https://github.com/baoqian426">@baoqian426</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30841">#30841</a></li>
<li><a class="user-mention notranslate" href="https://github.com/SongDI911">@SongDI911</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30852">#30852</a></li>
<li><a class="user-mention notranslate" href="https://github.com/www-spam">@www-spam</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30827">#30827</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Xunzhuo">@Xunzhuo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30844">#30844</a></li>
<li><a class="user-mention notranslate" href="https://github.com/TheCodeWrangler">@TheCodeWrangler</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30700">#30700</a></li>
<li><a class="user-mention notranslate" href="https://github.com/SungMinCho">@SungMinCho</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30738">#30738</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wzyrrr">@wzyrrr</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30949">#30949</a></li>
<li><a class="user-mention notranslate" href="https://github.com/navmarri14">@navmarri14</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30629">#30629</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HaloWorld">@HaloWorld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30867">#30867</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jeffreywang-anyscale">@jeffreywang-anyscale</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31013">#31013</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AmeenP">@AmeenP</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31093">#31093</a></li>
<li><a class="user-mention notranslate" href="https://github.com/westers">@westers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31071">#31071</a></li>
<li><a class="user-mention notranslate" href="https://github.com/CedricHwong">@CedricHwong</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30957">#30957</a></li>
<li><a class="user-mention notranslate" href="https://github.com/c0de128">@c0de128</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31114">#31114</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bounty-hunter">@Bounty-hunter</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30242">#30242</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jzakrzew">@jzakrzew</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30550">#30550</a></li>
<li><a class="user-mention notranslate" href="https://github.com/1643661061leo">@1643661061leo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30760">#30760</a></li>
<li><a class="user-mention notranslate" href="https://github.com/NickCao">@NickCao</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30070">#30070</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amithkk">@amithkk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31212">#31212</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gateremark">@gateremark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31218">#31218</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Tiiiktak">@Tiiiktak</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31274">#31274</a></li>
<li><a class="user-mention notranslate" href="https://github.com/oscardev256">@oscardev256</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28367">#28367</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Jzz1943">@Jzz1943</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31448">#31448</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mratsim">@mratsim</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31407">#31407</a></li>
<li><a class="user-mention notranslate" href="https://github.com/twjww">@twjww</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31445">#31445</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amittell">@amittell</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31438">#31438</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ricky-chaoju">@ricky-chaoju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30184">#30184</a></li>
<li><a class="user-mention notranslate" href="https://github.com/effortprogrammer">@effortprogrammer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31343">#31343</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZT-AIA">@ZT-AIA</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31408">#31408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rogerxfeng8">@rogerxfeng8</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31522">#31522</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kevin-pw">@kevin-pw</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31497">#31497</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vintipandey">@vintipandey</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31505">#31505</a></li>
<li><a class="user-mention notranslate" href="https://github.com/SameerAsal">@SameerAsal</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31520">#31520</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Dylan1229">@Dylan1229</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31546">#31546</a></li>
<li><a class="user-mention notranslate" href="https://github.com/reaganjlee">@reaganjlee</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29105">#29105</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhima771">@zhima771</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31569">#31569</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jayhemnani9910">@jayhemnani9910</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31513">#31513</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Tmn07">@Tmn07</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31572">#31572</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vsourirajan">@vsourirajan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31549">#31549</a></li>
<li><a class="user-mention notranslate" href="https://github.com/labAxiaoming">@labAxiaoming</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31601">#31601</a></li>
<li><a class="user-mention notranslate" href="https://github.com/massif-01">@massif-01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31604">#31604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PHOEBEMOON0802">@PHOEBEMOON0802</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31147">#31147</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tpopp">@tpopp</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29993">#29993</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ppppqp">@ppppqp</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31620">#31620</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zzzzwwjj">@zzzzwwjj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31674">#31674</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Catacomba">@Catacomba</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30322">#30322</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kunpengW-code">@kunpengW-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31669">#31669</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johncalesp">@johncalesp</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28874">#28874</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BlankRH">@BlankRH</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31800">#31800</a></li>
<li><a class="user-mention notranslate" href="https://github.com/guicho271828">@guicho271828</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20610">#20610</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ReinforcedKnowledge">@ReinforcedKnowledge</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31055">#31055</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vSeamar">@vSeamar</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29197">#29197</a></li>
<li><a class="user-mention notranslate" href="https://github.com/A1c0r-Z">@A1c0r-Z</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31656">#31656</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MrIceCreamMan">@MrIceCreamMan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31465">#31465</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tianshu-Michael-yu">@tianshu-Michael-yu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31841">#31841</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiyu0824">@weiyu0824</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30808">#30808</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andyl98">@andyl98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31757">#31757</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaredforReal">@JaredforReal</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31779">#31779</a></li>
<li><a class="user-mention notranslate" href="https://github.com/katec846">@katec846</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29213">#29213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kfirtoledo">@kfirtoledo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30761">#30761</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ayobami-00">@Ayobami-00</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31868">#31868</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ShaanveerS">@ShaanveerS</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31825">#31825</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Zyyeric">@Zyyeric</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31652">#31652</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wangshangsam">@wangshangsam</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31775">#31775</a></li>
<li><a class="user-mention notranslate" href="https://github.com/devbyteai">@devbyteai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31536">#31536</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BJWang-ant">@BJWang-ant</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31719">#31719</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dangoldbj">@dangoldbj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31847">#31847</a></li>
<li><a class="user-mention notranslate" href="https://github.com/maylikenoother">@maylikenoother</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31610">#31610</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yxing-bj">@yxing-bj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31575">#31575</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xbfs">@xbfs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31948">#31948</a></li>
<li><a class="user-mention notranslate" href="https://github.com/RunkaiTao">@RunkaiTao</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29354">#29354</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AkshatSh">@AkshatSh</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31550">#31550</a></li>
<li><a class="user-mention notranslate" href="https://github.com/frelam">@frelam</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31857">#31857</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shyeh25">@shyeh25</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31617">#31617</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andikarachman">@andikarachman</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32092">#32092</a></li>
<li><a class="user-mention notranslate" href="https://github.com/minimAluminiumalism">@minimAluminiumalism</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32158">#32158</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andyzhangx">@andyzhangx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32185">#32185</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sanghoon-yn">@sanghoon-yn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31956">#31956</a></li>
<li><a class="user-mention notranslate" href="https://github.com/potatosalad">@potatosalad</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32212">#32212</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.13.0...v0.14.0"><tt>v0.13.0...v0.14.0</tt></a></p>

---

### [v0.14.0rc2: [CI] Fix LM Eval Large Models (H100) (#32423)](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Matthew Bonanni <a href="mailto:mbonanni@redhat.com">mbonanni@redhat.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/bcf2333cd6514543f579d9bb6d309c5b8a0bfd0d"><tt>bcf2333</tt></a>)</p>

---

### [v0.14.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#‚Ä¶</p>

---

### [v0.14.0rc0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony mode‚Ä¶</p>

---

### [v0.13.0](https://github.com/vllm-project/vllm/releases/tag/v0.13.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.13.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features <strong>442 commits from 207 contributors (61 new contributors)!</strong></p>
<p><strong>Breaking Changes</strong>: This release includes deprecation removals, PassConfig flag renames, and attention configuration changes from environment variables to CLI arguments. Please review the breaking changes section carefully before upgrading.</p>
<h3>Model Support</h3>
<ul>
<li><strong>New models</strong>: BAGEL (AR only) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28439">#28439</a>), AudioFlamingo3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a>), JAIS 2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a>), latent MoE architecture support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a>).</li>
<li><strong>Tool parsers</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29848">#29848</a>), Gigachat 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a>), Holo2 reasoning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30048">#30048</a>).</li>
<li><strong>Model enhancements</strong>: Qwen3-VL embeddings support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a>), Qwen3-VL EVS (Efficient Video Sampling) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29752">#29752</a>), DeepSeek V3.2 proper <code>drop_thinking</code> logic (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a>), DeepSeek V3.2 top-k fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27568">#27568</a>).</li>
<li><strong>Task expansion</strong>: Automatic TokenClassification model conversion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30666">#30666</a>), Ultravox v0.7 transformer projector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30089">#30089</a>).</li>
<li><strong>Quantization</strong>: BitsAndBytes for Qwen3-Omni-MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29896">#29896</a>).</li>
<li><strong>Speculative decoding</strong>: Eagle/Eagle3 Transformers backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30340">#30340</a>), Mamba <code>selective_state_update</code> spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29488">#29488</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Compilation</strong>: Conditional compilation via <code>compile_ranges</code> for selective kernel compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24252">#24252</a>).</li>
<li><strong>Prefix caching</strong>: xxHash high-performance hash option (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a>).</li>
<li><strong>Attention</strong>: PrefixLM support for FlexAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27938">#27938</a>) and TritonAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30386">#30386</a>), CUDA graphs for 3D Triton attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28306">#28306</a>), <code>TRITON_MLA</code> without prefix-caching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29125">#29125</a>).</li>
<li><strong>Batch invariance</strong>: FA2 and LoRA batch-invariant support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30018">#30018</a>).</li>
<li><strong>Pooling</strong>: Chunked prefill for ALL pooling tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27145">#27145</a>), multi-vector retrieval API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26686">#26686</a>).</li>
<li><strong>Model Runner V2</strong>: Min-p sampling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30171">#30171</a>), NaN detection in logits (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30187">#30187</a>).</li>
<li><strong>Speculative decoding</strong>: Medusa GPU-CPU sync avoidance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29723">#29723</a>), async spec-decode improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29624">#29624</a>).</li>
<li><strong>Whisper</strong>: Major performance improvements - <a href="https://github.com/vllm-project/vllm/issues/24946#issuecomment-3680725754">V1 is now faster than V0</a> (~3x speedup vs v0.12.0). Encoder batching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29421">#29421</a>), <code>FULL_DECODE_ONLY</code> CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30072">#30072</a>), CPU backend support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>).</li>
<li><strong>Performance</strong>: Fused blockwise quant RMS norm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27883">#27883</a>), MoE LoRA loading reduction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30243">#30243</a>), encoder cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30475">#30475</a>), CPU KV offloading streams (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29013">#29013</a>).</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li><strong>NVIDIA Blackwell Ultra</strong>: SM103 (GB300) support with CUDA 13 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30484">#30484</a>).</li>
<li><strong>DeepSeek optimizations</strong> (benchmarked on DeepSeek-V3.1):
<ul>
<li>DeepEP High-Throughput CUDA graph enabled by default: <strong>5.3% throughput, 4.4% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29558">#29558</a>)</li>
<li>DeepGEMM fused layout kernel: <strong>4.3% throughput, 10.7% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29546">#29546</a>)</li>
<li>DeepGEMM experts initialization: <strong>3.9% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30494">#30494</a>)</li>
<li><code>group_topk</code> kernel: <strong>1.9% throughput, 2.1% TPOT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30159">#30159</a>)</li>
<li>Sparse prefill kernel for FP8 KV-cache in DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27532">#27532</a>)</li>
<li>MLA FP8 optimization with ReduceScatterSum (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29795">#29795</a>), direct k_nope/k_pe copy (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29710">#29710</a>)</li>
</ul>
</li>
<li><strong>CPU</strong>: Whisper support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>), Arm Optimized Routines vectorized exp (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a>), x86 CPU wheel pipeline (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28848">#28848</a>).</li>
<li><strong>AMD ROCm</strong>: Aiter quantization kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25552">#25552</a>), torch.compile layernorm/silu + FP8 quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25693">#25693</a>), Triton ScaledMM fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26668">#26668</a>), MXFP4 w4a4 inference (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29775">#29775</a>).</li>
<li><strong>Intel XPU</strong>: wNa16 compressed tensors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29484">#29484</a>).</li>
<li><strong>Build</strong>: CUDA 13 aarch64 wheels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30341">#30341</a>), Docker kernel build stage (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a>), Ascend NPU Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30015">#30015</a>).</li>
</ul>
<h3>Large Scale Serving &amp; Disaggregated Prefill/Decode</h3>
<ul>
<li><strong>KV connectors</strong>: Mooncake Transfer Engine (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a>), cache reset via <code>/reset_prefix_cache</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27170">#27170</a>), KV events (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28309">#28309</a>), failure recovery config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26813">#26813</a>).</li>
<li><strong>NIXL</strong>: Compatibility checking in handshake (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29503">#29503</a>), large batch proxy support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28782">#28782</a>).</li>
<li><strong>EPLB</strong>: NVFP4 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a>), algorithm abstraction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a>).</li>
<li><strong>Multi-node</strong>: External launcher mode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29833">#29833</a>).</li>
<li><strong>Hybrid allocator</strong>: Optional KV connector integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29805">#29805</a>).</li>
<li><strong>Performance</strong>: silu_mul_per_token_group_quant_fp8 kernel for DP/EP (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29470">#29470</a>).</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>New</strong>: W4A8 grouped GEMM on Hopper (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29691">#29691</a>), online FP8 with streaming post-processing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a>), FP8 weight reloading for RLHF (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28480">#28480</a>).</li>
<li><strong>MoE + LoRA</strong>: AWQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30442">#30442</a>) and GPTQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a>) support.</li>
<li><strong>GGUF</strong>: MoE + GGUF restored for Qwen3 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30116">#30116</a>), Qwen2 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30307">#30307</a>), HF defaults override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30118">#30118</a>).</li>
<li><strong>Compatibility</strong>: Transformers v5 RoPE support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30046">#30046</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>: MCP type infrastructure (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30054">#30054</a>), Browser/Container MCP tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29989">#29989</a>), full MCP Python loop (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29798">#29798</a>), extra body parameters (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a>).</li>
<li><strong>Configuration</strong>: <code>AttentionConfig</code> replaces <code>VLLM_ATTENTION_BACKEND</code> env var (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>).</li>
<li><strong>Chat templates</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29837">#29837</a>), DeepSeek-V3.2 developer tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a>).</li>
<li><strong>Anthropic API</strong>: Streaming fixes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29971">#29971</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30266">#30266</a>).</li>
<li><strong>Embeddings</strong>: Binary format with <code>encoding_format=bytes_only</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30249">#30249</a>), multiple image/audio per request (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29988">#29988</a>), tokenization_kwargs override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29794">#29794</a>).</li>
<li><strong>Metrics</strong>: Prefill KV compute metric excluding cached tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30189">#30189</a>).</li>
<li><strong>Profiling</strong>: Layer-wise NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29990">#29990</a>), profiling CLI config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29912">#29912</a>).</li>
<li><strong>UX</strong>: Better OOM errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28051">#28051</a>), ModelConfig validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a>), distributed executor errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30140">#30140</a>).</li>
</ul>
<h3>Security</h3>
<ul>
<li>Additional protection for <a href="https://github.com/advisories/GHSA-mrw7-hf4f-83pf" title="CVE-2025-62164">CVE-2025-62164</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a>).</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>NVSHMEM 3.3.24 + CUDA 13 fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a>).</li>
<li>TPU tpu-inference 0.12.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30221">#30221</a>).</li>
</ul>
<h3>Breaking Changes &amp; Deprecations</h3>
<ol>
<li><strong>PassConfig flags renamed</strong> per RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/27995">#27995</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29646">#29646</a>)</li>
<li><strong>Attention env vars ‚Üí CLI args</strong>: <code>VLLM_ATTENTION_BACKEND</code> replaced with <code>--attention-backend</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>)</li>
<li><strong>Removed <code>-O.xx</code> flag</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29991">#29991</a>)</li>
<li><strong>Removed deprecated plugin/compilation fields</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30396">#30396</a>)</li>
<li><strong>Removed deprecated task, seed, MM settings</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30397">#30397</a>)</li>
<li><strong>Removed <code>embed_input_ids</code>/<code>embed_multimodal</code> fallbacks</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30458">#30458</a>)</li>
<li><strong>Removed tokenizer setter</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30400">#30400</a>)</li>
<li><strong>Deprecations</strong>: <code>merge_by_field_config</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30035">#30035</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30170">#30170</a>), <code>--convert reward</code> ‚Üí <code>--convert embed</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30463">#30463</a>)</li>
</ol>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/ajpqs">@ajpqs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amitz-nv">@amitz-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29978">#29978</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amrmahdi">@amrmahdi</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andrewbriand">@andrewbriand</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/anker-c2">@anker-c2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30344">#30344</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AuruTus">@AuruTus</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30182">#30182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/avigny">@avigny</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19425">#19425</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bhanu068">@Bhanu068</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a></li>
<li>@Copilot made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29025">#29025</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dbotwinick">@dbotwinick</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30583">#30583</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30234">#30234</a></li>
<li><a class="user-mention notranslate" href="https://github.com/desertfire">@desertfire</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29919">#29919</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dmitry-tokarev-nv">@dmitry-tokarev-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a></li>
<li><a class="user-mention notranslate" href="https://github.com/drslark">@drslark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30632">#30632</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dtcccc">@dtcccc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elizabetht">@elizabetht</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28671">#28671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Elm8116">@Elm8116</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gausah01">@gausah01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29604">#29604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gh-wf">@gh-wf</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30285">#30285</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hdlj-h">@hdlj-h</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30056">#30056</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HF-001">@HF-001</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30051">#30051</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hzxuzhonghu">@hzxuzhonghu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29931">#29931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaviS-Rei">@JaviS-Rei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29882">#29882</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johannesflommersfeld">@johannesflommersfeld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30390">#30390</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KevinMusgrave">@KevinMusgrave</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30529">#30529</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kitaekatt">@kitaekatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30408">#30408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lashahub">@lashahub</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a></li>
<li><a class="user-mention notranslate" href="https://github.com/LuminolT">@LuminolT</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a></li>
<li><a class="user-mention notranslate" href="https://github.com/majiayu000">@majiayu000</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30615">#30615</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MaoJianwei">@MaoJianwei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29797">#29797</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Mercykid-bash">@Mercykid-bash</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mgehre-amd">@mgehre-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30364">#30364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mivehk">@mivehk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30512">#30512</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mondaylord">@mondaylord</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30671">#30671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/noa-neria">@noa-neria</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29320">#29320</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PatrykSaffer">@PatrykSaffer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30330">#30330</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Peng-YM">@Peng-YM</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29074">#29074</a></li>
<li><a class="user-mention notranslate" href="https://github.com/realliujiaxu">@realliujiaxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30059">#30059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/redwrasse">@redwrasse</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29261">#29261</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ri0S">@Ri0S</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scratch-ml">@scratch-ml</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30351">#30351</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seokhyunan">@seokhyunan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30648">#30648</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shaharmor98">@shaharmor98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a></li>
<li><a class="user-mention notranslate" href="https://github.com/taoyun951753">@taoyun951753</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tom-zju">@tom-zju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30057">#30057</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tomtomjhj">@tomtomjhj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29692">#29692</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vkuzo">@vkuzo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vladnosiv">@vladnosiv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiguihua2">@weiguihua2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30042">#30042</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wenqiglantz">@wenqiglantz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wkcn">@wkcn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29879">#29879</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wu-kan">@wu-kan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21804">#21804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wz1qqx">@wz1qqx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30376">#30376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xyDong0223">@xyDong0223</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30455">#30455</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yifant-code">@yifant-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yjc9696">@yjc9696</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yurekami">@yurekami</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30552">#30552</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuttian1">@yuttian1</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30102">#30102</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhijianJiang">@ZhijianJiang</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30219">#30219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhiweiYan-96">@ZhiweiYan-96</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29773">#29773</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.12.0...v0.13.0"><tt>v0.12.0...v0.13.0</tt></a></p>

---

### [v0.13.0rc4: [v1] Add PrefixLM support to TritonAttention backend (#30386)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc4)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/74a1ac38b00a8cf502db085d1bbd77712cf47e41"><tt>74a1ac3</tt></a>)</p>

---

### [v0.13.0rc3: [XPU] fix broken fp8 online quantization for XPU platform (#30831)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc3)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Yan Ma <a href="mailto:yan.ma@intel.com">yan.ma@intel.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/4f735babb7353987137b85ec0465e594e9ed1384"><tt>4f735ba</tt></a>)</p>

---

### [v0.13.0rc2: [ROCm] [Bugfix] Fix torch sdpa hallucination (#30789)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: tjtanaa <a href="mailto:tunjian.tan@embeddedllm.com">tunjian.tan@embeddedllm.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/2410132bb1f9faa5b252fad3f2b83dc926946b08"><tt>2410132</tt></a>)</p>

---

### [v0.13.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>fake rc release to fix nightly wheels</p>

---

## Operating Systems

### [GNU C Library 2.43 released](https://lwn.net/Articles/1055757/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <a href="https://lwn.net/ml/all/13932477.uLZWGnKmhe@pinacolada">Version 2.43</a> of the
GNU C Library has been released.  Changes include support for the <a href="https://lwn.net/Articles/978010/"><tt>mseal()</tt></a> and <a href="https://man7.org/linux/man-pages/man2/openat2.2.html"><tt>openat2()</tt></a>
system calls, experimental support for building with the Clang compiler,
Unicode 17.0.0 support, a number of security fixes, and much more.

---

## Systems

### [A data model for Git (and other docs updates)](https://jvns.ca/blog/2026/01/08/a-data-model-for-git/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! This past fall, I decided to take some time to work on Git&rsquo;s
documentation. I&rsquo;ve been thinking about working on open source docs for a long
time &ndash; usually if I think the documentation for something could be improved,
I&rsquo;ll write a blog post or a zine or something. But this time I wondered: could I
instead make a few improvements to the official documentation?</p>
<p>So <a href="https://marieflanagan.com/">Marie</a> and I made a few changes to the Git
documentation!</p>
<h3 id="a-data-model-for-git">a data model for Git</h3>
<p>After a while working on the documentation, we noticed that Git uses the terms
&ldquo;object&rdquo;, &ldquo;reference&rdquo;, or &ldquo;index&rdquo; in its documentation a lot, but that it didn&rsquo;t
have a great explanation of what those terms mean or how they relate to other
core concepts like &ldquo;commit&rdquo; and &ldquo;branch&rdquo;. So we wrote a new &ldquo;data model&rdquo; document!</p>
<p>You can <a href="https://github.com/git/git/blob/master/Documentation/gitdatamodel.adoc">read the data model here for now</a>.
I assume at some point (after the next release?) it&rsquo;ll also be on the <a href="https://git-scm.com">Git website</a>.</p>
<p>I&rsquo;m excited about this because understanding how Git organizes its commit and
branch data has really helped me reason about how Git works over the years,
and I think it&rsquo;s important to have a short (1600 words!) version of the data
model that&rsquo;s accurate.</p>
<p>The &ldquo;accurate&rdquo; part turned out to not be that easy: I knew the basics of how
Git&rsquo;s data model worked, but during the review process I learned some new
details and had to make quite a few changes (for example how merge conflicts are
stored in the staging area).</p>
<h3 id="updates-to-git-push-git-pull-and-more">updates to <code>git push</code>, <code>git pull</code>, and more</h3>
<p>I also worked on updating the introduction to some of Git&rsquo;s core man pages.
I quickly realized that &ldquo;just try to improve it according to my best judgement&rdquo;
was not going to work: why should the maintainers believe me that my version is
better?</p>
<p>I&rsquo;ve seen a problem a lot when discussing open source documentation changes
where 2 expert users of the software argue about whether an explanation
is clear or not (&ldquo;I think X would be a good way to explain it! Well, I think Y
would be better!&rdquo;)</p>
<p>I don&rsquo;t think this is very productive (expert users of a piece of software
are notoriously bad at being able to tell if an explanation will be clear to
non-experts), so I needed to find a way to identify problems with the man
pages that was a little more evidence-based.</p>
<h3 id="getting-test-readers-to-identify-problems">getting test readers to identify problems</h3>
<p>I asked for test readers on Mastodon to read the current version of
documentation and tell me what they find confusing or what questions they have.
About 80 test readers left comments, and I learned so much!</p>
<p>People left a huge amount of great feedback, for example:</p>
<ul>
<li>terminology they didn&rsquo;t understand (what&rsquo;s a pathspec? what does &ldquo;reference&rdquo; mean? does &ldquo;upstream&rdquo; have a specific meaning in Git?)</li>
<li>specific confusing sentences</li>
<li>suggestions of things things to add (&ldquo;I do X all the time, I think it should be included here&rdquo;)</li>
<li>inconsistencies (&ldquo;here it implies X is the default, but elsewhere it implies Y is the default&rdquo;)</li>
</ul>
<p>Most of the test readers had been using Git for at least 5-10 years, which
I think worked well &ndash; if a group of test readers who have been using Git
regularly for 5+ years find a sentence or term impossible to understand, it
makes it easy to argue that the documentation should be updated to make it
clearer.</p>
<p>I thought this &ldquo;get users of the software to comment on the existing
documentation and then fix the problems they find&rdquo; pattern worked really
well and I&rsquo;m excited about potentially trying it again in the future.</p>
<h3 id="the-man-page-changes">the man page changes</h3>
<p>We ended updating these 4 man pages:</p>
<ul>
<li><code>git add</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-add.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-add.adoc">after</a>)</li>
<li><code>git checkout</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-checkout.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-checkout.adoc">after</a>)</li>
<li><code>git push</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-push.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc">after</a>)</li>
<li><code>git pull</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-pull.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-pull.adoc">after</a>)</li>
</ul>
<p>The <code>git push</code> and <code>git pull</code> changes were the most interesting to me: in
addition to updating the intro to those pages, we also ended up writing:</p>
<ul>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/urls-remotes.adoc#upstream-branches">a section describing what the term &ldquo;upstream branch&rdquo; means</a> (which previously wasn&rsquo;t really explained)</li>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc#options">a cleaned-up description of what a &ldquo;push refspec&rdquo; is</a></li>
</ul>
<p>Making those changes really gave me an appreciation for how much work it is
to maintain open source documentation: it&rsquo;s not easy to write things that are
both clear and true, and sometimes we had to make compromises, for example the sentence
&ldquo;<code>git push</code> may fail if you haven‚Äôt set an upstream for the current branch,
depending on what <code>push.default</code> is set to.&rdquo; is a little vague, but the exact
details of what &ldquo;depending&rdquo; means are really complicated and untangling that is
a big project.</p>
<h3 id="on-the-process-for-contributing-to-git">on the process for contributing to Git</h3>
<p>It took me a while to understand Git&rsquo;s development process.
I&rsquo;m not going to try to describe it here (that could be a whole other post!), but a few quick notes:</p>
<ul>
<li>Git has a <a href="https://git-scm.com/community#discord">Discord server</a>
with a &ldquo;my first contribution&rdquo; channel for help with getting started contributing.
I found people to be very welcoming on the Discord.</li>
<li>I used <a href="https://gitgitgadget.github.io/">GitGitGadget</a> to make all of my contributions.
This meant that I could make a GitHub pull request (a workflow I&rsquo;m comfortable
with) and GitGitGadget would convert my PRs into the system the Git developers
use (emails with patches attached). GitGitGadget worked great and I was very
grateful to not have to learn how to send patches by email with Git.</li>
<li>Otherwise I used my normal email client (Fastmail&rsquo;s web interface) to reply
to emails, wrapping my text to 80 character lines since that&rsquo;s the mailing
list norm.</li>
</ul>
<p>I also found the mailing list archives on <a href="https://lore.kernel.org/git/">lore.kernel.org</a>
hard to navigate, so I hacked together <a href="https://github.com/jvns/git-list-viewer">my own git list viewer</a>
to make it easier to read the long mailing list threads.</p>
<p>Many people helped me navigate the contribution process and review the changes:
thanks to Emily Shaffer, Johannes Schindelin (the author of GitGitGadget),
Patrick Steinhardt, Ben Knoble, Junio Hamano, and more.</p>
<p>(I&rsquo;m experimenting with <a href="https://comments.jvns.ca/post/115861337435768520">comments on Mastodon, you can see the comments here</a>)</p>

---

### [Notes on switching to Helix from vim](https://jvns.ca/blog/2025/10/10/notes-on-switching-to-helix-from-vim/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Earlier this summer I was talking to a friend about how much I
<a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">love using fish</a>, and
how I love that I don&rsquo;t have to configure it. They said that they feel the same
way about the <a href="https://helix-editor.com/">helix</a> text editor, and so I decided
to give it a try.</p>
<p>I&rsquo;ve been using it for 3 months now and here are a few notes.</p>
<h3 id="why-helix-language-servers">why helix: language servers</h3>
<p>I think what motivated me to try Helix is that I&rsquo;ve been trying to get a working
language server setup (so I can do things like &ldquo;go to definition&rdquo;) and getting
a setup that feels good in Vim or Neovim just felt like too much work.</p>
<p>After using Vim/Neovim for 20 years, I&rsquo;ve tried both &ldquo;build my own custom
configuration from scratch&rdquo; and &ldquo;use someone else&rsquo;s pre-buld configuration
system&rdquo; and even though I love Vim I was excited about having things just work
without having to work on my configuration at all.</p>
<p>Helix comes with built in language server support, and it feels nice
to be able to do things like &ldquo;rename this symbol&rdquo; in any language.</p>
<h3 id="the-search-is-great">the search is great</h3>
<p>One of my favourite things about Helix is the search! If I&rsquo;m searching all the
files in my repository for a string, it lets me scroll through the potential
matching files and see the full context of the match, like this:</p>
<img src="https://jvns.ca/images/helix-search.png" />
<p>For comparison, here&rsquo;s what the vim ripgrep plugin I&rsquo;ve been using looks like:</p>
<img src="https://jvns.ca/images/vim-ripgrep.png" />
<p>There&rsquo;s no context for what else is around that line.</p>
<h3 id="the-quick-reference-is-nice">the quick reference is nice</h3>
<p>One thing I like about Helix is that when I press <code>g</code>, I get a little help popup
telling me places I can go. I really appreciate this because I don&rsquo;t often use
the &ldquo;go to definition&rdquo; or &ldquo;go to reference&rdquo; feature and I often forget the
keyboard shortcut.</p>
<img src="https://jvns.ca/images/goto.png" width="300px" />
<h3 id="some-vim-helix-translations">some vim -&gt; helix translations</h3>
<ul>
<li>Helix doesn&rsquo;t have marks like <code>ma</code>, <code>'a</code>, instead I&rsquo;ve been using <code>Ctrl+O</code> and
<code>Ctrl+I</code> to go back (or forward) to the last cursor location</li>
<li>I think Helix does have macros, but I&rsquo;ve been using multiple cursors in every
case that I would have previously used a macro. I like multiple cursors a lot
more than writing macros all the time. If I want to batch change something in
the document, my workflow is to press <code>%</code> (to highlight everything), then <code>s</code>
to select (with a regex) the things I want to change, then I can just edit
all of them as needed.</li>
<li>Helix doesn&rsquo;t have neovim-style tabs, instead it has a nice buffer switcher (<code>&lt;space&gt;b</code>)
I can use to switch to the buffer I want. There&rsquo;s a
<a href="https://github.com/helix-editor/helix/pull/7109">pull request here</a> to implement neovim-style tabs.
There&rsquo;s also a setting <code>bufferline=&quot;multiple&quot;</code> which can act a bit like tabs
with <code>gp</code>, <code>gn</code> for prev/next &ldquo;tab&rdquo; and <code>:bc</code> to close a &ldquo;tab&rdquo;.</li>
</ul>
<h3 id="some-helix-annoyances">some helix annoyances</h3>
<p>Here&rsquo;s everything that&rsquo;s annoyed me about Helix so far.</p>
<ul>
<li>I like the way Helix&rsquo;s <code>:reflow</code> works much less than how
vim reflows text with <code>gq</code>. It doesn&rsquo;t work as well with lists. (<a href="https://github.com/helix-editor/helix/issues/3332">github issue</a>)</li>
<li>If I&rsquo;m making a Markdown list, pressing &ldquo;enter&rdquo; at the end of a list item
won&rsquo;t continue the list. There&rsquo;s a <a href="https://github.com/helix-editor/helix/wiki/Recipes#continue-markdown-lists--quotes">partial workaround</a>
for bulleted lists but I don&rsquo;t know one for numbered lists.</li>
<li>No persistent undo yet: in vim I could use an
<a href="https://vimdoc.sourceforge.net/htmldoc/options.html#'undofile'">undofile</a> so
that I could undo changes even after quitting. Helix doesn&rsquo;t have that feature yet.
(<a href="https://github.com/helix-editor/helix/pull/9154">github PR</a>)</li>
<li>Helix doesn&rsquo;t autoreload files after they change on disk, I have to run
<code>:reload-all</code> (<code>:ra&lt;tab&gt;</code>) to manually reload them. Not a big deal.</li>
<li>Sometimes it crashes, maybe every week or so. I think it might be
<a href="https://github.com/helix-editor/helix/issues/12582">this issue</a>.</li>
</ul>
<p>The &ldquo;markdown list&rdquo; and reflowing issues come up a lot for me because I spend
a lot of time editing Markdown lists, but I keep using Helix anyway so I guess
they can&rsquo;t be making me that mad.</p>
<h3 id="switching-was-easier-than-i-thought">switching was easier than I thought</h3>
<p>I was worried that relearning 20 years of Vim muscle memory would be really hard.</p>
<p>It turned out to be easier than I expected, I started using Helix on a
vacation for a little low-stakes coding project I was doing on the side and
after a week or two it didn&rsquo;t feel so disorienting anymore. I think it might be
hard to switch back and forth between Vim and Helix, but I haven&rsquo;t needed to use
Vim recently so I don&rsquo;t know if that&rsquo;ll ever become an issue for me.</p>
<p>The first time I tried Helix I tried to force it to use keybindings that were
more similar to Vim and that did not work for me. Just learning the &ldquo;Helix way&rdquo;
was a lot easier.</p>
<p>There are still some things that throw me off: for example <code>w</code> in vim and <code>w</code> in
Helix don&rsquo;t have the same idea of what a &ldquo;word&rdquo; is (the Helix one includes the
space after the word, the Vim one doesn&rsquo;t).</p>
<h3 id="using-a-terminal-based-text-editor">using a terminal-based text editor</h3>
<p>For many years I&rsquo;d mostly been using a GUI version of vim/neovim, so switching
to actually using an editor in the terminal was a bit of an adjustment.</p>
<p>I ended up deciding on:</p>
<ol>
<li>Every project gets its own terminal window, and all of the tabs in that
window (mostly) have the same working directory</li>
<li>I make my Helix tab the first tab in the terminal window</li>
</ol>
<p>It works pretty well, I might actually like it better than my previous workflow.</p>
<h3 id="my-configuration">my configuration</h3>
<p>I appreciate that my configuration is really simple, compared to my neovim
configuration which is hundreds of lines. It&rsquo;s mostly just 4 keyboard
shortcuts.</p>
<pre><code>theme = &quot;solarized_light&quot;
[editor]
# Sync clipboard with system clipboard
default-yank-register = &quot;+&quot;

[keys.normal]
# I didn't like that Ctrl+C was the default &quot;toggle comments&quot; shortcut
&quot;#&quot; = &quot;toggle_comments&quot;

# I didn't feel like learning a different way
# to go to the beginning/end of a line so
# I remapped ^ and $
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.select]
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.normal.space]
# I write a lot of text so I need to constantly reflow,
# and missed vim's `gq` shortcut
l = &quot;:reflow&quot;
</code></pre>
<p>There&rsquo;s a separate <code>languages.toml</code> configuration where I set some language
preferences, like turning off autoformatting.
For example, here&rsquo;s my Python configuration:</p>
<pre><code>[[language]]
name = &quot;python&quot;
formatter = { command = &quot;black&quot;, args = [&quot;--stdin-filename&quot;, &quot;%{buffer_name}&quot;, &quot;-&quot;] }
language-servers = [&quot;pyright&quot;]
auto-format = false
</code></pre>
<h3 id="we-ll-see-how-it-goes">we&rsquo;ll see how it goes</h3>
<p>Three months is not that long, and it&rsquo;s possible that I&rsquo;ll decide to go back
to Vim at some point. For example, I wrote a <a href="https://jvns.ca/blog/2023/02/28/some-notes-on-using-nix/">post about switching to
nix</a> a while back but
after maybe 8 months I switched back to Homebrew (though I&rsquo;m still using NixOS
to manage one little server, and I&rsquo;m still satisfied with that).</p>

---

### [New zine: The Secret Rules of the Terminal](https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! After many months of writing deep dive blog posts about the terminal, on
Tuesday I released a new zine called &ldquo;The Secret Rules of the Terminal&rdquo;!</p>
<p>You can get it for $12 here:
<a href="https://wizardzines.com/zines/terminal">https://wizardzines.com/zines/terminal</a>, or get
an <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</p>
<p>Here&rsquo;s the cover:</p>
<div align="center">
<a href="https://wizardzines.com/zines/terminal">
  <img src="https://jvns.ca/images/terminal-cover-small.jpg" width="600px" />
  </a>
</div>
<h3 id="the-table-of-contents">the table of contents</h3>
<p>Here&rsquo;s the table of contents:</p>
<a href="https://wizardzines.com/zines/terminal/toc.png">
  <img src="https://jvns.ca/images/terminal-toc-small.png" width="600px" />
</a>
<h3 id="why-the-terminal">why the terminal?</h3>
<p>I&rsquo;ve been using the terminal every day for 20 years but even though I&rsquo;m very
confident in the terminal, I&rsquo;ve always had a bit of an uneasy feeling about it.
Usually things work fine, but sometimes something goes wrong and it just feels
like investigating it is impossible, or at least like it would open up a huge
can of worms.</p>
<p>So I started trying to write down a list of weird problems I&rsquo;ve run into in terminal and I realized
that the terminal has a lot of tiny inconsistencies like:</p>
<ul>
<li>sometimes you can use the arrow keys to move around, but sometimes pressing the arrow keys just prints <code>^[[D</code></li>
<li>sometimes you can use the mouse to select text, but sometimes you can&rsquo;t</li>
<li>sometimes your commands get saved to a history when you run them, and sometimes they don&rsquo;t</li>
<li>some shells let you use the up arrow to see the previous command, and some don&rsquo;t</li>
</ul>
<p>If you use the terminal daily for 10 or 20 years, even if you don&rsquo;t understand
exactly <em>why</em> these things happen, you&rsquo;ll probably build an intuition for them.</p>
<p>But having an intuition for them isn&rsquo;t the same as understanding why they
happen. When writing this zine I actually had to do a lot of work to figure out
exactly what was <em>happening</em> in the terminal to be able to talk about how to
reason about it.</p>
<h3 id="the-rules-aren-t-written-down-anywhere">the rules aren&rsquo;t written down anywhere</h3>
<p>It turns out that the &ldquo;rules&rdquo; for how the terminal works (how do
you edit a command you type in? how do you quit a program? how do you fix your
colours?) are extremely hard to fully understand, because &ldquo;the terminal&rdquo; is actually
made of many different pieces of software (your terminal emulator, your
operating system, your shell, the core utilities like <code>grep</code>, and every other random
terminal program you&rsquo;ve installed) which are written by different people with different
ideas about how things should work.</p>
<p>So I wanted to write something that would explain:</p>
<ul>
<li>how the 4 pieces of the terminal (your shell, terminal emulator, programs, and TTY driver) fit together to make everything work</li>
<li>some of the core conventions for how you can expect things in your terminal to work</li>
<li>lots of tips and tricks for how to use terminal programs</li>
</ul>
<h3 id="this-zine-explains-the-most-useful-parts-of-terminal-internals">this zine explains the most useful parts of terminal internals</h3>
<p>Terminal internals are a mess. A lot of it is just the way it is because
someone made a decision in the 80s and now it&rsquo;s impossible to change, and
honestly I don&rsquo;t think learning everything about terminal internals is worth
it.</p>
<p>But some parts are not that hard to understand and can really make your
experience in the terminal better, like:</p>
<ul>
<li>if you understand what <strong>your shell</strong> is responsible for, you can configure your shell (or use a different one!) to access your history more easily, get great tab completion, and so much more</li>
<li>if you understand <strong>escape codes</strong>, it&rsquo;s much less scary when <code>cat</code>ing a binary to stdout messes up your terminal, you can just type <code>reset</code> and move on</li>
<li>if you understand how <strong>colour</strong> works, you can get rid of bad colour contrast in your terminal so you can actually read the text</li>
</ul>
<h3 id="i-learned-a-surprising-amount-writing-this-zine">I learned a surprising amount writing this zine</h3>
<p>When I wrote <a href="https://wizardzines.com/zines/git">How Git Works</a>, I thought I
knew how Git worked, and I was right. But the terminal is different. Even
though I feel totally confident in the terminal and even though I&rsquo;ve used it
every day for 20 years, I had a lot of misunderstandings about how the terminal
works and (unless you&rsquo;re the author of <code>tmux</code> or something) I think there&rsquo;s a
good chance you do too.</p>
<p>A few things I learned that are actually useful to me:</p>
<ul>
<li>I understand the structure of the terminal better and so I feel more
confident debugging weird terminal stuff that happens to me (I was even able
to suggest a <a href="https://github.com/fish-shell/fish-shell/issues/10834">small improvement</a> to fish!). Identifying exactly which piece of software is causing a weird thing to happen in my terminal still isn&rsquo;t <em>easy</em> but I&rsquo;m a lot better at it now.</li>
<li>you can write a shell script to <a href="https://jvns.ca/til/vim-osc52/">copy to your clipboard over SSH</a></li>
<li>how <code>reset</code> works under the hood (it does the equivalent of <code>stty sane; sleep 1; tput reset</code>) ‚Äì basically I learned that I don&rsquo;t ever need to worry about
remembering <code>stty sane</code> or <code>tput reset</code> and I can just run <code>reset</code> instead</li>
<li>how to look at the invisible escape codes that a program is printing out (run <code>unbuffer program &gt; out; less out</code>)</li>
<li>why the builtin REPLs on my Mac like <code>sqlite3</code> are so annoying to use (they use <code>libedit</code> instead of <code>readline</code>)</li>
</ul>
<h3 id="blog-posts-i-wrote-along-the-way">blog posts I wrote along the way</h3>
<p>As usual these days I wrote a bunch of blog posts about various side quests:</p>
<ul>
<li><a href="https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/">How to add a directory to your PATH</a></li>
<li><a href="https://jvns.ca/blog/2024/11/26/terminal-rules/">&ldquo;rules&rdquo; that terminal problems follow</a></li>
<li><a href="https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/">why pipes sometimes get &ldquo;stuck&rdquo;: buffering</a></li>
<li><a href="https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/">some terminal frustrations</a></li>
<li><a href="https://jvns.ca/blog/2024/10/31/ascii-control-characters/">ASCII control characters in my terminal</a> on &ldquo;what&rsquo;s the deal with Ctrl+A, Ctrl+B, Ctrl+C, etc?&rdquo;</li>
<li><a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a></li>
<li><a href="https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/">what&rsquo;s involved in getting a &ldquo;modern&rdquo; terminal setup?</a></li>
<li><a href="https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/">reasons to use your shell&rsquo;s job control</a></li>
<li><a href="https://jvns.ca/blog/2025/03/07/escape-code-standards/">standards for ANSI escape codes</a>, which is really me trying to figure out if I think the <code>terminfo</code> database is serving us well today</li>
</ul>
<h3 id="people-who-helped-with-this-zine">people who helped with this zine</h3>
<p>A long time ago I used to write zines mostly by myself but with every project I get more
and more help. I met with <a href="https://marieflanagan.com">Marie Claire LeBlanc Flanagan</a> every weekday from September to June to work
on this one.</p>
<p>The cover is by Vladimir Ka≈°ikoviƒá,
Lesley Trites did copy editing,
Simon Tatham (who wrote <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a>) did technical review, our
Operations Manager Lee did the transcription as well as a million other
things, and <a href="https://github.com/doy">Jesse Luehrs</a> (who is one of the very few
people I know who actually understands the terminal&rsquo;s cursed inner workings)
had so many incredibly helpful conversations with me about what is going on in
the terminal.</p>
<h3 id="get-the-zine">get the zine</h3>
<p>Here are some links to get the zine again:</p>
<ul>
<li>get <a href="https://wizardzines.com/zines/terminal">The Secret Rules of the Terminal</a></li>
<li>get a <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</li>
</ul>
<p>As always, you can get either a PDF version to print at home or a print version
shipped to your house. The only caveat is print orders will ship in <strong>August</strong> &ndash; I
need to wait for orders to come in to get an idea of how many I should print
before sending it to the printer.</p>

---

### [Using `make` to compile C programs (for non-C-programmers)](https://jvns.ca/blog/2025/06/10/how-to-compile-a-c-program/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I have never been a C programmer but every so often I need to compile a C/C++
program from source. This has been kind of a struggle for me: for a
long time, my approach was basically &ldquo;install the dependencies, run <code>make</code>, if
it doesn&rsquo;t work, either try to find a binary someone has compiled or give up&rdquo;.</p>
<p>&ldquo;Hope someone else has compiled it&rdquo; worked pretty well when I was running Linux
but since I&rsquo;ve been using a Mac for the last couple of years I&rsquo;ve been running
into more situations where I have to actually compile programs myself.</p>
<p>So let&rsquo;s talk about what you might have to do to compile a C program! I&rsquo;ll use
a couple of examples of specific C programs I&rsquo;ve compiled and talk about a few
things that can go wrong. Here are three programs we&rsquo;ll be talking about
compiling:</p>
<ul>
<li><a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a></li>
<li><a href="https://www.sqlite.org/download.html">sqlite</a></li>
<li><a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> (a pager you can run to quickly open files from a search with <code>rg -n THING | qf</code>)</li>
</ul>
<h3 id="step-1-install-a-c-compiler">step 1: install a C compiler</h3>
<p>This is pretty simple: on an Ubuntu system if I don&rsquo;t already have a C compiler I&rsquo;ll install one with:</p>
<pre><code>sudo apt-get install build-essential
</code></pre>
<p>This installs <code>gcc</code>, <code>g++</code>, and <code>make</code>. The situation on a Mac is more
confusing but it&rsquo;s something like &ldquo;install xcode command line tools&rdquo;.</p>
<h3 id="step-2-install-the-program-s-dependencies">step 2: install the program&rsquo;s dependencies</h3>
<p>Unlike some newer programming languages, C doesn&rsquo;t have a dependency manager.
So if a program has any dependencies, you need to hunt them down yourself.
Thankfully because of this, C programmers usually keep their dependencies very
minimal and often the dependencies will be available in whatever package manager you&rsquo;re using.</p>
<p>There&rsquo;s almost always a section explaining how to get the dependencies in the
README, for example in <a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a>&rsquo;s README, it
says:</p>
<blockquote>
<p>To compile PaperJam, you need the headers for the libqpdf and libpaper libraries (usually available as libqpdf-dev and libpaper-dev packages).</p>
</blockquote>
<blockquote>
<p>You may need <code>a2x</code> (found in <a href="http://www.methods.co.nz/asciidoc/a2x.1.html">AsciiDoc</a>) for building manual pages.</p>
</blockquote>
<p>So on a Debian-based system you can install the dependencies like this.</p>
<pre><code>sudo apt install -y libqpdf-dev libpaper-dev
</code></pre>
<p>If a README gives a name for a package (like <code>libqpdf-dev</code>), I&rsquo;d basically
always assume that they mean &ldquo;in a Debian-based Linux distro&rdquo;: if you&rsquo;re on a
Mac <code>brew install libqpdf-dev</code> will not work. I still have not 100% gotten
the hang of developing on a Mac yet so I don&rsquo;t have many tips there yet. I
guess in this case it would be <code>brew install qpdf</code> if you&rsquo;re using Homebrew.</p>
<h3 id="step-3-run-configure-if-needed">step 3: run <code>./configure</code> (if needed)</h3>
<p>Some C programs come with a <code>Makefile</code> and some instead come with a script called
<code>./configure</code>. For example, if you download <a href="https://www.sqlite.org/download.html">sqlite&rsquo;s source code</a>, it has a <code>./configure</code> script in
it instead of a Makefile.</p>
<p>My understanding of this <code>./configure</code> script is:</p>
<ol>
<li>You run it, it prints out a lot of somewhat inscrutable output, and then it
either generates a <code>Makefile</code> or fails because you&rsquo;re missing some
dependency</li>
<li>The <code>./configure</code> script is part of a system called
<a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">autotools</a>
that I have never needed to learn anything about beyond &ldquo;run it to generate
a <code>Makefile</code>&rdquo;.</li>
</ol>
<p>I think there might be some options you can pass to get the <code>./configure</code>
script to produce a different <code>Makefile</code> but I have never done that.</p>
<h3 id="step-4-run-make">step 4: run <code>make</code></h3>
<p>The next step is to run <code>make</code> to try to build a program. Some notes about
<code>make</code>:</p>
<ul>
<li>Sometimes you can run <code>make -j8</code> to parallelize the build and make it go
faster</li>
<li>It usually prints out a million compiler warnings when compiling the program.
I always just ignore them. I didn&rsquo;t write the software! The compiler warnings
are not my problem.</li>
</ul>
<h3 id="compiler-errors-are-often-dependency-problems">compiler errors are often dependency problems</h3>
<p>Here&rsquo;s an error I got while compiling <code>paperjam</code> on my Mac:</p>
<pre><code>/opt/homebrew/Cellar/qpdf/12.0.0/include/qpdf/InputSource.hh:85:19: error: function definition does not declare parameters
   85 |     qpdf_offset_t last_offset{0};
      |                   ^
</code></pre>
<p>Over the years I&rsquo;ve learned it&rsquo;s usually best not to overthink problems like
this: if it&rsquo;s talking about <code>qpdf</code>, there&rsquo;s a good change it just means that
I&rsquo;ve done something wrong with how I&rsquo;m including the <code>qpdf</code> dependency.</p>
<p>Now let&rsquo;s talk about some ways to get the <code>qpdf</code> dependency included in the right way.</p>
<h3 id="the-world-s-shortest-introduction-to-the-compiler-and-linker">the world&rsquo;s shortest introduction to the compiler and linker</h3>
<p>Before we talk about how to fix dependency problems: building C programs is split into 2
steps:</p>
<ol>
<li><strong>Compiling</strong> the code into <strong>object files</strong> (with <code>gcc</code> or <code>clang</code>)</li>
<li><strong>Linking</strong> those object files into a final binary (with <code>ld</code>)</li>
</ol>
<p>It&rsquo;s important to know this when building a C program because sometimes you
need to pass the right flags to the compiler and linker to tell them where to
find the dependencies for the program you&rsquo;re compiling.</p>
<h3 id="make-uses-environment-variables-to-configure-the-compiler-and-linker"><code>make</code> uses environment variables to configure the compiler and linker</h3>
<p>If I run <code>make</code> on my Mac to install <code>paperjam</code>, I get this error:</p>
<pre><code>c++ -o paperjam paperjam.o pdf-tools.o parse.o cmds.o pdf.o -lqpdf -lpaper
ld: library 'qpdf' not found
</code></pre>
<p>This is not because <code>qpdf</code> is not installed on my system (it actually is!). But
the compiler and linker don&rsquo;t know how to <em>find</em> the <code>qpdf</code> library. To fix this, we need to:</p>
<ul>
<li>pass <code>&quot;-I/opt/homebrew/include&quot;</code> to the compiler (to tell it where to find the header files)</li>
<li>pass <code>&quot;-L/opt/homebrew/lib -liconv&quot;</code> to the linker (to tell it where to find library files and to link in <code>iconv</code>)</li>
</ul>
<p>And we can get <code>make</code> to pass those extra parameters to the compiler and linker using environment variables!
To see how this works: inside <code>paperjam</code>&rsquo;s Makefile you can see a bunch of environment variables, like <code>LDLIBS</code> here:</p>
<pre><code>paperjam: $(OBJS)
	$(LD) -o $@ $^ $(LDLIBS)
</code></pre>
<p>Everything you put into the <code>LDLIBS</code> environment variable gets passed to the
linker (<code>ld</code>) as a command line argument.</p>
<h3 id="secret-environment-variable-cppflags">secret environment variable: <code>CPPFLAGS</code></h3>
<p><code>Makefiles</code> sometimes define their own environment variables that they pass to
the compiler/linker, but <code>make</code> also has a bunch of &ldquo;implicit&rdquo; environment
variables which it will automatically pass to the C compiler and linker. There&rsquo;s a <a href="https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html#index-CFLAGS0">full list of implicit environment variables here</a>,
but one of them is <code>CPPFLAGS</code>, which gets automatically passed to the C compiler.</p>
<p>(technically it would be more normal to use <code>CXXFLAGS</code> for this, but this
particular <code>Makefile</code> hardcodes <code>CXXFLAGS</code> so setting <code>CPPFLAGS</code> was the only
way I could find to set the compiler flags without editing the <code>Makefile</code>)</p>
<small>
As an aside: it took me a long time to realize how closely tied to C/C++ `make` is -- I used
to think that `make` was just a general build system (and of course you can use it for
anything!) but it has a lot of affordances for building C/C++ programs that it
doesn't have for building any other kind of program.
</small>
<h3 id="two-ways-to-pass-environment-variables-to-make">two ways to pass environment variables to <code>make</code></h3>
<p>I learned thanks to <a href="https://www.owlfolio.org/">@zwol</a> that there are actually two ways to pass environment variables to <code>make</code>:</p>
<ol>
<li><code>CXXFLAGS=xyz make</code> (the usual way)</li>
<li><code>make CXXFLAGS=xyz</code></li>
</ol>
<p>The difference between them is that <code>make CXXFLAGS=xyz</code> will override the
value of <code>CXXFLAGS</code> set in the <code>Makefile</code> but <code>CXXFLAGS=xyz make</code> won&rsquo;t.</p>
<p>I&rsquo;m not sure which way is the norm but I&rsquo;m going to use the first way in this
post.</p>
<h3 id="how-to-use-cppflags-and-ldlibs-to-fix-this-compiler-error">how to use <code>CPPFLAGS</code> and <code>LDLIBS</code> to fix this compiler error</h3>
<p>Now that we&rsquo;ve talked about how <code>CPPFLAGS</code> and <code>LDLIBS</code> get passed to the
compiler and linker, here&rsquo;s the final incantation that I used to get the
program to build successfully!</p>
<pre><code>CPPFLAGS=&quot;-I/opt/homebrew/include&quot; LDLIBS=&quot;-L/opt/homebrew/lib -liconv&quot; make paperjam
</code></pre>
<p>This passes <code>-I/opt/homebrew/include</code> to the compiler and <code>-L/opt/homebrew/lib -liconv</code> to the linker.</p>
<p>Also I don&rsquo;t want to pretend that I &ldquo;magically&rdquo; knew that those were the right
arguments to pass, figuring them out involved a bunch of confused Googling that I
skipped over in this post. I will say that:</p>
<ul>
<li>the <code>-I</code> compiler flag tells the compiler which directory to find header files in, like <code>/opt/homebrew/include/qpdf/QPDF.hh</code></li>
<li>the <code>-L</code> linker flag tells the linker which directory to find libraries in, like <code>/opt/homebrew/lib/libqpdf.a</code></li>
<li>the <code>-l</code> linker flag tells the linker which libraries to link in, like <code>-liconv</code> means &ldquo;link in the <code>iconv</code> library&rdquo;, or <code>-lm</code> means &ldquo;link <code>math</code>&rdquo;</li>
</ul>
<h3 id="tip-how-to-just-build-1-specific-file-make-filename">tip: how to just build 1 specific file: <code>make $FILENAME</code></h3>
<p>Yesterday I discovered this cool tool called
<a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> which you can use to quickly
open files from the output of <code>ripgrep</code>.</p>
<p><code>qf</code> is in a big directory of various tools, but I only wanted to compile <code>qf</code>.
So I just compiled <code>qf</code>, like this:</p>
<pre><code>make qf
</code></pre>
<p>Basically if you know (or can guess) the output filename of the file you&rsquo;re
trying to build, you can tell <code>make</code> to just build that file by running <code>make $FILENAME</code></p>
<h3 id="tip-you-don-t-need-a-makefile">tip: you don&rsquo;t need a Makefile</h3>
<p>I sometimes write 5-line C programs with no dependencies, and I just learned
that if I have a file called <code>blah.c</code>, I can just compile it like this without creating a <code>Makefile</code>:</p>
<pre><code>make blah
</code></pre>
<p>It gets automaticaly expanded to <code>cc -o blah blah.c</code>, which saves a bit of
typing. I have no idea if I&rsquo;m going to remember this (I might just keep typing
<code>gcc -o blah blah.c</code> anyway) but it seems like a fun trick.</p>
<h3 id="tip-look-at-how-other-packaging-systems-built-the-same-c-program">tip: look at how other packaging systems built the same C program</h3>
<p>If you&rsquo;re having trouble building a C program, maybe other people had problems building it
too! Every Linux distribution has build files for every package that they
build, so even if you can&rsquo;t install packages from that distribution directly,
maybe you can get tips from that Linux distro for how to build the package.
Realizing this (thanks to my friend Dave) was a huge ah-ha moment for me.</p>
<p>For example, <a href="https://github.com/NixOS/nixpkgs/blob/405624e81a9b65378328accb0a11c3e5369e651c/pkgs/by-name/pa/paperjam/package.nix#L35">this line from the nix package for <code>paperjam</code></a> says:</p>
<pre><code>  env.NIX_LDFLAGS = lib.optionalString stdenv.hostPlatform.isDarwin &quot;-liconv&quot;;
</code></pre>
<p>This is basically saying &ldquo;pass the linker flag <code>-liconv</code> to build this on a
Mac&rdquo;, so that&rsquo;s a clue we could use to build it.</p>
<p>That same file also says <code>  env.NIX_CFLAGS_COMPILE = &quot;-DPOINTERHOLDER_TRANSITION=1&quot;;</code>. I&rsquo;m not sure what this means, but when I try
to build the <code>paperjam</code> package I do get an error about something called a
<code>PointerHolder</code>, so I guess that&rsquo;s somehow related to the &ldquo;PointerHolder
transition&rdquo;.</p>
<h3 id="step-5-installing-the-binary">step 5: installing the binary</h3>
<p>Once you&rsquo;ve managed to compile the program, probably you want to install it somewhere!
Some <code>Makefile</code>s have an <code>install</code> target that let you install the tool on your
system with <code>make install</code>. I&rsquo;m always a bit scared of this (where is it going
to put the files? what if I want to uninstall them later?), so if I&rsquo;m compiling
a pretty simple program I&rsquo;ll often just manually copy the binary to install it
instead, like this:</p>
<pre><code>cp qf ~/bin
</code></pre>
<h3 id="step-6-maybe-make-your-own-package">step 6: maybe make your own package!</h3>
<p>Once I figured out how to do all of this, I realized that I could use my new
<code>make</code> knowledge to contribute a <code>paperjam</code> package to Homebrew! Then I could
just <code>brew install paperjam</code> on future systems.</p>
<p>The good thing is that even if the details of how all of the different
packaging systems, they fundamentally all use C compilers and linkers.</p>
<h3 id="it-can-be-useful-to-understand-a-little-about-c-even-if-you-re-not-a-c-programmer">it can be useful to understand a little about C even if you&rsquo;re not a C programmer</h3>
<p>I think all of this is an interesting example of how it can useful to
understand some basics of how C programs work (like &ldquo;they have header files&rdquo;)
even if you&rsquo;re never planning to write a nontrivial C program if your life.</p>
<p>It feels good to have some ability to compile C/C++ programs myself, even
though I&rsquo;m still not totally confident about all of the compiler and linker
flags and I still plan to never learn anything about how autotools works other
than &ldquo;you run <code>./configure</code> to generate the <code>Makefile</code>&rdquo;.</p>
<p>Two things I left out of this post:</p>
<ul>
<li><code>LD_LIBRARY_PATH / DYLD_LIBRARY_PATH</code> (which you use to tell the dynamic
linker at runtime where to find dynamically linked files) because I can&rsquo;t
remember the last time I ran into an <code>LD_LIBRARY_PATH</code> issue and couldn&rsquo;t
find an example.</li>
<li><code>pkg-config</code>, which I think is important but I don&rsquo;t understand yet</li>
</ul>

---

### [Standards for ANSI escape codes](https://jvns.ca/blog/2025/03/07/escape-code-standards/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Today I want to talk about ANSI escape codes.</p>
<p>For a long time I was vaguely aware of ANSI escape codes (&ldquo;that&rsquo;s how you make
text red in the terminal and stuff&rdquo;) but I had no real understanding of where they were
supposed to be defined or whether or not there were standards for them. I just
had a kind of vague &ldquo;there be dragons&rdquo; feeling around them. While learning
about the terminal this year, I&rsquo;ve learned that:</p>
<ol>
<li>ANSI escape codes are responsible for a lot of usability improvements
in the terminal (did you know there&rsquo;s a way to copy to your system clipboard
when SSHed into a remote machine?? It&rsquo;s an escape code called <a href="https://jvns.ca/til/vim-osc52/">OSC 52</a>!)</li>
<li>They aren&rsquo;t completely standardized, and because of that they don&rsquo;t always
work reliably. And because they&rsquo;re also invisible, it&rsquo;s extremely
frustrating to troubleshoot escape code issues.</li>
</ol>
<p>So I wanted to put together a list for myself of some standards that exist
around escape codes, because I want to know if they <em>have</em> to feel unreliable
and frustrating, or if there&rsquo;s a future where we could all rely on them with
more confidence.</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#what-s-an-escape-code">what&rsquo;s an escape code?</a></li>
<li><a href="https://jvns.ca/atom.xml#ecma-48">ECMA-48</a></li>
<li><a href="https://jvns.ca/atom.xml#xterm-control-sequences">xterm control sequences</a></li>
<li><a href="https://jvns.ca/atom.xml#terminfo">terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#should-programs-use-terminfo">should programs use terminfo?</a></li>
<li><a href="https://jvns.ca/atom.xml#is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</a></li>
<li><a href="https://jvns.ca/atom.xml#some-reasons-to-use-terminfo">some reasons to use terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#some-more-documents-standards">some more documents/standards</a></li>
<li><a href="https://jvns.ca/atom.xml#why-i-think-this-is-interesting">why I think this is interesting</a></li>
</ul>
<h3 id="what-s-an-escape-code">what&rsquo;s an escape code?</h3>
<p>Have you ever pressed the left arrow key in your terminal and seen <code>^[[D</code>?
That&rsquo;s an escape code! It&rsquo;s called an &ldquo;escape code&rdquo; because the first character
is the &ldquo;escape&rdquo; character, which is usually written as <code>ESC</code>, <code>\x1b</code>, <code>\E</code>,
<code>\033</code>, or <code>^[</code>.</p>
<p>Escape codes are how your terminal emulator communicates various kinds of
information (colours, mouse movement, etc) with programs running in the
terminal. There are two kind of escape codes:</p>
<ol>
<li><strong>input codes</strong> which your terminal emulator sends for keypresses or mouse
movements that don&rsquo;t fit into Unicode. For example &ldquo;left arrow key&rdquo; is
<code>ESC[D</code>, &ldquo;Ctrl+left arrow&rdquo; might be <code>ESC[1;5D</code>, and clicking the mouse might
be something like <code>ESC[M :3</code>.</li>
<li><strong>output codes</strong> which programs can print out to colour text, move the
cursor around, clear the screen, hide the cursor, copy text to the
clipboard, enable mouse reporting, set the window title, etc.</li>
</ol>
<p>Now let&rsquo;s talk about standards!</p>
<h3 id="ecma-48">ECMA-48</h3>
<p>The first standard I found relating to escape codes was
<a href="https://ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA-48</a>,
which was originally published in 1976.</p>
<p>ECMA-48 does two things:</p>
<ol>
<li>Define some general <em>formats</em> for escape codes (like &ldquo;CSI&rdquo; codes, which are
<code>ESC[</code> + something and &ldquo;OSC&rdquo; codes, which are <code>ESC]</code> + something)</li>
<li>Define some specific escape codes, like how &ldquo;move the cursor to the left&rdquo; is
<code>ESC[D</code>, or &ldquo;turn text red&rdquo; is  <code>ESC[31m</code>. In the spec, the &ldquo;cursor left&rdquo;
one is called <code>CURSOR LEFT</code> and the one for changing colours is called
<code>SELECT GRAPHIC RENDITION</code>.</li>
</ol>
<p>The formats are extensible, so there&rsquo;s room for others to define more escape
codes in the future. Lots of escape codes that are popular today aren&rsquo;t defined
in ECMA-48: for example it&rsquo;s pretty common for terminal applications (like vim,
htop, or tmux) to support using the mouse, but ECMA-48 doesn&rsquo;t define escape
codes for the mouse.</p>
<h3 id="xterm-control-sequences">xterm control sequences</h3>
<p>There are a bunch of escape codes that aren&rsquo;t defined in ECMA-48, for example:</p>
<ul>
<li>enabling mouse reporting (where did you click in your terminal?)</li>
<li>bracketed paste (did you paste that text or type it in?)</li>
<li>OSC 52 (which terminal applications can use to copy text to your system clipboard)</li>
</ul>
<p>I believe (correct me if I&rsquo;m wrong!) that these and some others came from
xterm, are documented in <a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html">XTerm Control Sequences</a>, and have
been widely implemented by other terminal emulators.</p>
<p>This list of &ldquo;what xterm supports&rdquo; is not a standard exactly, but xterm is
extremely influential and so it seems like an important document.</p>
<h3 id="terminfo">terminfo</h3>
<p>In the 80s (and to some extent today, but my understanding is that it was MUCH
more dramatic in the 80s) there was a huge amount of variation in what escape
codes terminals actually supported.</p>
<p>To deal with this, there&rsquo;s a database of escape codes for various terminals
called &ldquo;terminfo&rdquo;.</p>
<p>It looks like the standard for terminfo is called <a href="https://publications.opengroup.org/c243-1">X/Open Curses</a>, though you need to create
an account to view that standard for some reason. It defines the database format as well
as a C library interface (&ldquo;curses&rdquo;) for accessing the database.</p>
<p>For example you can run this bash snippet to see every possible escape code for
&ldquo;clear screen&rdquo; for all of the different terminals your system knows about:</p>
<pre><code>for term in $(toe -a | awk '{print $1}')
do
  echo $term
  infocmp -1 -T &quot;$term&quot; 2&gt;/dev/null | grep 'clear=' | sed 's/clear=//g;s/,//g'
done
</code></pre>
<p>On my system (and probably every system I&rsquo;ve ever used?), the terminfo database is managed by ncurses.</p>
<h3 id="should-programs-use-terminfo">should programs use terminfo?</h3>
<p>I think it&rsquo;s interesting that there are two main approaches that applications
take to handling ANSI escape codes:</p>
<ol>
<li>Use the terminfo database to figure out which escape codes to use, depending
on what&rsquo;s in the <code>TERM</code> environment variable. Fish does this, for example.</li>
<li>Identify a &ldquo;single common set&rdquo; of escape codes which works in &ldquo;enough&rdquo;
terminal emulators and just hardcode those.</li>
</ol>
<p>Some examples of programs/libraries that take approach #2 (&ldquo;don&rsquo;t use terminfo&rdquo;) include:</p>
<ul>
<li><a href="https://github.com/mawww/kakoune/commit/c12699d2e9c2806d6ed184032078d0b84a3370bb">kakoune</a></li>
<li><a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/165258d2f3ae594b50f16c7b50ffb06627476269/src/prompt_toolkit/input/ansi_escape_sequences.py#L5-L8">python-prompt-toolkit</a></li>
<li><a href="https://github.com/antirez/linenoise">linenoise</a></li>
<li><a href="https://github.com/rockorager/libvaxis">libvaxis</a></li>
<li><a href="https://github.com/chalk/chalk">chalk</a></li>
</ul>
<p>I got curious about why folks might be moving away from terminfo and I found
this very interesting and extremely detailed
<a href="https://twoot.site/@bean/113056942625234032">rant about terminfo from one of the fish maintainers</a>, which argues that:</p>
<blockquote>
<p>[the terminfo authors] have done a lot of work that, at the time, was
extremely important and helpful. My point is that it no longer is.</p>
</blockquote>
<p>I&rsquo;m not going to do it justice so I&rsquo;m not going to summarize it, I think it&rsquo;s
worth reading.</p>
<h3 id="is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</h3>
<p>I was just talking about the idea that you can use a &ldquo;common set&rdquo; of escape
codes that will work for most people. But what is that set? Is there any agreement?</p>
<p>I really do not know the answer to this at all, but from doing some reading it
seems like it&rsquo;s some combination of:</p>
<ul>
<li>The codes that the VT100 supported (though some aren&rsquo;t relevant on modern terminals)</li>
<li>what&rsquo;s in ECMA-48 (which I think also has some things that are no longer relevant)</li>
<li>What xterm supports (though I&rsquo;d guess that not everything in there is actually widely supported enough)</li>
</ul>
<p>and maybe ultimately &ldquo;identify the terminal emulators you think your users are
going to use most frequently and test in those&rdquo;, the same way web developers do
when deciding which CSS features are okay to use</p>
<p>I don&rsquo;t think there are any resources like <a href="https://caniuse.com/">Can I use&hellip;?</a> or
<a href="https://web-platform-dx.github.io/web-features/">Baseline</a> for the terminal
though. (in theory terminfo is supposed to be the &ldquo;caniuse&rdquo; for the terminal
but it seems like it often takes 10+ years to add new terminal features when
people invent them which makes it very limited)</p>
<h3 id="some-reasons-to-use-terminfo">some reasons to use terminfo</h3>
<p>I also asked on Mastodon why people found terminfo valuable in 2025 and got a
few reasons that made sense to me:</p>
<ul>
<li>some people expect to be able to use the <code>TERM</code> environment variable to
control how programs behave (for example with <code>TERM=dumb</code>), and there&rsquo;s
no standard for how that should work in a post-terminfo world</li>
<li>even though there&rsquo;s <em>less</em> variation between terminal emulators than
there was in the 80s, there&rsquo;s far from zero variation: there are graphical
terminals, the Linux framebuffer console, the situation you&rsquo;re in when
connecting to a server via its serial console, Emacs shell mode, and probably
more that I&rsquo;m missing</li>
<li>there is no one standard for what the &ldquo;single common set&rdquo; of escape codes
is, and sometimes programs use escape codes which aren&rsquo;t actually widely
supported enough</li>
</ul>
<h3 id="terminfo-user-agent-detection">terminfo &amp; user agent detection</h3>
<p>The way that ncurses uses the <code>TERM</code> environment variable to decide which
escape codes to use reminds me of how webservers used to sometimes use the
browser user agent to decide which version of a website to serve.</p>
<p>It also seems like it&rsquo;s had some of the same results &ndash; the way iTerm2 reports
itself as being &ldquo;xterm-256color&rdquo; feels similar to how Safari&rsquo;s user agent is
&ldquo;Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7_4) AppleWebKit/605.1.15 (KHTML,
like Gecko) Version/18.3 Safari/605.1.15&rdquo;. In both cases the terminal emulator
/ browser ends up changing its user agent to get around user agent detection
that isn&rsquo;t working well.</p>
<p>On the web we ended up deciding that user agent detection was not a good
practice and to instead focus on standardization so we can serve the same
HTML/CSS to all browsers. I don&rsquo;t know if the same approach is the future in
the terminal though &ndash; I think the terminal landscape today is much more
fragmented than the web ever was as well as being much less well funded.</p>
<h3 id="some-more-documents-standards">some more documents/standards</h3>
<p>A few more documents and standards related to escape codes, in no particular order:</p>
<ul>
<li>the <a href="https://man7.org/linux/man-pages/man4/console_codes.4.html">Linux console_codes man page</a> documents
escape codes that Linux supports</li>
<li>how the <a href="https://vt100.net/docs/vt100-ug/chapter3.html">VT 100</a> handles escape codes &amp; control sequences</li>
<li>the <a href="https://sw.kovidgoyal.net/kitty/keyboard-protocol/">kitty keyboard protocol</a></li>
<li><a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda">OSC 8</a> for links in the terminal (and notes on <a href="https://github.com/Alhadis/OSC8-Adoption?tab=readme-ov-file">adoption</a>)</li>
<li>A <a href="https://github.com/tmux/tmux/blob/882fb4d295deb3e4b803eb444915763305114e4f/tools/ansicode.txt">summary of ANSI standards from tmux</a></li>
<li>this <a href="https://iterm2.com/feature-reporting/">terminal features reporting specification from iTerm</a></li>
<li>sixel graphics</li>
</ul>
<h3 id="why-i-think-this-is-interesting">why I think this is interesting</h3>
<p>I sometimes see people saying that the unix terminal is &ldquo;outdated&rdquo;, and since I
love the terminal so much I&rsquo;m always curious about what incremental changes
might make it feel less &ldquo;outdated&rdquo;.</p>
<p>Maybe if we had a clearer standards landscape (like we do on the web!) it would
be easier for terminal emulator developers to build new features and for
authors of terminal applications to more confidently adopt those features so
that we can all benefit from them and have a richer experience in the terminal.</p>
<p>Obviously standardizing ANSI escape codes is not easy (ECMA-48 was first
published almost 50 years ago and we&rsquo;re still not there!). I don&rsquo;t even know
what all of the challenges are. But the situation with HTML/CSS/JS used to be
extremely bad too and now it&rsquo;s MUCH better, so maybe there&rsquo;s hope.</p>

---

### [How to add a directory to your PATH](https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I was talking to a friend about how to add a directory to your PATH today. It&rsquo;s
something that feels &ldquo;obvious&rdquo; to me since I&rsquo;ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn&rsquo;t find something that explained all of the steps &ndash; a lot of them just
said &ldquo;add this to <code>~/.bashrc</code>&rdquo;, but what if you&rsquo;re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?</p>
<p>So I wanted to try to write down some more complete directions and mention some
of the gotchas I&rsquo;ve run into over the years.</p>
<p>Here&rsquo;s a table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-1-what-shell-are-you-using">step 1: what shell are you using?</a></li>
<li><a href="https://jvns.ca/atom.xml#step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-4-edit-your-shell-config">step 4: edit your shell config</a></li>
<li><a href="https://jvns.ca/atom.xml#step-5-restart-your-shell">step 5: restart your shell</a></li>
<li>problems:
<ul>
<li><a href="https://jvns.ca/atom.xml#problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate PATH entries making it harder to debug</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your PATH</a></li>
</ul>
</li>
<li>notes:
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-source">a note on source</a></li>
<li><a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">a note on fish_add_path</a></li>
</ul>
</li>
</ul>
<h3 id="step-1-what-shell-are-you-using">step 1: what shell are you using?</h3>
<p>If you&rsquo;re not sure what shell you&rsquo;re using, here&rsquo;s a way to find out. Run this:</p>
<pre><code>ps -p $$ -o pid,comm=
</code></pre>
<ul>
<li>if you&rsquo;re using <strong>bash</strong>, it&rsquo;ll print out <code>97295 bash</code></li>
<li>if you&rsquo;re using <strong>zsh</strong>, it&rsquo;ll print out <code>97295 zsh</code></li>
<li>if you&rsquo;re using <strong>fish</strong>, it&rsquo;ll print out an error like &ldquo;In fish, please use
$fish_pid&rdquo; (<code>$$</code> isn&rsquo;t valid syntax in fish, but in any case the error
message tells you that you&rsquo;re using fish, which you probably already knew)</li>
</ul>
<p>Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I&rsquo;ll only cover bash, zsh, and fish in these directions.</p>
<h3 id="step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</h3>
<ul>
<li>in zsh, it&rsquo;s probably <code>~/.zshrc</code></li>
<li>in bash, it might be <code>~/.bashrc</code>, but it&rsquo;s complicated, see the note in the next section</li>
<li>in fish, it&rsquo;s probably <code>~/.config/fish/config.fish</code> (you can run <code>echo $__fish_config_dir</code> if you want to be 100% sure)</li>
</ul>
<h3 id="a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</h3>
<p>Bash has three possible config files: <code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.profile</code>.</p>
<p>If you&rsquo;re not sure which one your system is set up to use, I&rsquo;d recommend
testing this way:</p>
<ol>
<li>add <code>echo hi there</code> to your <code>~/.bashrc</code></li>
<li>Restart your terminal</li>
<li>If you see &ldquo;hi there&rdquo;, that means <code>~/.bashrc</code> is being used! Hooray!</li>
<li>Otherwise remove it and try the same thing with <code>~/.bash_profile</code></li>
<li>You can also try <code>~/.profile</code> if the first two options don&rsquo;t work.</li>
</ol>
<p>(there are a lot of <a href="https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html">elaborate flow charts</a> out there that explain how bash
decides which config file to use but IMO it&rsquo;s not worth it to internalize them
and just testing is the fastest way to be sure)</p>
<h3 id="step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</h3>
<p>Let&rsquo;s say that you&rsquo;re trying to install and run a program called <code>http-server</code>
and it doesn&rsquo;t work, like this:</p>
<pre><code>$ npm install -g http-server
$ http-server
bash: http-server: command not found
</code></pre>
<p>How do you find what directory <code>http-server</code> is in? Honestly in general this is
not that easy &ndash; often the answer is something like &ldquo;it depends on how npm is
configured&rdquo;. A few ideas:</p>
<ul>
<li>Often when setting up a new installer (like <code>cargo</code>, <code>npm</code>, <code>homebrew</code>, etc),
when you first set it up it&rsquo;ll print out some directions about how to update
your PATH. So if you&rsquo;re paying attention you can get the directions then.</li>
<li>Sometimes installers will automatically update your shell&rsquo;s config file
to update your <code>PATH</code> for you</li>
<li>Sometimes just Googling &ldquo;where does npm install things?&rdquo; will turn up the
answer</li>
<li>Some tools have a subcommand that tells you where they&rsquo;re configured to
install things, like:
<ul>
<li>Node/npm: <code>npm config get prefix</code> (then append <code>/bin/</code>)</li>
<li>Go: <code>go env GOPATH</code> (then append <code>/bin/</code>)</li>
<li>asdf: <code>asdf info | grep ASDF_DIR</code> (then append <code>/bin/</code> and <code>/shims/</code>)</li>
</ul>
</li>
</ul>
<h3 id="step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</h3>
<p>Once you&rsquo;ve found a directory you think might be the right one, make sure it&rsquo;s
actually correct! For example, I found out that on my machine, <code>http-server</code> is
in <code>~/.npm-global/bin</code>. I can make sure that it&rsquo;s the right directory by trying to
run the program <code>http-server</code> in that directory like this:</p>
<pre><code>$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
</code></pre>
<p>It worked! Now that you know what directory you need to add to your <code>PATH</code>,
let&rsquo;s move to the next step!</p>
<h3 id="step-4-edit-your-shell-config">step 4: edit your shell config</h3>
<p>Now we have the 2 critical pieces of information we need:</p>
<ol>
<li>Which directory you&rsquo;re trying to add to your PATH (like  <code>~/.npm-global/bin/</code>)</li>
<li>Where your shell&rsquo;s config is (like <code>~/.bashrc</code>, <code>~/.zshrc</code>, or <code>~/.config/fish/config.fish</code>)</li>
</ol>
<p>Now what you need to add depends on your shell:</p>
<p><strong>bash instructions:</strong></p>
<p>Open your shell&rsquo;s config file, and add a line like this:</p>
<pre><code>export PATH=$PATH:~/.npm-global/bin/
</code></pre>
<p>(obviously replace <code>~/.npm-global/bin</code> with the actual directory you&rsquo;re trying to add)</p>
<p><strong>zsh instructions:</strong></p>
<p>You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:</p>
<pre><code>path=(
  $path
  ~/.npm-global/bin
)
</code></pre>
<p><strong>fish instructions:</strong></p>
<p>In fish, the syntax is different:</p>
<pre><code>set PATH $PATH ~/.npm-global/bin
</code></pre>
<p>(in fish you can also use <code>fish_add_path</code>, some notes on that <a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">further down</a>)</p>
<h3 id="step-5-restart-your-shell">step 5: restart your shell</h3>
<p>Now, an extremely important step: updating your shell&rsquo;s config won&rsquo;t take
effect if you don&rsquo;t restart it!</p>
<p>Two ways to do this:</p>
<ol>
<li>open a new terminal (or terminal tab), and maybe close the old one so you don&rsquo;t get confused</li>
<li>Run <code>bash</code> to start a new shell (or <code>zsh</code> if you&rsquo;re using zsh, or <code>fish</code> if you&rsquo;re using fish)</li>
</ol>
<p>I&rsquo;ve found that both of these usually work fine.</p>
<p>And you should be done! Try running the program you were trying to run and
hopefully it works now.</p>
<p>If not, here are a couple of problems that you might run into:</p>
<h3 id="problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</h3>
<p>If the wrong <strong>version</strong> of a program is running, you might need to add the
directory to the <em>beginning</em> of your PATH instead of the end.</p>
<p>For example, on my system I have two versions of <code>python3</code> installed, which I
can see by running <code>which -a</code>:</p>
<pre><code>$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
</code></pre>
<p>The one your shell will use is the <strong>first one listed</strong>.</p>
<p>If you want to use the Homebrew version, you need to add that directory
(<code>/opt/homebrew/bin</code>) to the <strong>beginning</strong> of your PATH instead, by putting this in
your shell&rsquo;s config file (it&rsquo;s <code>/opt/homebrew/bin/:$PATH</code> instead of the usual <code>$PATH:/opt/homebrew/bin/</code>)</p>
<pre><code>export PATH=/opt/homebrew/bin/:$PATH
</code></pre>
<p>or in fish:</p>
<pre><code>set PATH ~/.cargo/bin $PATH
</code></pre>
<h3 id="problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</h3>
<p>All of these directions only work if you&rsquo;re running the program <strong>from your
shell</strong>. If you&rsquo;re running the program from an IDE, from a GUI, in a cron job,
or some other way, you&rsquo;ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.</p>
<p><strong>in a cron job</strong></p>
<p>Some options:</p>
<ul>
<li>use the full path to the program you&rsquo;re running, like <code>/home/bork/bin/my-program</code></li>
<li>put the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:&hellip;.). You can get the full PATH you&rsquo;re
using in your shell by running <code>echo &quot;PATH=$PATH&quot;</code>.</li>
</ul>
<p>I&rsquo;m honestly not sure how to handle it in an IDE/GUI because I haven&rsquo;t run into
that in a long time, will add directions here if someone points me in the right
direction.</p>
<h3 id="problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate <code>PATH</code> entries making it harder to debug</h3>
<p>If you edit your path and start a new shell by running <code>bash</code> (or <code>zsh</code>, or
<code>fish</code>), you&rsquo;ll often end up with duplicate <code>PATH</code> entries, because the shell
keeps adding new things to your <code>PATH</code> every time you start your shell.</p>
<p>Personally I don&rsquo;t think I&rsquo;ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what&rsquo;s going on with your <code>PATH</code> if you&rsquo;re trying to understand its contents.</p>
<p>Some ways you could deal with this:</p>
<ol>
<li>If you&rsquo;re debugging your <code>PATH</code>, open a new terminal to do it in so you get
a &ldquo;fresh&rdquo; state. This should avoid the duplication.</li>
<li>Deduplicate your <code>PATH</code> at the end of your shell&rsquo;s config  (for example in
zsh apparently you can do this with <code>typeset -U path</code>)</li>
<li>Check that the directory isn&rsquo;t already in your <code>PATH</code> when adding it (for
example in fish I believe you can do this with <code>fish_add_path --path /some/directory</code>)</li>
</ol>
<p>How to deduplicate your <code>PATH</code> is shell-specific and there isn&rsquo;t always a
built in way to do it so you&rsquo;ll need to look up how to accomplish it in your
shell.</p>
<h3 id="problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your <code>PATH</code></h3>
<p>Here&rsquo;s a situation that&rsquo;s easy to get into in bash or zsh:</p>
<ol>
<li>Run a command (it fails)</li>
<li>Update your <code>PATH</code></li>
<li>Run <code>bash</code> to reload your config</li>
<li>Press the up arrow a couple of times to rerun the failed command (or open a new terminal)</li>
<li>The failed command isn&rsquo;t in your history! Why not?</li>
</ol>
<p>This happens because in bash, by default, history is not saved until you exit
the shell.</p>
<p>Some options for fixing this:</p>
<ul>
<li>Instead of running <code>bash</code> to reload your config, run <code>source ~/.bashrc</code> (or
<code>source ~/.zshrc</code> in zsh). This will reload the config inside your current
session.</li>
<li>Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you&rsquo;re
using bash or zsh, the history options in zsh are a bit complicated and I&rsquo;m
not exactly sure what the best way is)</li>
</ul>
<h3 id="a-note-on-source">a note on <code>source</code></h3>
<p>When you install <code>cargo</code> (Rust&rsquo;s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don&rsquo;t mention a specific
directory at all.</p>
<pre><code>This is usually done by running one of the following (note the leading DOT):

. &quot;$HOME/.cargo/env&quot;        	# For sh/bash/zsh/ash/dash/pdksh
source &quot;$HOME/.cargo/env.fish&quot;  # For fish
</code></pre>
<p>The idea is that you add that line to your shell&rsquo;s config, and their script
automatically sets up your <code>PATH</code> (and potentially other things) for you.</p>
<p>This is pretty common (for example <a href="https://github.com/Homebrew/install/blob/deacfa6a6e62e5f4002baf9e1fac7a96e9aa5d41/install.sh#L1072-L1087">Homebrew</a> suggests you eval <code>brew shellenv</code>), and there are
two ways to approach this:</p>
<ol>
<li>Just do what the tool suggests (like adding <code>. &quot;$HOME/.cargo/env&quot;</code> to your shell&rsquo;s config)</li>
<li>Figure out which directories the script they&rsquo;re telling you to run would add
to your PATH, and then add those manually. Here&rsquo;s how I&rsquo;d do that:
<ul>
<li>Run <code>. &quot;$HOME/.cargo/env&quot;</code> in my shell (or the fish version if using fish)</li>
<li>Run <code>echo &quot;$PATH&quot; | tr ':' '\n' | grep cargo</code> to figure out which directories it added</li>
<li>See that it says <code>/Users/bork/.cargo/bin</code> and shorten that to <code>~/.cargo/bin</code></li>
<li>Add the directory <code>~/.cargo/bin</code> to PATH (with the directions in this post)</li>
</ul>
</li>
</ol>
<p>I don&rsquo;t think there&rsquo;s anything wrong with doing what the tool suggests (it
might be the &ldquo;best way&rdquo;!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I&rsquo;m changing.</p>
<h3 id="a-note-on-fish-add-path">a note on <code>fish_add_path</code></h3>
<p>fish has a handy function called <code>fish_add_path</code> that you can run to add a directory to your <code>PATH</code> like this:</p>
<pre><code>fish_add_path /some/directory
</code></pre>
<p>This is cool (it&rsquo;s such a simple command!) but I&rsquo;ve stopped using it for a couple of reasons:</p>
<ol>
<li>Sometimes <code>fish_add_path</code> will update the <code>PATH</code> for every session in the
future (with a &ldquo;universal variable&rdquo;) and sometimes it will update the <code>PATH</code>
just for the current session and it&rsquo;s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.</li>
<li>If you ever need to <em>remove</em> the directory from your <code>PATH</code> a few weeks or
months later because maybe you made a mistake, it&rsquo;s kind of hard to do
(there are <a href="https://github.com/fish-shell/fish-shell/issues/8604">instructions in this comments of this github issue though</a>).</li>
</ol>
<h3 id="that-s-all">that&rsquo;s all</h3>
<p>Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!</p>

---

### [Some terminal frustrations](https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>A few weeks ago I ran a terminal survey (you can <a href="https://jvns.ca/terminal-survey/results-bsky.html">read the results here</a>) and at the end I asked:</p>
<blockquote>
<p>What‚Äôs the most frustrating thing about using the terminal for you?</p>
</blockquote>
<p>1600 people answered, and I decided to spend a few days categorizing all the
responses. Along the way I learned that classifying qualitative data is not
easy but I gave it my best shot. I ended up building a custom
<a href="https://github.com/jvns/classificator">tool</a> to make it faster to categorize
everything.</p>
<p>As with all of my surveys the methodology isn&rsquo;t particularly scientific. I just
posted the survey to Mastodon and Twitter, ran it for a couple of days, and got
answers from whoever happened to see it and felt like responding.</p>
<p>Here are the top categories of frustrations!</p>
<p>I think it&rsquo;s worth keeping in mind while reading these comments that</p>
<ul>
<li>40% of people answering this survey have been using the terminal for <strong>21+ years</strong></li>
<li>95% of people answering the survey have been using the terminal for at least 4 years</li>
</ul>
<p>These comments aren&rsquo;t coming from total beginners.</p>
<p>Here are the categories of frustrations! The number in brackets is the number
of people with that frustration. I&rsquo;m mostly writing this up for myself because
I&rsquo;m trying to write a zine about the terminal and I wanted to get a sense for
what people are having trouble with.</p>
<h3 id="remembering-syntax-115">remembering syntax (115)</h3>
<p>People talked about struggles remembering:</p>
<ul>
<li>the syntax for CLI tools like awk, jq, sed, etc</li>
<li>the syntax for redirects</li>
<li>keyboard shortcuts for tmux, text editing, etc</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>There are just so many little &ldquo;trivia&rdquo; details to remember for full
functionality. Even after all these years I&rsquo;ll sometimes forget where it&rsquo;s 2
or 1 for stderr, or forget which is which for <code>&gt;</code> and <code>&gt;&gt;</code>.</p>
</blockquote>
<h3 id="switching-terminals-is-hard-91">switching terminals is hard (91)</h3>
<p>People talked about struggling with switching systems (for example home/work
computer or when SSHing) and running into:</p>
<ul>
<li>OS differences in keyboard shortcuts (like Linux vs Mac)</li>
<li>systems which don&rsquo;t have their preferred text editor (&ldquo;no vim&rdquo; or &ldquo;only vim&rdquo;)</li>
<li>different versions of the same command (like Mac OS grep vs GNU grep)</li>
<li>no tab completion</li>
<li>a shell they aren&rsquo;t used to (&ldquo;the subtle differences between zsh and bash&rdquo;)</li>
</ul>
<p>as well as differences inside the same system like pagers being not consistent
with each other (git diff pagers, other pagers).</p>
<p>One example comment:</p>
<blockquote>
<p>I got used to fish and vi mode which are not available when I ssh into
servers, containers.</p>
</blockquote>
<h3 id="color-85">color (85)</h3>
<p>Lots of problems with color, like:</p>
<ul>
<li>programs setting colors that are unreadable with a light background color</li>
<li>finding a colorscheme they like (and getting it to work consistently across different apps)</li>
<li>color not working inside several layers of SSH/tmux/etc</li>
<li>not liking the defaults</li>
<li>not wanting color at all and struggling to turn it off</li>
</ul>
<p>This comment felt relatable to me:</p>
<blockquote>
<p>Getting my terminal theme configured in a reasonable way between the terminal
emulator and fish (I did this years ago and remember it being tedious and
fiddly and now feel like I&rsquo;m locked into my current theme because it works
and I dread touching any of that configuration ever again).</p>
</blockquote>
<h3 id="keyboard-shortcuts-84">keyboard shortcuts (84)</h3>
<p>Half of the comments on keyboard shortcuts were about how on Linux/Windows, the
keyboard shortcut to copy/paste in the terminal is different from in the rest
of the OS.</p>
<p>Some other issues with keyboard shortcuts other than copy/paste:</p>
<ul>
<li>using <code>Ctrl-W</code> in a browser-based terminal and closing the window</li>
<li>the terminal only supports a limited set of keyboard shortcuts (no
<code>Ctrl-Shift-</code>, no <code>Super</code>, no <code>Hyper</code>, lots of <code>ctrl-</code> shortcuts aren&rsquo;t
possible like <code>Ctrl-,</code>)</li>
<li>the OS stopping you from using a terminal keyboard shortcut (like by default
Mac OS uses <code>Ctrl+left arrow</code> for something else)</li>
<li>issues using emacs in the terminal</li>
<li>backspace not working (2)</li>
</ul>
<h3 id="other-copy-and-paste-issues-75">other copy and paste issues (75)</h3>
<p>Aside from &ldquo;the keyboard shortcut for copy and paste is different&rdquo;, there were
a lot of OTHER issues with copy and paste, like:</p>
<ul>
<li>copying over SSH</li>
<li>how tmux and the terminal emulator both do copy/paste in different ways</li>
<li>dealing with many different clipboards (system clipboard, vim clipboard, the
&ldquo;middle click&rdquo; clipboard on Linux, tmux&rsquo;s clipboard, etc) and potentially
synchronizing them</li>
<li>random spaces added when copying from the terminal</li>
<li>pasting multiline commands which automatically get run in a terrifying way</li>
<li>wanting a way to copy text without using the mouse</li>
</ul>
<h3 id="discoverability-55">discoverability (55)</h3>
<p>There were lots of comments about this, which all came down to the same basic
complaint &ndash; it&rsquo;s hard to discover useful tools or features! This comment kind of
summed it all up:</p>
<blockquote>
<p>How difficult it is to learn independently. Most of what I know is an
assorted collection of stuff I&rsquo;ve been told by random people over the years.</p>
</blockquote>
<h3 id="steep-learning-curve-44">steep learning curve (44)</h3>
<p>A lot of comments about it generally having a steep learning curve. A couple of
example comments:</p>
<blockquote>
<p>After 15 years of using it, I‚Äôm not much faster than using it than I was 5 or
maybe even 10 years ago.</p>
</blockquote>
<p>and</p>
<blockquote>
<p>That I know I could make my life easier by learning more about the shortcuts
and commands and configuring the terminal but I don&rsquo;t spend the time because it
feels overwhelming.</p>
</blockquote>
<h3 id="history-42">history  (42)</h3>
<p>Some issues with shell history:</p>
<ul>
<li>history not being shared between terminal tabs (16)</li>
<li>limits that are too short (4)</li>
<li>history not being restored when terminal tabs are restored</li>
<li>losing history because the terminal crashed</li>
<li>not knowing how to search history</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>It wasted a lot of time until I figured it out and still annoys me that
&ldquo;history&rdquo; on zsh has such a small buffer;  I have to type &ldquo;history 0&rdquo; to get
any useful length of history.</p>
</blockquote>
<h3 id="bad-documentation-37">bad documentation (37)</h3>
<p>People talked about:</p>
<ul>
<li>documentation being generally opaque</li>
<li>lack of examples in man pages</li>
<li>programs which don&rsquo;t have man pages</li>
</ul>
<p>Here&rsquo;s a representative comment:</p>
<blockquote>
<p>Finding good examples and docs. Man pages often not enough, have to wade
through stack overflow</p>
</blockquote>
<h3 id="scrollback-36">scrollback (36)</h3>
<p>A few issues with scrollback:</p>
<ul>
<li>programs printing out too much data making you lose scrollback history</li>
<li>resizing the terminal messes up the scrollback</li>
<li>lack of timestamps</li>
<li>GUI programs that you start in the background printing stuff out that gets in
the way of other programs&rsquo; outputs</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>When resizing the terminal (in particular: making it narrower) leads to
broken rewrapping of the scrollback content because the commands formatted
their output based on the terminal window width.</p>
</blockquote>
<h3 id="it-feels-outdated-33">&ldquo;it feels outdated&rdquo; (33)</h3>
<p>Lots of comments about how the terminal feels hampered by legacy decisions and
how users often end up needing to learn implementation details that feel very
esoteric. One example comment:</p>
<blockquote>
<p>Most of the legacy cruft, it would be great to have a green field
implementation of the CLI interface.</p>
</blockquote>
<h3 id="shell-scripting-32">shell scripting (32)</h3>
<p>Lots of complaints about POSIX shell scripting. There&rsquo;s a general feeling that
shell scripting is difficult but also that switching to a different less
standard scripting language (fish, nushell, etc) brings its own problems.</p>
<blockquote>
<p>Shell scripting. My tolerance to ditch a shell script and go to a scripting
language is pretty low. It‚Äôs just too messy and powerful. Screwing up can be
costly so I don‚Äôt even bother.</p>
</blockquote>
<h3 id="more-issues">more issues</h3>
<p>Some more issues that were mentioned at least 10 times:</p>
<ul>
<li>(31) inconsistent command line arguments: is it -h or help or &ndash;help?</li>
<li>(24) keeping dotfiles in sync across different systems</li>
<li>(23) performance (e.g. &ldquo;my shell takes too long to start&rdquo;)</li>
<li>(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)</li>
<li>(17) generally feeling scared/uneasy (&ldquo;The debilitating fear that I‚Äôm going
to do some mysterious Bad Thing with a command and I will have absolutely no
idea how to fix or undo it or even really figure out what happened&rdquo;)</li>
<li>(16) terminfo issues (&ldquo;Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.&rdquo;)</li>
<li>(16) lack of image support (sixel etc)</li>
<li>(15) SSH issues (like having to start over when you lose the SSH connection)</li>
<li>(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)</li>
<li>(15) typos &amp; slow typing</li>
<li>(13) the terminal getting messed up for various reasons (pressing <code>Ctrl-S</code>, <code>cat</code>ing a binary, etc)</li>
<li>(12) quoting/escaping in the shell</li>
<li>(11) various Windows/PowerShell issues</li>
</ul>
<h3 id="n-a-122">n/a (122)</h3>
<p>There were also 122 answers to the effect of &ldquo;nothing really&rdquo; or &ldquo;only that I
can&rsquo;t do EVERYTHING in the terminal&rdquo;</p>
<p>One example comment:</p>
<blockquote>
<p>Think I&rsquo;ve found work arounds for most/all frustrations</p>
</blockquote>
<h3 id="that-s-all">that&rsquo;s all!</h3>
<p>I&rsquo;m not going to make a lot of commentary on these results, but here are a
couple of categories that feel related to me:</p>
<ul>
<li>remembering syntax &amp; history (often the thing you need to remember is something you&rsquo;ve run before!)</li>
<li>discoverability &amp; the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)</li>
<li>&ldquo;switching systems is hard&rdquo; &amp; &ldquo;it feels outdated&rdquo; (tools that haven&rsquo;t really
changed in 30 or 40 years have many problems but they do tend to be always
<em>there</em> no matter what system you&rsquo;re on, which is very useful and makes them
hard to stop using)</li>
</ul>
<p>Trying to categorize all these results in a reasonable way really gave me an
appreciation for social science researchers&rsquo; skills.</p>

---

### [What's involved in getting a "modern" terminal setup?](https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Recently I ran a terminal survey and I asked people what frustrated
them. One person commented:</p>
<blockquote>
<p>There are so many pieces to having a modern terminal experience. I wish it
all came out of the box.</p>
</blockquote>
<p>My immediate reaction was &ldquo;oh, getting a modern terminal experience isn&rsquo;t that
hard, you just need to&hellip;.&rdquo;, but the more I thought about it, the longer the
&ldquo;you just need to&hellip;&rdquo; list got, and I kept thinking about more and more
caveats.</p>
<p>So I thought I would write down some notes about what it means to me personally
to have a &ldquo;modern&rdquo; terminal experience and what I think can make it hard for
people to get there.</p>
<h3 id="what-is-a-modern-terminal-experience">what is a &ldquo;modern terminal experience&rdquo;?</h3>
<p>Here are a few things that are important to me, with which part of the system
is responsible for them:</p>
<ul>
<li><strong>multiline support for copy and paste</strong>: if you paste 3 commands in your shell, it should not immediately run them all! That&rsquo;s scary! (<strong>shell</strong>, <strong>terminal emulator</strong>)</li>
<li><strong>infinite shell history</strong>: if I run a command in my shell, it should be saved forever, not deleted after 500 history entries or whatever. Also I want commands to be saved to the history immediately when I run them, not only when I exit the shell session (<strong>shell</strong>)</li>
<li><strong>a useful prompt</strong>: I can&rsquo;t live without having my <strong>current directory</strong> and <strong>current git branch</strong> in my prompt (<strong>shell</strong>)</li>
<li><strong>24-bit colour</strong>: this is important to me because I find it MUCH easier to theme neovim with 24-bit colour support than in a terminal with only 256 colours (<strong>terminal emulator</strong>)</li>
<li><strong>clipboard integration</strong> between vim and my operating system so that when I copy in Firefox, I can just press <code>p</code> in vim to paste (<strong>text editor</strong>, maybe the OS/terminal emulator too)</li>
<li><strong>good autocomplete</strong>: for example commands like git should have command-specific autocomplete (<strong>shell</strong>)</li>
<li><strong>having colours in <code>ls</code></strong> (<strong>shell config</strong>)</li>
<li><strong>a terminal theme I like</strong>: I spend a lot of time in my terminal, I want it to look nice and I want its theme to match my terminal editor&rsquo;s theme. (<strong>terminal emulator</strong>, <strong>text editor</strong>)</li>
<li><strong>automatic terminal fixing</strong>: If a programs prints out some weird escape
codes that mess up my terminal, I want that to automatically get reset so
that my terminal doesn&rsquo;t get messed up (<strong>shell</strong>)</li>
<li><strong>keybindings</strong>: I want <code>Ctrl+left arrow</code> to work (<strong>shell</strong> or <strong>application</strong>)</li>
<li><strong>being able to use the scroll wheel in programs like <code>less</code></strong>: (<strong>terminal emulator</strong> and <strong>applications</strong>)</li>
</ul>
<p>There are a million other terminal conveniences out there and different people
value different things, but those are the ones that I would be really unhappy
without.</p>
<h3 id="how-i-achieve-a-modern-experience">how I achieve a &ldquo;modern experience&rdquo;</h3>
<p>My basic approach is:</p>
<ol>
<li>use the <code>fish</code> shell. Mostly don&rsquo;t configure it, except to:
<ul>
<li>set the <code>EDITOR</code> environment variable to my favourite terminal editor</li>
<li>alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>use any terminal emulator with 24-bit colour support. In the past I&rsquo;ve used
GNOME Terminal, Terminator, and iTerm, but I&rsquo;m not picky about this. I don&rsquo;t really
configure it other than to choose a font.</li>
<li>use <code>neovim</code>, with a configuration that I&rsquo;ve been very slowly building over the last 9 years or so (the last time I deleted my vim config and started from scratch was 9 years ago)</li>
<li>use the <a href="https://github.com/chriskempson/base16">base16 framework</a> to theme everything</li>
</ol>
<p>A few things that affect my approach:</p>
<ul>
<li>I don&rsquo;t spend a lot of time SSHed into other machines</li>
<li>I&rsquo;d rather use the mouse a little than come up with keyboard-based ways to do everything</li>
<li>I work on a lot of small projects, not one big project</li>
</ul>
<h3 id="some-out-of-the-box-options-for-a-modern-experience">some &ldquo;out of the box&rdquo; options for a &ldquo;modern&rdquo; experience</h3>
<p>What if you want a nice experience, but don&rsquo;t want to spend a lot of time on
configuration? Figuring out how to configure vim in a way that I was satisfied
with really did take me like ten years, which is a long time!</p>
<p>My best ideas for how to get a reasonable terminal experience with minimal
config are:</p>
<ul>
<li>shell: either <code>fish</code> or <code>zsh</code> with <a href="https://ohmyz.sh/">oh-my-zsh</a></li>
<li>terminal emulator: almost anything with 24-bit colour support, for example all of these are popular:
<ul>
<li>linux: GNOME Terminal, Konsole, Terminator, xfce4-terminal</li>
<li>mac: iTerm (Terminal.app doesn&rsquo;t have 256-colour support)</li>
<li>cross-platform: kitty, alacritty, wezterm, or ghostty</li>
</ul>
</li>
<li>shell config:
<ul>
<li>set the <code>EDITOR</code> environment variable to your favourite terminal text
editor</li>
<li>maybe alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>text editor: this is a tough one, maybe <a href="https://micro-editor.github.io/">micro</a> or <a href="https://helix-editor.com/">helix</a>? I haven&rsquo;t used
either of them seriously but they both seem like very cool projects and I
think it&rsquo;s amazing that you can just use all the usual GUI editor commands
(<code>Ctrl-C</code> to copy, <code>Ctrl-V</code> to paste, <code>Ctrl-A</code> to select all) in micro and
they do what you&rsquo;d expect. I would probably try switching to helix except
that retraining my vim muscle memory seems way too hard. Also helix doesn&rsquo;t
have a GUI or plugin system yet.</li>
</ul>
<p>Personally I <strong>wouldn&rsquo;t</strong> use xterm, rxvt, or Terminal.app as a terminal emulator,
because I&rsquo;ve found in the past that they&rsquo;re missing core features (like 24-bit
colour in Terminal.app&rsquo;s case) that make the terminal harder to use for me.</p>
<p>I don&rsquo;t want to pretend that getting a &ldquo;modern&rdquo; terminal experience is easier
than it is though &ndash; I think there are two issues that make it hard. Let&rsquo;s talk
about them!</p>
<h3 id="issue-1-with-getting-to-a-modern-experience-the-shell">issue 1 with getting to a &ldquo;modern&rdquo; experience: the shell</h3>
<p>bash and zsh are by far the two most popular shells, and neither of them
provide a default experience that I would be happy using out of the box, for
example:</p>
<ul>
<li>you need to customize your prompt</li>
<li>they don&rsquo;t come with git completions by default, you have to set them up</li>
<li>by default, bash only stores 500 (!) lines of history and (at least on Mac OS)
zsh is only configured to store 2000 lines, which is still not a lot</li>
<li>I find bash&rsquo;s tab completion very frustrating, if there&rsquo;s more than
one match then you can&rsquo;t tab through them</li>
</ul>
<p>And even though <a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">I love fish</a>, the fact
that it isn&rsquo;t POSIX does make it hard for a lot of folks to make the switch.</p>
<p>Of course it&rsquo;s totally possible to learn how to customize your prompt in bash
or whatever, and it doesn&rsquo;t even need to be that complicated (in bash I&rsquo;d
probably start with something like <code>export PS1='[\u@\h \W$(__git_ps1 &quot; (%s)&quot;)]\$ '</code>, or maybe use <a href="https://starship.rs/">starship</a>).
But each of these &ldquo;not complicated&rdquo; things really does add up and it&rsquo;s
especially tough if you need to keep your config in sync across several
systems.</p>
<p>An extremely popular solution to getting a &ldquo;modern&rdquo; shell experience is
<a href="https://ohmyz.sh/">oh-my-zsh</a>. It seems like a great project and I know a lot
of people use it very happily, but I&rsquo;ve struggled with configuration systems
like that in the past &ndash; it looks like right now the base oh-my-zsh adds about
3000 lines of config, and often I find that having an extra configuration
system makes it harder to debug what&rsquo;s happening when things go wrong. I
personally have a tendency to use the system to add a lot of extra plugins,
make my system slow, get frustrated that it&rsquo;s slow, and then delete it
completely and write a new config from scratch.</p>
<h3 id="issue-2-with-getting-to-a-modern-experience-the-text-editor">issue 2 with getting to a &ldquo;modern&rdquo; experience: the text editor</h3>
<p>In the terminal survey I ran recently, the most popular terminal text editors
by far were <code>vim</code>, <code>emacs</code>, and <code>nano</code>.</p>
<p>I think the main options for terminal text editors are:</p>
<ul>
<li>use vim or emacs and configure it to your liking, you can probably have any
feature you want if you put in the work</li>
<li>use nano and accept that you&rsquo;re going to have a pretty limited experience
(for example I don&rsquo;t think you can select text with the mouse and then &ldquo;cut&rdquo;
it in nano)</li>
<li>use <code>micro</code> or <code>helix</code> which seem to offer a pretty good out-of-the-box
experience, potentially occasionally run into issues with using a less
mainstream text editor</li>
<li>just avoid using a terminal text editor as much as possible, maybe use VSCode, use
VSCode&rsquo;s terminal for all your terminal needs, and mostly never edit files in
the terminal. Or I know a lot of people use <code>code</code> as their <code>EDITOR</code> in the terminal.</li>
</ul>
<h3 id="issue-3-individual-applications">issue 3: individual applications</h3>
<p>The last issue is that sometimes individual programs that I use are kind of
annoying. For example on my Mac OS machine, <code>/usr/bin/sqlite3</code> doesn&rsquo;t support
the <code>Ctrl+Left Arrow</code> keyboard shortcut. Fixing this to get a reasonable
terminal experience in SQLite was a little complicated, I had to:</p>
<ul>
<li>realize why this is happening (Mac OS won&rsquo;t ship GNU tools, and &ldquo;Ctrl-Left arrow&rdquo; support comes from GNU readline)</li>
<li>find a workaround (install sqlite from homebrew, which does have readline support)</li>
<li>adjust my environment (put Homebrew&rsquo;s sqlite3 in my PATH)</li>
</ul>
<p>I find that debugging application-specific issues like this is really not easy
and often it doesn&rsquo;t feel &ldquo;worth it&rdquo; &ndash; often I&rsquo;ll end up just dealing with
various minor inconveniences because I don&rsquo;t want to spend hours investigating
them. The only reason I was even able to figure this one out at all is that
I&rsquo;ve been spending a huge amount of time thinking about the terminal recently.</p>
<p>A big part of having a &ldquo;modern&rdquo; experience using terminal programs is just
using newer terminal programs, for example I can&rsquo;t be bothered to learn a
keyboard shortcut to sort the columns in <code>top</code>, but in <code>htop</code>  I can just click
on a column heading with my mouse to sort it. So I use htop instead! But discovering new more &ldquo;modern&rdquo; command line tools isn&rsquo;t easy (though
I made <a href="https://jvns.ca/blog/2022/04/12/a-list-of-new-ish--command-line-tools/">a list here</a>),
finding ones that I actually like using in practice takes time, and if you&rsquo;re
SSHed into another machine, they won&rsquo;t always be there.</p>
<h3 id="everything-affects-everything-else">everything affects everything else</h3>
<p>Something I find tricky about configuring my terminal to make everything &ldquo;nice&rdquo;
is that changing one seemingly small thing about my workflow can really affect
everything else. For example right now I don&rsquo;t use tmux. But if I needed to use
tmux again (for example because I was doing a lot of work SSHed into another
machine), I&rsquo;d need to think about a few things, like:</p>
<ul>
<li>if I wanted tmux&rsquo;s copy to synchronize with my system clipboard over
SSH, I&rsquo;d need to make sure that my terminal emulator has <a href="https://old.reddit.com/r/vim/comments/k1ydpn/a_guide_on_how_to_copy_text_from_anywhere/">OSC 52 support</a></li>
<li>if I wanted to use iTerm&rsquo;s tmux integration (which makes tmux tabs into iTerm
tabs), I&rsquo;d need to change how I configure colours &ndash; right now I set them
with a <a href="https://github.com/chriskempson/base16-shell/blob/588691ba71b47e75793ed9edfcfaa058326a6f41/scripts/base16-solarized-light.sh">shell script</a> that I run when my shell starts, but that means the
colours get lost when restoring a tmux session.</li>
</ul>
<p>and probably more things I haven&rsquo;t thought of. &ldquo;Using tmux means that I have to
change how I manage my colours&rdquo; sounds unlikely, but that really did happen to
me and I decided &ldquo;well, I don&rsquo;t want to change how I manage colours right now,
so I guess I&rsquo;m not using that feature!&rdquo;.</p>
<p>It&rsquo;s also hard to remember which features I&rsquo;m relying on &ndash; for example maybe
my current terminal <em>does</em> have OSC 52 support and because copying from tmux over SSH
has always Just Worked I don&rsquo;t even realize that that&rsquo;s something I need, and
then it mysteriously stops working when I switch terminals.</p>
<h3 id="change-things-slowly">change things slowly</h3>
<p>Personally even though I think my setup is not <em>that</em> complicated, it&rsquo;s taken
me 20 years to get to this point! Because terminal config changes are so likely
to have unexpected and hard-to-understand consequences, I&rsquo;ve found that if I
change a lot of terminal configuration all at once it makes it much harder to
understand what went wrong if there&rsquo;s a problem, which can be really
disorienting.</p>
<p>So I usually prefer to make pretty small changes, and accept that changes can
might take me a REALLY long time to get used to. For example I switched from
using <code>ls</code> to <a href="https://github.com/eza-community/eza">eza</a> a year or two ago and
while I like it (because <code>eza -l</code> prints human-readable file sizes by default)
I&rsquo;m still not quite sure about it. But also sometimes it&rsquo;s worth it to make a
big change, like I made the switch to fish (from bash) 10 years ago and I&rsquo;m
very happy I did.</p>
<h3 id="getting-a-modern-terminal-is-not-that-easy">getting a &ldquo;modern&rdquo; terminal is not that easy</h3>
<p>Trying to explain how &ldquo;easy&rdquo; it is to configure your terminal really just made
me think that it&rsquo;s kind of hard and that I still sometimes get confused.</p>
<p>I&rsquo;ve found that there&rsquo;s never one perfect way to configure things in the
terminal that will be compatible with every single other thing. I just need to
try stuff, figure out some kind of locally stable state that works for me, and
accept that if I start using a new tool it might disrupt the system and I might
need to rethink things.</p>

---

### ["Rules" that terminal programs follow](https://jvns.ca/blog/2024/11/26/terminal-rules/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Recently I&rsquo;ve been thinking about how everything that happens in the terminal
is some combination of:</p>
<ol>
<li>Your <strong>operating system</strong>&rsquo;s job</li>
<li>Your <strong>shell</strong>&rsquo;s job</li>
<li>Your <strong>terminal emulator</strong>&rsquo;s job</li>
<li>The job of <strong>whatever program you happen to be running</strong> (like <code>top</code> or <code>vim</code> or <code>cat</code>)</li>
</ol>
<p>The first three (your operating system, shell, and terminal emulator) are all kind of
known quantities &ndash; if you&rsquo;re using bash in GNOME Terminal on Linux, you can
more or less reason about how how all of those things interact, and some of
their behaviour is standardized by POSIX.</p>
<p>But the fourth one (&ldquo;whatever program you happen to be running&rdquo;) feels like it
could do ANYTHING. How are you supposed to know how a program is going to
behave?</p>
<p>This post is kind of long so here&rsquo;s a quick table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#programs-behave-surprisingly-consistently">programs behave surprisingly consistently</a></li>
<li><a href="https://jvns.ca/atom.xml#these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</a></li>
<li><a href="https://jvns.ca/atom.xml#it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: <code>Ctrl-W</code> should delete the last word</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</a></li>
<li><a href="https://jvns.ca/atom.xml#these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</a></li>
</ul>
<h3 id="programs-behave-surprisingly-consistently">programs behave surprisingly consistently</h3>
<p>As far as I know, there are no real standards for how programs in the terminal
should behave &ndash; the closest things I know of are:</p>
<ul>
<li>POSIX, which mostly dictates how your terminal emulator / OS / shell should
work together. I think it does specify a few things about how core utilities like
<code>cp</code> should work but AFAIK it doesn&rsquo;t have anything to say about how for
example <code>htop</code> should behave.</li>
<li>these <a href="https://clig.dev/">command line interface guidelines</a></li>
</ul>
<p>But even though there are no standards, in my experience programs in the
terminal behave in a pretty consistent way. So I wanted to write down a list of
&ldquo;rules&rdquo; that in my experience programs mostly follow.</p>
<h3 id="these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</h3>
<p>My goal here isn&rsquo;t to convince authors of terminal programs that they <em>should</em>
follow any of these rules. There are lots of exceptions to these and often
there&rsquo;s a good reason for those exceptions.</p>
<p>But it&rsquo;s very useful for me to know what behaviour to expect from a random new
terminal program that I&rsquo;m using. Instead of &ldquo;uh, programs could do literally
anything&rdquo;, it&rsquo;s &ldquo;ok, here are the basic rules I expect, and then I can keep a
short mental list of exceptions&rdquo;.</p>
<p>So I&rsquo;m just writing down what I&rsquo;ve observed about how programs behave in my 20
years of using the terminal, why I think they behave that way, and some
examples of cases where that rule is &ldquo;broken&rdquo;.</p>
<h3 id="it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</h3>
<p>There are a bunch of common conventions that I think are pretty clearly the
program&rsquo;s responsibility to implement, like:</p>
<ul>
<li>config files should go in <code>~/.BLAHrc</code> or <code>~/.config/BLAH/FILE</code> or <code>/etc/BLAH/</code> or something</li>
<li><code>--help</code> should print help text</li>
<li>programs should print &ldquo;regular&rdquo; output to stdout and errors to stderr</li>
</ul>
<p>But in this post I&rsquo;m going to focus on things that it&rsquo;s not 100% obvious are
the program&rsquo;s responsibility. For example it feels to me like a &ldquo;law of nature&rdquo;
that pressing <code>Ctrl-D</code> should quit a REPL, but programs often
need to explicitly implement support for it &ndash; even though <code>cat</code> doesn&rsquo;t need
to implement <code>Ctrl-D</code> support, <code>ipython</code> <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">does</a>. (more about that in &ldquo;rule 3&rdquo; below)</p>
<p>Understanding which things are the program&rsquo;s responsibility makes it much less
surprising when different programs&rsquo; implementations are slightly different.</p>
<h3 id="rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></h3>
<p>The main reason for this rule is that noninteractive programs will quit by
default on <code>Ctrl-C</code> if they don&rsquo;t set up a <code>SIGINT</code> signal handler, so this is
kind of a &ldquo;you should act like the default&rdquo; rule.</p>
<p>Something that trips a lot of people up is that this doesn&rsquo;t apply to
<strong>interactive</strong> programs like <code>python3</code> or <code>bc</code> or <code>less</code>. This is because in
an interactive program, <code>Ctrl-C</code> has a different job &ndash; if the program is
running an operation (like for example a search in <code>less</code> or some Python code
in <code>python3</code>), then <code>Ctrl-C</code> will interrupt that operation but not stop the
program.</p>
<p>As an example of how this works in an interactive program: here&rsquo;s the code <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/vi.py#L2225">in prompt-toolkit</a> (the library that iPython uses for handling input)
that aborts a search when you press <code>Ctrl-C</code>.</p>
<h3 id="rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></h3>
<p>TUI programs (like <code>less</code> or <code>htop</code>) will usually quit when you press <code>q</code>.</p>
<p>This rule doesn&rsquo;t apply to any program where pressing <code>q</code> to quit wouldn&rsquo;t make
sense, like <code>tmux</code> or text editors.</p>
<h3 id="rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</h3>
<p>REPLs (like <code>python3</code> or <code>ed</code>) will usually quit when you press <code>Ctrl-D</code> on an
empty line. This rule is similar to the <code>Ctrl-C</code> rule &ndash; the reason for this is
that by default if you&rsquo;re running a program (like <code>cat</code>) in &ldquo;cooked mode&rdquo;, then
the operating system will return an <code>EOF</code> when you press <code>Ctrl-D</code> on an empty
line.</p>
<p>Most of the REPLs I use (sqlite3, python3, fish, bash, etc) don&rsquo;t actually use
cooked mode, but they all implement this keyboard shortcut anyway to mimic the
default behaviour.</p>
<p>For example, here&rsquo;s <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">the code in prompt-toolkit</a>
that quits when you press Ctrl-D, and here&rsquo;s <a href="https://github.com/bminor/bash/blob/6794b5478f660256a1023712b5fc169196ed0a22/lib/readline/readline.c#L658-L672">the same code in readline</a>.</p>
<p>I actually thought that this one was a &ldquo;Law of Terminal Physics&rdquo; until very
recently because I&rsquo;ve basically never seen it broken, but you can see that it&rsquo;s
just something that each individual input library has to implement in the links
above.</p>
<p>Someone pointed out that the Erlang REPL does not quit when you press <code>Ctrl-D</code>,
so I guess not every REPL follows this &ldquo;rule&rdquo;.</p>
<h3 id="rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</h3>
<p>Terminal programs rarely use colours other than the base 16 ANSI colours. This
is because if you specify colours with a hex code, it&rsquo;s very likely to clash
with some users&rsquo; background colour. For example if I print out some text as
<code>#EEEEEE</code>, it would be almost invisible on a white background, though it would
look fine on a dark background.</p>
<p>But if you stick to the default 16 base colours, you have a much better chance
that the user has configured those colours in their terminal emulator so that
they work reasonably well with their background color. Another reason to stick
to the default base 16 colours is that it makes less assumptions about what
colours the terminal emulator supports.</p>
<p>The only programs I usually see breaking this &ldquo;rule&rdquo; are text editors, for
example Helix by default will use a purple background which is not a default
ANSI colour. It seems fine for Helix to break this rule since Helix isn&rsquo;t a
&ldquo;core&rdquo; program and I assume any Helix user who doesn&rsquo;t like that colorscheme
will just change the theme.</p>
<h3 id="rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</h3>
<p>Almost every program I use supports <code>readline</code> keybindings if it would make
sense to do so. For example, here are a bunch of different programs and a link
to where they define <code>Ctrl-E</code> to go to the end of the line:</p>
<ul>
<li>ipython (<a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/emacs.py#L72">Ctrl-E defined here</a>)</li>
<li>atuin (<a href="https://github.com/atuinsh/atuin/blob/a67cfc82fe0dc907a01f07a0fd625701e062a33b/crates/atuin/src/command/client/search/interactive.rs#L407">Ctrl-E defined here</a>)</li>
<li>fzf (<a href="https://github.com/junegunn/fzf/blob/bb55045596d6d08445f3c6d320c3ec2b457462d1/src/terminal.go#L611">Ctrl-E defined here</a>)</li>
<li>zsh (<a href="https://github.com/zsh-users/zsh/blob/86d5f24a3d28541f242eb3807379301ea976de87/Src/Zle/zle_bindings.c#L94">Ctrl-E defined here</a>)</li>
<li>fish (<a href="https://github.com/fish-shell/fish-shell/blob/99fa8aaaa7956178973150a03ce4954ab17a197b/share/functions/fish_default_key_bindings.fish#L43">Ctrl-E defined here</a>)</li>
<li>tmux&rsquo;s command prompt (<a href="https://github.com/tmux/tmux/blob/ae8f2208c98e3c2d6e3fe4cad2281dce8fd11f31/key-bindings.c#L490">Ctrl-E defined here</a>)</li>
</ul>
<p>None of those programs actually uses <code>readline</code> directly, they just sort of
mimic emacs/readline keybindings. They don&rsquo;t always mimic them <em>exactly</em>: for
example atuin seems to use <code>Ctrl-A</code> as a prefix, so <code>Ctrl-A</code> doesn&rsquo;t go to the
beginning of the line.</p>
<p>Also all of these programs seem to implement their own internal cut and paste
buffers so you can delete a line with <code>Ctrl-U</code> and then paste it with <code>Ctrl-Y</code>.</p>
<p>The exceptions to this are:</p>
<ul>
<li>some programs (like <code>git</code>, <code>cat</code>, and <code>nc</code>) don&rsquo;t have any line editing support at all (except for backspace, <code>Ctrl-W</code>, and <code>Ctrl-U</code>)</li>
<li>as usual text editors are an exception, every text editor has its own
approach to editing text</li>
</ul>
<p>I wrote more about this &ldquo;what keybindings does a program support?&rdquo; question in
<a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a>.</p>
<h3 id="rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: Ctrl-W should delete the last word</h3>
<p>I&rsquo;ve never seen a program (other than a text editor) where <code>Ctrl-W</code> <em>doesn&rsquo;t</em>
delete the last word. This is similar to the <code>Ctrl-C</code> rule &ndash; by default if a
program is in &ldquo;cooked mode&rdquo;, the OS will delete the last word if you press
<code>Ctrl-W</code>, and delete the whole line if you press <code>Ctrl-U</code>. So usually programs
will imitate that behaviour.</p>
<p>I can&rsquo;t think of any exceptions to this other than text editors but if there
are I&rsquo;d love to hear about them!</p>
<h3 id="rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</h3>
<p>Most programs will disable colours when writing to a pipe. For example:</p>
<ul>
<li><code>rg blah</code> will highlight all occurrences of <code>blah</code> in the output, but if the
output is to a pipe or a file, it&rsquo;ll turn off the highlighting.</li>
<li><code>ls --color=auto</code> will use colour when writing to a terminal, but not when
writing to a pipe</li>
</ul>
<p>Both of those programs will also format their output differently when writing
to the terminal: <code>ls</code> will organize files into columns, and ripgrep will group
matches with headings.</p>
<p>If you want to force the program to use colour (for example because you want to
look at the colour), you can use <code>unbuffer</code> to force the program&rsquo;s output to be
a tty like this:</p>
<pre><code>unbuffer rg blah |  less -R
</code></pre>
<p>I&rsquo;m sure that there are some programs that &ldquo;break&rdquo; this rule but I can&rsquo;t think
of any examples right now. Some programs have an <code>--color</code> flag that you can
use to force colour to be on, in the example above you could also do <code>rg --color=always | less -R</code>.</p>
<h3 id="rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</h3>
<p>Usually if you pass <code>-</code> to a program instead of a filename, it&rsquo;ll read from
stdin or write to stdout (whichever is appropriate). For example, if you want
to format the Python code that&rsquo;s on your clipboard with <code>black</code> and then copy
it, you could run:</p>
<pre><code>pbpaste | black - | pbcopy
</code></pre>
<p>(<code>pbpaste</code> is a Mac program, you can do something similar on Linux with <code>xclip</code>)</p>
<p>My impression is that most programs implement this if it would make sense and I
can&rsquo;t think of any exceptions right now, but I&rsquo;m sure there are many
exceptions.</p>
<h3 id="these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</h3>
<p>These rules took me a long time for me to learn because I had to:</p>
<ol>
<li>learn that the rule applied anywhere at all (&quot;<code>Ctrl-C</code> will exit programs&quot;)</li>
<li>notice some exceptions (&ldquo;okay, <code>Ctrl-C</code> will exit <code>find</code> but not <code>less</code>&rdquo;)</li>
<li>subconsciously figure out what the pattern is (&quot;<code>Ctrl-C</code> will generally quit
noninteractive programs, but in interactive programs it might interrupt the
current operation instead of quitting the program&quot;)</li>
<li>eventually maybe formulate it into an explicit rule that I know</li>
</ol>
<p>A lot of my understanding of the terminal is honestly still in the
&ldquo;subconscious pattern recognition&rdquo; stage. The only reason I&rsquo;ve been taking the
time to make things explicit at all is because I&rsquo;ve been trying to explain how
it works to others. Hopefully writing down these &ldquo;rules&rdquo; explicitly will make
learning some of this stuff a little bit faster for others.</p>

---

### [Why pipes sometimes get "stuck": buffering](https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Here&rsquo;s a niche terminal problem that has bothered me for years but that I never
really understood until a few weeks ago. Let&rsquo;s say you&rsquo;re running this command
to watch for some specific output in a log file:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>If log lines are being added to the file relatively slowly, the result I&rsquo;d see
is&hellip; nothing! It doesn&rsquo;t matter if there were matches in the log file or not,
there just wouldn&rsquo;t be any output.</p>
<p>I internalized this as &ldquo;uh, I guess pipes just get stuck sometimes and don&rsquo;t
show me the output, that&rsquo;s weird&rdquo;, and I&rsquo;d handle it by just
running <code>grep thing1 /some/log/file | grep thing2</code> instead, which would work.</p>
<p>So as I&rsquo;ve been doing a terminal deep dive over the last few months I was
really excited to finally learn exactly why this happens.</p>
<h3 id="why-this-happens-buffering">why this happens: buffering</h3>
<p>The reason why &ldquo;pipes get stuck&rdquo; sometimes is that it&rsquo;s VERY common for
programs to buffer their output before writing it to a pipe or file. So the
pipe is working fine, the problem is that the program never even wrote the data
to the pipe!</p>
<p>This is for performance reasons: writing all output immediately as soon as you
can uses more system calls, so it&rsquo;s more efficient to save up data until you
have 8KB or so of data to write (or until the program exits) and THEN write it
to the pipe.</p>
<p>In this example:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>the problem is that <code>grep thing1</code> is saving up all of its matches until it has
8KB of data to write, which might literally never happen.</p>
<h3 id="programs-don-t-buffer-when-writing-to-a-terminal">programs don&rsquo;t buffer when writing to a terminal</h3>
<p>Part of why I found this so disorienting is that <code>tail -f file | grep thing</code>
will work totally fine, but then when you add the second <code>grep</code>, it stops
working!! The reason for this is that the way <code>grep</code> handles buffering depends
on whether it&rsquo;s writing to a terminal or not.</p>
<p>Here&rsquo;s how <code>grep</code> (and many other programs) decides to buffer its output:</p>
<ul>
<li>Check if stdout is a terminal or not using the <code>isatty</code> function
<ul>
<li>If it&rsquo;s a terminal, use line buffering (print every line immediately as soon as you have it)</li>
<li>Otherwise, use &ldquo;block buffering&rdquo; &ndash; only print data if you have at least 8KB or so of data to print</li>
</ul>
</li>
</ul>
<p>So if <code>grep</code> is writing directly to your terminal then you&rsquo;ll see the line as
soon as it&rsquo;s printed, but if it&rsquo;s writing to a pipe, you won&rsquo;t.</p>
<p>Of course the buffer size isn&rsquo;t always 8KB for every program, it depends on the implementation. For <code>grep</code> the buffering is handled by libc, and libc&rsquo;s buffer size is
defined in the <code>BUFSIZ</code> variable. <a href="https://github.com/bminor/glibc/blob/c69e8cccaff8f2d89cee43202623b33e6ef5d24a/libio/stdio.h#L100">Here&rsquo;s where that&rsquo;s defined in glibc</a>.</p>
<p>(as an aside: &ldquo;programs do not use 8KB output buffers when writing to a
terminal&rdquo; isn&rsquo;t, like, a law of terminal physics, a program COULD use an 8KB
buffer when writing output to a terminal if it wanted, it would just be
extremely weird if it did that, I can&rsquo;t think of any program that behaves that
way)</p>
<h3 id="commands-that-buffer-commands-that-don-t">commands that buffer &amp; commands that don&rsquo;t</h3>
<p>One annoying thing about this buffering behaviour is that you kind of need to
remember which commands buffer their output when writing to a pipe.</p>
<p>Some commands that <strong>don&rsquo;t</strong> buffer their output:</p>
<ul>
<li>tail</li>
<li>cat</li>
<li>tee</li>
</ul>
<p>I think almost everything else will buffer output, especially if it&rsquo;s a command
where you&rsquo;re likely to be using it for batch processing. Here&rsquo;s a list of some
common commands that buffer their output when writing to a pipe, along with the
flag that disables block buffering.</p>
<ul>
<li>grep (<code>--line-buffered</code>)</li>
<li>sed (<code>-u</code>)</li>
<li>awk (there&rsquo;s a <code>fflush()</code> function)</li>
<li>tcpdump (<code>-l</code>)</li>
<li>jq (<code>-u</code>)</li>
<li>tr (<code>-u</code>)</li>
<li>cut (can&rsquo;t disable buffering)</li>
</ul>
<p>Those are all the ones I can think of, lots of unix commands (like <code>sort</code>) may
or may not buffer their output but it doesn&rsquo;t matter because <code>sort</code> can&rsquo;t do
anything until it finishes receiving input anyway.</p>
<p>Also I did my best to test both the Mac OS and GNU versions of these but there
are a lot of variations and I might have made some mistakes.</p>
<h3 id="programming-languages-where-the-default-print-statement-buffers">programming languages where the default &ldquo;print&rdquo; statement buffers</h3>
<p>Also, here are a few programming language where the default print statement
will buffer output when writing to a pipe, and some ways to disable buffering
if you want:</p>
<ul>
<li>C (disable with <code>setvbuf</code>)</li>
<li>Python (disable with <code>python -u</code>, or <code>PYTHONUNBUFFERED=1</code>, or <code>sys.stdout.reconfigure(line_buffering=False)</code>, or <code>print(x, flush=True)</code>)</li>
<li>Ruby (disable with <code>STDOUT.sync = true</code>)</li>
<li>Perl (disable with <code>$| = 1</code>)</li>
</ul>
<p>I assume that these languages are designed this way so that the default print
function will be fast when you&rsquo;re doing batch processing.</p>
<p>Also whether output is buffered or not might depend on how you print, for
example in C++ <code>cout &lt;&lt; &quot;hello\n&quot;</code> buffers when writing to a pipe but <code>cout &lt;&lt; &quot;hello&quot; &lt;&lt; endl</code> will flush its output.</p>
<h3 id="when-you-press-ctrl-c-on-a-pipe-the-contents-of-the-buffer-are-lost">when you press <code>Ctrl-C</code> on a pipe, the contents of the buffer are lost</h3>
<p>Let&rsquo;s say you&rsquo;re running this command as a hacky way to watch for DNS requests
to <code>example.com</code>, and you forgot to pass <code>-l</code> to tcpdump:</p>
<pre><code>sudo tcpdump -ni any port 53 | grep example.com
</code></pre>
<p>When you press <code>Ctrl-C</code>, what happens? In a magical perfect world, what I would
<em>want</em> to happen is for <code>tcpdump</code> to flush its buffer, <code>grep</code> would search for
<code>example.com</code>, and I would see all the output I missed.</p>
<p>But in the real world, what happens is that all the programs get killed and the
output in <code>tcpdump</code>&rsquo;s buffer is lost.</p>
<p>I think this problem is probably unavoidable &ndash; I spent a little time with
<code>strace</code> to see how this works and <code>grep</code> receives the <code>SIGINT</code> before
<code>tcpdump</code> anyway so even if <code>tcpdump</code> tried to flush its buffer <code>grep</code> would
already be dead.</p>
<small>
<p>After a little more investigation, there is a workaround: if you find
<code>tcpdump</code>&rsquo;s PID and <code>kill -TERM $PID</code>, then tcpdump will flush the buffer so
you can see the output. That&rsquo;s kind of a pain but I tested it and it seems to
work.</p>
</small>
<h3 id="redirecting-to-a-file-also-buffers">redirecting to a file also buffers</h3>
<p>It&rsquo;s not just pipes, this will also buffer:</p>
<pre><code>sudo tcpdump -ni any port 53 &gt; output.txt
</code></pre>
<p>Redirecting to a file doesn&rsquo;t have the same &ldquo;<code>Ctrl-C</code> will totally destroy the
contents of the buffer&rdquo; problem though &ndash; in my experience it usually behaves
more like you&rsquo;d want, where the contents of the buffer get written to the file
before the program exits. I&rsquo;m not 100% sure whether this is something you can
always rely on or not.</p>
<h3 id="a-bunch-of-potential-ways-to-avoid-buffering">a bunch of potential ways to avoid buffering</h3>
<p>Okay, let&rsquo;s talk solutions. Let&rsquo;s say you&rsquo;ve run this command:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>I asked people on Mastodon how they would solve this in practice and there were
5 basic approaches. Here they are:</p>
<h4 id="solution-1-run-a-program-that-finishes-quickly">solution 1: run a program that finishes quickly</h4>
<p>Historically my solution to this has been to just avoid the &ldquo;command writing to
pipe slowly&rdquo; situation completely and instead run a program that will finish quickly
like this:</p>
<pre><code>cat /some/log/file | grep thing1 | grep thing2 | tail
</code></pre>
<p>This doesn&rsquo;t do the same thing as the original command but it does mean that
you get to avoid thinking about these weird buffering issues.</p>
<p>(you could also do <code>grep thing1 /some/log/file</code> but I often prefer to use an
&ldquo;unnecessary&rdquo; <code>cat</code>)</p>
<h4 id="solution-2-remember-the-line-buffer-flag-to-grep">solution 2: remember the &ldquo;line buffer&rdquo; flag to grep</h4>
<p>You could remember that grep has a flag to avoid buffering and pass it like this:</p>
<pre><code>tail -f /some/log/file | grep --line-buffered thing1 | grep thing2
</code></pre>
<h4 id="solution-3-use-awk">solution 3: use awk</h4>
<p>Some people said that if they&rsquo;re specifically dealing with a multiple greps
situation, they&rsquo;ll rewrite it to use a single <code>awk</code> instead, like this:</p>
<pre><code>tail -f /some/log/file |  awk '/thing1/ &amp;&amp; /thing2/'
</code></pre>
<p>Or you would write a more complicated <code>grep</code>, like this:</p>
<pre><code>tail -f /some/log/file |  grep -E 'thing1.*thing2'
</code></pre>
<p>(<code>awk</code> also buffers, so for this to work you&rsquo;ll want <code>awk</code> to be the last command in the pipeline)</p>
<h4 id="solution-4-use-stdbuf">solution 4: use <code>stdbuf</code></h4>
<p><code>stdbuf</code> uses LD_PRELOAD to turn off libc&rsquo;s buffering, and you can use it to turn off output buffering like this:</p>
<pre><code>tail -f /some/log/file | stdbuf -o0 grep thing1 | grep thing2
</code></pre>
<p>Like any <code>LD_PRELOAD</code> solution it&rsquo;s a bit unreliable &ndash; it doesn&rsquo;t work on
static binaries, I think won&rsquo;t work if the program isn&rsquo;t using libc&rsquo;s
buffering, and doesn&rsquo;t always work on Mac OS. Harry Marr has a really nice <a href="https://hmarr.com/blog/how-stdbuf-works/">How stdbuf works</a> post.</p>
<h4 id="solution-5-use-unbuffer">solution 5: use <code>unbuffer</code></h4>
<p><code>unbuffer program</code> will force the program&rsquo;s output to be a TTY, which means
that it&rsquo;ll behave the way it normally would on a TTY (less buffering, colour
output, etc). You could use it in this example like this:</p>
<pre><code>tail -f /some/log/file | unbuffer grep thing1 | grep thing2
</code></pre>
<p>Unlike <code>stdbuf</code> it will always work, though it might have unwanted side
effects, for example <code>grep thing1</code>&rsquo;s will also colour matches.</p>
<p>If you want to install unbuffer, it&rsquo;s in the <code>expect</code> package.</p>
<h3 id="that-s-all-the-solutions-i-know-about">that&rsquo;s all the solutions I know about!</h3>
<p>It&rsquo;s a bit hard for me to say which one is &ldquo;best&rdquo;, I think personally I&rsquo;m
mostly likely to use <code>unbuffer</code> because I know it&rsquo;s always going to work.</p>
<p>If I learn about more solutions I&rsquo;ll try to add them to this post.</p>
<h3 id="i-m-not-really-sure-how-often-this-comes-up">I&rsquo;m not really sure how often this comes up</h3>
<p>I think it&rsquo;s not very common for me to have a program that slowly trickles data
into a pipe like this, normally if I&rsquo;m using a pipe a bunch of data gets
written very quickly, processed by everything in the pipeline, and then
everything exits. The only examples I can come up with right now are:</p>
<ul>
<li>tcpdump</li>
<li><code>tail -f</code></li>
<li>watching log files in a different way like with <code>kubectl logs</code></li>
<li>the output of a slow computation</li>
</ul>
<h3 id="what-if-there-were-an-environment-variable-to-disable-buffering">what if there were an environment variable to disable buffering?</h3>
<p>I think it would be cool if there were a standard environment variable to turn
off buffering, like <code>PYTHONUNBUFFERED</code> in Python. I got this idea from a
<a href="https://blog.plover.com/Unix/stdio-buffering.html">couple</a> of <a href="https://blog.plover.com/Unix/stdio-buffering-2.html">blog posts</a> by Mark Dominus
in 2018. Maybe <code>NO_BUFFER</code> like <a href="https://no-color.org/">NO_COLOR</a>?</p>
<p>The design seems tricky to get right; Mark points out that NETBSD has <a href="https://man.netbsd.org/setbuf.3">environment variables called <code>STDBUF</code>, <code>STDBUF1</code>, etc</a> which gives you a
ton of control over buffering but I imagine most developers don&rsquo;t want to
implement many different environment variables to handle a relatively minor
edge case.</p>
<p>I&rsquo;m also curious about whether there are any programs that just automatically
flush their output buffers after some period of time (like 1 second). It feels
like it would be nice in theory but I can&rsquo;t think of any program that does that
so I imagine there are some downsides.</p>
<h3 id="stuff-i-left-out">stuff I left out</h3>
<p>Some things I didn&rsquo;t talk about in this post since these posts have been
getting pretty long recently and seriously does anyone REALLY want to read 3000
words about buffering?</p>
<ul>
<li>the difference between line buffering and having totally unbuffered output</li>
<li>how buffering to stderr is different from buffering to stdout</li>
<li>this post is only about buffering that happens <strong>inside the program</strong>, your
operating system&rsquo;s TTY driver also does a little bit of buffering sometimes</li>
<li>other reasons you might need to flush your output other than &ldquo;you&rsquo;re writing
to a pipe&rdquo;</li>
</ul>

---

### [2025 Recap: so many projects](https://fasterthanli.me/articles/2025-recap)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I‚Äôve been working on so many projects in 2025, I thought it was important for me
to make a recap, if only just to clear my head.</p>

<p>There are many, many, many things to go through and we don‚Äôt have a sponsor
today, so I‚Äôm gonna start right away with facet!</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#facet" id="facet"><h2>facet</h2></a>
<p>facet is a project that I started working on in March of this year ‚Äî that‚Äôs
right, it‚Äôs only been ten months, yet it feels like an eternity.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#in-the-beginning" id="in-the-beginning"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#the-first-golden-age" id="the-first-golden-age"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#disappointment" id="disappointment"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#refocus" id="refocus"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#volte-face" id="volte-face"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#just-in-time-for-the-new-year" id="just-in-time-for-the-new-year"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#dodeca" id="dodeca"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#rapace" id="rapace"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#tracey" id="tracey"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#picante" id="picante"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#pikru" id="pikru"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#aasvg-rs" id="aasvg-rs"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#facet-keeps-growing" id="facet-keeps-growing"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#fs-kitty" id="fs-kitty"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#vixen" id="vixen"></a>






























































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Introducing arborium, a tree-sitter distribution](https://fasterthanli.me/articles/introducing-arborium)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>About two weeks ago I entered a discussion with the docs.rs team about,
basically, why we have to look at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages but they're all white on black. It's sad.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-no-colors@2x~eb49252c206ebe40.jxl" title="" width="1083" /></p><p>When we could be looking at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages. this time it's colored.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-yes-colors@2x~708d0f07e1265747.jxl" title="" width="1083" /></p><p>And of course, as always, there are reasons why things are the way they are.
In an effort to understand those reasons, I opened a GitHub issue which resulted
in a <a href="https://github.com/rust-lang/docs.rs/issues/3040">short but productive</a> discussion.</p>

<p>I walked away discouraged, and then decided to, reasons be damned, attack this
problem from three different angles.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#background" id="background"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#problems" id="problems"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#solutions" id="solutions"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-1-just-include-this-script" id="angle-1-just-include-this-script"></a>

























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-2-it-goes-in-the-rustdoc-hole" id="angle-2-it-goes-in-the-rustdoc-hole"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#angle-3-only-in-the-backend" id="angle-3-only-in-the-backend"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#post-mortem" id="post-mortem"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Does Dioxus spark joy?](https://fasterthanli.me/articles/does-dioxus-spark-joy)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <div class="dialog right">
<div class="dialog-head" title="Amos says:">
  <source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Amos" height="42" src="https://cdn.fasterthanli.me/content/img/reimena/amos-neutral~55a3477398fe0cb1.jxl" width="42" />
</div>
<div class="dialog-text markup-container">
<p>Note: this article is adapted from a presentation I gave at a Rust Paris Meetup ‚Äî that‚Äôs why
it sounds a little different than usual. Enjoy!</p>

</div>
</div><p>Good evening! Tonight, I will attempt to answer the question: Does
<a href="https://github.com/dioxuslabs/dioxus">Dioxus</a> spark joy? Or at the very least,
whimsy.</p>

<p>What‚Äôs Dioxus, you ask? It is first and foremost a name that is quote: ‚Äúlegally
not inspired by any Pok√©mon‚Äù.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="The deoxys pokemon
" class="" height="449" src="https://cdn.fasterthanli.me/content/articles/does-dioxus-spark-joy/deoxys@2x~d76eb1e3eeda1b65.jxl" title="" width="422" /></p><p>Even if the author concedes <a href="https://news.ycombinator.com/item?id=39853385">in a Hacker News comment</a> that the ‚ÄúDeoxys‚Äù Pok√©mon
is, I quote: ‚Äúawesome‚Äù.</p>











<a class="anchor" href="https://fasterthanli.me/index.xml#a-short-and-mostly-wrong-history-of-web-apps" id="a-short-and-mostly-wrong-history-of-web-apps"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#a-practical-example" id="a-practical-example"></a>































<a class="anchor" href="https://fasterthanli.me/index.xml#love-hate" id="love-hate"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#does-dioxus-spark-joy" id="does-dioxus-spark-joy"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#afterword" id="afterword"></a>

---

### [Engineering a Rust optimization quiz](https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are several Rust quizzes online, including one that‚Äôs literally called the
‚ÄúUnfair Rust Quiz‚Äù at <a href="https://this.quiz.is.fckn.gay/">https:&#x2f;&#x2f;this.quiz.is.fckn.gay&#x2f;</a>, but when I was given the
opportunity to record an episode of the <a href="https://sdr-podcast.com/">Self-Directed Research
podcast</a> live on the main stage of <a href="https://eurorust.eu/2025/">EuroRust
2025</a>, I thought I‚Äôd come up with something special.</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Question Misc 6 of the unfair Rust quiz, about drop order. " height="714" src="https://cdn.fasterthanli.me/content/articles/engineering-a-rust-optimization-quiz/unfair-rust-quiz@2x~478a86d3446281d7.jxl" title="The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side. " width="795" /><figcaption><div class="markup-container figure-caption"><p>The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side.</p>
</div></figcaption>
</figure>



<a class="anchor" href="https://fasterthanli.me/index.xml#coming-up-with-the-questions" id="coming-up-with-the-questions"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#nih-syndrome-ppt" id="nih-syndrome-ppt"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#server-side-shenanigans-and-room-codes" id="server-side-shenanigans-and-room-codes"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#deploying-the-beast" id="deploying-the-beast"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#d-2-starting-fights-at-a-paris-meetup" id="d-2-starting-fights-at-a-paris-meetup"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#d-1-panic-mode-and-missing-explanations" id="d-1-panic-mode-and-missing-explanations"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#day-of-github-oauth-and-swipe-gestures" id="day-of-github-oauth-and-swipe-gestures"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#showtime" id="showtime"></a>

---

### [Making our own spectrogram](https://fasterthanli.me/articles/making-our-own-spectrogram)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>A couple months ago <a href="https://fasterthanli.me/articles/the-science-of-loudness">I made a loudness meter</a>
and went way too in-depth into how humans have measured loudness over time.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the fasterthanlime audio meter, with RMS, sample peak, true peak, and various loudness metrics.
" class="" height="512" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/fasterthanlime-audio-meter@2x~d5e6b54e3ade21f5.jxl" title="" width="800" /></p><p>Today we‚Äôre looking at a spectrogram visualization I made, which is a lot more entertaining!</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="" class="" height="632" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/spectrogram-sonata-op25-3@2x~c22af608785d58b2.jxl" title="" width="1100" /></p><p>We‚Äôre going to talk about how to extract frequencies from sound waves, but also
how my spectrogram app is assembled from different Rust crates, how it
handles audio and graphics threads, how it draws the spectrogram etc.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#the-humble-sine-wave" id="the-humble-sine-wave"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#approximating-a-square-wave" id="approximating-a-square-wave"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#a-real-world-sample" id="a-real-world-sample"></a>







<a class="anchor" href="https://fasterthanli.me/index.xml#chunking-and-windowing" id="chunking-and-windowing"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#the-gabor-limit" id="the-gabor-limit"></a>





















<a class="anchor" href="https://fasterthanli.me/index.xml#interpolation" id="interpolation"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#color-mapping" id="color-mapping"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#frequency-mapping" id="frequency-mapping"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#audio-input" id="audio-input"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#drawing-the-ui" id="drawing-the-ui"></a>



















































<a class="anchor" href="https://fasterthanli.me/index.xml#updating-the-texture" id="updating-the-texture"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#profiling-my-program" id="profiling-my-program"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#having-fun" id="having-fun"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#closing-words" id="closing-words"></a>

---

### [crates.io phishing attempt](https://fasterthanli.me/articles/crates-io-phishing-attempt)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>Earlier this week, an <a href="https://fasterthanli.me/articles/color-npm-package-compromised">npm supply chain attack</a>.</p>

<p>It‚Äôs turn for <a href="https://crates.io">crates.io</a>, the main public repository for <a href="https://rust-lang.org">Rust</a>
crates (packages).</p>

<p>The phishing e-mail looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="A phishing e-mail: Important: Breach notification regarding crates.io  Hi, BurntSushi! We recently discovered that an unauthorized actor had compromised the crates.io infrastructure and accessed a limited amount of user information. The attacker's access was revoked, and we are currently reviewing our security posture. We are currently drafting a blog post to outline the timeline and the steps we took to mitigate this. In the meantime, we strongly suggest you to rotate your login info by signing in here to our internal SSO, which is a temporary fix to ensure that the attacker cannot modify any packages published by you. " height="653" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/phishing-email~052f360399d29116.jxl" title="" width="708" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;burntsushi.net&#x2f;post&#x2f;3lynehptw6c2n">Andrew Gallant on BlueSky
</a></cite></figcaption>
</figure><p>And it leads to a GitHub login page that looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A fake GitHub sign-in page. " height="1378" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/github-phish~e78524d35ede5efb.jxl" title="" width="1322" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;github.com&#x2f;rust-lang&#x2f;crates.io&#x2f;discussions&#x2f;11889#discussion-8886064">Barre on GitHub
</a></cite></figcaption>
</figure><p>Several maintainers received it ‚Äî the issue is being discussed <a href="https://github.com/rust-lang/crates.io/discussions/11889">on GitHub</a>.</p>

<p>The <a href="https://www.rust-lang.org/governance/teams/dev-tools#team-crates-io">crates.io team</a> has acknowledged
the attack and said they‚Äôd see if they can do something about it.</p>

---

### [color npm package compromised](https://fasterthanli.me/articles/color-npm-package-compromised)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>On September 8 2025, around 13:00 UTC, someone compromised <a href="https://www.npmjs.com/~qix">Josh Junon‚Äôs npm
account (qix)</a> and started publishing backdoored
versions of his package.</p>

<p>Someone noticed and let Josh know:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Hey. Your npm account seems to have been compromised. 1 hour ago it started posting packages with backdoors to all your popular packages. " height="177" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/charlie-noticed@2x~4a9e74f87760a4af.jxl" title="" width="595" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;charlieeriksen.bsky.social&#x2f;post&#x2f;3lydffcyulc2n">Charlie Eriksen on BlueSky
</a></cite></figcaption>
</figure><p>Josh confirmed he‚Äôd gotten pwned by a fake 2FA (two-factor authentication) reset e-mail:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Yep, I've been pwned. 2FA reset email, looked very legitimate.  Only NPM affected. I've sent an email off to @npmjs.bsky.social  to see if I can get access again.  Sorry everyone, I should have paid more attention. Not like me; have had a stressful week. Will work to get this cleaned up. " height="396" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/josh-fake-2fa@2x~ca37f72d582a4442.jxl" title="" width="592" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;bad-at-computer.bsky.social&#x2f;post&#x2f;3lydioq5swk2y">Josh Junon on BlueSky
</a></cite></figcaption>
</figure><p>The phishing e-mail came from <code>npmsj.help</code> (registered 3 days prior) and claimed
users had to reset their 2FA:</p>







<a class="anchor" href="https://fasterthanli.me/index.xml#the-payload" id="the-payload"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#current-situation" id="current-situation"></a>

---

### [The science of loudness](https://fasterthanli.me/articles/the-science-of-loudness)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>My watch has a ‚ÄúNoise‚Äù app: it shows <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">B</mi></mrow></math>, for decibels.</p>

<p><video alt="A video of my Apple Watch showing me how loud the sound coming from my speakers is.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~d96a722c49fdda5d.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~e53858e76f36614d.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~fb169411c4d8653e.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>My amp has a volume knob, which also shows decibels, although.. negative ones, this time.</p>

<p><video alt="A video of me adjusting my volume on my Cambridge AXR100 amplifier. The
volume goes from -61 to -32 decibels in that video.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~46153fb1ad60f230.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~a8af11cbe6d90598.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~9a7388f3f768f948.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>And finally, my video editing software has a ton of meters ‚Äî which are all in decibel or
decibel-adjacent units.</p>

<p><video alt="A screenshot of DaVinci Resolve, showing various meters: we have Bus 1,
Control Room with TP, Loudness, YouTube (LUFS), then Loudness History
with Integrated, Momentary and Short Term. In the Mixer, each track has
its meter.
" class="" controls="controls" height="1127" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x.thumb" preload="none" title="" width="2002"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~9cc29f2809d1202a.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~da316f2ab0e0aeb5.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~e50f9e76cf48090c.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>How do all these decibels fit together?</p>






<a class="anchor" href="https://fasterthanli.me/index.xml#what-even-is-sound" id="what-even-is-sound"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#under-pressure" id="under-pressure"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#signal-processing" id="signal-processing"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#root-mean-square" id="root-mean-square"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#sample-peak-true-peak" id="sample-peak-true-peak"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#the-loudness-wars" id="the-loudness-wars"></a>











































































<a class="anchor" href="https://fasterthanli.me/index.xml#a-weighting" id="a-weighting"></a>

---

### [Summer fasterthanlime update](https://fasterthanli.me/articles/summer-fasterthanlime-update)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are news!</p>

<div class="tip markup-container">
<div class="tip-header bear-mark">
<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Cool bear" class="bear" height="48" src="https://cdn.fasterthanli.me/content/img/reimena/coolbear-idea~85823bd96ffd0bb6.jxl" width="48" />
Cool Bear's hot tip
</div>
<p>TL;DR: If you‚Äôre a patron or sponsor, check your <a href="https://fasterthanli.me/profile">Profile</a> page to
get detailed explainers of every perk. You‚Äôll need to log in. Duh.</p>

</div><p>Here are all the changes I‚Äôm implementing, summarized as a table:</p>

<div class="responsive-table"><table><thead><td>Before</td><td>After</td></thead><tr><td>üìö Articles remain exclusive for <strong>6 months</strong></td><td>Early access (<strong>couple weeks</strong>) for Silver tier</td></tr><tr><td>üéûÔ∏è No early access for video</td><td><strong>Video early access</strong> on Patreon and website</td></tr></table></div>

<a class="anchor" href="https://fasterthanli.me/index.xml#looking-back" id="looking-back"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#a-discord-server" id="a-discord-server"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#early-access-revamp" id="early-access-revamp"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#dual-rss-feeds" id="dual-rss-feeds"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#bye-ko-fi" id="bye-ko-fi"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#more-casual-posting" id="more-casual-posting"></a>




<a class="anchor" href="https://fasterthanli.me/index.xml#what-about-content-that-was-still-exclusive" id="what-about-content-that-was-still-exclusive"></a>

---

### [All color is best-effort](https://fasterthanli.me/articles/all-color-is-best-effort)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I do not come to you with answers today, but rather some observations and a lot of questions.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#the-weird-glitch" id="the-weird-glitch"><h2>The weird glitch</h2></a>
<p>Recently I was editing some video and I noticed this:</p>

<p>



<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/dvr-circles@2x~3c031f1f42693336.jxl" title="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." width="1966" /></p>

<p>Not what the finger is pointing at ‚Äî the dots.</p>

<p>Here are the separate layers this image is made up of: the background is a stock image
I‚Äôve licensed from Envato Elements:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A picture of a canyon, darker than you‚Äôd expect." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/canyon-background@2x~dcfb5771209ddbd5.jxl" title="A picture of a canyon, darker than you‚Äôd expect." width="1966" /></p>

<p>Because I use it as a background image, I‚Äôve cranked down the exposition in the Color tab:</p>













































































<a class="anchor" href="https://fasterthanli.me/index.xml#playing-with-color-spaces" id="playing-with-color-spaces"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#cie-chromaticity-diagram" id="cie-chromaticity-diagram"></a>


























































<a class="anchor" href="https://fasterthanli.me/index.xml#our-first-transfer-function" id="our-first-transfer-function"></a>






















































<a class="anchor" href="https://fasterthanli.me/index.xml#parade-scope" id="parade-scope"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#more-transfer-functions" id="more-transfer-functions"></a>































































































<a class="anchor" href="https://fasterthanli.me/index.xml#how-white-is-your-white" id="how-white-is-your-white"></a>









































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

## Tech News

### [AI tribalism](https://nolanlawson.com/2026/01/24/ai-tribalism/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/cw6f2s/ai_tribalism">Comments</a></p>

---

### [e9p - pure Erlang 9p implementation](https://tangled.org/hauleth.dev/e9p)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/2qsmw8/e9p_pure_erlang_9p_implementation">Comments</a></p>

---

### [Obvious Things C Should Do](https://www.digitalmars.com/articles/Cobvious.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/abqeyo/obvious_things_c_should_do">Comments</a></p>

---

### [Coi - WebAssembly for the Modern Web](https://io-eric.github.io/coi/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/sppr7m/coi_webassembly_for_modern_web">Comments</a></p>

---

### [MikroTik first look and getting started](https://rtfm.co.ua/en/mikrotik-first-look-and-getting-started/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/kgdkq3/mikrotik_first_look_getting_started">Comments</a></p>

---

### [December in Servo](https://servo.org/blog/2026/01/23/december-in-servo/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/9nwumc/december_servo">Comments</a></p>

---

### [I added a Bluesky comment section to my blog](https://micahcantor.com/blog/bluesky-comment-section.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/pdsego/i_added_bluesky_comment_section_my_blog">Comments</a></p>

---

### [modetc: Move your dotfiles from kernel space](https://maxwell.eurofusion.eu/git/rnhmjoj/modetc)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/lk5qjr/modetc_move_your_dotfiles_from_kernel">Comments</a></p>

---

### [Doing Gigabit Ethernet Over My British Phone Wires](https://thehftguy.com/2026/01/22/doing-gigabit-ethernet-over-my-british-phone-wires/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/iuzuw1/doing_gigabit_ethernet_over_my_british">Comments</a></p>

---


---

## üìö Â¶Ç‰Ωï‰ΩøÁî®

1. ÊµèËßàÊÑüÂÖ¥Ë∂£ÁöÑÊ†áÈ¢ò
2. ÈòÖËØªAIÁîüÊàêÁöÑÊëòË¶ÅÂø´ÈÄü‰∫ÜËß£ÂÜÖÂÆπ
3. ÁÇπÂáªÈìæÊé•Ê∑±ÂÖ•ÈòÖËØª
4. Êúâ‰ª∑ÂÄºÁöÑÂÜÖÂÆπÂèØ‰ª•Êï¥ÁêÜÂà∞ÂØπÂ∫îÁöÑ‰∏ªÈ¢òÁõÆÂΩï

## üîß ÈÖçÁΩÆ

‰øÆÊîπ `config/sources.yaml` ÂèØ‰ª•:
- Ê∑ªÂä†/Âà†Èô§RSSËÆ¢ÈòÖÊ∫ê
- Ë∞ÉÊï¥HackerNewsÊúÄÂ∞èÂàÜÊï∞ÈòàÂÄº
- ÈÖçÁΩÆÂÜÖÂÆπËøáÊª§ÂÖ≥ÈîÆËØç

*Êú¨ÊñáÊ°£Áî± [daily-digestËÑöÊú¨](../scripts/generate_digest.py) Ëá™Âä®ÁîüÊàê*