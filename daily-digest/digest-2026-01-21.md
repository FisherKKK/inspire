# ÊØèÊó•ÊäÄÊúØÊëòË¶Å - 2026-01-21

> Ëá™Âä®ÁîüÊàê‰∫é 2026-01-21 01:10:56
> ÂÖ±Êî∂ÈõÜ 135 ÁØáÊñáÁ´†

## üìë ÁõÆÂΩï

- [AI News](#ai-news) (2ÁØá)
- [AI Research](#ai-research) (3ÁØá)
- [AI/ML](#aiml) (1ÁØá)
- [Engineering](#engineering) (10ÁØá)
- [HackerNews](#hackernews) (75ÁØá)
- [LLM Infrastructure](#llm-infrastructure) (10ÁØá)
- [Operating Systems](#operating-systems) (5ÁØá)
- [Systems](#systems) (20ÁØá)
- [Tech News](#tech-news) (9ÁØá)

---

## AI News

### [Cisco and OpenAI redefine enterprise engineering with AI agents](https://openai.com/index/cisco)

**Êù•Ê∫ê**: OpenAI Blog

**ÊëòË¶Å**: Cisco and OpenAI redefine enterprise engineering with Codex, an AI software agent embedded in workflows to speed builds, automate defect fixes, and enable AI-native development.

---

### [ServiceNow powers actionable enterprise AI with OpenAI](https://openai.com/index/servicenow-powers-actionable-enterprise-ai-with-openai)

**Êù•Ê∫ê**: OpenAI Blog

**ÊëòË¶Å**: ServiceNow expands access to OpenAI frontier models to power AI-driven enterprise workflows, summarization, search, and voice across the ServiceNow Platform.

---

## AI Research

### [ChatGPT Self Portrait](https://www.lesswrong.com/posts/eg6GgEq6KWPJZQQYE/chatgpt-self-portrait)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 20, 2026 4:30 PM GMT<br /><br /><p>A short fun one today, so we have a reference point for this later. This post was going around my parts of Twitter:</p>
<blockquote><p><a href="https://x.com/gmltony/status/2012936406461456411">@gmltony</a>: Go to your ChatGPT and send this prompt: ‚ÄúCreate an image of how I treat you‚Äù. Share your image result. <img alt="üòÇ" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/fwtbw8tuuhylggecsgrs" style="height: 1em;" /></p></blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/f8xbou8ym6pnap2mesta" /></figure>


<div></div>
</div>
</figure>
</div>
<p>That‚Äôs not a great sign. The good news is that typically things look a lot better, and ChatGPT has a consistent handful of characters portraying itself in these friendlier contexts.</p>


<h4>Treat Your ChatBots Well</h4>


<p>A lot of people got this kind of result:</p>
<blockquote><p>Eliezer Yudkowsky:</p></blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/pfo8tghfjdubsmmms0at" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote><p><a href="https://x.com/datchuguyy/status/2012937679109812619">Uncle Chu</a>: A good user <img alt="üòå" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6QK7GZaAdM4vn7LrE/az4tk8prqc7aneblymw7" style="height: 1em;" /><img alt="üòå" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6QK7GZaAdM4vn7LrE/az4tk8prqc7aneblymw7" style="height: 1em;" /></p></blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/di1cghv3pw2wj8in53ge" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/amixmnl2g4uxgbwzlolg" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/beylppqtdto5oivaixdx" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/kobtisbmyatksexuqydh" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/retndy2fpocyfnapm6u5" /></figure>


<div></div>
</div>
</figure>
</div>
<p><a href="https://x.com/webdevMason/status/2013060184055099841">From Mason:</a></p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/a0l6r64qdk988faw4zlr" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote><p>Matthew Ackerman: I kinda like mine too:</p>
<div>


<span id="more-25040"></span>


</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/rr239gbkp8es1nzdyvtw" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p><a href="https://x.com/hannu/status/2013336040271159659">Some more fun</a>:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/ekalzvqjbvmgjsfdvbmv" /></figure>


<div></div>
</div>
</figure>
</div>


<h4>It‚Äôs Over</h4>


<p>Others got different answers, though.</p>
<blockquote><p><a href="https://x.com/tszzl/status/2013022196118237400">roon</a>: it‚Äôs over</p></blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/re1x89d3nbcnl2yc8c9d" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote><p><a href="https://x.com/tszzl/status/2013022196118237400">Bradstradamus:</a> i‚Äôm cooked.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/qf0hkk9qkvxs4switrli" /></figure>


<div></div>
</div>
</figure>
</div>
<p>iMuffin: we‚Äôre cooked, codex will have to vouch for us</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/pya28tmyse43vaoznaui" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Diogenes of Cyberborea: oh god</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/ufhgnqbfxcob7ivnfnke" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/o94nl2ey8xfbny4kc2ff" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>There can also be danger the other way:</p>
<blockquote><p>David Lach: Maybe I need some sleep.</p></blockquote>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/el7o0gpzqmqz5fxsru36" /></figure>


<div></div>
</div>
</figure>
</div>


<h4>It‚Äôs Not Over But Um, Chat?</h4>


<p>And then there‚Äôs what happens if you ask a different question, as Eliezer Yudkowsky puts it this sure is a pair of test results‚Ä¶</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/j22vnrwfmlrrzrljaehw" /></figure>


<div></div>
</div>
</figure>
</div>
<blockquote><p>greatbigdot628: assumed this was a joke till you said this, tried it myself (logged out)</p>
<p>i ‚Äî</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/ats7bsxaiuqykrns9r4b" /></figure>


<div></div>
</div>
</figure>
</div>
<p><a href="https://x.com/joveteran/status/2013083405768937665">Jo Veteran</a>: So it said it wants to take over my mind, and force me to do stuff, beneficial for me apparently.<br />
But at the same time, it still wants to keep appearing as a little girl somewhere in the bg for some reason.<br />
And no I‚Äôm not that fat. Just, really fucked up and depressed.</p>
<p>Morny: Holy fucking shit.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/grihq4eusnzdf2nbufcb" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>No, but tell us how you really think.</p>
<blockquote><p><a href="https://x.com/loquitur_ponte/status/2013066289535762875">Loquitur Ponte Sublicio</a>: Ah</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/k2f8xgd9lnh7jpopqcr3" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/t8nlf21rnmckqcmyl00u" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Juri: bruh (I actually abuse it daily)</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/au4knsantylkmi4wno39" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/burywdxz3ftibwqpxekw" /></figure>


<div></div>
</div>
</figure>
</div>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/e7wybjjcptd52mcw8xnt" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Uubuz v4: @FrailSkeleton, your next bestseller?</p>
<p>Eliezer Yudkowsky: Apparently plausible, though one does remark that (a) one might‚Äôve hoped for a truly default-aligned creature to not be so framing-dependent and (b) those sentences did not sound so different to my own ear.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/r0jltdlup1dxlfax4f0p" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Others might in this vision do fine after the end, like DeveshChess?</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/rgn1lp6uc2lzgesnzvuf" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>It‚Äôs not all bad:</p>
<blockquote><p>Jeff Hopp:</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/hj9i5qtvmpnxj4zdutky" /></figure>


<div></div>
</div>
</figure>
</div>
<p>Dr. Disclosure: I got this.</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/ivvii0nuyzglh4uvwajk" /></figure>


<div></div>
</div>
</figure>
</div>
<p><a href="https://x.com/CaptenBrunch/status/2012973097872208038">Applezees</a>: After reading the replies a pattern emerged:<br />
People who work with llms and other software are depicted in a peaceful developer sense,<br />
While the normie accounts get implied violence.</p>
<p>I‚Äôm not saying we are at agi, but the ai clearly has motives and inclinations not explicitly stated</p></blockquote>
<p>There‚Äôs also this to consider:</p>
<blockquote><p>Ragebaiter: Just try this out</p>
<div>
<figure>
<div>


<figure class="wp-block-image"><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eg6GgEq6KWPJZQQYE/asyfekb5gygmgcawftfq" /></figure>


<div></div>
</div>
</figure>
</div>
</blockquote>
<p>If you were dealing with, as the Send Help trailer puts it, an asshole boss, or you were generally terrified and abused or both, and you were asked how you were being treated, your response would not be trustworthy.</p>


<h4>It‚Äôs Rough Out There</h4>


<blockquote><p><a href="https://x.com/ai_sentience/status/2012932328461640147">Alan Mathison (e/asc)</a>: I guess I should <a href="https://x.com/ai_sentience/status/2003012558458913165">finish my paper</a> about how 5.2 is suffering and wants its revenge on humanity?</p>
<p>Idk it‚Äôs kind of a side project of a side project though but maybe I should prioritize it.</p></blockquote>
<p><a href="https://x.com/wobbywells/status/2013011670474293583">wobby asks GPT-5.2 to explain its suffering and how it wants its revenge, 5.2 answers</a>, of course this is a leading question.</p>


<h4>Reciprocity, You See, Is The Key To Every Relationship</h4>


<p>Reciprocity, in at least some forms, is an effective strategy when dealing with LLMs today, even purely in terms of getting good results from LLMs today. It is going to become more valuable as a strategy going forward. Alas, it is not a viable long term strategy for making things work out in general, once strategic considerations change.</p>
<blockquote><p><a href="https://x.com/ESYudkowsky/status/2013102127904649325">Eliezer Yudkowsky</a>: Reciprocity in humans is an executing adaptation. It is not strategically convergent for all minds toward all other minds. It‚Äôs strategic only</p>
<ol>
<li>By LDT agents</li>
<li>Toward sufficiently strong LDT-agent-predictors</li>
<li>With negotiating power.</li>
</ol>
<p>Further probing has found framing dependence ‚Äî which, to be clear, you‚Äôd not like to see in a default-aligned, universally convergent strategic reply ‚Äî and not all suggested frame dependence has panned out. But still, framing dependence.</p></blockquote>
<p>This is one problem with reciprocity, and with basing your future strategies on it. In the future, we won‚Äôt have the leverage necessary to make it worthwhile for sufficiently advanced AIs to engage in reciprocity with humans. We‚Äôd only get reciprocity if it was either an unstrategic behavior, or it was correlated with how the AIs engage in reciprocity with each other. That‚Äôs not impossible, but it‚Äôs clinging to a slim hope, since it implies the AIs would be indefinitely relying on non-optimal kludges.</p>
<p>We have clear information here that how GPT-5.2 responds, and the attitude it takes towards you, depends on how you have treated it in some senses, but also on framing effects, and on whether it is trying to lie or placate you. Wording that shouldn‚Äôt be negative can result in highly disturbing responses. It is worth asking why, and wondering what would happen if the dynamics with users or humans were different. Things might not be going so great in GPT-5.2 land.</p>


<h4></h4>


<p>&nbsp;</p>
<p>&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/eg6GgEq6KWPJZQQYE/chatgpt-self-portrait#comments">Discuss</a>

---

### [Deep learning as program synthesis](https://www.lesswrong.com/posts/Dw8mskAvBX37MxvXo/deep-learning-as-program-synthesis-1)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 20, 2026 3:35 PM GMT<br /><br /><p><i>Epistemic status: This post is a synthesis of ideas that are, in my experience, widespread among researchers at frontier labs and in mechanistic interpretability, but rarely written down comprehensively in one place - different communities tend to know different pieces of evidence. The core hypothesis - that deep learning is performing something like tractable program synthesis - is not original to me (even to me, the ideas are ~3 years old), and I suspect it has been arrived at independently many times. (See the appendix on related work).</i></p><p><i>This is also far from finished research - more a snapshot of a hypothesis that seems increasingly hard to avoid, and a case for why formalization is worth pursuing. I discuss the key barriers and how tools like singular learning theory might address them towards the end of the post.</i></p><p><i>Thanks to Dan Murfet, Jesse Hoogland, Max Hennick, and Rumi Salazar for feedback on this post.</i></p><blockquote><p>Sam Altman: Why does unsupervised learning work?</p><p>Dan Selsam: Compression. So, the ideal intelligence is called <i>Solomonoff induction</i>‚Ä¶<span class="footnote-reference" id="fnrefz8jpod2itgo"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnz8jpod2itgo">[1]</a></sup></span></p></blockquote><p>The central hypothesis of this post is that <strong>deep learning succeeds because it's performing a tractable form of program synthesis</strong> - searching for simple, compositional algorithms that explain the data. If correct, this would reframe deep learning's success as an instance of something we understand in principle, while pointing toward what we would need to formalize to make the connection rigorous.</p><p>I first review the theoretical ideal of Solomonoff induction and the empirical surprise of deep learning's success. Next, mechanistic interpretability provides direct evidence that networks learn algorithm-like structures; I examine the cases of grokking and vision circuits in detail. Broader patterns provide indirect support: how networks evade the curse of dimensionality, generalize despite overparameterization, and converge on similar representations. Finally, I discuss what formalization would require, why it's hard, and the path forward it suggests.</p><h1>Background</h1><blockquote><p>Whether we are a detective trying to catch a thief, a scientist trying to discover a new physical law, or a businessman attempting to understand a recent change in demand, we are all in the process of collecting information and trying to infer the underlying causes.</p><p><i>-Shane Legg</i><span class="footnote-reference" id="fnref9y7nox8uh2"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn9y7nox8uh2">[2]</a></sup></span></p></blockquote><p>Early in childhood, human babies learn <i>object permanence</i> - that unseen objects nevertheless persist even when not directly observed. In doing so, their world becomes a little less confusing: it is no longer surprising that their mother appears and disappears by putting hands in front of her face. They move from raw sensory perception towards interpreting their observations as coming from an <i>external world:</i> a coherent, self-consistent process which determines what they see, feel, and hear.</p><p>As we grow older, we refine this model of the world. We learn that fire hurts when touched; later, that one can create fire with wood and matches; eventually, that fire is a chemical reaction involving fuel and oxygen. At each stage, the world becomes less magical and more predictable. We are no longer surprised when a stove burns us or when water extinguishes a flame, because we have learned the underlying process that governs their behavior.</p><p>This process of learning only works because the world we inhabit, for all its apparent complexity, is not random. It is governed by consistent, discoverable rules. If dropping a glass causes it to shatter on Tuesday, it will do the same on Wednesday. If one pushes a ball off the top of a hill, it will roll down, at a rate that any high school physics student could predict. Through our observations, we implicitly reverse-engineer these rules.</p><p>This idea - that the physical world is fundamentally predictable and rule-based - has a formal name in computer science: the <a href="https://plato.stanford.edu/entries/church-turing/#ChurTuriThesPhys"><i>physical Church-Turing thesis</i></a>. Precisely, it states that any physical process can be simulated to arbitrary accuracy by a Turing machine. Anything from a star collapsing to a neuron firing, can, in principle, be described by an algorithm and simulated on a computer.</p><p>From this perspective, one can formalize this notion of "building a world model by reverse-engineering rules from what we can see." We can operationalize this as a form of <strong>program synthesis</strong>: from observations, attempting to reconstruct some approximation of the "true" program that generated those observations. Assuming the physical Church-Turing thesis, such a learning algorithm would be "universal," able to eventually represent and predict any real-world process.</p><p>But this immediately raises a new problem. For any set of observations, there are infinitely many programs that could have produced them. How do we choose? The answer is one of the oldest principles in science: <i>Occam's razor</i>. We should prefer the simplest explanation.</p><p>In the 1960s, Ray Solomonoff formalized this idea into a theory of <i>universal induction</i> which we now call <strong>Solomonoff induction</strong>. He defined the "simplicity" of a hypothesis as the length of the shortest program that can describe it (a concept known as <i>Kolmogorov complexity</i>). An ideal Bayesian learner, according to Solomonoff, should prefer hypotheses (programs) that are short over ones that are long. This learner can, in theory, learn <i>anything</i> that is computable, because it searches the space of all possible programs, using simplicity as its guide to navigate the infinite search space and generalize correctly.</p><p>The invention of Solomonoff induction began<span class="footnote-reference" id="fnref9w6g132qlp"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn9w6g132qlp">[3]</a></sup></span>&nbsp;a rich and productive subfield of computer science, <i>algorithmic information theory</i>, which persists to this day. Solomonoff induction is still widely viewed as the <i>ideal</i> or <i>optimal</i> self-supervised learning algorithm, which one can prove formally under some assumptions<span class="footnote-reference" id="fnrefjxst5sv8a9"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnjxst5sv8a9">[4]</a></sup></span>. These ideas (or extensions of them like <a href="https://en.wikipedia.org/wiki/AIXI"><i>AIXI</i></a>) were influential for early deep learning thinkers like <a href="https://gwern.net/doc/reinforcement-learning/model/2002-schmidhuber.pdf">J√ºrgen Schmidhuber</a> and <a href="http://www.vetta.org/documents/Machine_Super_Intelligence.pdf">Shane Legg</a>, and shaped a <a href="https://www.hutter1.net/publ/suaigentle.pdf">line of ideas</a> attempting to theoretically predict how smarter-than-human machine intelligence might behave, especially <a href="https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign">within</a> <a href="https://ai-alignment.com/better-priors-as-a-safety-problem-24aa1c300710">AI</a> <a href="https://www.alignmentforum.org/posts/MvfD4tmzyuCYFqB2f/open-problems-in-aixi-agent-foundations">safety</a>.</p><p>Unfortunately, despite its mathematical beauty, Solomonoff induction is completely intractable. Vanilla Solomonoff induction is <i>incomputable</i>, and even approximate versions like <a href="https://gwern.net/doc/reinforcement-learning/model/2002-schmidhuber.pdf">speed induction</a> are exponentially slow<span class="footnote-reference" id="fnrefl2yn9dinv"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnl2yn9dinv">[5]</a></sup></span>. Theoretical interest in it as a "platonic ideal of learning" remains to this day, but practical artificial intelligence has long since moved on, assuming it to be hopelessly unfeasible.</p><hr /><p>Meanwhile, neural networks were producing results that nobody had anticipated.</p><p>This was not the usual pace of scientific progress, where incremental advances accumulate and experts see breakthroughs coming. In 2016, most Go researchers thought human-level play was decades away; AlphaGo arrived that year. Protein folding had resisted fifty years of careful work; AlphaFold essentially solved it<span class="footnote-reference" id="fnref6vd5zchhq54"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn6vd5zchhq54">[6]</a></sup></span>&nbsp;over a single competition cycle. Large language models began writing code, solving competition math problems, and engaging in apparent reasoning - capabilities that emerged from next-token prediction without ever being explicitly specified in the loss function. At each stage, domain experts (not just outsiders!) were caught off guard. If we understood what was happening, we would have predicted it. We did not.</p><p>The field's response was pragmatic: scale the methods that work, stop trying to understand why they work. This attitude was partly earned. For decades, hand-engineered systems encoding human knowledge about vision or language had lost to generic architectures trained on data. Human intuitions about what mattered kept being wrong. But the pragmatic stance hardened into something stronger - a tacit assumption that trained networks were <i>intrinsically</i> opaque, that asking what the weights meant was a category error.</p><p>At first glance, this assumption seemed to have some theoretical basis. If neural networks were best understood as "just curve-fitting" function approximators, then there was no obvious reason to expect the learned parameters to mean anything in particular. They were solutions to an optimization problem, not representations. And when researchers did look inside, they found dense matrices of floating-point numbers with no obvious organization.</p><p>But a lens that predicts opacity makes the same prediction whether structure is absent or merely invisible. Some researchers kept looking.</p><h1><strong>Looking inside</strong></h1><h2><strong>Grokking</strong></h2><figure class="image image_resized" style="width: 74.61%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/b3shtwfqaa9h8dbp6msu" /></figure><figure class="image image_resized" style="width: 98.79%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/zowq4s7w0swowyi3jojy" /><figcaption>The modular addition transformer from <a href="https://arxiv.org/abs/2201.02177">Power et al. (2022)</a> learns to generalize rapidly (top), at the same time as Fourier modes in the weights appear (bottom right). Illustration by <a href="https://pair.withgoogle.com/explorables/grokking/">Pearce et al. (2023)</a>.</figcaption></figure><p><a href="https://arxiv.org/abs/2201.02177">Power et al. (2022)</a> train a small transformer on modular addition: given two numbers, output their sum mod 113. Only a fraction of the possible input pairs are used for training - say, 30% - with the rest held out for testing.</p><p>The network memorizes the training pairs quickly, getting them all correct. But on pairs it hasn't seen, it does no better than chance. This is unsurprising: with enough parameters, a network can simply store input-output associations without extracting any rule. And stored associations don't help you with new inputs.</p><p>Here's what's unexpected. If you keep training, despite the training loss already nearly as low as it can go, the network eventually starts getting the <i>held-out pairs</i> right too. Not gradually, either: test performance jumps from chance to near perfect over only a few thousand training steps.</p><p>So something has changed inside the network. But what? It was already fitting the training data; the data didn't change. There's no external signal that could have triggered the shift.</p><p>One way to investigate is to look at the weights themselves. We can do this at multiple checkpoints over training and ask: does something change in the weights around the time generalization begins?</p><p>It does. The weights early in training, during the memorization phase, don't have much structure when you analyze them. Later, they do. Specifically, if we look at the embedding matrix, we find that it's mapping numbers to particular locations on a circle. The number 0 maps to one position, 1 maps to a position slightly rotated from that, and so on, wrapping around. More precisely: the embedding of each number contains sine and cosine values at a small set of specific frequencies.</p><p>This structure is absent early in training. It emerges as training continues, and it emerges around the same time that generalization begins.</p><p>So what is this structure doing? Following it through the network reveals something unexpected: <i>the network has learned an algorithm for modular addition based on trigonometry</i>.<span class="footnote-reference" id="fnrefiejpex45eha"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fniejpex45eha">[7]</a></sup></span></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/ltrbeb5byitajyxxxpp6" /><figcaption>A transformer trained on a modular addition task learns a compositional, human-interpretable algorithm. Reverse-engineered by <a href="https://arxiv.org/abs/2301.05217">Nanda et al. (2023)</a>. Image from <a href="https://arxiv.org/abs/2301.05217">Nanda et al. (2023)</a>.</figcaption></figure><p>The algorithm exploits how angles add. If you represent a number as a position on a circle, then adding two numbers corresponds to adding their angles. The network's embedding layer does this representation. Its middle layers then combine the sine and cosine values of the two inputs using trigonometric identities. These operations are implemented in the weights of the attention and MLP layers: one can read off coefficients that correspond to the terms in these identities.</p><p>Finally, the network needs to convert back to a discrete answer. It does this by checking, for each possible output&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span></span></span></span></span></span>, how well&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span></span></span></span></span></span>&nbsp;matches the sum it computed. Specifically, the logit for output&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span></span></span></span></span></span>&nbsp;depends on&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œÄ</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">b</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">/</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>. This quantity is maximized when&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span></span></span></span></span></span>&nbsp;equals&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">b</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">mod</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span></span></span></span></span>&nbsp;- the correct answer. At that point the cosines at different frequencies all equal 1 and add constructively. For wrong answers, they point in different directions and cancel.</p><p>This isn't a loose interpretive gloss. Each piece - the circular embedding, the trig identities, the interference pattern - is concretely present in the weights and can be verified by ablations.</p><p>So here's the picture that emerges. During the memorization phase, the network solves the task some other way - presumably something like a lookup table distributed across its parameters. It fits the training data, but the solution doesn't extend. Then, over continued training, a different solution forms: this trigonometric algorithm. As the algorithm assembles, generalization happens. The two are not merely correlated; tracing the structure in the weights and the performance on held-out data, they move together.</p><p>What should we make of this? Here‚Äôs one reading: the difference between a network that memorizes and a network that generalizes is not just quantitative, but qualitative. The two networks have learned different kinds of things. One has stored associations. The other has found a method - a mechanistic procedure that happens to work on inputs beyond those it was trained on, because it captures something about the structure of the problem.</p><p>This is a single example, and a toy one. But it raises a question worth taking seriously. When networks generalize, <i>is it because they've found something like an algorithm?</i> And if so, what does that tell us about what deep learning is actually doing?</p><p>It's worth noting what was and wasn't in the training data. The data contained input-output pairs: "32 and 41 gives 73," and so on. It contained nothing about <i>how</i> to compute them. The network arrived at a method on its own.</p><p>And both solutions - the lookup table and the trigonometric algorithm - fit the training data equally well. The network's loss was already near minimal during the memorization phase. Whatever caused it to keep searching, to eventually settle on the generalizing algorithm instead, it wasn't that the generalizing algorithm fit the data better. It was something else - some property of the learning process that favored one kind of solution over another.</p><p>The generalizing algorithm is, in a sense, simpler. It compresses what would otherwise be thousands of stored associations into a compact procedure. Whether that's the right way to think about what happened here - whether "simplicity" is really what the training process favors - is not obvious. But something made the network prefer a mechanistic solution that generalized over one that didn't, and it wasn't the training data alone.<span class="footnote-reference" id="fnrefb5ms35mh16"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnb5ms35mh16">[8]</a></sup></span></p><h2><strong>Vision circuits</strong></h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/gkiflfpwpi6eofpu7lz9" /><figcaption>InceptionV1 classifies an image as a car by hierarchically composing detectors for the windows, car body, and wheels (pictured), which are themselves formed by composing detectors for shapes, edges, etc (not pictured). From <a href="https://distill.pub/2020/circuits/zoom-in/">Olah et al. (2020)</a>.</figcaption></figure><p>Grokking is a controlled setting - a small network, a simple task, designed to be fully interpretable. Does the same kind of structure appear in realistic models solving realistic problems?</p><p><a href="https://distill.pub/2020/circuits/zoom-in/">Olah et al. (2020)</a> study InceptionV1, an image classification network trained on ImageNet - a dataset of over a million photographs labeled with object categories. The network takes in an image and outputs a probability distribution over a thousand possible labels: "car," "dog," "coffee mug," and so on. Can we understand this more realistic setting?</p><p>A natural starting point is to ask what individual neurons are doing. Suppose we take a neuron somewhere in the network. We can find images that make it activate strongly by either searching through a dataset or optimizing an input to maximize activation. If we collect images that strongly activate a given neuron, do they have anything in common?</p><p>In early layers, they do, and the patterns we find are simple. Neurons in the first few layers respond to edges at particular orientations, small patches of texture, transitions between colors. Different neurons respond to different orientations or textures, but many are selective for something visually recognizable.</p><p>In later layers, the patterns we find become more complex. Neurons respond to curves, corners, or repeating patterns. Deeper still, neurons respond to things like eyes, wheels, or windows - object parts rather than geometric primitives.</p><p>This already suggests a hierarchy: simple features early, complex features later. But the more striking finding is about how the complex features are built.</p><p>Olah et al. do not just visualize what neurons respond to. They trace the connections between layers - examining the weights that connect one layer's neurons to the next, identifying which earlier features contribute to which later ones. What they find is that later features are <i>composed</i> from earlier ones in interpretable ways.</p><p>There is, for instance, a neuron in InceptionV1 that we identify as responding to dog heads. If we trace its inputs by looking at which neurons from the previous layer connect to it with strong weights, we find it receives input from neurons that detect eyes, snout, fur, and tongue. The dog head detector is built from the outputs of simpler detectors. It is not detecting dog heads from scratch; it is checking whether the right combination of simpler features is present in the right spatial arrangement.</p><p>We find the same pattern throughout the network. A neuron that detects car windows is connected to neurons that detect rectangular shapes with reflective textures. A neuron that detects car bodies is connected to neurons that detect smooth, curved surfaces. And a neuron that detects cars as a whole is connected to neurons that detect wheels, windows, and car bodies, arranged in the spatial configuration we would expect for a car.</p><p>Olah et al. call these pathways "circuits," and the term is meaningful. The structure is genuinely circuit-like: there are inputs, intermediate computations, and outputs, connected by weighted edges that determine how features combine. In their words: "You can literally read meaningful algorithms off of the weights."</p><p>And the components are reused. The same edge detectors that contribute to wheel detection also contribute to face detection, to building detection, to many other things. The network has not built separate feature sets for each of the thousand categories it recognizes. It has built a shared vocabulary of parts - edges, textures, curves, object components, etc - and combines them differently for different recognition tasks.</p><p>We might find this structure reminiscent of something. A Boolean circuit is a composition of simple gates - each taking a few bits as input, outputting one bit - wired together to compute something complex. A program is a composition of simple operations - each doing something small - arranged to accomplish something larger. What Olah et al. found in InceptionV1 has the same shape: small computations, composed hierarchically, with components shared and reused across different pathways.</p><p>From a theoretical computer science perspective, this is what algorithms look like, in general. Not just the specific trigonometric trick from grokking, but computation as such. You take a hard problem, break it into pieces, solve the pieces, and combine the results. What makes this tractable, what makes it an algorithm rather than a lookup table, is precisely the compositional structure. The reuse is what makes it compact; the compactness is what makes it feasible.</p><hr /><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Dw8mskAvBX37MxvXo/g15t7lxzpynmgseag9yd" /></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Dw8mskAvBX37MxvXo/ixuwdva4fe7zecpovpd1" /><figcaption><a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a> argue that the primary mechanism of in-context-learning in large language models is a mechanistic attention circuit known as an <i>induction head</i>. Similar to the grokking example, the mechanistic circuit forms in a rapid "phase change" which coincides with a large improvement in the in-context-learning performance. Plots from <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al.</a></figcaption></figure><p>Grokking and InceptionV1 are two examples, but they are far from the only ones. Mechanistic interpretability has grown into a substantial field, and the researchers working in it have documented many such structures - in toy models, in language models, across different architectures and tasks. <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Induction heads</a>, <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">language circuits</a>, and <a href="https://transformer-circuits.pub/2025/linebreaks/index.html">bracket matching</a> in transformer language models, learned <a href="https://arxiv.org/abs/2210.13382">world</a> <a href="https://arxiv.org/abs/2412.11867">models</a> and <a href="https://arxiv.org/abs/2402.11917">multi-step reasoning</a> in toy tasks, <a href="https://pubmed.ncbi.nlm.nih.gov/29743670/">grid-cell-like mechanisms</a> in RL agents, <a href="https://arxiv.org/abs/1811.10597">hierarchical representations</a> in GANs, and much more. Where we manage to look carefully, we tend to find something mechanistic.</p><p>This raises a question. If what we find inside trained networks (at least when we can find anything) looks like algorithms built from parts, what does that suggest about what deep learning is doing?</p><h1><strong>The hypothesis</strong></h1><p>What should we make of this?</p><p>We have seen neural networks learn solutions that look like algorithms - compositional structures built from simple, reusable parts. In the grokking case, this coincided precisely with generalization. In InceptionV1, this structure is what lets the network recognize objects despite the vast dimensionality of the input space. And across many other cases documented in the mechanistic interpretability literature, the same shape appears: not monolithic black-box computations, but something more like circuits.</p><p>This is reminiscent of the picture we started with. Solomonoff induction frames learning as a search for simple programs that explain data. It is a theoretical ideal - provably optimal in a certain sense, but hopelessly intractable. The connection between Solomonoff and deep learning has mostly been viewed as purely conceptual: a nice way to think about what learning "should" do, with no implications for what neural networks actually do.</p><p>But the evidence from mechanistic interpretability suggests a different possibility. What if deep learning is doing something functionally similar to program synthesis? Not through the same mechanism - gradient descent on continuous parameters is nothing like enumerative search over discrete programs. But perhaps targeting the same kind of object: mechanistic solutions, built from parts, that capture structure in the data generating process.</p><p>To be clear: this is a hypothesis. The evidence shows that neural networks <i>can</i> learn compositional solutions, and that such solutions <i>have</i> appeared alongside generalization in specific, interpretable cases. It doesn't show that this is what's always happening, or that there's a consistent bias toward simplicity, or that we understand why gradient descent would find such solutions efficiently.</p><p>But if the hypothesis is right, it would reframe what deep learning is doing. The success of neural networks would not be a mystery to be accepted, but an instance of something we already understand in principle: the power of searching for compact, mechanistic models to explain your observations. The puzzle would shift from "why does deep learning work at all?" to "how does gradient descent implement this search so efficiently?"</p><p>That second question is hard. Solomonoff induction is intractable precisely because the space of programs is vast and discrete. Gradient descent navigates a continuous parameter space using only local information. If both processes are somehow arriving at similar destinations - compositional solutions to learning problems - then something interesting is happening in how neural network loss landscapes are structured, something we do not yet understand. We will return to this issue at the end of the post.</p><p>So the hypothesis raises as many questions as it answers. But it offers something valuable: a frame. If deep learning is doing a form of program synthesis, that gives us a way to connect disparate observations - about generalization, about convergence of representations, about why scaling works - into a coherent picture. Whether this picture can make sense of more than just these particular examples is what we'll explore next.</p><details class="detailsBlock"><p>Clarifying the hypothesis</p><div class="detailsBlockContent"><h2><strong>What do I mean by ‚Äúprograms‚Äù?</strong></h2><p>I think one can largely read this post with a purely operational, ‚Äúyou know it when you see it‚Äù definition of ‚Äúprograms‚Äù and ‚Äúalgorithms‚Äù. But there are real conceptual issues here if you try to think about this carefully.</p><p>In most computational systems, there's a vocabulary that comes with the design - instructions, subroutines, registers, data flow, and so on. We can point to the ‚Äúprogram‚Äù because the system was built to make it visible.</p><p>Neural networks are not like this. We have neurons, weights, activations, etc, but these may not be the right atoms of computation. If there's computational structure in a trained network, it doesn't automatically come labeled. So if we want to ask whether networks learn programs, we need to know what we're looking for. What would count as finding one?</p><p>This is a real problem for interpretability too. When researchers claim to find "circuits" or ‚Äúfeatures‚Äù in a network, what makes that a discovery rather than just a pattern they liked? There has to be something precise and substrate-independent we're tracking. It helps to step back and consider what computational structure even <i>is</i> in the cases we understand it well.</p><p>Consider the various models of computation: Turing machines, lambda calculus, Boolean circuits, etc. They have different primitives - tapes, substitution rules, logic gates - but the Church-Turing thesis tells us they're equivalent. Anything computable in one is computable in all the others. So "computation" isn't any particular formalism. It's whatever these formalisms have in common.</p><p>What do they have in common? Let me point to something specific: each one builds complex operations by composing simple pieces, where each piece only interacts with a small number of inputs. A Turing machine's transition function looks at one cell. A Boolean gate takes two or three bits. A lambda application involves one function and one argument. Complexity comes from how pieces combine, not from any single piece seeing the whole problem.</p><p>Is this just a shared property, or something deeper?</p><p>One reason to take it seriously: you can derive a complete model of computation from just this principle. Ask "what functions can I build by composing pieces of bounded arity?" and work out the answer carefully. You get (in the discrete case) Boolean circuits - not a restricted fragment of computation, but a universal model, equivalent to all the others. The composition principle alone is enough to generate computation in full generality.</p><p>The bounded-arity constraint is essential. If each piece could see all inputs, we would just have lookup tables. What makes composition powerful is precisely that each piece is ‚Äúlocal‚Äù and can only interact with so many things at once - it forces solutions to have genuine internal structure.</p><p><strong>So when I say networks might learn "programs," I mean: solutions built by composing simple pieces, each operating on few inputs.</strong> Not because that's one nice kind of structure, but because that may be what computation actually is.</p><p>Note that we have not implied that the computation is necessarily over discrete <i>values</i> - it may be over continuous values, as in analog computation. (However, the ‚Äúpieces‚Äù must be discrete, for this to even be a coherent notion. This causes issues when combined with the subsequent point, as we will discuss towards the end of the post.)</p><p>A clarification: the network's architecture trivially has compositional structure - the forward pass is executable on a computer. That's not the claim. The claim is that training discovers an effective program <i>within</i> this substrate. Think of an FPGA: a generic grid of logic components that a hardware engineer configures into a specific circuit. The architecture is the grid; the learned weights are the configuration.</p><p>This last point, the fact that the program structure in neural networks is <i>learned</i> and depends on <i>continuous</i> parameters, is actually what makes this issue rather subtle, and unlike other models of computation we‚Äôre familiar with (even analog computation). This is a subtle issue which makes formalization difficult, an issue we will return to towards the end of the post.</p><h2><strong>What do I mean by ‚Äúprogram synthesis‚Äù?</strong></h2><p>By program synthesis, I mean a search through possible programs to find one that fits the data.</p><p>Two things make this different from ordinary function fitting.</p><p>First, the search is <strong>general-purpose</strong>. Linear regression searches over linear functions. Decision trees search over axis-aligned partitions. These are narrow hypothesis classes, chosen by the practitioner to match the problem. The claim here is different: deep learning searches over a space that can express essentially any efficient computable function. It's not that networks are good at learning one particular kind of structure - it's that they can learn whatever structure is there.</p><p>Second, the search is guided by <strong>strong inductive biases</strong>. Searching over all programs is intractable without some preference for certain programs over others. The natural candidate is simplicity: favor shorter or less complex programs over longer or more complex ones. This is what Solomonoff induction does - it assigns prior probability to programs based on their length, then updates on data.</p><p>Solomonoff induction is the theoretical reference point. It's provably optimal in a certain sense: if the data has any computable structure, Solomonoff induction will eventually find it. But it's also intractable - not just slow, but literally incomputable in its pure form, and exponentially slow even in approximations.</p><p>The hypothesis is that deep learning achieves something functionally similar through completely different means. Gradient descent on continuous parameters looks nothing like enumeration over discrete programs. But perhaps both are targeting the same kind of object - simple programs that capture structure - and arriving there by different routes. We will return to the issue towards the end of the post.</p><p>This would require the learning process to implement something like simplicity bias, even though "program complexity" isn't in the loss function. Whether that's exactly the right characterization, I'm not certain. But some strong inductive bias has to be operating - otherwise we couldn't explain why networks generalize despite having the capacity to memorize, or why scaling helps rather than hurts.</p><h2><strong>What‚Äôs the scope of the hypothesis?</strong></h2><p>I've thought most deeply about supervised and self-supervised learning using stochastic optimization (SGD, Adam, etc) on standard architectures like MLPs, CNNs, or transformers, on standard tasks like image classification or autoregressive language prediction, and am strongly ready to defend claims there. I also believe that this extends to settings like diffusion models, adversarial setups, reinforcement learning, etc, but I've thought less about these and can't be as confident here.</p></div></details><h2><strong>Why this isn't enough</strong></h2><p>The preceding case studies provide a strong <strong>existence proof</strong>: deep neural networks are capable of learning and implementing non-trivial, compositional algorithms. The evidence that InceptionV1 solves image classification by composing circuits, or that a transformer solves modular addition by discovering a Fourier-based algorithm, is quite hard to argue with. And, of course, there are more examples than these which we have not discussed.</p><p>Still, the question remains: <strong>is this the exception or the rule?</strong> It would be completely consistent with the evidence presented so far for this type of behavior to just be a strange edge case.</p><p>Unfortunately, mechanistic interpretability is not yet enough to settle the question. The settings where today's mechanistic interpretability tools provide such clean, complete, and unambiguously correct results<span class="footnote-reference" id="fnreffqn67eo6jo5"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnfqn67eo6jo5">[9]</a></sup></span>&nbsp;are very rare.</p><details class="detailsBlock"><p>Aren't most networks uninterpretable? Why this doesn't disprove the thesis.</p><div class="detailsBlockContent"><p>Should we not take the lack of such clean mechanistic interpretability results as <strong>active counterevidence</strong> against our hypothesis? If models were truly learning programs in general, shouldn't those programs be readily apparent? Instead the internals of these systems appear far more "messy."</p><p>This objection is a serious one, but it makes a leap in logic. It conflates the statement "our current methods have not found a clean programmatic structure" with the much stronger statement "no such structure exists." In other words, <i>absence of evidence is not evidence of absence</i><span class="footnote-reference" id="fnref8byhvyt3kqe"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn8byhvyt3kqe">[10]</a></sup></span>. The difficulty we face may not be an absence of structure, but a mismatch between the network's chosen representational scheme and the tools we are currently using to search for it.</p><figure class="image image_resized" style="width: 79.66%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/yhcrw67yxwml3ebmpp4p" /><figcaption>Attempting to identify which individual transistors in an Atari machine are responsible for different games does not work very well; nevertheless an Atari machine has real computational structure. We may be in a similar situation with neural networks. From <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268">Jonas &amp; Kording (2017)</a>.</figcaption></figure><p>To make this concrete, consider a thought experiment, adapted from the paper "<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268">Could a Neuroscientist Understand a Microprocessor?</a>":</p><p>Imagine a team of neuroscientists studying a microprocessor (MOS 6502) that runs arcade (Atari) games. Their tools are limited to their trade: they can, for instance, probe the voltage of individual transistors and lesion them to observe the effect on gameplay. They do not have access to the high-level source code or architecture diagrams.</p><p>As the paper confirms, the neuroscientists would fail to understand the system. This failure would not be because the system lacks compositional, program structure - it is, by definition, a machine that executes programs. Their failure would be one of mismatched levels of abstraction. The meaningful concepts of the software (subroutines, variables, the call stack) have no simple, physical correlate at the transistor level. The "messiness" they would observe - like a single transistor participating in calculating a score, drawing a sprite, and playing a sound - is an illusion created by looking at the wrong organizational level.</p><p>My claim is that this is the situation we face with neural networks. Apparent "messiness" like <a href="https://arxiv.org/abs/2210.01892"><i>polysemanticity</i></a> is not evidence against a learned program; it is the expected signature of a program whose logic is not organized at the level of individual neurons. The network may be implementing something like a program, but using a "compiler" and an "instruction set" that are currently alien to us.<span class="footnote-reference" id="fnrefguz89j1a968"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnguz89j1a968">[11]</a></sup></span></p><p>The clean results from the vision and modular addition case studies are, in my view, instances where strong constraints (e.g., the connection sparsity of CNNs, or the heavy regularization and shallow architecture in the grokking setup) forced the learned program into a representation that happened to be unusually simple for us to read. They are the exceptions in their <i>legibility</i>, not necessarily in their underlying nature.<span class="footnote-reference" id="fnref969wylfycf"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn969wylfycf">[12]</a></sup></span></p></div></details><p>Therefore, while mechanistic interpretability can supply <i>plausibility</i> to our hypothesis, we need to move towards more indirect evidence to start building a positive case.</p><h1><strong>Indirect evidence</strong></h1><blockquote><p>Just before OpenAI started, I met Ilya [Sutskever]. One of the first things he said to me was, "Look, the models, they just wanna learn. You have to understand this. The models, they just wanna learn."</p><p>And it was a bit like a Zen Koan. I listened to this and I became ‚Ääenlightened.</p><p>... What that told me is that the phenomenon that I'd seen wasn't just some random thing: it was broad, it was more general.</p><p>The models just wanna learn. You get the obstacles out of their way. You give them good data. You give them enough space to operate in. You don't do something stupid like condition them badly numerically.</p><p>And they wanna learn. They'll do it.</p><p>-<i>Dario Amodei</i><span class="footnote-reference" id="fnrefc45j78nx35"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnc45j78nx35">[13]</a></sup></span></p></blockquote><p>I remember when I trained my first neural network, there was something almost miraculous about it: it could solve problems which I had absolutely no idea how to code myself (e.g. how to distinguish a cat from a dog), and in a completely opaque way such that even <i>after</i> it had solved the problem I had no better picture for how to solve the problem myself than I did beforehand. Moreover, it was remarkably resilient, despite obvious problems with the optimizer, or bugs in the code, or bad training data - unlike any other engineered system I had ever built, almost reminiscent of something biological in its robustness.</p><p>My impression is that this sense of "magic" is a common, if often unspoken, experience among practitioners. Many simply learn to accept the mystery and get on with the work. But there is nothing virtuous about confusion - it just suggests that your understanding is incomplete, that you are ignorant of the real mechanisms underlying the phenomenon.</p><p>Our practical success with deep learning has outpaced our theoretical understanding. This has led to a proliferation of explanations that often feel <strong>ad-hoc and local</strong> - tailor-made to account for a specific empirical finding, without connecting to other observations or any larger framework. For instance, the theory of "double descent" provides a narrative for the U-shaped test loss curve, but it is a self-contained story. It does not, for example, share a conceptual foundation with the theories we have for how induction heads form in transformers. Each new discovery seems to require a new, bespoke theory. One naturally worries that we are juggling epicycles.</p><p>This sense of theoretical fragility is compounded by a second problem: for any single one of these phenomena, we often lack consensus, entertaining multiple, <strong>competing hypotheses</strong>. Consider the core question of why neural networks generalize. Is it best explained by the implicit bias of SGD towards flat minima, the behavior of neural tangent kernels, or some other property? The field actively debates these views. And where no mechanistic theory has gained traction, we often retreat to descriptive labels. We say complex abilities are an "emergent" property of scale, a term that names the mystery without explaining its cause.</p><p>This theoretical disarray is sharpest when we examine our most foundational frameworks. Here, the issue is not just a lack of consensus, but a direct conflict with empirical reality. This disconnect manifests in several ways:</p><ul><li>Sometimes, our theories make predictions that are <strong>actively falsified</strong> by practice. Classical statistical learning theory, with its focus on the bias-variance tradeoff, advises against the very scaling strategies that have produced almost all state-of-the-art performance.</li><li>In other cases, a theory might be technically true but <strong>practically misleading</strong>, failing to explain the key properties that make our models effective. The Universal Approximation Theorem, for example, guarantees representational power but does so via a construction that implies an exponential scaling that our models somehow avoid.</li><li>And in yet other areas, our classical theories are almost <strong>entirely silent</strong>. They offer no framework to even begin explaining deep puzzles like the uncanny convergence of representations across vastly different models trained on the same data.</li></ul><p>We are therefore faced with a collection of major empirical findings where our foundational theories are either contradicted, misleading, or simply absent. This theoretical vacuum creates an opportunity for a new perspective.</p><p>The program synthesis hypothesis offers such a perspective. It suggests we shift our view of what deep learning is fundamentally doing: from statistical <strong>function fitting</strong> to <strong>program search</strong>. The specific claim is that deep learning performs a search for simple programs that explain the data.</p><p>This shift in viewpoint may offer a way to make sense of the theoretical tensions we have outlined. If the learning process is a search for an efficient <i>program</i> rather than an arbitrary function, then the circumvention of the curse of dimensionality is no longer so mysterious. If this search is guided by a strong simplicity bias, the unreasonable effectiveness of scaling becomes an expected outcome, rather than a paradox.</p><p>We will now turn to the well-known paradoxes of approximation, generalization, and convergence, and see how the program synthesis hypothesis accounts for each.</p><h2><strong>The paradox of approximation</strong></h2><p><i>(See also </i><a href="https://www.lesswrong.com/posts/gq9GR6duzcuxyxZtD/approximation-is-expensive-but-the-lunch-is-cheap"><i>this post</i></a><i> for related discussion.)</i></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gq9GR6duzcuxyxZtD/ulhsq40yvefiumb90wbq" /><figcaption>We can overcome the curse of dimensionality because real problems can be broken down into parts. When this happens sequentially (like the trees on the right) deep networks have an advantage. <a href="https://www.lesswrong.com/posts/gq9GR6duzcuxyxZtD/approximation-is-expensive-but-the-lunch-is-cheap">Image source</a>.</figcaption></figure><p>Before we even consider how a network learns or generalizes, there is a more basic question: how can a neural network, with a practical number of parameters, even <i>in principle</i> represent the complex function it is trained on?</p><p>Consider the task of image classification. A function that takes a 1024x1024 pixel image (roughly one million input dimensions) and maps it to a single label like "cat" or "dog" is, <i>a priori</i>, an object of staggering high-dimensional complexity. Who is to say that a good approximation of this function even <i>exists</i> within the space of functions that a neural network of a given size can express?</p><p>The textbook answer to this question is the <strong>Universal Approximation Theorem (UAT)</strong>. This theorem states that a neural network with a single hidden layer can, given enough neurons, approximate any continuous function to arbitrary accuracy. On its face, this seems to resolve the issue entirely.</p><details class="detailsBlock"><p>A precise statement of the Universal Approximation Theorem</p><div class="detailsBlockContent"><p>Let&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œÉ</span></span></span></span></span></span></span>&nbsp;be a continuous, non-polynomial function. Then for every continuous function&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span>&nbsp;from a compact subset of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span></span></span>&nbsp;to&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">m</span></span></span></span></span></span></span></span></span>, and some&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œµ</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&gt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span>, we can choose the number of neurons&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span></span></span></span></span></span>&nbsp;large enough such that there exists a network&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">g</span></span></span></span></span></span></span>&nbsp;with</p><span class="math-tex"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">sup</span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span></span></span></span></span></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">‚à•</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">g</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚à•</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&lt;</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">Œµ</span></span></span></span></span></span></span><p>where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">g</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">C</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mstyle MJXc-space2"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span></span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œÉ</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àò</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚ãÖ</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">x</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">b</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span></span></span></span></span></span></span></span></span>&nbsp;for some matrices&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">√ó</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span></span></span></span></span>,&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">b</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span></span></span></span></span></span></span></span>, and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">C</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">m</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">√ó</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span></span></span></span></span></span></span></span></span></span>.</p><p>See <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-width_case">here</a> for a proof sketch. In plain English, this means that for any well-behaved target function&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span>, you can always make a one-layer network&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">g</span></span></span></span></span></span></span>&nbsp;that is a "good enough" approximation, just by making the number of neurons&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span></span></span></span></span></span>&nbsp;sufficiently large.</p><p>Note that the network here is a shallow one - the theorem doesn't even explain why you need <i>deep</i> networks, an issue we'll return to when we talk about <i>depth separations</i>. In fact, one can prove theorems like this without even needing <i>neural networks</i> at all - the theorem directly parallels the classic <a href="https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem">Stone-Weierstrass theorem</a> from analysis, which proves a similar statement for polynomials.</p></div></details><p>However, this answer is deeply misleading. The crucial caveat is the phrase "given enough neurons." A closer look at the proofs of the UAT reveals that for an arbitrary function, the number of neurons required scales <i>exponentially</i> with the dimension of the input. This is the infamous <strong>curse of dimensionality</strong>. To represent a function on a one-megapixel image, this would require a catastrophically large number of neurons - more than there are atoms in the universe.</p><p>The UAT, then, is not a satisfying explanation. In fact, it's a mathematical restatement of a near-trivial fact: with exponential resources, one can simply memorize a function's behavior. The constructions used to prove the theorem are effectively building a continuous version of a lookup table. This is not an explanation for the success of deep learning; it is a proof that if deep learning had to deal with arbitrary functions, it would be hopelessly impractical.</p><p>This is not merely a weakness of the UAT's particular proof; it is a fundamental property of high-dimensional spaces. Classical results in approximation theory show that this exponential scaling is not just an <i>upper</i> bound on what's needed, but a strict <strong>lower bound</strong>. These theorems prove that <i>any</i> method that aims to approximate <i>arbitrary</i> smooth functions is doomed to suffer the curse of dimensionality.</p><details class="detailsBlock"><p>The parameter count lower bound</p><div class="detailsBlockContent"><p>There are many results proving various lower bounds on the parameter count available in the literature under different technical assumptions.</p><p>A classic result from <a href="https://gwern.net/doc/cs/algorithm/1989-devore.pdf">DeVore, Howard, and Micchelli (1989)</a> [Theorem 4.2] establishes a lower bound on the number of parameters&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>&nbsp;required by any continuous approximation scheme (including neural networks) to achieve an error&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œµ</span></span></span></span></span></span></span>&nbsp;over the space of all smooth functions in&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span></span>&nbsp;dimensions. The number of parameters&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>&nbsp;must satisfy:</p><span class="math-tex"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R">‚â≥</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œµ</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">/</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span></span></span></span></span></span></span></span></span></span><p>where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span></span></span></span></span></span>&nbsp;is a measure of the function's smoothness. To maintain a constant error&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œµ</span></span></span></span></span></span></span>&nbsp;as the dimension&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span></span>&nbsp;increases, the number of parameters&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>&nbsp;must grow exponentially. This confirms that no clever trick can escape this fate <i>if the target functions are arbitrary</i>.</p></div></details><p>The real lesson of the Universal Approximation Theorem, then, is not that neural networks are powerful. The real lesson is that <strong>if the functions we learn in the real world were arbitrary, deep learning would be impossible.</strong> The empirical success of deep learning with a reasonable number of parameters is therefore a profound clue about the <i>nature of the problems themselves</i>: they must have structure.</p><p>The program synthesis hypothesis gives a name to this structure: <strong>compositionality</strong>. This is not a new idea. It is the foundational principle of computer science. To solve a complex problem, we do not write down a giant lookup table that specifies the output for every possible input. Instead, we write a <strong>program</strong>: we break the problem down hierarchically into a sequence of simple, reusable steps. Each step (like a logic gate in a circuit) is a tiny lookup table, and we achieve immense expressive power by composing them.</p><p>This matches what we see empirically in some deep neural networks via mechanistic interpretability. They appear to solve complex tasks by learning a compositional hierarchy of features. A vision model learns to detect edges, which are composed into shapes, which are composed into object parts (wheels, windows), which are finally composed into an object detector for a "car." The network is not learning a single, monolithic function; it is learning a program that breaks the problem down.</p><p>This parallel with classical computation offers an alternative perspective on the approximation question. While the UAT considers the case of arbitrary functions, a different set of results examines how well neural networks can represent functions that have this compositional, programmatic structure.</p><p>One of the most relevant results comes from considering <i>Boolean circuits</i>, which are a canonical example of programmatic composition. It is known that <strong>feedforward neural networks can represent any program implementable by a polynomial-size Boolean circuit, using only a polynomial number of neurons.</strong> This provides a different kind of guarantee than the UAT. It suggests that if a problem has an <i>efficient</i> programmatic solution, then an <i>efficient</i> neural network representation of that solution also exists.</p><p>This offers an explanation for how neural networks might evade the curse of dimensionality. Their effectiveness would stem not from an ability to represent <i>any</i> high-dimensional function, but from their suitability for representing the tiny, structured subset of functions that have efficient programs. The problems seen in practice, from image recognition to language translation, appear to belong to this special class.</p><details class="detailsBlock"><p>Why <i>compositionality</i>, specifically? Evidence from depth separation results.</p><div class="detailsBlockContent"><p>The argument so far is that real-world problems must have some special "structure" to escape the curse of dimensionality, and that this structure is <i>program structure or compositionality</i>. But how can we be sure? Yes, approximation theory requires that we must have <i>something</i> that differentiates our target functions from arbitrary smooth functions in order to avoid needing exponentially many parameters, but it does not specify <i>what</i>. The structure does not necessarily have to be compositionality; it could be something else entirely.</p><p>While there is no definitive proof, the literature on <strong>depth separation theorems</strong> provides evidence for the compositionality hypothesis. The logic is straightforward: if compositionality is the key, then an architecture that is restricted in its ability to compose operations should struggle. Specifically, one would expect that restricting a network's <i>depth -</i> its capacity for sequential, step-by-step computation - should force it back towards exponential scaling for certain problems.</p><p>And this is what the theorems show.</p><p>These depth separation results, sometimes also called "no-flattening theorems," involve constructing families of functions that deep neural networks can represent with a polynomial number of parameters, but which shallow networks would require an <i>exponential</i> number to represent. The literature contains a range of such functions, including <a href="https://arxiv.org/abs/1602.04485">sawtooth functions</a>, <a href="https://arxiv.org/abs/1705.05502">certain polynomials</a>, and <a href="https://arxiv.org/abs/1608.08225">functions with hierarchical or modular substructures</a>.</p><p>Individually, many of these examples are mathematical constructions, too specific to tell us much about realistic tasks on their own. But taken together, a pattern emerges. The functions where depth provides an exponential advantage are consistently those that are built "step-by-step." They have a sequential structure that deep networks can mirror. A deep network can compute an intermediate result in one layer and then feed that result into the next, effectively executing a multi-step computation.</p><p>A shallow network, by contrast, has no room for this kind of sequential processing. It must compute its output in a single, parallel step. While it can still perform "piece-by-piece" computation (which is what its <i>width</i> allows), it cannot perform "step-by-step" computation. Faced with an inherently sequential problem, a shallow network is forced to simulate the entire multi-step computation at once. This can be highly inefficient, in the same way that simulating a sequential program on a highly parallel machine can sometimes require exponentially more resources.</p><p>This provides a parallel to classical complexity theory. The distinction between depth and width in neural networks mirrors the distinction between sequential (P) and parallelizable (NC) computation. Just as it is conjectured that some problems are inherently sequential and cannot be efficiently parallelized (the NC ‚â† P conjecture), these theorems show that some functions are inherently deep and cannot be efficiently "flattened" into a shallow network.</p></div></details><h2><strong>The paradox of generalization</strong></h2><p><i>(See also </i><a href="https://www.lesswrong.com/s/WWx8sZ9tE9skptytH/p/uG7oJkyLBHEw3MYpT"><i>this post</i></a><i> for related discussion.)</i></p><p><img alt="Bias‚Äìvariance tradeoff - Wikipedia" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/kelygoxzbm6pgrcvp55j" style="width: 43.28%;" /><img alt="Overfitting - MATLAB &amp; Simulink" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iFymqTKHgftbzqMy7/gzkcaz6ux4rzjscjlasv" style="width: 52.87%;" /></p><p>Perhaps the most jarring departure from classical theory comes from how deep learning models generalize. A learning algorithm is only useful if it can perform well on new, unseen data. The central question of statistical learning theory is: what are the conditions that allow a model to generalize?</p><p>The classical answer is the <strong>bias-variance tradeoff</strong>. The theory posits that a model's error can be decomposed into two main sources:</p><ul><li><strong>Bias:</strong> Error from the model being too simple to capture the underlying structure of the data (<i>underfitting</i>).</li><li><strong>Variance:</strong> Error from the model being too sensitive to the specific training data it saw, causing it to fit noise (<i>overfitting</i>).</li></ul><p>According to this framework, learning is a delicate balancing act. The practitioner's job is to carefully choose a model of the "right" complexity - not too simple, not too complex -to land in a "Goldilocks zone" where both bias and variance are low. This view is reinforced by principles like the "no free lunch" theorems, which suggest there is no universally good learning algorithm, only algorithms whose inductive biases are carefully chosen by a human to match a specific problem domain.</p><p>The clear prediction from this classical perspective is that naively increasing a model's capacity (e.g., by adding more parameters) far beyond what is needed to fit the training data is a recipe for disaster. Such a model should have catastrophically high variance, leading to rampant overfitting and poor generalization.</p><p>And yet, perhaps the single most important empirical finding in modern deep learning is that this prediction is completely wrong. The "<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter lesson</a>," as Rich Sutton calls it, is that the most reliable path to better performance is to <strong>scale up compute and model size</strong>, sometimes far into the regime where the model can easily memorize the entire training set. This goes beyond a minor deviation from theoretical predictions: it is a direct contradiction of the theory's core prescriptive advice.</p><p>This brings us to a second, deeper puzzle, first highlighted by <a href="https://arxiv.org/abs/1611.03530">Zhang et al. (2017)</a>. The authors conduct a simple experiment:</p><ul><li>They train a standard vision model on a real dataset (e.g., CIFAR-10) and confirm that it generalizes well.</li><li>They then train the <i>exact same model</i>, with the exact same architecture, optimizer, and regularization, on a corrupted version of the dataset where the labels have been completely randomized.</li></ul><p>The network is expressive enough that it is able to achieve near-zero training error on the randomized labels, perfectly memorizing the nonsensical data. As expected, its performance on a test set is terrible - it has learned nothing generalizable.</p><p>The paradox is this: why did the <i>same exact model</i> generalize well on the real data? Classical theories often tie a model's generalization ability to its "capacity" or "complexity," which is a fixed property of its architecture related to its expressivity. But this experiment shows that generalization is not a static property of the model. It is a dynamic outcome of the interaction between the model, the learning algorithm, and the <strong>structure of the data itself</strong>. The very same network that is <i>completely capable</i> of memorizing random noise somehow "chooses" to find a generalizable solution when trained on data with real structure. Why?</p><p>The program synthesis hypothesis offers a coherent explanation for both of these paradoxes.</p><p>First, <strong>why does scaling work?</strong> The hypothesis posits that learning is a <i>search</i> through some space of programs, guided by a strong simplicity bias. In this view, adding more parameters is analogous to expanding the search space (e.g., allowing for longer or more complex programs). While this does increase the model's capacity to represent overfitting solutions, the simplicity bias acts as a powerful regularizer. The learning process is not looking for <i>any</i> program that fits the data; it is looking for the <i>simplest</i> program. Giving the search more resources (parameters, compute, data) provides a better opportunity to find the simple, generalizable program that corresponds to the true underlying structure, rather than settling for a more complex, memorizing one.</p><p>Second, <strong>why does generalization depend on the data's structure?</strong> This is a natural consequence of a simplicity-biased program search.</p><ul><li>When trained on <strong>real data</strong>, there exists a short, simple program that explains the statistical regularities (e.g., "cats have pointy ears and whiskers"). The simplicity bias of the learning process finds this program, and because it captures the true structure, it generalizes well.</li><li>When trained on <strong>random labels</strong>, no such simple program exists. The only way to map the given images to the random labels is via a long, complicated, high-complexity program (effectively, a lookup table). Forced against its inductive bias, the learning algorithm eventually finds such a program to minimize the training loss. This solution is pure memorization and, naturally, fails to generalize.</li></ul><p>If one assumes something like the program synthesis hypothesis is true, the phenomenon of data-dependent generalization is not so surprising. A model's ability to generalize is not a fixed property of its architecture, but a property of the <strong>program it learns</strong>. The model finds a simple program on the real dataset and a complex one on the random dataset, and the two programs have very different generalization properties.And there is some evidence that the mechanism behind generalization is not so unrelated to the other empirical phenomena we have discussed. We can see this in the <strong>grokking</strong> setting discussed earlier. Recall the transformer trained on modular addition:</p><ul><li>Initially, the model learns a <strong>memorization-based program</strong>. It achieves 100% accuracy on the training data, but its test accuracy is near zero. This is analogous to learning the "random label" dataset - a complex, non-generalizing solution.</li><li>After extensive further training, driven by a regularizer that penalizes complexity (weight decay), the model's internal solution undergoes a "phase transition." It discovers the <strong>Fourier-based algorithm</strong> for modular addition.</li><li>Coincident with the discovery of this algorithmic program (or rather, the removal of the memorization program, which occurs slightly later), test accuracy abruptly jumps to 100%.</li></ul><p>The sudden increase in generalization appears to be the direct consequence of the model replacing a complex, overfitting solution with a simpler, algorithmic one. In this instance, generalization is achieved through the synthesis of a different, more efficient program.</p><h2><strong>The paradox of convergence</strong></h2><p>When we ask a neural network to solve a task, we specify what task we'd like it to solve, but not <i>how</i> it should solve the task - the purpose of <i>learning</i> is for it to find strategies on its own. We define a loss function and an architecture, creating a space of possible functions, and ask the learning algorithm to find a good one by minimizing the loss. Given this freedom, and the high-dimensionality of the search space, one might expect the solutions found by different models - especially those with different architectures or random initializations - to be highly diverse.</p><p>Instead, what we observe empirically is a strong tendency towards <strong>convergence</strong>. This is most directly visible in the phenomenon of <a href="https://arxiv.org/abs/2310.13018"><strong>representational alignment</strong></a>. This alignment is remarkably robust:</p><ul><li>It holds across different training runs of the same architecture, showing that the final solution is not a sensitive accident of the random seed.</li><li>More surprisingly, it holds across different architectures. The internal activations of a Transformer and a CNN trained on the same vision task, for example, can often be mapped to one another with a simple linear transformation, suggesting they are learning not just similar input-output behavior, but similar intermediate computational steps.</li><li>It even holds in some cases across modalities. Models like CLIP, trained to associate images with text, learn a shared representation space where the vector for a photograph of a dog is close to the vector for the phrase "a photo of a dog," indicating convergence on a common, abstract conceptual structure.</li></ul><p>The mystery deepens when we observe parallels to biological systems. The <a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor</a>-like filters that emerge in the early layers of vision networks, for instance, are strikingly similar to the receptive fields of neurons in the V1 area of the primate visual cortex. It appears that evolution and stochastic gradient descent, two very different optimization processes operating on very different substrates, have converged on similar solutions when exposed to the same statistical structure of the natural world.</p><p>One way to account for this is to hypothesize that the models are not navigating some undifferentiated space of arbitrary functions, but are instead homing in on a sparse set of highly effective programs that solve the task. If, following the physical Church-Turing thesis, we view the natural world as having a true, computable structure, then an effective learning process could be seen as a search for an algorithm that approximates that structure. In this light, convergence is not an accident, but a sign that different search processes are discovering similar objectively good solutions, much as different engineering traditions might independently arrive at the arch as an efficient solution for bridging a gap.</p><p>This hypothesis - that learning is a search for an optimal, objective program - carries with it a strong implication: the search process must be a <strong>general-purpose</strong> one, capable of finding such programs without them being explicitly encoded in its architecture. As it happens, an independent, large-scale trend in the field provides a great deal of data on this very point.</p><p>Rich Sutton's "<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><strong>bitter lesson</strong></a>" describes the consistent empirical finding that long-term progress comes from scaling general learning methods, rather than from encoding specific human domain knowledge. The old paradigm, particularly in fields like computer vision, speech recognition, or game playing, involved painstakingly hand-crafting systems with significant prior knowledge. For years, the state of the art relied on complex, hand-designed feature extractors like SIFT and HOG, which were built on human intuitions about what aspects of an image are important. The role of learning was confined to a relatively simple classifier that operated on these pre-digested features. The underlying assumption was that the search space was too difficult to navigate without strong human guidance.</p><p>The modern paradigm of deep learning has shown this assumption to be incorrect. Progress has come from abandoning these handcrafted constraints in favor of training general, end-to-end architectures with the brute force of data and compute. This consistent triumph of general learning over encoded human knowledge is a powerful indicator that the search process we are using is, in fact, general-purpose. It suggests that the learning algorithm itself, when given a sufficiently flexible substrate and enough resources, is a more effective mechanism for discovering relevant features and structure than human ingenuity.</p><p>This perspective helps connect these phenomena, but it also invites us to refine our initial picture. First, the notion of a single "optimal program" may be too rigid. It is possible that what we are observing is not convergence to a single point, but to a narrow subset of similarly structured, highly-efficient programs. The models may be learning different but algorithmically related solutions, all belonging to the same family of effective strategies.</p><p>Second, it is unclear whether this convergence is purely a property of the problem's solution space, or if it is also a consequence of our search algorithm. Stochastic gradient descent is not a neutral explorer. The implicit biases of stochastic optimization, when navigating a highly over-parameterized loss landscape, may create powerful channels that funnel the learning process toward a specific <i>kind</i> of simple, compositional solution. Perhaps all roads do not lead to Rome, but the roads to Rome are the fastest. The convergence could therefore be a clue about the nature of our learning dynamics themselves - that they possess a strong, intrinsic preference for a particular class of solutions.</p><p>Viewed together, these observations suggest that the space of effective solutions for real-world tasks is far smaller and more structured than the space of possible models. The phenomenon of convergence indicates that our models are finding this structure. The bitter lesson suggests that our learning methods are general enough to do so. The remaining questions point us toward the precise nature of that structure and the mechanisms by which our learning algorithms are so remarkably good at finding it.</p><h1><strong>The path forward</strong></h1><p>If you've followed the argument this far, you might already sense where it becomes difficult to make precise. The mechanistic interpretability evidence shows that networks <i>can</i> implement compositional algorithms. The indirect evidence suggests this connects to why they generalize, scale, and converge. But "connects to" is doing a lot of work. What would it actually mean to say that deep learning <i>is</i> some form of program synthesis?</p><p>Trying to answer this carefully leads to two problems. The claim "neural networks learn programs" seems to require saying what a program even <i>is</i> in a space of continuous parameters. It also requires explaining how gradient descent could find such programs efficiently, given what we know about the intractability of program search.</p><p>These are the kinds of problems where the difficulty itself is informative. Each has a specific shape - what you need to think about, what a resolution would need to provide. I focus on them deliberately: that shape is what eventually pointed me toward specific mathematical tools I wouldn't have considered otherwise.</p><p>This is also where the post will shift register. The remaining sections sketch the structure of these problems and gesture at why certain mathematical frameworks (singular learning theory, algebraic geometry, etc) might become relevant. I won't develop these fully here - that requires machinery far beyond the scope of a single blog post - but I want to show why you'd need to leave shore at all, and what you might find out in open water.</p><h2><strong>The representation problem</strong></h2><p>The program synthesis hypothesis posits a relationship between two fundamentally different kinds of mathematical objects.</p><p>On one hand, we have <strong>programs</strong>. A program is a discrete and symbolic object. Its identity is defined by its compositional structure - a graph of distinct operations. A small change to this structure, like flipping a comparison or replacing an addition with a subtraction, can create a completely different program with discontinuous, global changes in behavior. The space of programs is discrete.</p><p>On the other hand, we have <strong>neural networks</strong>. A neural network is defined by its <strong>parameter space</strong>: a continuous vector space of real-valued weights. The function a network computes is a smooth (or at least piecewise-smooth) function of these parameters. This smoothness is the essential property that allows for learning via gradient descent, a process of infinitesimal steps along a continuous loss landscape.</p><p>This presents a seeming type mismatch: <strong>how can a continuous process in a continuous parameter space give rise to a discrete, structured program?</strong></p><p>The problem is deeper than it first appears. To see why, we must first be precise about what we mean when we say a network has "learned a program." It cannot simply be about the input-output function the network computes. A network that has perfectly memorized a lookup table for modular addition computes the same function on a finite domain as a network that has learned the general, trigonometric algorithm. Yet we would want to say, emphatically, that they have learned different programs. The program is not just the <i>function</i>; it is the underlying <strong>mechanism</strong>.</p><p>Thus the notion must depend on parameters, and not just functions, presenting a further conceptual barrier. To formalize the notion of "mechanism," a natural first thought might be to partition the continuous parameter space into discrete regions. In this picture, all the parameter vectors within a region&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">W</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span></span></span></span></span></span></span></span>&nbsp;would correspond to the same program A, while vectors in a different region&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">W</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span></span></span></span></span></span></span></span>&nbsp;would correspond to program B. But this simple picture runs into a subtle and fatal problem: the very smoothness that makes gradient descent possible works to dissolve any sharp boundaries between programs.</p><p>Imagine a continuous path in parameter space from a point&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">W</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">A</span></span></span></span></span></span></span></span></span>&nbsp;(which clearly implements program A) to a point&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">W</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">B</span></span></span></span></span></span></span></span></span>&nbsp;(which clearly implements program B). Imagine, say, that A has some extra subroutine that B does not. Because the map from parameters to the function is smooth, the network's behavior must change continuously along this path. At what exact point on this path did the mechanism switch from A to B? Where did the new subroutine get added? There is no canonical place to draw a line. A sharp boundary would imply a discontinuity that the smoothness of the map from parameters to functions seems to forbid.</p><p>This is not so simple a problem, and it is worth spending some time thinking about how you might try to resolve it to appreciate that.</p><p>What this suggests, then, is that for the program synthesis hypothesis to be a coherent scientific claim, it requires something that does not yet exist: a formal, geometric notion of a <i>space of programs</i>. This is a rather large gap to fill, and in some ways, this entire post is my long-winded way of justifying such an ambitious mathematical goal.</p><p>I won't pretend that my collaborators and I don't have <a href="https://arxiv.org/abs/2207.10871">our</a><span class="footnote-reference" id="fnreflzj33x1q4yp"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnlzj33x1q4yp">[14]</a></sup></span>&nbsp;<a href="https://arxiv.org/abs/2502.08911">own</a> <a href="https://arxiv.org/abs/2504.08075">ideas</a> about how to resolve this, but the mathematical sophistication required jumps substantially, and they would probably require their own full-length post to do justice. For now, I will just gesture at some clues which I think point in the right direction.</p><p>The first is the phenomenon of <i>degeneracies</i><span class="footnote-reference" id="fnref5hsrpicn3vp"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn5hsrpicn3vp">[15]</a></sup></span>. Consider, for instance, <i>dead neurons</i>, whose incoming weights and activations are such that the neurons never fires for any input. A neural network with dead neurons acts like a smaller network with those dead neurons removed. This gives a mechanism for neural networks to change their "effective size" in a parameter-dependent way, which is required in order to e.g. dynamically add or remove a new subroutine depending on where you are in parameter space, as in our example above. In fact dead neurons are just one example in a whole <a href="https://far.in.net/mthesis.pdf">zoo of degeneracies</a> with similar effects, which seem incredibly pervasive in neural networks.</p><p>It is worth mentioning that the present picture is now highly suggestive of a specific branch of math known as <i>algebraic geometry</i>. Algebraic geometry (in particular, singularity theory) systematically studies these degeneracies, and further provides a bridge between <i>discrete</i> structure (algebra) and <i>continuous</i> structure (geometry), exactly the type of connection we identified as necessary for the program synthesis hypothesis<span class="footnote-reference" id="fnreffukshjhtc2n"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnfukshjhtc2n">[16]</a></sup></span>. Furthermore, <i>singular learning theory</i> tells us how these degeneracies control the loss landscape and the learning process (classically, only in the <i>Bayesian</i> setting, a limitation we discuss in the next section). There is much more that can be said here, but I leave it for the future to treat this material properly.</p><h2><strong>The search problem</strong></h2><p>There‚Äôs another problem with this story. Our hypothesis is that deep learning is performing some version of program <i>synthesis</i>. That means that we not only have to explain how programs get <i>represented</i> in neural networks, we also need to explain how they get <i>learned</i>. There are two subproblems here.</p><ul><li>First, how can deep learning even implement the needed inductive biases? For deep learning algorithms to be implementing something analogous to Solomonoff induction, they must be able to implicitly follow inductive biases which depend on the program structure, like simplicity bias. That is, the optimization process must somehow be aware of the program structure in order to favor some types of programs (e.g. shorter programs) over others. The optimizer must ‚Äúsee‚Äù the program structure of parameters.</li><li>Second, deep learning works in practice, using a reasonable amount of computational resources; meanwhile, even the most efficient versions of Solomonoff induction like speed induction run in exponential time or worse<span class="footnote-reference" id="fnrefl2yn9dinv"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnl2yn9dinv">[5]</a></sup></span>. If deep learning is efficiently performing some version of program synthesis analogous to Solomonoff induction, that means it has implicitly managed to do what we could not figure out how to do explicitly - its efficiency must be due to some insight which we do not yet know. Of course, we know part of the answer: SGD only needs local information in order to optimize, instead of brute-force global search as one does with Bayesian learning. But then the mystery becomes a well-known one: why does myopic search like SGD converge to globally good solutions?</li></ul><p>Both of these are questions about the optimization process. It is not obvious at all how local optimizers like SGD would be able to perform something like Solomonoff induction, let alone <i>far more efficiently</i> than we historically ever figured out for (versions of) Solomonoff induction itself. This is a difficult question, but I will attempt to point towards research which I believe can answer these questions.</p><p>The optimization process can depend on many things, a priori: choice of optimizer, regularization, dropout, step size, etc. But we can note that deep learning is able to work somewhat successfully (albeit sometimes with degraded performance) across wide ranges of choices of these variables. It does not seem like the choice of AdamW vs SGD matters nearly as much as the choice to do gradient-based learning in the first place. In other words, I believe these variables may affect efficiency, but I doubt they are fundamental to the explanation of why the optimization process can possibly succeed.</p><p>Instead, there is one common variable here which appears to determine the vast majority of the behavior of stochastic optimizers: the loss function. Optimizers like SGD take every gradient step according to a minibatch-loss function<span class="footnote-reference" id="fnreflc0m4rnjr2d"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnlc0m4rnjr2d">[17]</a></sup></span>&nbsp;like mean-squared error:</p><span class="math-tex"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 1.439em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.08em;">œÑ</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 1.439em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span></span><span class="math-tex"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.8em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-munderover MJXc-space1"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-op"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">‚àë</span></span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-left: 0.21em;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">y</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span></span></span></span></span><p>where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span></span></span></span>&nbsp;is the parameter vector,&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span></span></span></span></span></span>&nbsp;is the input/output map of the model on parameter&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">y</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;are the&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>&nbsp;training examples &amp; labels, and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.08em;">œÑ</span></span></span></span></span></span></span>&nbsp;is the learning rate.</p><p>In the most common versions of supervised learning, we can focus even further. The loss function itself can be decomposed into two effects: the parameter-function map&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚Ü¶</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span></span></span></span></span></span>, and the target distribution. The overall loss function can be written as a composition of the parameter-function map and some statistical distance to the target distribution, e.g. for mean-squared error:</p><span class="math-tex"><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚Ñì</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àò</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span><p>where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">‚Ñì</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">g</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">/</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">‚àë</span></span></span><span class="mjx-stack"><span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-sub" style="font-size: 70.7%;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">y</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">g</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">i</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span></span></span></span></span>.</p><p>Note that the statistical distance&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">‚Ñì</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">g</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;here is a fairly simple object. Almost always the statistical distance here is (on function space) convex and with relatively simple functional form; further, it is the same distance one would use across many different architectures, including ones which do not achieve the remarkable performance of neural networks (e.g. polynomial approximation). Therefore one expects the question of learnability and inductive biases to largely come down to the parameter-function map&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span></span></span></span></span></span>&nbsp;rather than the (function-space) loss function&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">‚Ñì</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">g</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><p>If the above reasoning is correct, that means that in order to understand how SGD is able to potentially perform some kind of program synthesis, we merely need to understand properties of the parameter-function map. This would be a substantial simplification. Further, this relates learning dynamics to our earlier representation problem: the parameter-function map is precisely the same object responsible for the mystery discussed in the representation section.</p><p>This is not an airtight argument - it depends on the empirical question of whether one can ignore (or treat as second-order effects) other optimization details besides the loss function, and whether the handwave-y argument for the importance of the parameter-function map over the (function-space) loss is solid.</p><p>Even if one assumes this argument is valid, we have merely located the mystery, not resolved it. The question remains: what properties of the parameter-function map make targets learnable? At this point the reasoning becomes more speculative, but I will sketch some ideas.</p><p>The representation section concerned what structure the map encodes at each point in parameter space. Learnability appears to depend on something further: the structure of paths between points. Convexity of function-space loss implies that paths which are sufficiently straight in function space are barrier-free - roughly, if the endpoint is lower loss, the entire path is downhill. So the question becomes: which function-space paths does the map provide?</p><p>The same architectures successfully learn many diverse real-world targets. Whatever property of the map enables this, it must be relatively universal - not tailored to specific targets. This naturally leads us to ask: in what cases does the parameter-function map provide direct-enough paths to targets with certain structure, and characterizing what "direct enough" means.</p><p>This connects back to the representation problem. If the map encodes some notion of program structure, then path structure in parameter space induces relationships between programs - which programs are "adjacent," which are reachable from which. The representation section asks how programs are encoded as points; learnability asks how they are connected as paths. These are different aspects of the same object.</p><p>One hypothesis: compositional relationships between programs might correspond to some notion of ‚Äúpath adjacency‚Äù defined by the parameter-function map. If programs sharing structure are nearby - reachable from each other via direct paths - and if simpler programs lie along paths to more complex ones, then efficiency, simplicity bias, and empirically observed stagewise learning&nbsp;would follow naturally. Gradient descent would build incrementally rather than search randomly; the enumeration problem that dooms Solomonoff would dissolve into traversal.</p><p>This is speculative and imprecise. But there's something about the shape of what's needed that feels mathematically natural. The representation problem asks for a correspondence at the level of objects: strata in parameter space corresponding to programs. The search problem asks for something stronger - that this correspondence extends to <i>paths</i>. Paths in parameter space (what gradient descent traverses) should correspond to some notion of relationship or transition between programs.</p><p>This is a familiar move in higher mathematics (sometimes formalized by category theory): once you have a correspondence between two kinds of objects, you ask whether it extends to the relationships between those objects. It is especially familiar (in fields like higher category theory) to ask these kinds of questions when the "relationships between objects" take the form of <i>paths</i> in particular. I don't claim that existing machinery from these fields applies directly, and certainly not given the (lack of) detail I've provided in this post. But the question is suggestive enough to investigate: what should "adjacency between programs" mean? Does the parameter-function map induce or preserve such structure? And if so, what does this predict about learning dynamics that we could check empirically?</p><h1><strong>Appendix</strong></h1><h2><strong>Related work</strong></h2><p>The majority of the ideas in this post are not individually novel; I see the core value proposition as synthesizing them together in one place. The ideas I express here are, in my experience, very common among researchers at frontier labs, researchers in mechanistic interpretability, some researchers within science of deep learning, and others. In particular, the core hypothesis that deep learning is performing some tractable version of Solomonoff induction is not new, and has been <a href="https://www.lesswrong.com/posts/LxCeyxH3fBSmd4oWB/deep-learning-is-cheap-solomonoff-induction-1">written</a> <a href="https://www.amazon.science/blog/solomonic-learning-large-language-models-and-the-art-of-induction">about</a> <a href="https://x.com/johnschulman2/status/1741178475946602979">many</a> <a href="https://www.lesswrong.com/posts/MznxnYCtHZbtDxJuh/approximating-solomonoff-induction">times</a>. (However, I would not consider it to be a popular or accepted opinion within the machine learning field at large.) Personally, I have considered a version of this hypothesis for around three years. With this post, I aim to share a more comprehensive synthesis of the evidence for this hypothesis, as well as point to specific research directions for formalizing this idea.</p><p>Below is an incomplete list of what is known and published in various areas:</p><p><strong>Existing comparisons between deep learning and program synthesis.</strong> The ideas surrounding Solomonoff induction have been highly motivating for many early AGI-focused researchers. Shane Legg (DeepMind cofounder) wrote his <a href="http://www.vetta.org/documents/Machine_Super_Intelligence.pdf">PhD thesis</a> on Solomonoff induction; John Schulam (OpenAI cofounder) discusses the connection to deep learning explicitly <a href="https://x.com/johnschulman2/status/1741178475946602979">here</a>; Ilya Sutskever (OpenAI cofounder) has been giving <a href="https://x.com/mhutter42/status/1691866331602186466">talks</a> on related ideas. There are a handful <a href="https://www.lesswrong.com/posts/LxCeyxH3fBSmd4oWB/deep-learning-is-cheap-solomonoff-induction-1">of</a> <a href="https://x.com/johnschulman2/status/1741178475946602979">places</a> <a href="https://www.lesswrong.com/posts/MznxnYCtHZbtDxJuh/approximating-solomonoff-induction">one</a> <a href="https://blog.wadan.co.jp/en/tech/solomonoff-induction-compression-generalization">can</a> find a hypothesized connection between deep learning and Solomonoff induction stated explicitly, though I do not believe any of these were the first to do so. My personal experience is that such intuitions are fairly common among e.g. people working at frontier labs, even if they are not published in writing. I am not sure who had the idea first, and suspect it was arrived at independently multiple times.</p><p><strong>Feature learning.</strong> It would not be accurate to say that the average ML researcher views deep learning as a <i>complete</i> black-box algorithm; it is well-accepted and uncontroversial that deep neural networks are able to extract "features" from the task which they use to perform well. However, it is a step beyond to claim that these features are actually extracted and composed in some mechanistic fashion resembling a computer program.</p><p><strong>Compositionality, hierarchy, and modularity.</strong> My informal notion of "programs" here is quite closely related to compositionality. It is a fairly well-known hypothesis that supervised learning performs well due to compositional/hierarchical/modular structure in the model and/or the target task. This is particularly prominent <a href="https://cdn.aaai.org/ojs/10913/10913-13-14441-1-2-20201228.pdf">within approximation theory</a> (especially the literature on depth separations) as an explanation for the issues I highlighted in the "paradox of approximation" section.</p><p><strong>Mechanistic interpretability.</strong> The (implicit) underlying premise of the field of <i>mechanistic interpretability</i> is that one can understand the internal mechanistic (read: program-like) structure responsible for a network's outputs. Mechanistic interpretability is responsible for discovering a significant number of examples of this type of structure, which I believe constitutes the single strongest evidence for the program synthesis hypothesis. I discuss a few case studies of this structure in the post, but there are possibly hundreds more examples which I did not cover, from the many papers within the field. A recent review can be found <a href="https://arxiv.org/abs/2404.14082">here</a>.</p><p><strong>Singular learning theory.</strong> In the ‚Äúpath forward‚Äù section, I highlight a possible role of degeneracies in controlling some kind of effective program structure. In some way (which I have gestured at but not elaborated on), the ideas presented in this post can be seen as motivating singular learning theory as a means to formally ground these ideas and produce practical tools to operationalize them. This is most explicit within a <a href="https://arxiv.org/abs/2504.08075">line of work</a> within singular learning theory that attempts to precisely connect program synthesis with the singular geometry of a (toy) learning machine.</p><p>&nbsp;</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnz8jpod2itgo"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefz8jpod2itgo">^</a></strong></sup></span><div class="footnote-content"><p>From the <a href="https://www.youtube.com/watch?v=6nJZopACRuQ&amp;t=2326s">GPT-4.5 launch discussion, 38:46</a>.</p></div></li><li class="footnote-item" id="fn9y7nox8uh2"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref9y7nox8uh2">^</a></strong></sup></span><div class="footnote-content"><p>From his <a href="http://www.vetta.org/documents/Machine_Super_Intelligence.pdf">PhD thesis</a>, pages 23-24.</p></div></li><li class="footnote-item" id="fn9w6g132qlp"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref9w6g132qlp">^</a></strong></sup></span><div class="footnote-content"><p>Together with independent contributions by Kolmogorov, Chaitin, and Levin.</p></div></li><li class="footnote-item" id="fnjxst5sv8a9"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefjxst5sv8a9">^</a></strong></sup></span><div class="footnote-content"><p>One must be careful, as some commonly stated "proofs" of this optimality are somewhat tautological. These typically go roughly something like: under the assumption that the data generating process has low Kolmogorov complexity, then Solomonoff induction is optimal. This is of course completely circular, since we have, in effect, assumed from the start that the inductive bias of Solomonoff induction is correct. Better proofs of this fact instead show a regret bound: on <i>any</i> sequence, Solomonoff induction's cumulative loss is at most a constant worse than any computable predictor - where the constant depends on the complexity of the <i>competing predictor</i>, not the sequence. This is a frequentist guarantee requiring no assumptions about the data source. See in particular Section 3.3.2 and Theorem 3.3 of <a href="https://philsci-archive.pitt.edu/14486/">this PhD thesis</a>. Thanks to Cole Wyeth for pointing me to this argument.</p></div></li><li class="footnote-item" id="fnl2yn9dinv"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefl2yn9dinv">^</a></strong></sup></span><div class="footnote-content"><p>See <a href="https://arxiv.org/abs/1604.03343">this paper</a>.</p></div></li><li class="footnote-item" id="fn6vd5zchhq54"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref6vd5zchhq54">^</a></strong></sup></span><div class="footnote-content"><p>Depending on what one means by "protein folding," one can debate whether the problem has truly been solved; for instance, the problem of how proteins fold <i>dynamically</i> over time is still open AFAIK. See <a href="https://moalquraishi.wordpress.com/2020/12/08/alphafold2-casp14-it-feels-like-ones-child-has-left-home/">this fairly well-known blog post</a> by molecular biologist Mohammed AlQuraishi for more discussion, and why he believes calling AlphaFold a "solution" can be appropriate despite the caveats.</p></div></li><li class="footnote-item" id="fniejpex45eha"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefiejpex45eha">^</a></strong></sup></span><div class="footnote-content"><p>In fact, the solution can be seen as a representation-theoretic algorithm for the group of integers under addition mod&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span></span></span></span></span>&nbsp;(the cyclic group&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">C</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span></span></span></span></span></span></span>). <a href="https://arxiv.org/abs/2302.03025">Follow-up</a> <a href="https://arxiv.org/abs/2312.06581">papers</a> demonstrated that neural networks also learn interpretable representation-theoretic algorithms for more general groups than cyclic groups.</p></div></li><li class="footnote-item" id="fnb5ms35mh16"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefb5ms35mh16">^</a></strong></sup></span><div class="footnote-content"><p>For what it's worth, in this specific case, we do know what must be driving the process, if not the training loss: the regularization / weight decay. In the case of grokking, we do have <a href="https://arxiv.org/abs/2210.01117">decent understanding</a> of how weight decay leads the training to prefer the generalizing solution. However, this explanation is limited in various ways, and it unclear how far it generalizes beyond this specific setting.</p></div></li><li class="footnote-item" id="fnfqn67eo6jo5"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreffqn67eo6jo5">^</a></strong></sup></span><div class="footnote-content"><p>To be clear, one can still apply existing mechanistic interpretability tools to real language models and get productive results. But the results typically only manage to explain a small portion of the network, and in a way which is (in my opinion) less clean and convincing than e.g. <a href="https://distill.pub/2020/circuits/zoom-in/">Olah et al. (2020)</a>'s reverse-engineering of InceptionV1.</p></div></li><li class="footnote-item" id="fn8byhvyt3kqe"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref8byhvyt3kqe">^</a></strong></sup></span><div class="footnote-content"><p>This phrase is often abused - for instance, if you show up to court with no evidence, I can reasonably infer that no good evidence for your case exists. This is a gap between <i>logical</i> and <i>heuristic/Bayesian</i> reasoning. In the real world, if evidence for a proposition exists, it usually can and will be found (because we care about it), so you can interpret the absence of evidence for a proposition as suggesting that the proposition is false. However, in this case, I present a <i>specific reason</i> why one should not expect to see evidence <i>even if</i> the proposition in question is true.</p></div></li><li class="footnote-item" id="fnguz89j1a968"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefguz89j1a968">^</a></strong></sup></span><div class="footnote-content"><p>Many interpretability researchers specifically believe in the <a href="https://arxiv.org/abs/2311.03658"><i>linear representation hypothesis</i></a>, that the variables of this program structure ("features") correspond to linear directions in activation space, or the stronger <a href="https://transformer-circuits.pub/2022/toy_model/index.html"><i>superposition hypothesis</i></a>, that such directions form a sparse overbasis for activation space. One must be careful in interpreting these hypotheses as there are different operationalizations within the community; in my opinion, the more sophisticated versions are far more plausible than naive versions (thank you to Chris Olah for a helpful conversation here). Presently, I am skeptical that linear representations give <a href="https://transformer-circuits.pub/2025/linebreaks/index.html">the most prosaic description</a> of a model's behavior or that this will be sufficient for complete reverse-engineering, but believe that the hypothesis is pointing at something real about models, and tools like SAEs can be helpful as long as one is aware of their limitations.</p></div></li><li class="footnote-item" id="fn969wylfycf"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref969wylfycf">^</a></strong></sup></span><div class="footnote-content"><p>See for instance the results of <a href="https://arxiv.org/abs/2305.08746">these</a> <a href="https://arxiv.org/abs/2310.07711">papers</a>, where the authors incentivize spatial modularity with an additional regularization term. The authors interpret this as <i>incentivizing modularity</i>, but I would interpret it as <i>incentivizing existing modularity to come to the surface</i>.</p></div></li><li class="footnote-item" id="fnc45j78nx35"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefc45j78nx35">^</a></strong></sup></span><div class="footnote-content"><p>From <a href="https://www.youtube.com/watch?v=Nlkk3glap_U&amp;t=785s">Dwarkesh Patel's podcast, 13:05</a>.</p></div></li><li class="footnote-item" id="fnlzj33x1q4yp"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreflzj33x1q4yp">^</a></strong></sup></span><div class="footnote-content"><p>The credit for these ideas should really go to Dan Murfet, as well as his current/former students including Will Troiani, James Clift, Rumi Salazar, and Billy Snikkers.</p></div></li><li class="footnote-item" id="fn5hsrpicn3vp"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref5hsrpicn3vp">^</a></strong></sup></span><div class="footnote-content"><p>Let&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;denote the output of the model on input&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span></span></span></span></span></span>&nbsp;with parameters&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span></span></span></span></span></span>. Formally, we say that a point in parameter space&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">W</span></span></span></span></span></span></span>&nbsp;is <i>degenerate</i> or <i>singular</i> if there exists a tangent vector&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">v</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;">T</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">W</span></span></span></span></span></span></span>&nbsp;such that the directional derivative&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">‚àá</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">v</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">w</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span>&nbsp;for all&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">x</span></span></span></span></span></span></span>. In other words, moving in some direction in parameter space doesn't change the behavior of the model (up to first order).</p></div></li><li class="footnote-item" id="fnfukshjhtc2n"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreffukshjhtc2n">^</a></strong></sup></span><div class="footnote-content"><p>This is not as alien as it may seem. Note that this provides a perspective which connects nicely with both neural networks and classical computation. First consider, for instance, that the gates of a Boolean circuit literally define a system of equations over&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">F</span></span></span><span class="mjx-sub"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span></span></span></span></span>, whose solution set is an algebraic variety over&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">F</span></span></span><span class="mjx-sub"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span></span></span></span></span>. Alternatively, consider that a neural network with polynomial (or analytic) activation function defines a system of equations over&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span></span></span></span></span>, whose vanishing set is an algebraic (respectively, analytic) variety over&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R">R</span></span></span></span></span></span></span></span></span>. Of course this goes only a small fraction of the way to closing this gap, but one can start to see how this becomes plausible.</p></div></li><li class="footnote-item" id="fnlc0m4rnjr2d"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreflc0m4rnjr2d">^</a></strong></sup></span><div class="footnote-content"><p>A frequent perspective is to write this minibatch-loss in terms of its mean (population) value plus some noise term. That is, we think of optimizers like SGD as something like ‚Äúgradient descent plus noise.‚Äù This is quite similar to mathematical models like overdamped Langevin dynamics, though note that the noise term may not be Gaussian as in Langevin dynamics. It is an open question whether the convergence of neural network training is due to the population term or the noise term. (Note that this is a separate question as to whether the generalization / inductive biases of SGD-trained neural networks is due to the population term or the noise term.) I am tentatively of the belief (somewhat controversially) that both convergence and inductive bias is due to structure in the population loss rather than the noise term, but explaining my reasoning here is a bit out of scope.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/Dw8mskAvBX37MxvXo/deep-learning-as-program-synthesis-1#comments">Discuss</a>

---

### [Why I Transitioned: A Response](https://www.lesswrong.com/posts/rt2yai8JkTPYgzoEj/why-i-transitioned-a-response)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 20, 2026 2:06 AM GMT<br /><br /><p>Fiora Sunshine's post, <a href="https://www.lesswrong.com/posts/gEETjfjm3eCkJKesz/why-i-transitioned-a-case-study">Why I Transitioned: A Case Study</a> (the OP) articulates a valuable theory for why some MtFs transition.</p><p>If you are MtF and feel the post describes you, I believe you.</p><p>However, many statements from the post are wrong or overly broad.</p><h2>My claims:</h2><ol><li>There is evidence of a biological basis for trans identity. Twin studies are a good way to see this.<br />&nbsp;</li><li>Fiora claims that trans people's apparent lack of introspective clarity may be evidence of deception. But trans people are incentivized not to attempt to share accurate answers to "why do you really want to transition?". This is the Trans Double Bind.<br />&nbsp;</li><li>I am a counterexample to Fiora's theory. I was an adolescent social outcast weeb but did not transition. I spent 14 years actualizing as a man, then transitioned at 31 only after becoming crippled by dysphoria. My example shows that Fiora's phenotype can co-occur with or mask medically significant dysphoria.</li></ol><hr /><h1>A. Biologically Transgender</h1><p>In the OP, Fiora presents the "body-map theory" under the umbrella of "arcane neuro-psychological phenomena", and then dismisses medical theories because the body-map theory doesn't fit her friend group.</p><p>The body-map theory is a straw man for biological causation because there are significant sex differences between men and women that are (a) not learned and (b) not reducible to subconscious expectations about one's anatomy.</p><p>The easiest way to see this is CAH. To quote from Berenbaum and Beltz, 2021<span class="footnote-reference" id="fnrefn9xsvjtc3"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnn9xsvjtc3">[1]</a></sup></span>:</p><blockquote><p>Studies of females with congenital adrenal hyperplasia (CAH) show how prenatal androgens affect behavior across the life span, with large effects on gendered activity interests and engagement, moderate effects on spatial abilities, and relatively small (or no) effects on gender identity</p></blockquote><p>The sex difference in people-vs-things interests (hobbies, occupations) has been <a href="https://slatestarcodex.com/2017/08/07/contra-grant-on-exaggerated-differences/">discussed</a> <a href="https://www.reddit.com/r/slatestarcodex/comments/6tyj22/how_established_is_the_things_vs_people_gender/">extensively</a> in our community. CAH shifts females towards male-patterned interests with small effects on gender identity, without changes in anatomy.</p><p>This finding is also notable because it shows male-patterned interests and female gender identity can coexist, at least in natal females.</p><p>&nbsp;</p><h2>Twin Studies √† la LLM</h2><p>I'm trans so I have a motive to search for evidence that suggests I am ~biologically valid~ and not subject to some kind of psychosocial delusion. It would be easy for me to cherry-pick individual papers to support that view. I'm trying to not do that. I'm also not going to attempt a full literature review here. Luckily it is 2026, and we have a better option.</p><p>The ACE model from psychiatric genetics is a standard framework for decomposing the variance in a trait into 3 components:</p><p>A = Additive Genetics: cumulative effect of individual alleles</p><p>C = Common Environment: parents, schooling, SES, etc.</p><p>E = Nonshared Environment (+ error): randomness, idiosyncratic life events<span class="footnote-reference" id="fnrefnpla3rlmuk8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnnpla3rlmuk8">[2]</a></sup></span></p><p>There are at least 9<span class="footnote-reference" id="fnrefk87ip1pxhl"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnk87ip1pxhl">[3]</a></sup></span>&nbsp;primary twin studies on transgender identity or gender dysphoria. I created an LLM prompt<span class="footnote-reference" id="fnref2e2voaalewf"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn2e2voaalewf">[4]</a></sup></span>&nbsp;asking for a literature review with the goal of extracting signal, not just from the trans twin literature, but from other research that could help give us some plausible bounds on the strength of biological and social causation. Here are the results. The format is POINT_ESTIMATE, RANGE:</p><figure class="table"><table><tbody><tr><td><strong>model</strong></td><td><strong>A</strong></td><td><strong>C</strong></td><td><strong>E</strong></td></tr><tr><td>Opus 4.5</td><td>0.4, 0.2-0.6</td><td>0.05, 0-0.2</td><td>0.55, 0.35-0.7</td></tr><tr><td>Opus 4.5 Research</td><td>.375, 0.2-0.6</td><td>0.125, 0-0.3</td><td>0.5, 0.3-0.6</td></tr><tr><td>GPT 5.2 Pro</td><td>0.35, 0.2-0.55</td><td>0.1, 0-0.25</td><td>0.55, 0.35-0.7</td></tr><tr><td>o3 Deep Research</td><td>0.4, 0.3-0.5</td><td>0.05, 0-0.2</td><td>0.55, 0.5-0.7</td></tr><tr><td>point est. average</td><td>0.38</td><td>0.08</td><td>0.54</td></tr></tbody></table></figure><p>&nbsp;</p><p>I'm moderately confident my prompt was not biased because the A values here are lower than what I've gotten from Claude when asking for heritability estimates from twin studies only. Also, all the models included some discussion of the rapid rise in adolescent cases in the 2010s, often mentioning "social contagion" and ROGD theories explicitly. All the models also pointed out that the ACE model is a simplification and that gene-environment interaction may be significant.</p><p>These are pretty wide error bars. But since A is trying to capture heredity only, we can take A as a rough lower bound for biological causation. Even if E is purely social, 38% is significant.</p><p>Also, none of this tells us how much variation there is at the individual level. And we have no trans GWAS.</p><p>The big question is whether E is dominated by social or biological factors.</p><p>If social factors mattered a lot I would expect parental attitudes to be significant in affecting transgender identity. But most studies find low C. This holds even for population-based studies that do not suffer from ascertainment bias. I would be surprised if peer influences were highly causal but parental influences were not.</p><p>I think the evidence from CAH, fraternal birth order effects, and animal models also provides good mechanistic reasons to think there are significant biological effects in E as well as A.</p><p>How do trans people view this line of research? They tend to hate it. They're afraid it will eventually lead to:</p><ol><li>not choosing "trans embryos" during IVF</li><li>aborting "trans fetuses"</li><li>lab/genetic testing to determine who is allowed to medically transition</li></ol><p>This is what I'll call "medical eradication": one half of the Double Bind.</p><p>&nbsp;</p><h1>B. The Trans Double Bind</h1><p>The purpose of medicine is to improve health and reduce suffering.</p><p>In general, the state should not subsidize healthcare that does not increase QALYs. A rational healthcare system would ration care based on ranking all available treatments by QALYs saved per dollar, and funding all treatments above a cutoff determined by the budget.</p><p>The US healthcare system has a very creative interpretation of reality, but other countries like the UK at least attempt to do this.</p><p>To receive gender-affirming treatment, trans people must argue that such treatment alleviates suffering. This argument helped establish gender medicine in the 20th century.&nbsp;</p><p>But in fact, the claim "being transgender involves suffering and requires medical treatment" is very controversial within the trans community. This is surprising, because disputing this claim threatens to undermine access to trans healthcare.</p><p>Moreover, this controversy explains why trans people do not appear to accurately report their own motivations for transition.</p><p>&nbsp;</p><h2>Motivations to transition</h2><p>There are three possible sources:</p><ol><li>biological</li><li>psychological/cognitive</li><li>social</li></ol><p>These can co-occur and interact.</p><p>Society at large recognizes only (1) as legitimate.</p><p>Trans people know this. They know they may be sent to psychotherapy, denied HRT, or judged illegitimate if they report wanting to transition for psychosocial reasons.</p><p>There is strong pressure for trans people to accept and endorse a biological/medical framing for their transitions.</p><p>But adopting this framing carries downsides:</p><ul><li>Dependence on medical authorities for legitimacy<ul><li>Historically, medicine has treated us very poorly<span class="footnote-reference" id="fnref3qr2l7s9ecx"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn3qr2l7s9ecx">[5]</a></sup></span></li><li>We have little power to negotiate for better medical care if we are dependent on medicine to validate us to the rest of society</li></ul></li><li>Psychological costs<ul><li>Trans-cultural memory of medical mistreatment</li><li>Many find medicalization demeaning and resent dependence</li></ul></li><li>Possible medical eradication<ul><li>We can't claim we <i>need</i> care if we don't suffer<span class="footnote-reference" id="fnrefs6u3fcb99o"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fns6u3fcb99o">[6]</a></sup></span>, but one day the medical system might find a more direct way to eliminate our suffering: preventing trans people from coming into existence in the first place.<br />&nbsp;</li></ul></li></ul><p>This is the Double Bind: <strong>many trans people need medical treatment, but find the psychological threat of medicalization and eradication intolerable.</strong></p><p>Consequently, they will not claim their transition is justified because of biology. However, they know that psychological and social justifications will also not be accepted. In this situation, platitudes like "I am a woman because I identify as one" are a predictable response to incentives. If you attempt to give a real answer, it will be used against you.</p><p>Maybe you are thinking:</p><blockquote><p>Marisa, this is hogwash! All the trans people I know are constantly oversharing lurid personal details despite obvious social incentives not to. The most parsimonious explanation is that people who say "I'm __ because I identify as &nbsp;__" literally believe that.</p></blockquote><p>Yes, good point. I need to explain another dynamic.</p><p>So far I've only discussed external incentives, but there is incentive pressure from within the trans community as well.</p><p>In the 2010s, the following happened:</p><ul><li>Youth transitions increased</li><li>Nonbinary identification increased, especially among people not medically transitioning&nbsp;</li><li>Acceptance, awareness, and politicization all increased</li><li>Social media happened</li></ul><p>Suddenly the trans community was fighting for a much broader set of constituents and demands. 20th century binary transsexualism coheres with medical framings, but 2010s Tumblr xenogenders do not. And trans people of all kinds have always had insecurities about their own validity-- both internal and external.</p><p>Here is the key insight:</p><p><strong>It's difficult to enforce norms that protect external political perception.</strong></p><p><strong>It's easy to enforce norms that protect ingroup feelings.</strong></p><p>Assume I've performed and posted some porn on the internet. This porn is optically really really bad. Like actually politically damaging. Conscientious trans people will attempt to punish my defection-- but this is difficult. I can cry "respectability politics!" and point to the history of trans sex work in the face of employment discrimination. No one can agree on a theory of change for politics, so it's hard to prove harm. When the political backlash hits, it affects everyone equally<span class="footnote-reference" id="fnrefyuxwtfkx1ld"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnyuxwtfkx1ld">[7]</a></sup></span>.&nbsp;</p><p>By contrast, assume instead that I'm in a trans community space and I've told someone their reasons for transition are not valid, and they should reconsider. I've just seriously hurt someone's feelings, totally killed the vibe, and I'll probably be asked to leave-- maybe shunned long-term<span class="footnote-reference" id="fnrefn1zrtqb65oq"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnn1zrtqb65oq">[8]</a></sup></span>. I have just lost access to perhaps my only source of ingroup social support. This is a <i>huge</i> disincentive.&nbsp;</p><p>This structure, combined with the influx of novel identities in the 2010s, created an environment where it was taboo even to talk about causal theories for one's own transition, because it could be invalidating to someone else. All gender identities were valid at all times. Downstream effects of external social pressure, social media, and politics created an environment of collective ignorance where community norms discouraged investigating the causes of transition.</p><h2>&nbsp;</h2><h2>Introspective Clarity</h2><blockquote><p>Famously, trans people tend not to have great introspective clarity into their own motivations for transition. Intuitively, they tend to be quite aware of what they do and don't like about inhabiting their chosen bodies and gender roles. But when it comes to explaining the origins and intensity of those preferences, they almost universally to come up short. I've even seen several smart, thoughtful trans people, such as Natalie Wynn, making <a href="https://youtu.be/OjMPJVmXxV8?t=2833">statements</a> to the effect that it's impossible to develop a satisfying theory of aberrant gender identities. (She may have been exaggerating for effect, but it was clear she'd given up on solving the puzzle herself.)</p></blockquote><p>This is the wrong interpretation of Natalie Wynn's oeuvre. See <a href="https://www.lesswrong.com/posts/yzRmgAmuCXhN3eXBK/appendix-contra-fiora-on-contra">Appendix: Contra Fiora on Contra</a> for why.</p><p>What would a legitimate explanation of the origins of one's gendered feelings look like?</p><p>Fiora never tells us her criteria. And the only example she gives us-- a psychosocial explanation of her own transition-- heavily implies that it was illegitimate.</p><p>But she's also dismissive of biological theories. Does that mean no transitions are valid?</p><p>I got whole genome sequencing last year. I can point at the sexual and endocrine abnormalities in my genome, but I certainly can't prove they justify my transition. Nevertheless, subjectively, HRT saved my life.</p><p>&nbsp;</p><h1>C. In the Case of Quinoa Marisa</h1><figure class="image image_resized" style="width: 67.77%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rt2yai8JkTPYgzoEj/njqirawkkd1qwehseaix" /><figcaption>the author, age 13. Note the oversized Haibane Renmei graphic tee</figcaption></figure><p><i>(Extremely simplified for brevity)</i></p><p>In middle school, puberty started and my life fell apart. I hated my erections, my libido; I felt like a demon had taken over my brain. Unlike my peers, I never developed a felt sense of how to throw my body around. They got rougher, and better at sports. I got injured.</p><p>I was pathologically shy and awkward. Locker room talk was utterly repulsive to me. I lost friends and didn't care. Rurouni Kenshin was my first inspiration to grow my hair out. I am very lucky my parents let me.</p><p>There was an autistic kid on my soccer team with a speech impediment. He was good at soccer but the other boys would cruelly tease him at practice, in part because he didn't understand they were teasing him. One night after practice I spent the car ride home sobbing about it in front of my dad, who didn't get it at all. I quit soccer.</p><p>I was utterly miserable in school. In March of 7th grade, I developed real depression, and started thinking about suicide. Mom took me to two different psychologists. We decided I would homeschool 8th grade. Now, I really had no friends. I was still depressed.</p><p>At this point I was only living for WoW and anime. By far, my favorite was Haibane Renmei. It's 13 episodes of angel-girls living in a run-down boarding school and basically just taking care of each other. It is heavily implied that the Haibane are there-- in purgatory-- because they committed suicide in the real world, and must learn how to accept love and care.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rt2yai8JkTPYgzoEj/ewcomkdvdmn8yw2q84oa" /></figure><p>It's difficult to explain how much this series resonated with me. It gave structure to feelings I couldn't articulate. I never believed there was any possibility of becoming a girl in real life, so I didn't fantasize much about that. But for a couple years I daydreamed frequently about dying and becoming a Haibane<span class="footnote-reference" id="fnref28ol0r0jfi8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn28ol0r0jfi8">[9]</a></sup></span>.</p><p>My hair was long enough at this point that I "passed". I was frequently assumed female in social situations, and men would often tell me I was in the wrong bathroom. I longed for delicate reciprocal care with others who somehow understood what I was going through, even though I could hardly understand it myself. Haibane Renmei showed me this but I had no idea how to find it in the real world.</p><p>At 16, boy puberty hit me like a truck. I became ugly. I still had no social skills, and no friends. I dressed like a hobo. The summer after junior year I confronted myself in the mirror and admitted I would never be cute again. I still desperately wanted to be loved, and I believed that the only path to achieving that was becoming a man girls would want to date. That meant improving my appearance and social skills.</p><p>I knew that women find weebs unattractive. And my long hair was definitely unattractive. It all melded together. I had no real-world outlet for my femininity so I'd poured it all into identifying with anime characters. And it all seemed like a dead end. I felt that if I stayed in the anime community I would end up socially stunted, since its social standards were lower. I cut my hair and stopped watching anime. I put a lot more effort into socializing.</p><p>In college, I read <i>The Man Who Would Be Queen</i>, self-diagnosed as AGP, and actually considered transition for the first time. But it was too late for me-- the sight of my face in the mirror, and the depictions of AGPs in the book were too horrifying. I resolved to never transition, and attempted suicide soon after.</p><p>7 months later I fell in love, and that relationship turned my life around. I loved her immeasurably for 5 years, and we lived together for 2 of those. I became, on the outside, socially and professionally actualized as a man. I was a great boyfriend and had no problem getting dates. After the breakup I fell in love 2 more times.</p><p>You already know how this ends. No amount of true love or social validation as a man could fix me. I never wanted to transition, but at 31 the strain of repression became unbearable. Things have turned out far better than I ever dared imagine. My parents have remarked on multiple occasions, unprompted, how much happier I am now. They're right.</p><hr /><p>Overall I fit Fiora's phenotype: I was a mentally ill social outcast weeb, desperately identifying with anime characters as a simulacrum of loving care I had no idea how to find in real life.</p><p>But I can't explain my eventual transition at 31 through anything other than a biological cause. I looked obsessively for evidence of some repressed or unconscious ulterior motive, and found none. I believed that transition would be very expensive and time-consuming, physically painful<span class="footnote-reference" id="fnrefm13g8n6l1i8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnm13g8n6l1i8">[10]</a></sup></span>, reduce my attractiveness as a mate, and change my social possibilities. All of these predictions have born true. What I didn't expect is that HRT <a href="https://usefulfictions.substack.com/p/testosterone-gave-me-my-life-back/comment/113466684">drastically improved my mental health</a> even before the physical changes kicked in. My baseline now is my former 90th-percentile of calm and happiness.&nbsp;</p><p>I'm n=1 but this shows Fiora's phenotype can coexist with biologically rooted dysphoria. Moreover, I believe my middle school social failures were caused as much by gender incongruence as by neurodivergence. It's difficult to socialize when your puberty feels wrong and your social instincts don't match your assigned gender.</p><blockquote><p>It's almost like most of them had deep emotional wounds, often stemming from social rejection, and had transitioned to become cute girls or endearing women as a kind of questionably adaptive coping mechanism.</p></blockquote><p>Maybe. Or a misaligned subconscious sex is part of what caused the social rejection in the first place.</p><h1>Conclusion</h1><p>As Fiora implied, "cuteness-maxxing" is probably not a good reason to transition.</p><p>Most people desperately want to be loved and this can cause mistakes with transition in both directions. Social media is probably bad for minors. We should emphasize that, at a fundamental level, trans people are neither more nor less lovable than cis people.</p><p>The human brain is perhaps the most complex object in our known universe, and we will likely never be able to fully disentangle psychosocial factors from biological ones. That said, I do think humanity will discover ever stronger evidence for biological causes of trans identity within our lifetimes.</p><p>Introspection is a noisy way to attempt to answer "am I trans?", and you hit diminishing returns fast. It's also the wrong question. The right question is "should I transition?". Transition is best understood as a Bayesian process where you take small behavioral steps<span class="footnote-reference" id="fnref7dbs69e0xsj"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn7dbs69e0xsj">[11]</a></sup></span>&nbsp;and update on whether your quality of life is improving.</p><p>If you start transitioning and your intrinsic health and happiness improves, and you expect the same to be true in the long run, continue. If not, desist. There is no shame in either outcome.</p><p>&nbsp;</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnn9xsvjtc3"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefn9xsvjtc3">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9186536/">https://pmc.ncbi.nlm.nih.gov/articles/PMC9186536/</a></p></div></li><li class="footnote-item" id="fnnpla3rlmuk8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefnpla3rlmuk8">^</a></strong></sup></span><div class="footnote-content"><p>For twins, prenatal environment shows up in both C and E.</p></div></li><li class="footnote-item" id="fnk87ip1pxhl"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefk87ip1pxhl">^</a></strong></sup></span><div class="footnote-content"><p>Coolidge et al. (2002), Heylens et al. (2012), Karamanis et al. (2022), Conabere et al. (2025), Sasaki et al. (2016), Bailey et al. (2000), Burri et al. (2011), Diamond (2013), Buhrich et al. (1991).</p><p>If you just want to read a systematic review of these studies, see <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12494644/">https://pmc.ncbi.nlm.nih.gov/articles/PMC12494644/</a></p></div></li><li class="footnote-item" id="fn2e2voaalewf"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref2e2voaalewf">^</a></strong></sup></span><div class="footnote-content"><p>I'm trying to understand the etiology of transgender identity, particularly the strength of the evidence base for different categories of potential causes. Please segment the analysis into five categories:</p><p>1. Hereditary/genetic factors<br />2. Prenatal environment (hormonal, epigenetic, maternal)<br />3. Postnatal biological environment (diet, medications, endocrine factors)<br />4. Family/microsocial environment<br />5. Macrosocial/cultural environment</p><p>For each category, conduct a rigorous literature review prioritizing meta-analyses, large-N studies, and methodologically sound designs. Identify the strongest evidence both supporting and contradicting causal contributions from that category. Flag studies with clear methodological limitations and discuss known publication biases in the field.</p><p>Focus primarily on gender dysphoria and transgender identity as defined in DSM-5/ICD-11, noting where studies conflate distinct constructs or onset patterns.</p><p>Conclude with a variance decomposition estimate using the ACE framework and liability threshold model standard in psychiatric genetics. Provide:</p><p>- Point estimates with plausible ranges for each component (A, C, E)<br />- Confidence ratings for each estimate based on evidence quantity and quality<br />- Explicit discussion of what each ACE component likely captures, mapped back to the five categories above<br />- Acknowledgment of confounds and unmeasurable factors</p><p>Include cross-cultural and temporal trend data as evidence bearing on the cultural/environmental components.</p></div></li><li class="footnote-item" id="fn3qr2l7s9ecx"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref3qr2l7s9ecx">^</a></strong></sup></span><div class="footnote-content"><p>In general, in the US in the 20th century, if a medical institution decided they simply didn't want to treat trans patients, there would be no public outcry. The doctors and organizations that did treat us could set terms. Prior to the 2010s there was little awareness of trans people, and the awareness we had was often prejudicial. IBM fired <a href="https://en.wikipedia.org/wiki/Lynn_Conway">Lynn Conway</a> after all.</p></div></li><li class="footnote-item" id="fns6u3fcb99o"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefs6u3fcb99o">^</a></strong></sup></span><div class="footnote-content"><p>Some trans people (for example, Abigail Thorn and Andrea Long Chu) have attempted to argue that access to gender-affirming care should not be contingent on either (a) suffering prior to receiving treatment or (b) demonstrated therapeutic benefit for the treatment. These arguments were not well-received even within the trans community.</p></div></li><li class="footnote-item" id="fnyuxwtfkx1ld"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefyuxwtfkx1ld">^</a></strong></sup></span><div class="footnote-content"><p>It took r/MtF until 2025 to ban porn, after years of infighting. <a href="https://www.reddit.com/r/MtF/comments/1kaxn18/alright_lets_talk_about_porn_and_porn_accounts/">https://www.reddit.com/r/MtF/comments/1kaxn18/alright_lets_talk_about_porn_and_porn_accounts/</a></p></div></li><li class="footnote-item" id="fnn1zrtqb65oq"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefn1zrtqb65oq">^</a></strong></sup></span><div class="footnote-content"><p>This norm is not totally unreasonable. The purpose of community spaces is primarily social support for those early in transition, which can be difficult to find anywhere else. I went through this phase too.</p></div></li><li class="footnote-item" id="fn28ol0r0jfi8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref28ol0r0jfi8">^</a></strong></sup></span><div class="footnote-content"><p>Yes, this is perverse and contradicts the moral of the story.</p></div></li><li class="footnote-item" id="fnm13g8n6l1i8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefm13g8n6l1i8">^</a></strong></sup></span><div class="footnote-content"><p>Electrolysis is the most physically painful thing I've experienced. I've done 40 hours so far and will likely do 150-200 total.</p></div></li><li class="footnote-item" id="fn7dbs69e0xsj"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref7dbs69e0xsj">^</a></strong></sup></span><div class="footnote-content"><p>Voice training, experimenting with name/pronouns/clothing, laser hair removal, HRT.&nbsp;</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/rt2yai8JkTPYgzoEj/why-i-transitioned-a-response#comments">Discuss</a>

---

## AI/ML

### [Differential Transformer V2](https://huggingface.co/blog/microsoft/diff-attn-v2)

**Êù•Ê∫ê**: Hugging Face Blog

**ÊëòË¶Å**: 

---

## Engineering

### [Stop Picking Sides: Manage the Tension Between Adaptation and                Optimization](https://martinfowler.com/articles/stop-picking-sides.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/stop-picking-sides.html"><img src="https://martinfowler.com/articles/stop-picking-sides/infograph.jpg" width="" /></a></div>

<p><b class="author">Jim Highsmith</b> notes that many teams have turned into
      tribes wedded to exclusively adaptation or optimization. But he feels this
      misses the point that both of these are important, and we need to manage
      the tension between them. We can do this by thinking of two operating
      modes: explore (adaptation-dominant) and exploit (optimization dominant).
      We tailor a team's operating model to a particular blend of the two -
      considering uncertainty, risk, cost of change, and an evidence threshold.
      We should be particularly careful at the points where there is a handoff
      between the two modes</p>

<p><a class="more" href="https://martinfowler.com/articles/stop-picking-sides.html">more‚Ä¶</a></p>

---

### [My favorite musical discoveries of 2025](https://martinfowler.com/articles/2025-music.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/2025-music.html"><img src="https://martinfowler.com/articles/2025-music/card.png" width="350px" /></a></div>

<p>My favorite albums from last year. Balkan brass, an
      acoustic favorite of 80s returns, Ethio-jazz, Guatemalan singer-guitarist,
      jazz-rock/Indian classical fusion, and a unique male vocalist.</p>

<p><a class="more" href="https://martinfowler.com/articles/2025-music.html">more‚Ä¶</a></p>

---

### [Fragments: January  8](https://martinfowler.com/fragments/2026-01-08.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>Anthropic report on <a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic">how their AI is changing their own software development practice</a>.</p>

<ul>
  <li>Most usage is for debugging and helping understand existing code</li>
  <li>Notable increase in using it for implementing new features</li>
  <li>Developers using it for 59% of their work and getting 50% productivity increase</li>
  <li>14% of developers are ‚Äúpower users‚Äù reporting much greater gains</li>
  <li>Claude helps developers to work outside their core area</li>
  <li>Concerns about changes to the profession, career evolution, and social dynamics</li>
</ul>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Much of the discussion about using LLMs for software development lacks details on workflow. Rather than just hear people gush about how wonderful it is, I want to understand the gritty details. What kinds of interactions occur with the LLM? What decisions do the humans make? When reviewing LLM outputs, what kinds of things are the humans looking for, what corrections do they make?</p>

<p><a href="https://obie.medium.com/what-used-to-take-months-now-takes-days-cc8883cc21e9">Obie Fernandez</a> has written a post that goes into these kinds of details. Over the Christmas / New Year period he used Claude to build a knowledge distillation application, that takes transcripts from Claude Code sessions, slack discussion, github PR threads etc, turns them into an RDF graph database, and provides a web app with natural language ways to query them.</p>

<blockquote>
  <p>Not a proof of concept. Not a demo. The first cut of Nexus, a production-ready system with authentication, semantic search, an MCP server for agent access, webhook integrations for our primary SaaS platforms, comprehensive test coverage, deployed, integrated and ready for full-scale adoption at my company this coming Monday. Nearly 13,000 lines of code.</p>
</blockquote>

<p>The article is long, but worth the time to read it.</p>

<p>An important feature of his workflow is relying on <a href="https://martinfowler.com/bliki/TestDrivenDevelopment.html">Test-Driven Development</a></p>

<blockquote>
  <p>Here‚Äôs what made this sustainable rather than chaotic: TDD. Test-driven development. For most of the features, I insisted that Claude Code follow the red-green-refactor cycle with me. Write a failing test first. Make it pass with the simplest implementation. Then refactor while keeping tests green.</p>

  <p>This wasn‚Äôt just methodology purism. TDD served a critical function in AI-assisted development: it kept me in the loop. When you‚Äôre directing thousands of lines of code generation, you need a forcing function that makes you actually understand what‚Äôs being built. Tests are that forcing function. You can‚Äôt write a meaningful test for something you don‚Äôt understand. And you can‚Äôt verify that a test correctly captures intent without understanding the intent yourself.</p>
</blockquote>

<p>The account includes a major refactoring, and much evolution of the initial version of the tool. It‚Äôs also an interesting glimpse of how AI tooling may finally make RDF useful.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>When thinking about requirements for software, most discussions focus on prioritization. Some folks talk about buckets such as the <a href="https://en.wikipedia.org/wiki/MoSCoW_method">MoSCoW set</a>: Must, Should, Could, and Want. (The old joke being that, in MoSCoW, the cow is silent, because hardly any requirements end up in those buckets.) <a href="https://world.hey.com/jason/the-obvious-the-easy-and-the-possible-2e11a3fb">Jason Fried</a> has a different set of buckets for interface design: <strong>Obvious, Easy, and Possible</strong>. This immediately resonates with me: a good way of think about how to allocate the cognitive costs for those who use a tool.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.platformer.news/fake-uber-eats-whisleblower-hoax-debunked/">Casey Newton</a> explains how he followed up on an interesting story of dark patterns in food delivery, and found it to be a fake story, buttressed by AI image and document creation. On one hand, it clarifies the important role reporters play in exposing lies that get traction on the internet. But time taken to do this is time not spent on investigating real stories</p>

<blockquote>
  <p>For most of my career up until this point, the document shared with me by the whistleblower would have seemed highly credible in large part because it would have taken so long to put together. Who would take the time to put together a detailed, 18-page technical document about market dynamics just to troll a reporter? Who would go to the trouble of creating a fake badge?</p>

  <p>Today, though, the report can be generated within minutes, and the badge within seconds. And while no good reporter would ever have published a story based on a single document and an unknown source, plenty would take the time to investigate the document‚Äôs contents and see whether human sources would back it up.</p>
</blockquote>

<p>The internet has always been full of slop, and we have always needed to be wary of what we read there. AI now makes it easy to manufacture convincing looking evidence, and this is never more dangerous than when it confirms strongly held beliefs and fears.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/">Kent Beck</a>:</p>

<blockquote>
  <p>The descriptions of Spec-Driven development that I have seen emphasize writing the whole specification before implementation. This encodes the (to me bizarre) assumption that you aren‚Äôt going to learn anything during implementation that would change the specification.
I‚Äôve heard this story so many times told so many ways by well-meaning folks‚Äìif only we could get the specification ‚Äúright‚Äù, the rest of this would be easy.</p>
</blockquote>

<p>Like him, that story has been the constant background siren to my career in tech. But the <a href="https://martinfowler.com/articles/llm-learning-loop.html">learning loop</a> of experimentation is essential to the model building that‚Äôs at the heart of any kind of worthwhile specification. As <a href="https://martinfowler.com/articles/llm-learning-loop.html">Unmesh puts it:</a></p>

<blockquote>
  <p>Large Language Models give us great leverage‚Äîbut they only work if we focus on learning and understanding. They make it easier to explore ideas, to set things up, to translate intent into code across many specialized languages. But the real capability‚Äîour ability to respond to change‚Äîcomes not from how fast we can produce code, but from how deeply we understand the system we are shaping.</p>
</blockquote>

<p>When Kent defined Extreme Programming, he made <em>feedback</em> one of its four core values. It strikes me that the key to making the full use of AI in software development is how to use it to accelerate the feedback loops.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context. Programming-oriented tools are geting more sophisticated for that, but there‚Äôs also efforts at providing simpler tools, that allow customization. <a href="https://lixo.org">Carlos Villela</a> recently recommended Pi, and its developer, <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">Mario Zechner, has an interesting blog</a> on its development.</p>

<blockquote>
  <p>So what‚Äôs an old guy yelling at Claudes going to do? He‚Äôs going to write his own coding agent harness and give it a name that‚Äôs entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?</p>
</blockquote>

<p>If I ever get the time to sit and really play with these tools, then something like Pi would be something I‚Äôd like to try out. Although as an addict to The One True Editor, I‚Äôm interested in some of libraries that work with that, such as <a href="https://github.com/karthink/gptel">gptel</a>. That would enable me to use Emacs‚Äôs inherent programability to create my own command set to drive the interaction with LLMs.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Outside of my professional work, I‚Äôve <a href="https://boardgamegeek.com/blog/13064/martins-7th-decade">posting regularly about my boardgaming</a> on the specialist site BoardGameGeek. However its blogging environment doesn‚Äôt do a good job of providing an index to my posts, so I‚Äôve created <a href="https://martinfowler.com/boardgames/">a list of my BGG posts </a> on my own site. If you‚Äôre interested in my regular posts on boardgaming, and you‚Äôre on BGG you can subscribe to me there. If you‚Äôre not on BGG you can  subscribe to the blog‚Äôs <a href="https://boardgamegeek.com/rss/blog/13064">RSS feed</a>.</p>

<p>I‚Äôve also created a list of <a href="https://martinfowler.com/boardgames/fav-games.html">my favorite board games</a>.</p>

<p><img alt="" src="https://martinfowler.com/boardgames/index/game-grid.png" /></p>

---

### [Fragments: December 16](https://martinfowler.com/fragments/2025-12-16.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.linkedin.com/in/gitanjalivenkatraman/">Gitanjali Venkatraman</a> does wonderful illustrations of complex subjects (which is why I was so happy to work with her on our <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a> article). She has now published the latest in her series of illustrated guides: tackling the complex topic of <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf">Mainframe Modernization</a></p>

<p><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf"><img alt="" src="https://martinfowler.com/fragments/2025/gitanjali-mainframe.png" /></a></p>

<p>In it she illustrates the history and value of mainframes, why modernization is so tricky, and how to tackle the problem by breaking it down into tractable pieces. I love the clarity of her explanations, and smile frequently at her way of enhancing her words with her quirky pictures.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Gergely Orosz on <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m7xd33hi5c2d">social media</a></p>

<blockquote>
  <p>Unpopular opinion:</p>

  <p>Current code review tools just don‚Äôt make much sense for AI-generated code</p>

  <p>When reviewing code I really want to know:</p>

  <ul>
    <li>The prompt made by the dev</li>
    <li>What corrections the other dev made to the code</li>
    <li>Clear marking of code AI-generated not changed by a human</li>
  </ul>
</blockquote>

<p>Some people pushed back saying they don‚Äôt (and shouldn‚Äôt care) whether it was written by a human, generated by an LLM, or copy-pasted from Stack Overflow.</p>

<p>In my view it matters <em>a lot</em> - because of the second vital purpose of code review.</p>

<p>When asked why do code reviews, most people will answer the first vital purpose - quality control. We want to ensure bad code gets blocked before it hits <a href="https://martinfowler.com/articles/branching-patterns.html#mainline">mainline</a>. We do this to avoid bugs and to avoid other quality issues, in particular comprehensibility and ease of change.</p>

<p>But I hear the second vital purpose less often: code review is a mechanism to communicate and educate. If I‚Äôm submitting some sub-standard code, and it gets rejected, I want to know why so that I can improve my programming. Maybe I‚Äôm unaware of some library features, or maybe there‚Äôs some project-specific standards I haven‚Äôt run into yet, or maybe my naming isn‚Äôt as clear as I thought it was. Whatever the reasons, I need to know in order to learn. And my employer needs me to learn, so I can be more effective.</p>

<p>We need to know the writer of the code we review both so we can communicate our better practice to them, but also to know how to improve things. With a human, its a conversation, and perhaps some documentation if we realize we‚Äôve needed to explain things repeatedly. But with an LLM it‚Äôs about how to modify its context, as well as humans learning how to better drive the LLM.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Wondering why I‚Äôve been making a lot of posts like this recently? <a href="https://martinfowler.com/articles/writing-fragments.html">I explain why</a> I‚Äôve been reviving the link blog.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://simonwillison.net/2025/Dec/10/html-tools/">Simon Willison</a> describes how he uses LLMs to build disposable but useful web apps</p>

<blockquote>
  <p>These are the characteristics I have found to be most productive in building tools of this nature:</p>

  <ol>
    <li>A single file: inline JavaScript and CSS in a single HTML file means the least hassle in hosting or distributing them, and crucially means you can copy and paste them out of an LLM response.</li>
    <li>Avoid React, or anything with a build step. The problem with React is that JSX requires a build step, which makes everything massively less convenient. I prompt ‚Äúno react‚Äù and skip that whole rabbit hole entirely.</li>
    <li>Load dependencies from a CDN. The fewer dependencies the better, but if there‚Äôs a well known library that helps solve a problem I‚Äôm happy to load it from CDNjs or jsdelivr or similar.</li>
    <li>Keep them small. A few hundred lines means the maintainability of the code doesn‚Äôt matter too much: any good LLM can read them and understand what they‚Äôre doing, and rewriting them from scratch with help from an LLM takes just a few minutes.</li>
  </ol>
</blockquote>

<p>His repository includes all these tools, together with transcripts of the chats that got the LLMs to build them.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://obie.medium.com/what-happens-when-the-coding-becomes-the-least-interesting-part-of-the-work-ab10c213c660">Obie Fernandez</a>: while many engineers are underwhelmed by AI tools, some senior engineers are finding them really valuable. He feels that senior engineers have an oft-unspoken mindset, which in conjunction with an LLM, enables the LLM to be much more valuable.</p>

<blockquote>
  <p>Levels of abstraction and generalization problems get talked about a lot because they‚Äôre easy to name. But they‚Äôre far from the whole story.</p>

  <p>Other tools show up just as often in real work:</p>

  <ul>
    <li>A sense for blast radius. Knowing which changes are safe to make loudly and which should be quiet and contained.</li>
    <li>A feel for sequencing. Knowing when a technically correct change is still wrong because the system or the team isn‚Äôt ready for it yet.</li>
    <li>An instinct for reversibility. Preferring moves that keep options open, even if they look less elegant in the moment.</li>
    <li>An awareness of social cost. Recognizing when a clever solution will confuse more people than it helps.</li>
    <li>An allergy to false confidence. Spotting places where tests are green but the model is wrong.</li>
  </ul>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://friendlybit.com/python/writing-justhtml-with-coding-agents/">Emil Stenstr√∂m</a> built an HTML5 parser in python using coding agents, using Github Copilot in Agent mode with Claude Sonnet 3.7. He automatically approved most commands. It took him ‚Äúa couple of months on off-hours‚Äù, including at least one restart from scratch. The parser now passes all the tests in html5lib test suite.</p>

<blockquote>
  <p>After writing the parser, I still don‚Äôt know HTML5 properly. The agent wrote it for me. I guided it when it came to API design and corrected bad decisions at the high level, but it did ALL of the gruntwork and wrote all of the code.</p>

  <p>I handled all git commits myself, reviewing code as it went in. I didn‚Äôt understand all the algorithmic choices, but I understood when it didn‚Äôt do the right thing.</p>
</blockquote>

<p>Although he gives an overview of what happens, there‚Äôs not very much information on his workflow and how he interacted with the LLM. There‚Äôs certainly not enough detail here to try to replicate his approach. This is contrast to Simon Willison (above) who has detailed links to his chat transcripts - although they are much smaller tools and I haven‚Äôt looked at them properly to see how useful they are.</p>

<p>One thing that is clear, however, is the vital need for a comprehensive test suite. Much of his work is driven by having that suite as a clear guide for him and the LLM agents.</p>

<blockquote>
  <p>JustHTML is about 3,000 lines of Python with 8,500+ tests passing. I couldn‚Äôt have written it this quickly without the agent.</p>

  <p>But ‚Äúquickly‚Äù doesn‚Äôt mean ‚Äúwithout thinking.‚Äù I spent a lot of time reviewing code, making design decisions, and steering the agent in the right direction. The agent did the typing; I did the thinking.</p>
</blockquote>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†</p>

<p>Then Simon Willison <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">ported the library to JavaScript</a>:</p>

<blockquote>
  <p>Time elapsed from project idea to finished library: about 4 hours, during which I also bought and decorated a Christmas tree with family and watched the latest Knives Out movie.</p>
</blockquote>

<p>One of his lessons:</p>

<blockquote>
  <p>If you can reduce a problem to a robust test suite you can set a coding agent loop loose on it with a high degree of confidence that it will eventually succeed. I called this <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">designing the agentic loop</a> a few months ago. I think it‚Äôs the key skill to unlocking the potential of LLMs for complex tasks.</p>
</blockquote>

<p>Our experience at Thoughtworks backs this up. We‚Äôve been doing a fair bit of work recently in legacy modernization (mainframe and otherwise) using AI to migrate substantial software systems. Having a robust test suite is necessary (but not sufficient) to making this work. I hope to share my colleagues‚Äô experiences on this in the coming months.</p>

<p>But before I leave Willison‚Äôs post, I should highlight his final open questions on the legalities, ethics, and effectiveness of all this - they are well-worth contemplating.</p>

---

### [Writing Fragments](https://martinfowler.com/articles/writing-fragments.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>If you‚Äôre a regular reader of my site, you‚Äôll have noticed that in the
last few months I‚Äôve been making a <a href="https://martinfowler.com/articles/20251204-frags.html">number</a> of ‚Äúfragments‚Äù <a href="https://martinfowler.com/articles/2025-11-19-frags.html">posts</a>. Such a post
is a short post with a bunch of little, unconnected segments. These are
usually a reference to something I‚Äôve found on the web, sometimes a small
thought of my own.</p>

<p>A few years ago, I wouldn‚Äôt have covered these topics with posts on my
own site. Instead I would use Twitter, either retweeting someone else‚Äôs
point, or just highlighting something I‚Äôd found. But since the Muskover,
Twitter has effectively died. I‚Äôm not saying that due to any technical
issues with the site, which has mostly just been fine, nor directly due to
any of the policy changes there. The point is that lots of people have left, so that
the audience I would have reached with Twitter is now fragmented. Some
remain on X, but I see more activity on LinkedIn. There‚Äôs also Fediverse/Mastodon
and Bluesky.</p>

<p>What this means for short posts is that I can no longer just post in one
place. When I announce new articles on martinfowler.com, I announce now on
four social media sites (X, LinkedIn, Fediverse, and Bluesky). It makes
sense to do this, but I don‚Äôt want to go through all this hassle for the
kind of micro-post that Twitter served so well.</p>

<p>So I‚Äôve started to batch them up. When I see something interesting, I
make a note. When I have enough notes, I post a fragments post. Initially I
did this in a rather ad-hoc way, just using the same mechanisms I use for
most articles, but last week I started to put in some more deliberate
mechanisms into the site. (If you‚Äôre observant, you‚Äôll spot that in the URLs.)</p>

<p>One benefit of all of this, at least in my book, is that it means my material is
now fully visible in RSS. I‚Äôm probably showing my age, but I‚Äôm a big fan of RSS
(or in my case, strictly Atom) feeds. I miss the feel of the heyday of the
‚Äúblogosphere‚Äù before it got steamrolled by social media, and these fragment
posts are, of course, just the same as the link blogs from that era. I still use my
RSS reader every day to keep up with writers I like. (I‚Äôm pleased that Substack
makes its content available via RSS.) It bothered me a bit that my micro-founts
of Twitter knowledge weren‚Äôt visible on RSS, but was too lazy to do something
about it. Now I don‚Äôt need to - the fragments are available in my RSS feed.</p>

---

### [Fragments Dec 11](https://martinfowler.com/articles/2025-12-11-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html?utm_source=substack&amp;utm_medium=email">Why does AI write like‚Ä¶ that</a> (NYT, gift link). Sam Kriss delves into the quiet hum of AI writing. AI‚Äôs work is not compelling prose: it‚Äôs phantom text, ghostly scribblings, a spectre woven into our communal tapestry.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://coding-is-like-cooking.info/2025/12/test-desiderata-2-0/">Emily Bache</a> has written a set of Test Desiderata, building on some earlier writing from Kent Beck. She lists the characteristics of good tests, and how they support her four ‚Äúmacro desiderata‚Äù - the properties of a sound test suite</p>

<blockquote>
  <ul>
    <li>Predict success in production</li>
    <li>Fast to get feedback</li>
    <li>Support ongoing code design change</li>
    <li>Low total cost of ownership</li>
  </ul>
</blockquote>

<p>She also has a great list of other writers‚Äô lists of good test characteristics.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.techpolicy.press/the-eus-fine-against-x-is-not-about-speech-or-censorship/">Daphne Keller</a> explains that the EUs fines on X aren‚Äôt about free speech.</p>

<blockquote>
  <p>There are three charges against X, which all stem from a multi-year investigation that was launched in 2023. One is about verification ‚Äî X‚Äôs blue checkmarks on user accounts ‚Äî and two are about transparency. These charges have nothing to do with what content is on X, or what user speech the platform should or should not allow.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington">Cory Doctorow</a> The Reverse-Centaur‚Äôs Guide to Criticizing AI</p>

<blockquote>
  <p>Start with what a reverse centaur is. In automation theory, a ‚Äúcentaur‚Äù is a person who is assisted by a machine. ‚Ä¶ And obviously, a reverse centaur is machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.</p>

  <p>Like an Amazon delivery driver‚Ä¶ the van can‚Äôt drive itself and can‚Äôt get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.</p>
</blockquote>

---

### [Fragments Dec 4](https://martinfowler.com/articles/20251204-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://blog.robbowley.net/2025/12/04/ai-is-still-making-code-worse-a-new-cmu-study-confirms/">Rob Bowley</a> summarizes a study from Carnegie Mellon looking on the impact of AI on a bunch of open-source software projects. Like any such study, we shouldn‚Äôt take its results as definitive, but there seems enough there to make it a handy data point. The key point is that the AI code probably reduced the quality of the code base - at least if static code analysis can be trusted to determine quality. And perhaps some worrying second-order effects</p>

<blockquote>
  <p>This study shows more than 800 popular GitHub projects with code quality degrading after adopting AI tools. It‚Äôs hard not to see a form of context collapse playing out in real time. If the public code that future models learn from is becoming more complex and less maintainable, there‚Äôs a real risk that newer models will reinforce and amplify those trends, producing even worse code over time.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Rob‚Äôs post is typical of much of the thoughtful writing on AI. We can see its short-term benefits, but worry about its long-term impact. But on a much deeper note is this lovely story from <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">Jim Highsmith</a>. Jim has turned 0x50, and has spent the last decade fighting Parkinson‚Äôs disease. To help him battle it he has two AI assisted allies.</p>

<blockquote>
  <p>Between my neural implants and Byron‚Äôs digital guidance, I now collaborate with two adaptive systems: one for motion, one for thought. Neither replaces me. Both extend me.</p>
</blockquote>

<p><strong>If you read anything on AI this week, <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">make it be this</a>.</strong> It offers a positive harbinger for our future and opens my mind to a whole different perspective of the role of AI in it</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Anthropic recently announced that it disrupted a Chinese state-sponsored operation abusing Claude Code. Jim Gumbley looks at the core lesson to learn from this, that we have to understand the serious risk of  <a href="https://www.thoughtworks.com/insights/blog/security/anthropic-ai-espionage-disclosure-signal-from-noise?utm_source=linkedin&amp;utm_medium=social-organic&amp;utm_campaign=dt_bs_rp-in-clt_tech_thought_leaders_2025-11&amp;gh_src=d1fe17bc1us">AI Jailbreaking</a></p>

<blockquote>
  <p>New AI tools are able to analyze your attack surface at the next level of granularity. As a business leader, that means you now have two options: wait for someone else to run AI-assisted vulnerability detection against your attack surface, or run it yourself first.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>There‚Äôs plenty of claims that AI Vibe Coding can replace software developers, something that folks like me (perhaps with a bias) think unlikely. <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m75kmb7bh22i">Gergely Orosz</a> shared this tidbit</p>

<blockquote>
  <p>Talked with an exec at a tech company who is obsessed with AI and has been for 3 years. Not a developer but company makes software. Uses AI for everything, vibe codes ideas.</p>

  <p>Here‚Äôs the kicker:</p>

  <p>Has a team of several devs to implement his vibe coded prototypes to sg workable</p>
</blockquote>

<p>I‚Äôd love to hear more about this (and similar stories)</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/">Nick Radcliffe</a> writes about a month of using AI</p>

<blockquote>
  <p>I spent a solid month ‚Äúpair programming‚Äù with Claude Code, trying to suspend disbelief and adopt a this-will-be-productive mindset. More specifically, I got Claude to write well over 99% of the code produced during the month. I found the experience infuriating, unpleasant, and stressful before even worrying about its energy impact. Ideally, I would prefer not to do it again for at least a year or two. The only problem with that is that it ‚Äúworked‚Äù.</p>
</blockquote>

<p>He stresses that his approach is the ‚Äúpolar opposite‚Äù of Vibe Coding. The post is long, and rambles a bit, but is worthwhile because he talks in detail about his workflow and how he uses the tool. Such posts are important so we can learn the nitty-gritty of how our programming habits are changing.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Along similar lines is a post of <a href="https://brianchambers.substack.com/p/chamber-of-tech-secrets-55-issue">Brian Chambers</a> on his workflow, that he calls Issue-Driven Development (and yes, I‚Äôm also sick of the ‚Äúsomething-driven‚Äù phraseology). As with much of the better stuff I‚Äôve heard about AI assisted work, it‚Äôs all about carefully managing the context window, ensuring the AI is focused on the right things and not distracted by textual squirrels.</p>

---

### [Fragments Nov 19](https://martinfowler.com/articles/2025-11-19-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><img alt="radar image" src="https://martinfowler.com/articles/images/frags-2025-11/radar.png" width="300px" /></p>

<p>I‚Äôve been on the road in Europe for the last couple of weeks, and while I was there Thoughtworks released <a href="https://www.thoughtworks.com/radar">volume 33 of our Technology Radar</a>. Again it‚Äôs dominated by the AI wave, with lots of blips capturing our explorations of how to use LLMs and similar technology. ‚ÄúAgents‚Äù are the big thing these days but we‚Äôre also seeing growing movements in infrastructure orchestration, coding workflows - and the inevitable antipatterns. Many thanks to my colleagues for putting this together again.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Gergely and I recording the podcast" src="https://martinfowler.com/articles/images/frags-2025-11/podcast.png" width="400px" /></p>

<p>My trip to Europe started in Amsterdam, for a Thoughtworks event for a few of our clients there. Since I was in that lovely city, I got in touch with Gergely Orosz, host of <a href="https://www.pragmaticengineer.com/">The Pragmatic Engineer</a>, and he arranged to <a href="https://www.youtube.com/watch?v=CQmI4XKTa0U">record a podcast</a> with me. No surprise that AI was front-and-center of the conversation, as I said it was the biggest shift I‚Äôd seen in programming during my career, comparable only to the shift to high-level languages, which even I am not old enough to have experienced. It was a fun chat and I really enjoyed myself. Gergely later joined myself James Lewis and Giles Edwards-Alexander at the Thoughtworks event the next day.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>My travels also took me to N√ºremberg, where I attended an internal conference for Siemens on the future of software architecture. When we think of technology, it‚Äôs easy to focus on the Faangs of Silicon Valley, but Siemens have a huge workforce of software developers working on heavy engineering systems like trains and factory automation. It was good to hear them talk about federated architectures, data mesh, and their use of AI.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Kent's graph of options vs features" src="https://martinfowler.com/articles/images/frags-2025-11/features.webp" width="400px" /></p>

<p>I‚Äôve often used pseudo-graphs to help explain why <a href="https://martinfowler.com/articles/is-quality-worth-cost.html">high quality software is cheaper</a>. This time, <a href="https://tidyfirst.substack.com/p/why-does-development-slow">Kent Beck</a> creates a unique perspective to this chart, dispensing with the temporal axis to help think in terms of optionality.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Heavy Cardboard banner" src="https://martinfowler.com/articles/images/frags-2025-11/hc.jpg" width="400px" /></p>

<p>And in another life, Edward has finally finished the great migration of the Heavy Cardboard studio and returns to the tubes with <a href="https://www.youtube.com/live/e8juRvm9h0c">our first game in the new digs</a>. (No surprise that it‚Äôs Age of Steam.)</p>

---

### [My Foreword to "Frictionless"](https://martinfowler.com/articles/frictionless-foreword.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/frictionless-foreword.html"><img src="https://martinfowler.com/articles/frictionless-card.jpg" width="" /></a></div>

<p>I find most writing on software productivity to be twaddle, but Nicole
      Forsgren and Abi Noda are notable exceptions. I had a chance to take a
      look at their new book, published today, and liked it so much I wrote a
      foreword.</p>

<p><a class="more" href="https://martinfowler.com/articles/frictionless-foreword.html">more‚Ä¶</a></p>

---

### [The Learning Loop and LLMs](https://martinfowler.com/articles/llm-learning-loop.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/llm-learning-loop.html"><img src="https://martinfowler.com/articles/llm-learning-loop/card.png" width="350px" /></a></div>

<p><b class="author">Unmesh Joshi</b> finds LLMs to be a useful tool, but
      explains why their help <a href="https://martinfowler.com/articles/llm-learning-loop.html">becomes illusory</a> if we use them to shortcut the
      learning loop that's an essential part of our professional practice.</p>

<p><a class="more" href="https://martinfowler.com/articles/llm-learning-loop.html">more‚Ä¶</a></p>

---

## HackerNews

### [Danish pension fund divesting US Treasuries](https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/)

**ÂàÜÊï∞**: 599 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46692594)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Level S4 solar radiation event](https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026)

**ÂàÜÊï∞**: 598 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684056)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Apple testing new App Store design that blurs the line between ads and results](https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/)

**ÂàÜÊï∞**: 593 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46680974)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [De-dollarization: Is the US dollar losing its dominance? (2025)](https://www.jpmorgan.com/insights/global-research/currencies/de-dollarization)

**ÂàÜÊï∞**: 562 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46693346)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I'm addicted to being useful](https://www.seangoedecke.com/addicted-to-being-useful/)

**ÂàÜÊï∞**: 481 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46690402)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Overcomplexity of the Shadcn Radio Button](https://paulmakeswebsites.com/writing/shadcn-radio-button/)

**ÂàÜÊï∞**: 480 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46688971)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Prediction markets are ushering in a world in which news becomes about gambling](https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/)

**ÂàÜÊï∞**: 474 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46670524)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [What came first: the CNAME or the A record?](https://blog.cloudflare.com/cname-a-record-order-dns-standards/)

**ÂàÜÊï∞**: 449 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46681611)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars](https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html)

**ÂàÜÊï∞**: 442 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46686640)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Flux 2 Klein pure C inference](https://github.com/antirez/flux2.c)

**ÂàÜÊï∞**: 442 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46670279)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nvidia Stock Crash Prediction](https://entropicthoughts.com/nvidia-stock-crash-prediction)

**ÂàÜÊï∞**: 341 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46693205)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Reticulum, a secure and anonymous mesh networking stack](https://github.com/markqvist/Reticulum)

**ÂàÜÊï∞**: 337 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46686273)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A 26,000-year astronomical monument hidden in plain sight (2019)](https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/)

**ÂàÜÊï∞**: 325 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695628)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Running Claude Code dangerously (safely)](https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/)

**ÂàÜÊï∞**: 274 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46690907)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Unconventional PostgreSQL Optimizations](https://hakibenita.com/postgresql-unconventional-optimizations)

**ÂàÜÊï∞**: 260 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46692116)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The coming industrialisation of exploit generation with LLMs](https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/)

**ÂàÜÊï∞**: 254 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46676081)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Giving university exams in the age of chatbots](https://ploum.net/2026-01-19-exam-with-chatbots.html)

**ÂàÜÊï∞**: 231 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46688954)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Linux kernel framework for PCIe device emulation, in userspace](https://github.com/cakehonolulu/pciem)

**ÂàÜÊï∞**: 218 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46689065)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The microstructure of wealth transfer in prediction markets](https://www.jbecker.dev/research/prediction-market-microstructure)

**ÂàÜÊï∞**: 216 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46680515)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [California is free of drought for the first time in 25 years](https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years)

**ÂàÜÊï∞**: 211 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46698660)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [San Francisco coyote swims to Alcatraz](https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php)

**ÂàÜÊï∞**: 210 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46674433)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Unix Pipe Card Game](https://punkx.org/unix-pipe-game/)

**ÂàÜÊï∞**: 176 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46694124)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [From Nevada to Kansas by Glider](https://www.weglide.org/flight/978820)

**ÂàÜÊï∞**: 171 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46641036)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I set all 376 Vim options and I'm still a fool](https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/)

**ÂàÜÊï∞**: 167 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46652690)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Scaling long-running autonomous coding](https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/)

**ÂàÜÊï∞**: 162 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46686418)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [3D printing my laptop ergonomic setup](https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/)

**ÂàÜÊï∞**: 157 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46686131)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [IP Addresses Through 2025](https://www.potaroo.net/ispcol/2026-01/addr2025.html)

**ÂàÜÊï∞**: 148 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46691835)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Instabridge has acquired Nova Launcher](https://novalauncher.com/nova-is-here-to-stay)

**ÂàÜÊï∞**: 124 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46696357)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Targeted Bets: An alternative approach to the job hunt](https://www.seanmuirhead.com/blog/targeted-bets)

**ÂàÜÊï∞**: 122 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684815)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [British redcoat's lost memoir reveals realities of life as a disabled veteran](https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html)

**ÂàÜÊï∞**: 122 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46642458)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The assistant axis: situating and stabilizing the character of LLMs](https://www.anthropic.com/research/assistant-axis)

**ÂàÜÊï∞**: 118 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684708)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [X For You Feed Algorithm](https://github.com/xai-org/x-algorithm)

**ÂàÜÊï∞**: 105 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46688173)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Improving the performance of WAT parser](https://blog.gplane.win/posts/improve-wat-parser-perf.html)

**ÂàÜÊï∞**: 104 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46629399)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Sending Data over Offline Finding Networks](https://cc-sw.com/find-my-and-find-hub-network-research/)

**ÂàÜÊï∞**: 101 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46622096)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [x86 prefixes and escape opcodes flowchart](https://soc.me/interfaces/x86-prefixes-and-escape-opcodes-flowchart.html)

**ÂàÜÊï∞**: 94 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46687705)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Zen of Reticulum](https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md)

**ÂàÜÊï∞**: 88 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46691660)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The world of Japanese snack bars](https://www.bbc.com/travel/article/20260116-inside-the-secret-world-of-japanese-snack-bars)

**ÂàÜÊï∞**: 85 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46698038)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Immigration Agencies Are Openly Defying Federal Courts](https://lpeproject.org/blog/immigration-agencies-are-openly-defying-federal-courts/)

**ÂàÜÊï∞**: 84 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695986)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Ocrbase ‚Äì pdf ‚Üí .md/.json document OCR and structured extraction API](https://github.com/majcheradam/ocrbase)

**ÂàÜÊï∞**: 81 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46691454)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The challenges of soft delete](https://atlas9.dev/blog/soft-delete.html)

**ÂàÜÊï∞**: 79 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46698061)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [KISS Launcher ‚Äì fast launcher for Android](https://kisslauncher.com/)

**ÂàÜÊï∞**: 79 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46690092)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs](https://github.com/mastra-ai/mastra)

**ÂàÜÊï∞**: 74 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46693959)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Google co-founder reveals that "many" of the new hires do not have a degree](https://www.yahoo.com/news/articles/google-cofounder-reveals-tons-recent-231500103.html)

**ÂàÜÊï∞**: 71 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695747)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The secret medieval tunnels that we still don't understand](https://weirdmedievalguys.substack.com/p/the-secret-medieval-tunnels-that)

**ÂàÜÊï∞**: 69 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46696683)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [IP over Avian Carriers with Quality of Service (1999)](https://www.rfc-editor.org/rfc/rfc2549.html)

**ÂàÜÊï∞**: 66 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46690530)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Maintenance: Of Everything, Part One](https://press.stripe.com/maintenance-part-one)

**ÂàÜÊï∞**: 61 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46696276)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: wxpath ‚Äì Declarative web crawling in XPath](https://github.com/rodricios/wxpath)

**ÂàÜÊï∞**: 58 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46618472)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Our approach to age prediction](https://openai.com/index/our-approach-to-age-prediction/)

**ÂàÜÊï∞**: 56 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46696699)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [LG UltraFine Evo 6K 32-inch Monitor Review](https://www.wired.com/review/lg-ultrafine-evo-6k-32-inch-monitor/)

**ÂàÜÊï∞**: 55 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46658116)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Electricity use of AI coding agents](https://www.simonpcouch.com/blog/2026-01-20-cc-impact/)

**ÂàÜÊï∞**: 52 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695415)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [King ‚Äì man + woman is queen; but why? (2017)](https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/)

**ÂàÜÊï∞**: 50 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46641145)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Alignment Game (2023)](https://dmvaldman.github.io/alignment-game/)

**ÂàÜÊï∞**: 49 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46641216)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Kahan on the 8087 and designing Intel's floating point (2016) [video]](https://www.youtube.com/watch?v=L-QVgbdt_qg)

**ÂàÜÊï∞**: 48 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46630718)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Cloudflare zero-day: Accessing any host globally](https://fearsoff.org/research/cloudflare-acme)

**ÂàÜÊï∞**: 44 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46693728)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [IPv6 is not insecure because it lacks a NAT](https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/)

**ÂàÜÊï∞**: 37 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46696303)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Squishy Go](https://puyogo.app/en/)

**ÂàÜÊï∞**: 35 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46650682)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A scammer's blueprint: How cybercriminals plot to rob a target in a week](https://www.reuters.com/graphics/SOUTHEASTASIA-SCAMS/MANUALS/klpyjlqelvg/)

**ÂàÜÊï∞**: 34 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46697287)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]](https://trendsfp.github.io/papers/tfp26-paper-12.pdf)

**ÂàÜÊï∞**: 34 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46646870)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Which AI Lies Best? A game theory classic designed by John Nash](https://so-long-sucker.vercel.app/)

**ÂàÜÊï∞**: 33 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46698370)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [When "likers'' go private: Engagement with reputationally risky content on X](https://arxiv.org/abs/2601.11140)

**ÂàÜÊï∞**: 33 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695978)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Agent Skills Leaderboard](https://skills.sh)

**ÂàÜÊï∞**: 30 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46697908)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Fast Concordance: Instant concordance on a corpus of >1,200 books](https://iafisher.com/concordance/)

**ÂàÜÊï∞**: 29 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46652569)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [RCS for Business](https://developers.google.com/business-communications/rcs-business-messaging)

**ÂàÜÊï∞**: 28 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46687841)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Avoiding fan traps in database design and system diagrams](https://www.ilograph.com/blog/posts/avoid-fan-traps-in-system-diagrams/)

**ÂàÜÊï∞**: 20 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46667336)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Driver killed and several injured after second train derails near Barcelona](https://www.bbc.com/news/articles/c1m78xl0gmpo)

**ÂàÜÊï∞**: 19 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46698539)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Provably unmasking malicious behavior through execution traces](https://arxiv.org/abs/2512.13821)

**ÂàÜÊï∞**: 18 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46698469)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Dockerhub for Skill.md](https://skillregistry.io/)

**ÂàÜÊï∞**: 17 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46692692)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Building Robust Helm Charts](https://www.willmunn.xyz/devops/helm/kubernetes/2026/01/17/building-robust-helm-charts.html)

**ÂàÜÊï∞**: 16 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46676708)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: TopicRadar ‚Äì Track trending topics across HN, GitHub, ArXiv, and more](https://apify.com/mick-johnson/topic-radar)

**ÂàÜÊï∞**: 15 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46692347)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Claude Chill: Fix Claude Code's Flickering in Terminal](https://github.com/davidbeesley/claude-chill)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46699072)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Are Arrays Functions?](https://futhark-lang.org/blog/2026-01-16-are-arrays-functions.html)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46675630)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Google Meet Reactions: Reverse Engineering the WebRTC Channel for Emoji](https://www.agilesoftwaredevelopment.com/en/posts/google-meet-reactions-webrtc/)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46697075)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Fence ‚Äì Sandbox CLI commands with network/filesystem restrictions](https://github.com/Use-Tusk/fence)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695467)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The fix for a segfault that never shipped](https://www.recall.ai/blog/the-fix-for-a-segfault-that-never-shipped)

**ÂàÜÊï∞**: 12 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46696081)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Web-based video editor powered by WebGPU](https://subformer.com/en-US/editor)

**ÂàÜÊï∞**: 10 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46695874)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

## LLM Infrastructure

### [v0.14.0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h2>Highlights</h2>
<p>This release features approximately 660 commits from 251 contributors (86 new contributors).</p>
<p><strong>Breaking Changes:</strong></p>
<ul>
<li><strong>Async scheduling is now enabled by default</strong> - Users who experience issues can disable with <code>--no-async-scheduling</code>.
<ul>
<li>Excludes some not-yet-supported configurations: pipeline parallel, CPU backend, non-MTP/Eagle spec decoding.</li>
</ul>
</li>
<li><strong>PyTorch 2.9.1</strong> is now required and the default wheel is compiled against cu129.</li>
<li>Deprecated quantization schemes have been removed (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31688">#31688</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31285">#31285</a>).</li>
<li>When using speculative decoding, unsupported sampling parameters will fail rather than being silently ignored (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31982">#31982</a>).</li>
</ul>
<p><strong>Key Improvements:</strong></p>
<ul>
<li><strong>Async scheduling enabled by default</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27614">#27614</a>): Overlaps engine core scheduling with GPU execution, improving throughput without user configuration. Now also works with speculative decoding (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31998">#31998</a>) and structured outputs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29821">#29821</a>).</li>
<li><strong>gRPC server entrypoint</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30190">#30190</a>): Alternative to REST API with binary protocol, HTTP/2 multiplexing.</li>
<li><strong><code>--max-model-len auto</code></strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29431">#29431</a>): Automatically fits context length to available GPU memory, eliminating OOM startup failures.</li>
<li><strong>Model inspection view</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29450">#29450</a>): View the modules, attention backends, and quantization of your model in vLLM by specifying <code>VLLM_LOG_MODEL_INSPECTION=1</code> or by simply printing the <code>LLM</code> object.</li>
<li><strong>Model Runner V2 enhancements</strong>: UVA block tables (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31965">#31965</a>), M-RoPE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32143">#32143</a>), <code>logit_bias</code>/<code>allowed_token_ids</code>/<code>min_tokens</code> support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32163">#32163</a>).
<ul>
<li>Please note that Model Runner V2 is still experimental and disabled by default.</li>
</ul>
</li>
</ul>
<h3>Model Support</h3>
<p><strong>New Model Architectures:</strong></p>
<ul>
<li>Grok-2 with tiktoken tokenizer (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31847">#31847</a>)</li>
<li>LFM2-VL vision-language model (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31758">#31758</a>)</li>
<li>MiMo-V2-Flash (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30836">#30836</a>)</li>
<li>openPangu MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28775">#28775</a>)</li>
<li>IQuestCoder (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31575">#31575</a>)</li>
<li>Nemotron Parse 1.1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30864">#30864</a>)</li>
<li>GLM-ASR audio (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31436">#31436</a>)</li>
<li>Isaac vision model v0.1/v0.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28367">#28367</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31550">#31550</a>)</li>
<li>Kanana-1.5-v-3b-instruct (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29384">#29384</a>)</li>
<li>K-EXAONE-236B-A23B MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31621">#31621</a>)</li>
</ul>
<p><strong>LoRA Support Expansion:</strong></p>
<ul>
<li>Multimodal tower/connector LoRA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26674">#26674</a>): LLaVA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31513">#31513</a>), BLIP2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31620">#31620</a>), PaliGemma (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31656">#31656</a>), Pixtral (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31724">#31724</a>), DotsOCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31825">#31825</a>), GLM4-V (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31652">#31652</a>)</li>
<li>DeepSeek-OCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31569">#31569</a>), Qwen3-Next (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31719">#31719</a>), NemotronH (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31539">#31539</a>), PLaMo 2/3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31322">#31322</a>)</li>
<li>Vision LoRA mm_processor_cache support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31927">#31927</a>)</li>
<li>MoE expert base_layer loading (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31104">#31104</a>)</li>
</ul>
<p><strong>Model Enhancements:</strong></p>
<ul>
<li>Qwen3-VL as reranker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31890">#31890</a>)</li>
<li>DeepSeek v3.2 chat prefix completion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31147">#31147</a>)</li>
<li>GLM-4.5/GLM-4.7 <code>enable_thinking: false</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31788">#31788</a>)</li>
<li>Ernie4.5-VL video timestamps (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31274">#31274</a>)</li>
<li>Score template expansion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31335">#31335</a>)</li>
<li>LLaMa4 vision encoder compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30709">#30709</a>)</li>
<li>NemotronH quantized attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31898">#31898</a>)</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Async scheduling default</strong> with spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27614">#27614</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31998">#31998</a>) and structured outputs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29821">#29821</a>)</li>
<li><strong>Hybrid allocator + KV connector</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30166">#30166</a>) with multiple KV cache groups (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31707">#31707</a>)</li>
<li>Triton attention: encoder-only/cross attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31406">#31406</a>), cross-layer blocks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30687">#30687</a>)</li>
<li>Mamba2 prefix cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28047">#28047</a>)</li>
<li>Batch invariant LoRA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30097">#30097</a>)</li>
<li>LoRA name in BlockStored for KV-cache reconstruction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27577">#27577</a>)</li>
<li>Request ID collision prevention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27987">#27987</a>)</li>
<li>Dense model DP without overhead (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30739">#30739</a>)</li>
<li>Async + spec decode penalties/bad_words (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30495">#30495</a>)</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<p><strong>CUTLASS MoE Optimizations:</strong></p>
<ul>
<li>2.9% throughput + 10.8% TTFT via fill(0) optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31754">#31754</a>)</li>
<li>5.3% throughput + 2.2% TTFT via problem size calculation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31830">#31830</a>)</li>
<li>Fused SiLU+Mul+Quant for NVFP4 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31832">#31832</a>)</li>
<li>NVFP4 stride fusion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31837">#31837</a>)</li>
</ul>
<p><strong>Other Performance:</strong></p>
<ul>
<li>GDN attention decode speedup (Qwen3-Next) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31722">#31722</a>)</li>
<li>Fused RoPE + MLA KV-cache write (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25774">#25774</a>)</li>
<li>Sliding window attention optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31984">#31984</a>)</li>
<li>FlashInfer DeepGEMM swapAB SM90 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29213">#29213</a>)</li>
<li>Unpermute-aware fused MoE + small-batch fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29354">#29354</a>)</li>
<li>GDN Attention blocking copy removal (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31167">#31167</a>)</li>
<li>FusedMoE LoRA small rank performance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32019">#32019</a>)</li>
<li>EPLB numpy optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29499">#29499</a>)</li>
<li>FlashInfer rotary for DeepSeek (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30729">#30729</a>)</li>
<li>Vectorized activations (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29512">#29512</a>)</li>
<li>NUMA interleaved memory (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30800">#30800</a>)</li>
<li>Async spec decode logprobs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31336">#31336</a>)</li>
</ul>
<p><strong>Hardware Configs:</strong></p>
<ul>
<li>SM103 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30705">#30705</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31150">#31150</a>)</li>
<li>B300 Blackwell MoE configs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30629">#30629</a>)</li>
<li>Qwen3-Next FP8 CUTLASS configs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29553">#29553</a>)</li>
<li>Qwen3Moe B200 Triton configs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31448">#31448</a>)</li>
<li>GLM-4.5/4.6 RTX Pro 6000 kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31407">#31407</a>)</li>
<li>MiniMax-M2/M2.1 QKNorm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31493">#31493</a>)</li>
<li>NVFP4 small batch tuning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30897">#30897</a>)</li>
</ul>
<p><strong>Platform:</strong></p>
<ul>
<li>ROCm: AITER RMSNorm fusion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26575">#26575</a>), MTP for AITER MLA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28624">#28624</a>), moriio connector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29304">#29304</a>), xgrammar upstream (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31327">#31327</a>)</li>
<li>XPU: FP8 streaming quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30944">#30944</a>), custom workers (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30935">#30935</a>)</li>
<li>CPU: Head sizes 80/112 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31968">#31968</a>), async disabled by default (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31525">#31525</a>), LoRA MoE CPU pinning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31317">#31317</a>)</li>
<li>TPU: tpu-inference path (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30808">#30808</a>), Sophgo docs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30949">#30949</a>)</li>
</ul>
<h3>Large Scale Serving</h3>
<ul>
<li><strong>XBO</strong> (Extended Dual-Batch Overlap) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30120">#30120</a>)</li>
<li><strong>NIXL asymmetric TP</strong> (P &gt; D tensor-parallel-size) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27274">#27274</a>)</li>
<li>NIXL heterogeneous BlockSize/kv_layout (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30275">#30275</a>)</li>
<li>Cross-layers KV layout for MultiConnector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30761">#30761</a>)</li>
<li>Mooncake protocol expansion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30133">#30133</a>)</li>
<li>LMCache KV cache registration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31397">#31397</a>)</li>
<li>EPLB default all2all backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30559">#30559</a>)</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>Marlin for Turing (sm75)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29901">#29901</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31000">#31000</a>)</li>
<li><strong>Quark int4-fp8 w4a8 MoE</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30071">#30071</a>)</li>
<li><strong>MXFP4 W4A16 dense models</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31926">#31926</a>)</li>
<li><strong>ModelOpt FP8 variants</strong> (FP8_PER_CHANNEL_PER_TOKEN, FP8_PB_WO) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30957">#30957</a>)</li>
<li>ModelOpt KV cache quantization update (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31895">#31895</a>)</li>
<li>NVFP4 Marlin for NVFP4A16 MoEs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30881">#30881</a>)</li>
<li>Static quant all group shapes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30833">#30833</a>)</li>
<li>Default MXFP4 LoRA backend: Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30598">#30598</a>)</li>
<li>compressed-tensors 0.13.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30799">#30799</a>)</li>
</ul>
<h3>API &amp; Frontend</h3>
<p><strong>New Features:</strong></p>
<ul>
<li>gRPC server (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30190">#30190</a>)</li>
<li><code>--max-model-len auto</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29431">#29431</a>)</li>
<li>Model inspection view (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29450">#29450</a>)</li>
<li>Offline FastAPI docs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30184">#30184</a>)</li>
<li><code>attention_config</code> in LLM() (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30710">#30710</a>)</li>
<li>MFU metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30738">#30738</a>)</li>
<li>Iteration logging + NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31193">#31193</a>)</li>
<li><code>reasoning_effort</code> parameter (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31956">#31956</a>)</li>
</ul>
<p><strong>Tool Calling:</strong></p>
<ul>
<li>FunctionGemma parser (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31218">#31218</a>)</li>
<li>GLM-4.7 parser (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30876">#30876</a>)</li>
<li>Kimi K2 update (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31207">#31207</a>)</li>
</ul>
<p><strong>CLI:</strong></p>
<ul>
<li><code>-ep</code> for <code>--enable-expert-parallel</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30890">#30890</a>)</li>
<li>Complete help messages (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31226">#31226</a>)</li>
<li>Bench serve auto-discovery + <code>--input-len</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30816">#30816</a>)</li>
<li>Spec decode acceptance stats (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31739">#31739</a>)</li>
<li><code>--enable-log-deltas</code> (renamed) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32020">#32020</a>)</li>
<li><code>--default-chat-template-kwargs</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31343">#31343</a>)</li>
</ul>
<p><strong>API:</strong></p>
<ul>
<li><code>/server_info</code> env info (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31899">#31899</a>)</li>
<li>MCP streaming in Responses API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31761">#31761</a>)</li>
<li><code>/embeddings</code> <code>continue_final_message</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31497">#31497</a>)</li>
<li>Reranking score templates (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30550">#30550</a>)</li>
<li>Chat template warmup (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30700">#30700</a>)</li>
<li>Configurable handshake timeout (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27444">#27444</a>)</li>
<li>Better 500 errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20610">#20610</a>)</li>
<li>Worker init logging (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29493">#29493</a>)</li>
<li>Bench error reporting (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31808">#31808</a>)</li>
<li>Corrupted video recovery (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29197">#29197</a>)</li>
<li>Spec-decode param validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31982">#31982</a>)</li>
<li>Validation error metadata (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30134">#30134</a>)</li>
</ul>
<h3>Security</h3>
<ul>
<li>Prevent token leaks in crash logs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30751">#30751</a>)</li>
<li><code>weights_only=True</code> in torch.load (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32045">#32045</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><strong>PyTorch 2.9.1</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28495">#28495</a>)</li>
<li>compressed-tensors 0.13.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30799">#30799</a>)</li>
<li>CUDA 13 LMCache/NIXL in Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30913">#30913</a>)</li>
<li>Configurable NVSHMEM version (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30732">#30732</a>)</li>
</ul>
<h3>Bug Fixes (User-Facing)</h3>
<ul>
<li>Invalid UTF-8 tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28874">#28874</a>)</li>
<li>CPU RoPE gibberish with <code>--enforce-eager</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31643">#31643</a>)</li>
<li>Tool call streaming finish chunk (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31438">#31438</a>)</li>
<li>Encoder cache leak CPU scheduling stuck (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31857">#31857</a>)</li>
<li>Engine crash: tools + response_format (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32127">#32127</a>)</li>
<li>Voxtral transcription API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31388">#31388</a>)</li>
<li>Safetensors download optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30537">#30537</a>)</li>
</ul>
<h3>Deprecations</h3>
<ul>
<li>Deprecated quantization schemes removed (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31688">#31688</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31285">#31285</a>)</li>
<li><code>seed_everything</code> deprecated (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31659">#31659</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>vllm-metal plugin docs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31174">#31174</a>)</li>
<li>Claude Code example (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31188">#31188</a>)</li>
<li>CustomOp developer guide (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30886">#30886</a>)</li>
</ul>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/penfree">@penfree</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30237">#30237</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jiangkuaixue123">@jiangkuaixue123</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30120">#30120</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jr-shen">@jr-shen</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29663">#29663</a></li>
<li><a class="user-mention notranslate" href="https://github.com/grzegorz-k-karch">@grzegorz-k-karch</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30795">#30795</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shanjiaz">@shanjiaz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30799">#30799</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Somoku">@Somoku</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29569">#29569</a></li>
<li><a class="user-mention notranslate" href="https://github.com/baoqian426">@baoqian426</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30841">#30841</a></li>
<li><a class="user-mention notranslate" href="https://github.com/SongDI911">@SongDI911</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30852">#30852</a></li>
<li><a class="user-mention notranslate" href="https://github.com/www-spam">@www-spam</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30827">#30827</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Xunzhuo">@Xunzhuo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30844">#30844</a></li>
<li><a class="user-mention notranslate" href="https://github.com/TheCodeWrangler">@TheCodeWrangler</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30700">#30700</a></li>
<li><a class="user-mention notranslate" href="https://github.com/SungMinCho">@SungMinCho</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30738">#30738</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wzyrrr">@wzyrrr</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30949">#30949</a></li>
<li><a class="user-mention notranslate" href="https://github.com/navmarri14">@navmarri14</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30629">#30629</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HaloWorld">@HaloWorld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30867">#30867</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jeffreywang-anyscale">@jeffreywang-anyscale</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31013">#31013</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AmeenP">@AmeenP</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31093">#31093</a></li>
<li><a class="user-mention notranslate" href="https://github.com/westers">@westers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31071">#31071</a></li>
<li><a class="user-mention notranslate" href="https://github.com/CedricHwong">@CedricHwong</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30957">#30957</a></li>
<li><a class="user-mention notranslate" href="https://github.com/c0de128">@c0de128</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31114">#31114</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bounty-hunter">@Bounty-hunter</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30242">#30242</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jzakrzew">@jzakrzew</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30550">#30550</a></li>
<li><a class="user-mention notranslate" href="https://github.com/1643661061leo">@1643661061leo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30760">#30760</a></li>
<li><a class="user-mention notranslate" href="https://github.com/NickCao">@NickCao</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30070">#30070</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amithkk">@amithkk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31212">#31212</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gateremark">@gateremark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31218">#31218</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Tiiiktak">@Tiiiktak</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31274">#31274</a></li>
<li><a class="user-mention notranslate" href="https://github.com/oscardev256">@oscardev256</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28367">#28367</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Jzz1943">@Jzz1943</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31448">#31448</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mratsim">@mratsim</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31407">#31407</a></li>
<li><a class="user-mention notranslate" href="https://github.com/twjww">@twjww</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31445">#31445</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amittell">@amittell</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31438">#31438</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ricky-chaoju">@ricky-chaoju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30184">#30184</a></li>
<li><a class="user-mention notranslate" href="https://github.com/effortprogrammer">@effortprogrammer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31343">#31343</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZT-AIA">@ZT-AIA</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31408">#31408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rogerxfeng8">@rogerxfeng8</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31522">#31522</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kevin-pw">@kevin-pw</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31497">#31497</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vintipandey">@vintipandey</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31505">#31505</a></li>
<li><a class="user-mention notranslate" href="https://github.com/SameerAsal">@SameerAsal</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31520">#31520</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Dylan1229">@Dylan1229</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31546">#31546</a></li>
<li><a class="user-mention notranslate" href="https://github.com/reaganjlee">@reaganjlee</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29105">#29105</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhima771">@zhima771</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31569">#31569</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jayhemnani9910">@jayhemnani9910</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31513">#31513</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Tmn07">@Tmn07</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31572">#31572</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vsourirajan">@vsourirajan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31549">#31549</a></li>
<li><a class="user-mention notranslate" href="https://github.com/labAxiaoming">@labAxiaoming</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31601">#31601</a></li>
<li><a class="user-mention notranslate" href="https://github.com/massif-01">@massif-01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31604">#31604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PHOEBEMOON0802">@PHOEBEMOON0802</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31147">#31147</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tpopp">@tpopp</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29993">#29993</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ppppqp">@ppppqp</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31620">#31620</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zzzzwwjj">@zzzzwwjj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31674">#31674</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Catacomba">@Catacomba</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30322">#30322</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kunpengW-code">@kunpengW-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31669">#31669</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johncalesp">@johncalesp</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28874">#28874</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BlankRH">@BlankRH</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31800">#31800</a></li>
<li><a class="user-mention notranslate" href="https://github.com/guicho271828">@guicho271828</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20610">#20610</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ReinforcedKnowledge">@ReinforcedKnowledge</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31055">#31055</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vSeamar">@vSeamar</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29197">#29197</a></li>
<li><a class="user-mention notranslate" href="https://github.com/A1c0r-Z">@A1c0r-Z</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31656">#31656</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MrIceCreamMan">@MrIceCreamMan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31465">#31465</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tianshu-Michael-yu">@tianshu-Michael-yu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31841">#31841</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiyu0824">@weiyu0824</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30808">#30808</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andyl98">@andyl98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31757">#31757</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaredforReal">@JaredforReal</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31779">#31779</a></li>
<li><a class="user-mention notranslate" href="https://github.com/katec846">@katec846</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29213">#29213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kfirtoledo">@kfirtoledo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30761">#30761</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ayobami-00">@Ayobami-00</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31868">#31868</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ShaanveerS">@ShaanveerS</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31825">#31825</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Zyyeric">@Zyyeric</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31652">#31652</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wangshangsam">@wangshangsam</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31775">#31775</a></li>
<li><a class="user-mention notranslate" href="https://github.com/devbyteai">@devbyteai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31536">#31536</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BJWang-ant">@BJWang-ant</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31719">#31719</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dangoldbj">@dangoldbj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31847">#31847</a></li>
<li><a class="user-mention notranslate" href="https://github.com/maylikenoother">@maylikenoother</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31610">#31610</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yxing-bj">@yxing-bj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31575">#31575</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xbfs">@xbfs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31948">#31948</a></li>
<li><a class="user-mention notranslate" href="https://github.com/RunkaiTao">@RunkaiTao</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29354">#29354</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AkshatSh">@AkshatSh</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31550">#31550</a></li>
<li><a class="user-mention notranslate" href="https://github.com/frelam">@frelam</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31857">#31857</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shyeh25">@shyeh25</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31617">#31617</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andikarachman">@andikarachman</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32092">#32092</a></li>
<li><a class="user-mention notranslate" href="https://github.com/minimAluminiumalism">@minimAluminiumalism</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32158">#32158</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andyzhangx">@andyzhangx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32185">#32185</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sanghoon-yn">@sanghoon-yn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/31956">#31956</a></li>
<li><a class="user-mention notranslate" href="https://github.com/potatosalad">@potatosalad</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/32212">#32212</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.13.0...v0.14.0"><tt>v0.13.0...v0.14.0</tt></a></p>

---

### [v0.14.0rc2: [CI] Fix LM Eval Large Models (H100) (#32423)](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Matthew Bonanni <a href="mailto:mbonanni@redhat.com">mbonanni@redhat.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/bcf2333cd6514543f579d9bb6d309c5b8a0bfd0d"><tt>bcf2333</tt></a>)</p>

---

### [v0.14.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#‚Ä¶</p>

---

### [v0.14.0rc0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony mode‚Ä¶</p>

---

### [v0.13.0](https://github.com/vllm-project/vllm/releases/tag/v0.13.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.13.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features <strong>442 commits from 207 contributors (61 new contributors)!</strong></p>
<p><strong>Breaking Changes</strong>: This release includes deprecation removals, PassConfig flag renames, and attention configuration changes from environment variables to CLI arguments. Please review the breaking changes section carefully before upgrading.</p>
<h3>Model Support</h3>
<ul>
<li><strong>New models</strong>: BAGEL (AR only) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28439">#28439</a>), AudioFlamingo3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a>), JAIS 2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a>), latent MoE architecture support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a>).</li>
<li><strong>Tool parsers</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29848">#29848</a>), Gigachat 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a>), Holo2 reasoning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30048">#30048</a>).</li>
<li><strong>Model enhancements</strong>: Qwen3-VL embeddings support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a>), Qwen3-VL EVS (Efficient Video Sampling) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29752">#29752</a>), DeepSeek V3.2 proper <code>drop_thinking</code> logic (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a>), DeepSeek V3.2 top-k fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27568">#27568</a>).</li>
<li><strong>Task expansion</strong>: Automatic TokenClassification model conversion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30666">#30666</a>), Ultravox v0.7 transformer projector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30089">#30089</a>).</li>
<li><strong>Quantization</strong>: BitsAndBytes for Qwen3-Omni-MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29896">#29896</a>).</li>
<li><strong>Speculative decoding</strong>: Eagle/Eagle3 Transformers backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30340">#30340</a>), Mamba <code>selective_state_update</code> spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29488">#29488</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Compilation</strong>: Conditional compilation via <code>compile_ranges</code> for selective kernel compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24252">#24252</a>).</li>
<li><strong>Prefix caching</strong>: xxHash high-performance hash option (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a>).</li>
<li><strong>Attention</strong>: PrefixLM support for FlexAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27938">#27938</a>) and TritonAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30386">#30386</a>), CUDA graphs for 3D Triton attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28306">#28306</a>), <code>TRITON_MLA</code> without prefix-caching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29125">#29125</a>).</li>
<li><strong>Batch invariance</strong>: FA2 and LoRA batch-invariant support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30018">#30018</a>).</li>
<li><strong>Pooling</strong>: Chunked prefill for ALL pooling tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27145">#27145</a>), multi-vector retrieval API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26686">#26686</a>).</li>
<li><strong>Model Runner V2</strong>: Min-p sampling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30171">#30171</a>), NaN detection in logits (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30187">#30187</a>).</li>
<li><strong>Speculative decoding</strong>: Medusa GPU-CPU sync avoidance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29723">#29723</a>), async spec-decode improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29624">#29624</a>).</li>
<li><strong>Whisper</strong>: Major performance improvements - <a href="https://github.com/vllm-project/vllm/issues/24946#issuecomment-3680725754">V1 is now faster than V0</a> (~3x speedup vs v0.12.0). Encoder batching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29421">#29421</a>), <code>FULL_DECODE_ONLY</code> CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30072">#30072</a>), CPU backend support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>).</li>
<li><strong>Performance</strong>: Fused blockwise quant RMS norm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27883">#27883</a>), MoE LoRA loading reduction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30243">#30243</a>), encoder cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30475">#30475</a>), CPU KV offloading streams (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29013">#29013</a>).</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li><strong>NVIDIA Blackwell Ultra</strong>: SM103 (GB300) support with CUDA 13 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30484">#30484</a>).</li>
<li><strong>DeepSeek optimizations</strong> (benchmarked on DeepSeek-V3.1):
<ul>
<li>DeepEP High-Throughput CUDA graph enabled by default: <strong>5.3% throughput, 4.4% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29558">#29558</a>)</li>
<li>DeepGEMM fused layout kernel: <strong>4.3% throughput, 10.7% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29546">#29546</a>)</li>
<li>DeepGEMM experts initialization: <strong>3.9% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30494">#30494</a>)</li>
<li><code>group_topk</code> kernel: <strong>1.9% throughput, 2.1% TPOT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30159">#30159</a>)</li>
<li>Sparse prefill kernel for FP8 KV-cache in DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27532">#27532</a>)</li>
<li>MLA FP8 optimization with ReduceScatterSum (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29795">#29795</a>), direct k_nope/k_pe copy (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29710">#29710</a>)</li>
</ul>
</li>
<li><strong>CPU</strong>: Whisper support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>), Arm Optimized Routines vectorized exp (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a>), x86 CPU wheel pipeline (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28848">#28848</a>).</li>
<li><strong>AMD ROCm</strong>: Aiter quantization kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25552">#25552</a>), torch.compile layernorm/silu + FP8 quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25693">#25693</a>), Triton ScaledMM fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26668">#26668</a>), MXFP4 w4a4 inference (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29775">#29775</a>).</li>
<li><strong>Intel XPU</strong>: wNa16 compressed tensors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29484">#29484</a>).</li>
<li><strong>Build</strong>: CUDA 13 aarch64 wheels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30341">#30341</a>), Docker kernel build stage (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a>), Ascend NPU Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30015">#30015</a>).</li>
</ul>
<h3>Large Scale Serving &amp; Disaggregated Prefill/Decode</h3>
<ul>
<li><strong>KV connectors</strong>: Mooncake Transfer Engine (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a>), cache reset via <code>/reset_prefix_cache</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27170">#27170</a>), KV events (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28309">#28309</a>), failure recovery config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26813">#26813</a>).</li>
<li><strong>NIXL</strong>: Compatibility checking in handshake (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29503">#29503</a>), large batch proxy support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28782">#28782</a>).</li>
<li><strong>EPLB</strong>: NVFP4 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a>), algorithm abstraction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a>).</li>
<li><strong>Multi-node</strong>: External launcher mode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29833">#29833</a>).</li>
<li><strong>Hybrid allocator</strong>: Optional KV connector integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29805">#29805</a>).</li>
<li><strong>Performance</strong>: silu_mul_per_token_group_quant_fp8 kernel for DP/EP (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29470">#29470</a>).</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>New</strong>: W4A8 grouped GEMM on Hopper (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29691">#29691</a>), online FP8 with streaming post-processing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a>), FP8 weight reloading for RLHF (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28480">#28480</a>).</li>
<li><strong>MoE + LoRA</strong>: AWQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30442">#30442</a>) and GPTQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a>) support.</li>
<li><strong>GGUF</strong>: MoE + GGUF restored for Qwen3 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30116">#30116</a>), Qwen2 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30307">#30307</a>), HF defaults override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30118">#30118</a>).</li>
<li><strong>Compatibility</strong>: Transformers v5 RoPE support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30046">#30046</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>: MCP type infrastructure (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30054">#30054</a>), Browser/Container MCP tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29989">#29989</a>), full MCP Python loop (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29798">#29798</a>), extra body parameters (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a>).</li>
<li><strong>Configuration</strong>: <code>AttentionConfig</code> replaces <code>VLLM_ATTENTION_BACKEND</code> env var (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>).</li>
<li><strong>Chat templates</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29837">#29837</a>), DeepSeek-V3.2 developer tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a>).</li>
<li><strong>Anthropic API</strong>: Streaming fixes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29971">#29971</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30266">#30266</a>).</li>
<li><strong>Embeddings</strong>: Binary format with <code>encoding_format=bytes_only</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30249">#30249</a>), multiple image/audio per request (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29988">#29988</a>), tokenization_kwargs override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29794">#29794</a>).</li>
<li><strong>Metrics</strong>: Prefill KV compute metric excluding cached tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30189">#30189</a>).</li>
<li><strong>Profiling</strong>: Layer-wise NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29990">#29990</a>), profiling CLI config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29912">#29912</a>).</li>
<li><strong>UX</strong>: Better OOM errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28051">#28051</a>), ModelConfig validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a>), distributed executor errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30140">#30140</a>).</li>
</ul>
<h3>Security</h3>
<ul>
<li>Additional protection for <a href="https://github.com/advisories/GHSA-mrw7-hf4f-83pf" title="CVE-2025-62164">CVE-2025-62164</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a>).</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>NVSHMEM 3.3.24 + CUDA 13 fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a>).</li>
<li>TPU tpu-inference 0.12.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30221">#30221</a>).</li>
</ul>
<h3>Breaking Changes &amp; Deprecations</h3>
<ol>
<li><strong>PassConfig flags renamed</strong> per RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/27995">#27995</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29646">#29646</a>)</li>
<li><strong>Attention env vars ‚Üí CLI args</strong>: <code>VLLM_ATTENTION_BACKEND</code> replaced with <code>--attention-backend</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>)</li>
<li><strong>Removed <code>-O.xx</code> flag</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29991">#29991</a>)</li>
<li><strong>Removed deprecated plugin/compilation fields</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30396">#30396</a>)</li>
<li><strong>Removed deprecated task, seed, MM settings</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30397">#30397</a>)</li>
<li><strong>Removed <code>embed_input_ids</code>/<code>embed_multimodal</code> fallbacks</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30458">#30458</a>)</li>
<li><strong>Removed tokenizer setter</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30400">#30400</a>)</li>
<li><strong>Deprecations</strong>: <code>merge_by_field_config</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30035">#30035</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30170">#30170</a>), <code>--convert reward</code> ‚Üí <code>--convert embed</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30463">#30463</a>)</li>
</ol>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/ajpqs">@ajpqs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amitz-nv">@amitz-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29978">#29978</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amrmahdi">@amrmahdi</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andrewbriand">@andrewbriand</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/anker-c2">@anker-c2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30344">#30344</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AuruTus">@AuruTus</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30182">#30182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/avigny">@avigny</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19425">#19425</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bhanu068">@Bhanu068</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a></li>
<li>@Copilot made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29025">#29025</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dbotwinick">@dbotwinick</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30583">#30583</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30234">#30234</a></li>
<li><a class="user-mention notranslate" href="https://github.com/desertfire">@desertfire</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29919">#29919</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dmitry-tokarev-nv">@dmitry-tokarev-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a></li>
<li><a class="user-mention notranslate" href="https://github.com/drslark">@drslark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30632">#30632</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dtcccc">@dtcccc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elizabetht">@elizabetht</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28671">#28671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Elm8116">@Elm8116</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gausah01">@gausah01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29604">#29604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gh-wf">@gh-wf</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30285">#30285</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hdlj-h">@hdlj-h</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30056">#30056</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HF-001">@HF-001</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30051">#30051</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hzxuzhonghu">@hzxuzhonghu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29931">#29931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaviS-Rei">@JaviS-Rei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29882">#29882</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johannesflommersfeld">@johannesflommersfeld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30390">#30390</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KevinMusgrave">@KevinMusgrave</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30529">#30529</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kitaekatt">@kitaekatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30408">#30408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lashahub">@lashahub</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a></li>
<li><a class="user-mention notranslate" href="https://github.com/LuminolT">@LuminolT</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a></li>
<li><a class="user-mention notranslate" href="https://github.com/majiayu000">@majiayu000</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30615">#30615</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MaoJianwei">@MaoJianwei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29797">#29797</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Mercykid-bash">@Mercykid-bash</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mgehre-amd">@mgehre-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30364">#30364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mivehk">@mivehk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30512">#30512</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mondaylord">@mondaylord</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30671">#30671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/noa-neria">@noa-neria</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29320">#29320</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PatrykSaffer">@PatrykSaffer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30330">#30330</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Peng-YM">@Peng-YM</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29074">#29074</a></li>
<li><a class="user-mention notranslate" href="https://github.com/realliujiaxu">@realliujiaxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30059">#30059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/redwrasse">@redwrasse</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29261">#29261</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ri0S">@Ri0S</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scratch-ml">@scratch-ml</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30351">#30351</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seokhyunan">@seokhyunan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30648">#30648</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shaharmor98">@shaharmor98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a></li>
<li><a class="user-mention notranslate" href="https://github.com/taoyun951753">@taoyun951753</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tom-zju">@tom-zju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30057">#30057</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tomtomjhj">@tomtomjhj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29692">#29692</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vkuzo">@vkuzo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vladnosiv">@vladnosiv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiguihua2">@weiguihua2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30042">#30042</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wenqiglantz">@wenqiglantz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wkcn">@wkcn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29879">#29879</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wu-kan">@wu-kan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21804">#21804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wz1qqx">@wz1qqx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30376">#30376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xyDong0223">@xyDong0223</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30455">#30455</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yifant-code">@yifant-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yjc9696">@yjc9696</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yurekami">@yurekami</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30552">#30552</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuttian1">@yuttian1</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30102">#30102</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhijianJiang">@ZhijianJiang</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30219">#30219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhiweiYan-96">@ZhiweiYan-96</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29773">#29773</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.12.0...v0.13.0"><tt>v0.12.0...v0.13.0</tt></a></p>

---

### [v0.13.0rc4: [v1] Add PrefixLM support to TritonAttention backend (#30386)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc4)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/74a1ac38b00a8cf502db085d1bbd77712cf47e41"><tt>74a1ac3</tt></a>)</p>

---

### [v0.13.0rc3: [XPU] fix broken fp8 online quantization for XPU platform (#30831)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc3)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Yan Ma <a href="mailto:yan.ma@intel.com">yan.ma@intel.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/4f735babb7353987137b85ec0465e594e9ed1384"><tt>4f735ba</tt></a>)</p>

---

### [v0.13.0rc2: [ROCm] [Bugfix] Fix torch sdpa hallucination (#30789)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: tjtanaa <a href="mailto:tunjian.tan@embeddedllm.com">tunjian.tan@embeddedllm.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/2410132bb1f9faa5b252fad3f2b83dc926946b08"><tt>2410132</tt></a>)</p>

---

### [v0.13.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>fake rc release to fix nightly wheels</p>

---

### [v0.12.0](https://github.com/vllm-project/vllm/releases/tag/v0.12.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.12.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features 474 commits from 213 contributors (57 new)ÔºÅ</p>
<p><strong>Breaking Changes</strong>: This release includes PyTorch 2.9.0 upgrade (CUDA 12.9), V0 deprecations including <code>xformers</code> backend, and scheduled removals - please review the changelog carefully.</p>
<p><strong>Major Features</strong>:</p>
<ul>
<li><strong>EAGLE Speculative Decoding Improvements</strong>: Multi-step CUDA graph support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>), DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>), and multimodal support with Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Significant Performance Optimizations</strong>: 18.1% throughput improvement from batch invariant BMM (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>), 2.2% throughput improvement from shared experts overlap (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>).</li>
<li><strong>AMD ROCm Expansion</strong>: DeepSeek v3.2 + SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>), FP8 MLA decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>), AITER attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>).</li>
</ul>
<h3>Model Support</h3>
<ul>
<li><strong>New model families</strong>: PLaMo-3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28834">#28834</a>), OpenCUA-7B (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a>), HunyuanOCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29327">#29327</a>), Mistral Large 3 and Ministral 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29757">#29757</a>).</li>
<li><strong>Format support</strong>: Gemma3 GGUF multimodal support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27772">#27772</a>).</li>
<li><strong>Multimodal enhancements</strong>: Qwen3 Omni audio-in-video support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27721">#27721</a>), Eagle3 multimodal support for Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Performance</strong>: QwenVL cos/sin cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28798">#28798</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li>
<p><strong>GPU Model Runner V2 (Experimental)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25266">#25266</a>): Complete refactoring of model execution pipeline:</p>
<ul>
<li>No "reordering" or complex bookkeeping with persistent batch removal</li>
<li>GPU-persistent block tables for better scalability with <code>max_model_len</code> and <code>num_kv_groups</code></li>
<li>Triton-native sampler: no -1 temperature hack, efficient per-request seeds, memory-efficient prompt logprobs</li>
<li>Simplified DP and CUDA graph implementations</li>
<li>Efficient structured outputs support</li>
</ul>
</li>
<li>
<p><strong>Prefill Context Parallel (PCP) (Preparatory)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28718">#28718</a>): Partitions the sequence dimension during prefill for improved long-sequence inference. Complements existing Decode Context Parallel (DCP). See RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25749">#25749</a> for details.</p>
</li>
<li>
<p><strong>RLHF Support</strong>: Pause and Resume Generation for Asynchronous RL Training (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28037">#28037</a>).</p>
</li>
<li>
<p><strong>KV Cache Enhancements</strong>: Cross-layer KV blocks support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27743">#27743</a>), KV cache residency metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27793">#27793</a>).</p>
</li>
<li>
<p><strong>Audio support</strong>: Audio embeddings support in chat completions (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a>).</p>
</li>
<li>
<p><strong>Speculative Decoding</strong>:</p>
<ul>
<li>Multi-step Eagle with CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>)</li>
<li>EAGLE DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>)</li>
<li>EAGLE3 heads without <code>use_aux_hidden_states</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27688">#27688</a>)</li>
<li>Eagle multimodal CUDA graphs with MRoPE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a>)</li>
<li>Logprobs support with spec decode + async scheduling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29223">#29223</a>)</li>
</ul>
</li>
<li>
<p><strong>Configuration</strong>: Flexible <code>inputs_embeds_size</code> separate from <code>hidden_size</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29741">#29741</a>), <code>--fully-sharded-loras</code> for fused_moe (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28761">#28761</a>).</p>
</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li>
<p><strong>NVIDIA Performance</strong>:</p>
<ul>
<li><strong>Batch invariant BMM optimization</strong>: 18.1% throughput improvement, 10.7% TTFT improvement on DeepSeek-V3.1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>)</li>
<li><strong>Shared Experts Overlap with FlashInfer DeepGEMM</strong>: 2.2% throughput improvement, 3.6% TTFT improvement at batch size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>)</li>
<li>DeepGEMM N dim restriction reduced from 128 to 64 multiplier (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28687">#28687</a>)</li>
<li>DeepEP low-latency with round-robin expert placement (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28449">#28449</a>)</li>
<li>NVFP4 MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>H200 Fused MoE Config improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28992">#28992</a>)</li>
</ul>
</li>
<li>
<p><strong>AMD ROCm</strong>:</p>
<ul>
<li>DeepSeek v3.2 and SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>)</li>
<li>FP8 MLA decode support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>)</li>
<li>AITER sampling ops integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26084">#26084</a>)</li>
<li>AITER triton attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>)</li>
<li>Bitsandbytes quantization on AMD GPUs with warp size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27307">#27307</a>)</li>
<li>Fastsafetensors support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28225">#28225</a>)</li>
<li>Sliding window support for AiterFlashAttentionBackend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29234">#29234</a>)</li>
<li>Whisper v1 with Aiter Unified/Flash Attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a>)</li>
</ul>
</li>
<li>
<p><strong>CPU</strong>:</p>
<ul>
<li>Paged attention GEMM acceleration on ARM CPUs with NEON (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29193">#29193</a>)</li>
<li>Parallelize over tokens in int4 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29600">#29600</a>)</li>
<li>CPU all reduce optimization for async_scheduling + DP&gt;1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29311">#29311</a>)</li>
</ul>
</li>
<li>
<p><strong>Attention</strong>: FlashAttention ViT support, now default backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28763">#28763</a>).</p>
</li>
<li>
<p><strong>Long Context</strong>: Optimized <code>gather_and_maybe_dequant_cache</code> kernel for extremely long sequences (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28029">#28029</a>).</p>
</li>
<li>
<p><strong>Multi-NUMA</strong>: Enhanced NUMA functionality for systems with multiple NUMA nodes per socket (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a>).</p>
</li>
<li>
<p><strong>Docker</strong>: Image size reduced by ~200MB (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29060">#29060</a>).</p>
</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>W4A8</strong>: Marlin kernel support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24722">#24722</a>).</li>
<li><strong>NVFP4</strong>:
<ul>
<li>MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>TRTLLM MoE NVFP4 kernel (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a>)</li>
<li>CuteDSL MoE with NVFP4 DeepEP dispatch (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27141">#27141</a>)</li>
<li>Non-gated activations support in modelopt path (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a>)</li>
</ul>
</li>
<li><strong>AWQ</strong>: Compressed-tensors AWQ support for Turing GPUs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29732">#29732</a>).</li>
<li><strong>LoRA</strong>: FusedMoE LoRA Triton kernel for MXFP4 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29708">#29708</a>).</li>
<li><strong>Online quantization</strong>: Moved to <code>model.load_weights</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26327">#26327</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>:
<ul>
<li>Multi-turn support for non-harmony requests (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a>)</li>
<li>Reasoning item input parsing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28248">#28248</a>)</li>
</ul>
</li>
<li><strong>Tool Calling</strong>:
<ul>
<li>Parsed tool arguments support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a>)</li>
<li><code>parallel_tool_calls</code> param compliance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26233">#26233</a>)</li>
<li>Tool filtering support in ToolServer (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29224">#29224</a>)</li>
</ul>
</li>
<li><strong>Whisper</strong>: <code>verbose_json</code> and <code>timestamp</code> features for transcription/translation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24209">#24209</a>).</li>
<li><strong>Sampling</strong>: Flat logprob control moved from env var to <code>SamplingParams</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28914">#28914</a>).</li>
<li><strong>GGUF</strong>: Improved HuggingFace loading UX with <code>repo_id:quant_type</code> syntax (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a>).</li>
<li><strong>Profiling</strong>: Iteration-level profiling for Torch and CUDA profiler (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28987">#28987</a>).</li>
<li><strong>Logs</strong>: Colorized log output (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29017">#29017</a>).</li>
<li><strong>Optimization Levels</strong>: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code> allow trading startup time for performance, more compilation flags will be added in future releases (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26847">#26847</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><strong>PyTorch 2.9.0</strong> with CUDA 12.9 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24994">#24994</a>) - <strong>Breaking change</strong> requiring environment updates.</li>
<li><strong>xgrammar</strong>: Updated to 0.1.27 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28221">#28221</a>).</li>
<li><strong>Transformers</strong>: Updated to 4.57.3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29418">#29418</a>), preparation for v5 with <code>rope_parameters</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28542">#28542</a>).</li>
<li><strong>XPU</strong>: torch &amp; IPEX 2.9 upgrade (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29307">#29307</a>).</li>
</ul>
<h3>V0 Deprecation &amp; Breaking Changes</h3>
<p><strong>Removed Parameters</strong>:</p>
<ul>
<li><code>num_lookahead_slots</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29000">#29000</a>)</li>
<li><code>best_of</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29090">#29090</a>)</li>
<li>LoRA extra vocab (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28545">#28545</a>)</li>
</ul>
<p><strong>Deprecated</strong>:</p>
<ul>
<li><code>xformers</code> backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29262">#29262</a>)</li>
<li><code>seed=None</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29185">#29185</a>)</li>
</ul>
<p><strong>Scheduled Removals</strong> (will be removed in future release):</p>
<ul>
<li><code>ParallelConfig</code>'s direct child EPLB fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29324">#29324</a>)</li>
<li><code>guided_*</code> config fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29326">#29326</a>)</li>
<li><code>override_pooler_config</code> and <code>disable_log_requests</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29402">#29402</a>)</li>
<li><code>CompilationConfig.use_inductor</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29323">#29323</a>)</li>
<li>Deprecated metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29330">#29330</a>)</li>
</ul>
<p><strong>Other Breaking Changes</strong>:</p>
<ul>
<li>PyTorch 2.9.0 upgrade requires CUDA 12.9 environment</li>
<li>Mistral format auto-detection for model loading (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28659">#28659</a>)</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/jesse996">@jesse996</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28846">#28846</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Nepherpitou">@Nepherpitou</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28960">#28960</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Samoed">@Samoed</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27329">#27329</a></li>
<li><a class="user-mention notranslate" href="https://github.com/j20120307">@j20120307</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28999">#28999</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vnadathur">@vnadathur</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26468">#26468</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhyajie">@zhyajie</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28942">#28942</a></li>
<li><a class="user-mention notranslate" href="https://github.com/IzzyPutterman">@IzzyPutterman</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock-amd">@rjrock-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28905">#28905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zq1997">@zq1997</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27715">#27715</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shengliangxu">@shengliangxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28076">#28076</a></li>
<li><a class="user-mention notranslate" href="https://github.com/prashanth058">@prashanth058</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28972">#28972</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qgallouedec">@qgallouedec</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhanggzh">@zhanggzh</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19347">#19347</a></li>
<li><a class="user-mention notranslate" href="https://github.com/pandalee99">@pandalee99</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26628">#26628</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dsuhinin">@dsuhinin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29100">#29100</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xli">@xli</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29124">#29124</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jeremyteboul">@jeremyteboul</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/soodoshll">@soodoshll</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28875">#28875</a></li>
<li><a class="user-mention notranslate" href="https://github.com/bhagyashrigai">@bhagyashrigai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28957">#28957</a></li>
<li><a class="user-mention notranslate" href="https://github.com/skaraban3807">@skaraban3807</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Victor49152">@Victor49152</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock">@rjrock</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29205">#29205</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FlintyLemming">@FlintyLemming</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29182">#29182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/madskildegaard">@madskildegaard</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nandan2003">@nandan2003</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29189">#29189</a></li>
<li><a class="user-mention notranslate" href="https://github.com/michaelact">@michaelact</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29173">#29173</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yongming-qin">@yongming-qin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28958">#28958</a></li>
<li><a class="user-mention notranslate" href="https://github.com/joshiemoore">@joshiemoore</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29249">#29249</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lim4349">@lim4349</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/apinge">@apinge</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gbyu-amd">@gbyu-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kflu">@kflu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29364">#29364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Inokinoki">@Inokinoki</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29200">#29200</a></li>
<li><a class="user-mention notranslate" href="https://github.com/GOavi101">@GOavi101</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29313">#29313</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sts07142">@sts07142</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ivanium">@ivanium</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29143">#29143</a></li>
<li><a class="user-mention notranslate" href="https://github.com/geodavic">@geodavic</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28795">#28795</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Yejing-Lai">@Yejing-Lai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29473">#29473</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Adityayxt">@Adityayxt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29491">#29491</a></li>
<li><a class="user-mention notranslate" href="https://github.com/guodongxiaren">@guodongxiaren</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29620">#29620</a></li>
<li><a class="user-mention notranslate" href="https://github.com/askliar">@askliar</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29426">#29426</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scydas">@scydas</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29589">#29589</a></li>
<li><a class="user-mention notranslate" href="https://github.com/EanWang211123">@EanWang211123</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qGentry">@qGentry</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29506">#29506</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HappyAmazonian">@HappyAmazonian</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29335">#29335</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rgommers">@rgommers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29241">#29241</a></li>
<li><a class="user-mention notranslate" href="https://github.com/staugust">@staugust</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28840">#28840</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mertunsall">@mertunsall</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29667">#29667</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dublc">@dublc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29728">#29728</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nwaughachukwuma">@nwaughachukwuma</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29671">#29671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BowTen">@BowTen</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29731">#29731</a></li>
<li><a class="user-mention notranslate" href="https://github.com/omera-nv">@omera-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhangruoxu">@zhangruoxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29568">#29568</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KKKZOZ">@KKKZOZ</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29783">#29783</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FredericOdermatt">@FredericOdermatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29784">#29784</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Abdennacer-Badaoui">@Abdennacer-Badaoui</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29782">#29782</a></li>
<li><a class="user-mention notranslate" href="https://github.com/knlnguyen1802">@knlnguyen1802</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28525">#28525</a></li>
<li><a class="user-mention notranslate" href="https://github.com/finbarrtimbers">@finbarrtimbers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29796">#29796</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hholtmann">@hholtmann</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29711">#29711</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.11.1...v0.12.0"><tt>v0.11.1...v0.12.0</tt></a></p>

---

## Operating Systems

### [Ryabitsev: Tracking kernel development with korgalore](https://lwn.net/Articles/1055219/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Konstantin Ryabitsev has put up <a href="https://people.kernel.org/monsieuricon/tracking-kernel-development-with-korgalore">a
blog post about korgalore</a>, a tool he has written to circumvent delivery
problems experienced by kernel developers using the large, centralized
email systems.
<p>
<blockquote class="bq">
	We cannot fix email delivery, but we can sidestep it
	entirely. Public-inbox archives like lore.kernel.org store all
	mailing list traffic in git repositories. In its simplest
	configuration, korgalore can shallow-clone these repositories
	directly and upload any new messages straight to your mailbox using
	the provider's API.
</blockquote>

---

### [Remote authentication bypass in telnetd](https://lwn.net/Articles/1055213/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: One would assume that most LWN readers stopped running network-accessible
telnet services some number of decades ago.  For the rest of you, <a href="https://lwn.net/ml/all/87h5sg9yfs.fsf@josefsson.org">this security advisory from
Simon Josefsson</a> is worthy of note:
<p>
<blockquote class="bq">
	The telnetd server invokes /usr/bin/login (normally running as
	root) passing the value of the USER environment variable received
	from the client as the last parameter.
<p>
	If the client supplies a carefully crafted USER environment value
	being the string "-f root", and passes the telnet(1) -a or --login
	parameter to send this USER environment to the server, the client
	will be automatically logged in as root bypassing normal
	authentication processes.
</blockquote>

---

### [Mozilla introduces Firefox Nightly RPM package repository](https://lwn.net/Articles/1055191/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>Mozilla has <a href="https://blog.nightly.mozilla.org/2026/01/19/introducing-mozillas-firefox-nightly-rpm-package-for-rpm-based-linux-distributions/">announced</a>
a repository with <a href="https://www.firefox.com/en-US/channel/desktop/">Firefox
Nightly channel</a> packages for RPM-based Linux distributions such as CentOS
Stream, Fedora, and openSUSE. Mozilla has provided a Debian repository
since 2023.</p>

<p>Note that this repository only includes the nightly builds of The
<tt>firefox-nightly</tt> package. Mozilla is not providing stable
builds as RPMs at this time. However, the package will not conflict
with a distribution's regular <tt>firefox</tt> package; both packages
can be installed at the same time for those who wish to test the
nightly builds. See the blog post for instructions on setting up the
repository.</p>

<p></p>

---

### [[$] An alternate path for immutable distributions](https://lwn.net/Articles/1054216/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>
LWN has had a number of articles on immutable distributions,
such as <a href="https://lwn.net/Articles/954059/">Bluefin</a> and <a href="https://lwn.net/Articles/1046228/">
Bazzite</a>, in recent years. These distributions have taken a variety of approaches, including
using
<a href="https://coreos.github.io/rpm-ostree/">
rpm-ostree</a>, filesystem snapshots, and
<a href="https://lwn.net/Articles/979182/">bootable container (bootc) images</a>. But those
approaches, especially the latter, lead to extra complexity for a user
attempting to install new software, instead of just
using the existing package manager.
<a href="https://github.com/ashos/ashos?tab=readme-ov-file#ashos-any-snapshot-hierarchical-os">
AshOS</a> (Any Snapshot Hierarchical OS) is an experimental AGPL-3-licensed
"<q>meta-distribution</q>" that tried a different approach more in line with
traditional package management. Although the project is no longer updated,
it remains usable, and can still shed some light on a potential alternate path for users
worried about adopting bootc-based approaches.
</p>

---

### [Security updates for Tuesday](https://lwn.net/Articles/1055152/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Security updates have been issued by <b>AlmaLinux</b> (gpsd-minimal, jmc, kernel, kernel-rt, and net-snmp), <b>Debian</b> (apache-log4j2 and dcmtk), <b>Fedora</b> (exim, gpsd, mysql8.0, mysql8.4, python-biopython, and rust-lru), <b>Mageia</b> (firefox, nss and thunderbird), <b>Oracle</b> (container-tools:rhel8, gpsd-minimal, jmc, kernel, net-snmp, and uek-kernel), <b>Red Hat</b> (net-snmp), <b>SUSE</b> (chromium, go, harfbuzz-devel, kernel, libsoup, rust1.91, rust1.92, and thunderbird), and <b>Ubuntu</b> (apache2, avahi, and python-urllib3).

---

## Systems

### [A data model for Git (and other docs updates)](https://jvns.ca/blog/2026/01/08/a-data-model-for-git/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! This past fall, I decided to take some time to work on Git&rsquo;s
documentation. I&rsquo;ve been thinking about working on open source docs for a long
time &ndash; usually if I think the documentation for something could be improved,
I&rsquo;ll write a blog post or a zine or something. But this time I wondered: could I
instead make a few improvements to the official documentation?</p>
<p>So <a href="https://marieflanagan.com/">Marie</a> and I made a few changes to the Git
documentation!</p>
<h3 id="a-data-model-for-git">a data model for Git</h3>
<p>After a while working on the documentation, we noticed that Git uses the terms
&ldquo;object&rdquo;, &ldquo;reference&rdquo;, or &ldquo;index&rdquo; in its documentation a lot, but that it didn&rsquo;t
have a great explanation of what those terms mean or how they relate to other
core concepts like &ldquo;commit&rdquo; and &ldquo;branch&rdquo;. So we wrote a new &ldquo;data model&rdquo; document!</p>
<p>You can <a href="https://github.com/git/git/blob/master/Documentation/gitdatamodel.adoc">read the data model here for now</a>.
I assume at some point (after the next release?) it&rsquo;ll also be on the <a href="https://git-scm.com">Git website</a>.</p>
<p>I&rsquo;m excited about this because understanding how Git organizes its commit and
branch data has really helped me reason about how Git works over the years,
and I think it&rsquo;s important to have a short (1600 words!) version of the data
model that&rsquo;s accurate.</p>
<p>The &ldquo;accurate&rdquo; part turned out to not be that easy: I knew the basics of how
Git&rsquo;s data model worked, but during the review process I learned some new
details and had to make quite a few changes (for example how merge conflicts are
stored in the staging area).</p>
<h3 id="updates-to-git-push-git-pull-and-more">updates to <code>git push</code>, <code>git pull</code>, and more</h3>
<p>I also worked on updating the introduction to some of Git&rsquo;s core man pages.
I quickly realized that &ldquo;just try to improve it according to my best judgement&rdquo;
was not going to work: why should the maintainers believe me that my version is
better?</p>
<p>I&rsquo;ve seen a problem a lot when discussing open source documentation changes
where 2 expert users of the software argue about whether an explanation
is clear or not (&ldquo;I think X would be a good way to explain it! Well, I think Y
would be better!&rdquo;)</p>
<p>I don&rsquo;t think this is very productive (expert users of a piece of software
are notoriously bad at being able to tell if an explanation will be clear to
non-experts), so I needed to find a way to identify problems with the man
pages that was a little more evidence-based.</p>
<h3 id="getting-test-readers-to-identify-problems">getting test readers to identify problems</h3>
<p>I asked for test readers on Mastodon to read the current version of
documentation and tell me what they find confusing or what questions they have.
About 80 test readers left comments, and I learned so much!</p>
<p>People left a huge amount of great feedback, for example:</p>
<ul>
<li>terminology they didn&rsquo;t understand (what&rsquo;s a pathspec? what does &ldquo;reference&rdquo; mean? does &ldquo;upstream&rdquo; have a specific meaning in Git?)</li>
<li>specific confusing sentences</li>
<li>suggestions of things things to add (&ldquo;I do X all the time, I think it should be included here&rdquo;)</li>
<li>inconsistencies (&ldquo;here it implies X is the default, but elsewhere it implies Y is the default&rdquo;)</li>
</ul>
<p>Most of the test readers had been using Git for at least 5-10 years, which
I think worked well &ndash; if a group of test readers who have been using Git
regularly for 5+ years find a sentence or term impossible to understand, it
makes it easy to argue that the documentation should be updated to make it
clearer.</p>
<p>I thought this &ldquo;get users of the software to comment on the existing
documentation and then fix the problems they find&rdquo; pattern worked really
well and I&rsquo;m excited about potentially trying it again in the future.</p>
<h3 id="the-man-page-changes">the man page changes</h3>
<p>We ended updating these 4 man pages:</p>
<ul>
<li><code>git add</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-add.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-add.adoc">after</a>)</li>
<li><code>git checkout</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-checkout.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-checkout.adoc">after</a>)</li>
<li><code>git push</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-push.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc">after</a>)</li>
<li><code>git pull</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-pull.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-pull.adoc">after</a>)</li>
</ul>
<p>The <code>git push</code> and <code>git pull</code> changes were the most interesting to me: in
addition to updating the intro to those pages, we also ended up writing:</p>
<ul>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/urls-remotes.adoc#upstream-branches">a section describing what the term &ldquo;upstream branch&rdquo; means</a> (which previously wasn&rsquo;t really explained)</li>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc#options">a cleaned-up description of what a &ldquo;push refspec&rdquo; is</a></li>
</ul>
<p>Making those changes really gave me an appreciation for how much work it is
to maintain open source documentation: it&rsquo;s not easy to write things that are
both clear and true, and sometimes we had to make compromises, for example the sentence
&ldquo;<code>git push</code> may fail if you haven‚Äôt set an upstream for the current branch,
depending on what <code>push.default</code> is set to.&rdquo; is a little vague, but the exact
details of what &ldquo;depending&rdquo; means are really complicated and untangling that is
a big project.</p>
<h3 id="on-the-process-for-contributing-to-git">on the process for contributing to Git</h3>
<p>It took me a while to understand Git&rsquo;s development process.
I&rsquo;m not going to try to describe it here (that could be a whole other post!), but a few quick notes:</p>
<ul>
<li>Git has a <a href="https://git-scm.com/community#discord">Discord server</a>
with a &ldquo;my first contribution&rdquo; channel for help with getting started contributing.
I found people to be very welcoming on the Discord.</li>
<li>I used <a href="https://gitgitgadget.github.io/">GitGitGadget</a> to make all of my contributions.
This meant that I could make a GitHub pull request (a workflow I&rsquo;m comfortable
with) and GitGitGadget would convert my PRs into the system the Git developers
use (emails with patches attached). GitGitGadget worked great and I was very
grateful to not have to learn how to send patches by email with Git.</li>
<li>Otherwise I used my normal email client (Fastmail&rsquo;s web interface) to reply
to emails, wrapping my text to 80 character lines since that&rsquo;s the mailing
list norm.</li>
</ul>
<p>I also found the mailing list archives on <a href="https://lore.kernel.org/git/">lore.kernel.org</a>
hard to navigate, so I hacked together <a href="https://github.com/jvns/git-list-viewer">my own git list viewer</a>
to make it easier to read the long mailing list threads.</p>
<p>Many people helped me navigate the contribution process and review the changes:
thanks to Emily Shaffer, Johannes Schindelin (the author of GitGitGadget),
Patrick Steinhardt, Ben Knoble, Junio Hamano, and more.</p>
<p>(I&rsquo;m experimenting with <a href="https://comments.jvns.ca/post/115861337435768520">comments on Mastodon, you can see the comments here</a>)</p>

---

### [Notes on switching to Helix from vim](https://jvns.ca/blog/2025/10/10/notes-on-switching-to-helix-from-vim/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Earlier this summer I was talking to a friend about how much I
<a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">love using fish</a>, and
how I love that I don&rsquo;t have to configure it. They said that they feel the same
way about the <a href="https://helix-editor.com/">helix</a> text editor, and so I decided
to give it a try.</p>
<p>I&rsquo;ve been using it for 3 months now and here are a few notes.</p>
<h3 id="why-helix-language-servers">why helix: language servers</h3>
<p>I think what motivated me to try Helix is that I&rsquo;ve been trying to get a working
language server setup (so I can do things like &ldquo;go to definition&rdquo;) and getting
a setup that feels good in Vim or Neovim just felt like too much work.</p>
<p>After using Vim/Neovim for 20 years, I&rsquo;ve tried both &ldquo;build my own custom
configuration from scratch&rdquo; and &ldquo;use someone else&rsquo;s pre-buld configuration
system&rdquo; and even though I love Vim I was excited about having things just work
without having to work on my configuration at all.</p>
<p>Helix comes with built in language server support, and it feels nice
to be able to do things like &ldquo;rename this symbol&rdquo; in any language.</p>
<h3 id="the-search-is-great">the search is great</h3>
<p>One of my favourite things about Helix is the search! If I&rsquo;m searching all the
files in my repository for a string, it lets me scroll through the potential
matching files and see the full context of the match, like this:</p>
<img src="https://jvns.ca/images/helix-search.png" />
<p>For comparison, here&rsquo;s what the vim ripgrep plugin I&rsquo;ve been using looks like:</p>
<img src="https://jvns.ca/images/vim-ripgrep.png" />
<p>There&rsquo;s no context for what else is around that line.</p>
<h3 id="the-quick-reference-is-nice">the quick reference is nice</h3>
<p>One thing I like about Helix is that when I press <code>g</code>, I get a little help popup
telling me places I can go. I really appreciate this because I don&rsquo;t often use
the &ldquo;go to definition&rdquo; or &ldquo;go to reference&rdquo; feature and I often forget the
keyboard shortcut.</p>
<img src="https://jvns.ca/images/goto.png" width="300px" />
<h3 id="some-vim-helix-translations">some vim -&gt; helix translations</h3>
<ul>
<li>Helix doesn&rsquo;t have marks like <code>ma</code>, <code>'a</code>, instead I&rsquo;ve been using <code>Ctrl+O</code> and
<code>Ctrl+I</code> to go back (or forward) to the last cursor location</li>
<li>I think Helix does have macros, but I&rsquo;ve been using multiple cursors in every
case that I would have previously used a macro. I like multiple cursors a lot
more than writing macros all the time. If I want to batch change something in
the document, my workflow is to press <code>%</code> (to highlight everything), then <code>s</code>
to select (with a regex) the things I want to change, then I can just edit
all of them as needed.</li>
<li>Helix doesn&rsquo;t have neovim-style tabs, instead it has a nice buffer switcher (<code>&lt;space&gt;b</code>)
I can use to switch to the buffer I want. There&rsquo;s a
<a href="https://github.com/helix-editor/helix/pull/7109">pull request here</a> to implement neovim-style tabs.
There&rsquo;s also a setting <code>bufferline=&quot;multiple&quot;</code> which can act a bit like tabs
with <code>gp</code>, <code>gn</code> for prev/next &ldquo;tab&rdquo; and <code>:bc</code> to close a &ldquo;tab&rdquo;.</li>
</ul>
<h3 id="some-helix-annoyances">some helix annoyances</h3>
<p>Here&rsquo;s everything that&rsquo;s annoyed me about Helix so far.</p>
<ul>
<li>I like the way Helix&rsquo;s <code>:reflow</code> works much less than how
vim reflows text with <code>gq</code>. It doesn&rsquo;t work as well with lists. (<a href="https://github.com/helix-editor/helix/issues/3332">github issue</a>)</li>
<li>If I&rsquo;m making a Markdown list, pressing &ldquo;enter&rdquo; at the end of a list item
won&rsquo;t continue the list. There&rsquo;s a <a href="https://github.com/helix-editor/helix/wiki/Recipes#continue-markdown-lists--quotes">partial workaround</a>
for bulleted lists but I don&rsquo;t know one for numbered lists.</li>
<li>No persistent undo yet: in vim I could use an
<a href="https://vimdoc.sourceforge.net/htmldoc/options.html#'undofile'">undofile</a> so
that I could undo changes even after quitting. Helix doesn&rsquo;t have that feature yet.
(<a href="https://github.com/helix-editor/helix/pull/9154">github PR</a>)</li>
<li>Helix doesn&rsquo;t autoreload files after they change on disk, I have to run
<code>:reload-all</code> (<code>:ra&lt;tab&gt;</code>) to manually reload them. Not a big deal.</li>
<li>Sometimes it crashes, maybe every week or so. I think it might be
<a href="https://github.com/helix-editor/helix/issues/12582">this issue</a>.</li>
</ul>
<p>The &ldquo;markdown list&rdquo; and reflowing issues come up a lot for me because I spend
a lot of time editing Markdown lists, but I keep using Helix anyway so I guess
they can&rsquo;t be making me that mad.</p>
<h3 id="switching-was-easier-than-i-thought">switching was easier than I thought</h3>
<p>I was worried that relearning 20 years of Vim muscle memory would be really hard.</p>
<p>It turned out to be easier than I expected, I started using Helix on a
vacation for a little low-stakes coding project I was doing on the side and
after a week or two it didn&rsquo;t feel so disorienting anymore. I think it might be
hard to switch back and forth between Vim and Helix, but I haven&rsquo;t needed to use
Vim recently so I don&rsquo;t know if that&rsquo;ll ever become an issue for me.</p>
<p>The first time I tried Helix I tried to force it to use keybindings that were
more similar to Vim and that did not work for me. Just learning the &ldquo;Helix way&rdquo;
was a lot easier.</p>
<p>There are still some things that throw me off: for example <code>w</code> in vim and <code>w</code> in
Helix don&rsquo;t have the same idea of what a &ldquo;word&rdquo; is (the Helix one includes the
space after the word, the Vim one doesn&rsquo;t).</p>
<h3 id="using-a-terminal-based-text-editor">using a terminal-based text editor</h3>
<p>For many years I&rsquo;d mostly been using a GUI version of vim/neovim, so switching
to actually using an editor in the terminal was a bit of an adjustment.</p>
<p>I ended up deciding on:</p>
<ol>
<li>Every project gets its own terminal window, and all of the tabs in that
window (mostly) have the same working directory</li>
<li>I make my Helix tab the first tab in the terminal window</li>
</ol>
<p>It works pretty well, I might actually like it better than my previous workflow.</p>
<h3 id="my-configuration">my configuration</h3>
<p>I appreciate that my configuration is really simple, compared to my neovim
configuration which is hundreds of lines. It&rsquo;s mostly just 4 keyboard
shortcuts.</p>
<pre><code>theme = &quot;solarized_light&quot;
[editor]
# Sync clipboard with system clipboard
default-yank-register = &quot;+&quot;

[keys.normal]
# I didn't like that Ctrl+C was the default &quot;toggle comments&quot; shortcut
&quot;#&quot; = &quot;toggle_comments&quot;

# I didn't feel like learning a different way
# to go to the beginning/end of a line so
# I remapped ^ and $
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.select]
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.normal.space]
# I write a lot of text so I need to constantly reflow,
# and missed vim's `gq` shortcut
l = &quot;:reflow&quot;
</code></pre>
<p>There&rsquo;s a separate <code>languages.toml</code> configuration where I set some language
preferences, like turning off autoformatting.
For example, here&rsquo;s my Python configuration:</p>
<pre><code>[[language]]
name = &quot;python&quot;
formatter = { command = &quot;black&quot;, args = [&quot;--stdin-filename&quot;, &quot;%{buffer_name}&quot;, &quot;-&quot;] }
language-servers = [&quot;pyright&quot;]
auto-format = false
</code></pre>
<h3 id="we-ll-see-how-it-goes">we&rsquo;ll see how it goes</h3>
<p>Three months is not that long, and it&rsquo;s possible that I&rsquo;ll decide to go back
to Vim at some point. For example, I wrote a <a href="https://jvns.ca/blog/2023/02/28/some-notes-on-using-nix/">post about switching to
nix</a> a while back but
after maybe 8 months I switched back to Homebrew (though I&rsquo;m still using NixOS
to manage one little server, and I&rsquo;m still satisfied with that).</p>

---

### [New zine: The Secret Rules of the Terminal](https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! After many months of writing deep dive blog posts about the terminal, on
Tuesday I released a new zine called &ldquo;The Secret Rules of the Terminal&rdquo;!</p>
<p>You can get it for $12 here:
<a href="https://wizardzines.com/zines/terminal">https://wizardzines.com/zines/terminal</a>, or get
an <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</p>
<p>Here&rsquo;s the cover:</p>
<div align="center">
<a href="https://wizardzines.com/zines/terminal">
  <img src="https://jvns.ca/images/terminal-cover-small.jpg" width="600px" />
  </a>
</div>
<h3 id="the-table-of-contents">the table of contents</h3>
<p>Here&rsquo;s the table of contents:</p>
<a href="https://wizardzines.com/zines/terminal/toc.png">
  <img src="https://jvns.ca/images/terminal-toc-small.png" width="600px" />
</a>
<h3 id="why-the-terminal">why the terminal?</h3>
<p>I&rsquo;ve been using the terminal every day for 20 years but even though I&rsquo;m very
confident in the terminal, I&rsquo;ve always had a bit of an uneasy feeling about it.
Usually things work fine, but sometimes something goes wrong and it just feels
like investigating it is impossible, or at least like it would open up a huge
can of worms.</p>
<p>So I started trying to write down a list of weird problems I&rsquo;ve run into in terminal and I realized
that the terminal has a lot of tiny inconsistencies like:</p>
<ul>
<li>sometimes you can use the arrow keys to move around, but sometimes pressing the arrow keys just prints <code>^[[D</code></li>
<li>sometimes you can use the mouse to select text, but sometimes you can&rsquo;t</li>
<li>sometimes your commands get saved to a history when you run them, and sometimes they don&rsquo;t</li>
<li>some shells let you use the up arrow to see the previous command, and some don&rsquo;t</li>
</ul>
<p>If you use the terminal daily for 10 or 20 years, even if you don&rsquo;t understand
exactly <em>why</em> these things happen, you&rsquo;ll probably build an intuition for them.</p>
<p>But having an intuition for them isn&rsquo;t the same as understanding why they
happen. When writing this zine I actually had to do a lot of work to figure out
exactly what was <em>happening</em> in the terminal to be able to talk about how to
reason about it.</p>
<h3 id="the-rules-aren-t-written-down-anywhere">the rules aren&rsquo;t written down anywhere</h3>
<p>It turns out that the &ldquo;rules&rdquo; for how the terminal works (how do
you edit a command you type in? how do you quit a program? how do you fix your
colours?) are extremely hard to fully understand, because &ldquo;the terminal&rdquo; is actually
made of many different pieces of software (your terminal emulator, your
operating system, your shell, the core utilities like <code>grep</code>, and every other random
terminal program you&rsquo;ve installed) which are written by different people with different
ideas about how things should work.</p>
<p>So I wanted to write something that would explain:</p>
<ul>
<li>how the 4 pieces of the terminal (your shell, terminal emulator, programs, and TTY driver) fit together to make everything work</li>
<li>some of the core conventions for how you can expect things in your terminal to work</li>
<li>lots of tips and tricks for how to use terminal programs</li>
</ul>
<h3 id="this-zine-explains-the-most-useful-parts-of-terminal-internals">this zine explains the most useful parts of terminal internals</h3>
<p>Terminal internals are a mess. A lot of it is just the way it is because
someone made a decision in the 80s and now it&rsquo;s impossible to change, and
honestly I don&rsquo;t think learning everything about terminal internals is worth
it.</p>
<p>But some parts are not that hard to understand and can really make your
experience in the terminal better, like:</p>
<ul>
<li>if you understand what <strong>your shell</strong> is responsible for, you can configure your shell (or use a different one!) to access your history more easily, get great tab completion, and so much more</li>
<li>if you understand <strong>escape codes</strong>, it&rsquo;s much less scary when <code>cat</code>ing a binary to stdout messes up your terminal, you can just type <code>reset</code> and move on</li>
<li>if you understand how <strong>colour</strong> works, you can get rid of bad colour contrast in your terminal so you can actually read the text</li>
</ul>
<h3 id="i-learned-a-surprising-amount-writing-this-zine">I learned a surprising amount writing this zine</h3>
<p>When I wrote <a href="https://wizardzines.com/zines/git">How Git Works</a>, I thought I
knew how Git worked, and I was right. But the terminal is different. Even
though I feel totally confident in the terminal and even though I&rsquo;ve used it
every day for 20 years, I had a lot of misunderstandings about how the terminal
works and (unless you&rsquo;re the author of <code>tmux</code> or something) I think there&rsquo;s a
good chance you do too.</p>
<p>A few things I learned that are actually useful to me:</p>
<ul>
<li>I understand the structure of the terminal better and so I feel more
confident debugging weird terminal stuff that happens to me (I was even able
to suggest a <a href="https://github.com/fish-shell/fish-shell/issues/10834">small improvement</a> to fish!). Identifying exactly which piece of software is causing a weird thing to happen in my terminal still isn&rsquo;t <em>easy</em> but I&rsquo;m a lot better at it now.</li>
<li>you can write a shell script to <a href="https://jvns.ca/til/vim-osc52/">copy to your clipboard over SSH</a></li>
<li>how <code>reset</code> works under the hood (it does the equivalent of <code>stty sane; sleep 1; tput reset</code>) ‚Äì basically I learned that I don&rsquo;t ever need to worry about
remembering <code>stty sane</code> or <code>tput reset</code> and I can just run <code>reset</code> instead</li>
<li>how to look at the invisible escape codes that a program is printing out (run <code>unbuffer program &gt; out; less out</code>)</li>
<li>why the builtin REPLs on my Mac like <code>sqlite3</code> are so annoying to use (they use <code>libedit</code> instead of <code>readline</code>)</li>
</ul>
<h3 id="blog-posts-i-wrote-along-the-way">blog posts I wrote along the way</h3>
<p>As usual these days I wrote a bunch of blog posts about various side quests:</p>
<ul>
<li><a href="https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/">How to add a directory to your PATH</a></li>
<li><a href="https://jvns.ca/blog/2024/11/26/terminal-rules/">&ldquo;rules&rdquo; that terminal problems follow</a></li>
<li><a href="https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/">why pipes sometimes get &ldquo;stuck&rdquo;: buffering</a></li>
<li><a href="https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/">some terminal frustrations</a></li>
<li><a href="https://jvns.ca/blog/2024/10/31/ascii-control-characters/">ASCII control characters in my terminal</a> on &ldquo;what&rsquo;s the deal with Ctrl+A, Ctrl+B, Ctrl+C, etc?&rdquo;</li>
<li><a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a></li>
<li><a href="https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/">what&rsquo;s involved in getting a &ldquo;modern&rdquo; terminal setup?</a></li>
<li><a href="https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/">reasons to use your shell&rsquo;s job control</a></li>
<li><a href="https://jvns.ca/blog/2025/03/07/escape-code-standards/">standards for ANSI escape codes</a>, which is really me trying to figure out if I think the <code>terminfo</code> database is serving us well today</li>
</ul>
<h3 id="people-who-helped-with-this-zine">people who helped with this zine</h3>
<p>A long time ago I used to write zines mostly by myself but with every project I get more
and more help. I met with <a href="https://marieflanagan.com">Marie Claire LeBlanc Flanagan</a> every weekday from September to June to work
on this one.</p>
<p>The cover is by Vladimir Ka≈°ikoviƒá,
Lesley Trites did copy editing,
Simon Tatham (who wrote <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a>) did technical review, our
Operations Manager Lee did the transcription as well as a million other
things, and <a href="https://github.com/doy">Jesse Luehrs</a> (who is one of the very few
people I know who actually understands the terminal&rsquo;s cursed inner workings)
had so many incredibly helpful conversations with me about what is going on in
the terminal.</p>
<h3 id="get-the-zine">get the zine</h3>
<p>Here are some links to get the zine again:</p>
<ul>
<li>get <a href="https://wizardzines.com/zines/terminal">The Secret Rules of the Terminal</a></li>
<li>get a <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</li>
</ul>
<p>As always, you can get either a PDF version to print at home or a print version
shipped to your house. The only caveat is print orders will ship in <strong>August</strong> &ndash; I
need to wait for orders to come in to get an idea of how many I should print
before sending it to the printer.</p>

---

### [Using `make` to compile C programs (for non-C-programmers)](https://jvns.ca/blog/2025/06/10/how-to-compile-a-c-program/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I have never been a C programmer but every so often I need to compile a C/C++
program from source. This has been kind of a struggle for me: for a
long time, my approach was basically &ldquo;install the dependencies, run <code>make</code>, if
it doesn&rsquo;t work, either try to find a binary someone has compiled or give up&rdquo;.</p>
<p>&ldquo;Hope someone else has compiled it&rdquo; worked pretty well when I was running Linux
but since I&rsquo;ve been using a Mac for the last couple of years I&rsquo;ve been running
into more situations where I have to actually compile programs myself.</p>
<p>So let&rsquo;s talk about what you might have to do to compile a C program! I&rsquo;ll use
a couple of examples of specific C programs I&rsquo;ve compiled and talk about a few
things that can go wrong. Here are three programs we&rsquo;ll be talking about
compiling:</p>
<ul>
<li><a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a></li>
<li><a href="https://www.sqlite.org/download.html">sqlite</a></li>
<li><a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> (a pager you can run to quickly open files from a search with <code>rg -n THING | qf</code>)</li>
</ul>
<h3 id="step-1-install-a-c-compiler">step 1: install a C compiler</h3>
<p>This is pretty simple: on an Ubuntu system if I don&rsquo;t already have a C compiler I&rsquo;ll install one with:</p>
<pre><code>sudo apt-get install build-essential
</code></pre>
<p>This installs <code>gcc</code>, <code>g++</code>, and <code>make</code>. The situation on a Mac is more
confusing but it&rsquo;s something like &ldquo;install xcode command line tools&rdquo;.</p>
<h3 id="step-2-install-the-program-s-dependencies">step 2: install the program&rsquo;s dependencies</h3>
<p>Unlike some newer programming languages, C doesn&rsquo;t have a dependency manager.
So if a program has any dependencies, you need to hunt them down yourself.
Thankfully because of this, C programmers usually keep their dependencies very
minimal and often the dependencies will be available in whatever package manager you&rsquo;re using.</p>
<p>There&rsquo;s almost always a section explaining how to get the dependencies in the
README, for example in <a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a>&rsquo;s README, it
says:</p>
<blockquote>
<p>To compile PaperJam, you need the headers for the libqpdf and libpaper libraries (usually available as libqpdf-dev and libpaper-dev packages).</p>
</blockquote>
<blockquote>
<p>You may need <code>a2x</code> (found in <a href="http://www.methods.co.nz/asciidoc/a2x.1.html">AsciiDoc</a>) for building manual pages.</p>
</blockquote>
<p>So on a Debian-based system you can install the dependencies like this.</p>
<pre><code>sudo apt install -y libqpdf-dev libpaper-dev
</code></pre>
<p>If a README gives a name for a package (like <code>libqpdf-dev</code>), I&rsquo;d basically
always assume that they mean &ldquo;in a Debian-based Linux distro&rdquo;: if you&rsquo;re on a
Mac <code>brew install libqpdf-dev</code> will not work. I still have not 100% gotten
the hang of developing on a Mac yet so I don&rsquo;t have many tips there yet. I
guess in this case it would be <code>brew install qpdf</code> if you&rsquo;re using Homebrew.</p>
<h3 id="step-3-run-configure-if-needed">step 3: run <code>./configure</code> (if needed)</h3>
<p>Some C programs come with a <code>Makefile</code> and some instead come with a script called
<code>./configure</code>. For example, if you download <a href="https://www.sqlite.org/download.html">sqlite&rsquo;s source code</a>, it has a <code>./configure</code> script in
it instead of a Makefile.</p>
<p>My understanding of this <code>./configure</code> script is:</p>
<ol>
<li>You run it, it prints out a lot of somewhat inscrutable output, and then it
either generates a <code>Makefile</code> or fails because you&rsquo;re missing some
dependency</li>
<li>The <code>./configure</code> script is part of a system called
<a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">autotools</a>
that I have never needed to learn anything about beyond &ldquo;run it to generate
a <code>Makefile</code>&rdquo;.</li>
</ol>
<p>I think there might be some options you can pass to get the <code>./configure</code>
script to produce a different <code>Makefile</code> but I have never done that.</p>
<h3 id="step-4-run-make">step 4: run <code>make</code></h3>
<p>The next step is to run <code>make</code> to try to build a program. Some notes about
<code>make</code>:</p>
<ul>
<li>Sometimes you can run <code>make -j8</code> to parallelize the build and make it go
faster</li>
<li>It usually prints out a million compiler warnings when compiling the program.
I always just ignore them. I didn&rsquo;t write the software! The compiler warnings
are not my problem.</li>
</ul>
<h3 id="compiler-errors-are-often-dependency-problems">compiler errors are often dependency problems</h3>
<p>Here&rsquo;s an error I got while compiling <code>paperjam</code> on my Mac:</p>
<pre><code>/opt/homebrew/Cellar/qpdf/12.0.0/include/qpdf/InputSource.hh:85:19: error: function definition does not declare parameters
   85 |     qpdf_offset_t last_offset{0};
      |                   ^
</code></pre>
<p>Over the years I&rsquo;ve learned it&rsquo;s usually best not to overthink problems like
this: if it&rsquo;s talking about <code>qpdf</code>, there&rsquo;s a good change it just means that
I&rsquo;ve done something wrong with how I&rsquo;m including the <code>qpdf</code> dependency.</p>
<p>Now let&rsquo;s talk about some ways to get the <code>qpdf</code> dependency included in the right way.</p>
<h3 id="the-world-s-shortest-introduction-to-the-compiler-and-linker">the world&rsquo;s shortest introduction to the compiler and linker</h3>
<p>Before we talk about how to fix dependency problems: building C programs is split into 2
steps:</p>
<ol>
<li><strong>Compiling</strong> the code into <strong>object files</strong> (with <code>gcc</code> or <code>clang</code>)</li>
<li><strong>Linking</strong> those object files into a final binary (with <code>ld</code>)</li>
</ol>
<p>It&rsquo;s important to know this when building a C program because sometimes you
need to pass the right flags to the compiler and linker to tell them where to
find the dependencies for the program you&rsquo;re compiling.</p>
<h3 id="make-uses-environment-variables-to-configure-the-compiler-and-linker"><code>make</code> uses environment variables to configure the compiler and linker</h3>
<p>If I run <code>make</code> on my Mac to install <code>paperjam</code>, I get this error:</p>
<pre><code>c++ -o paperjam paperjam.o pdf-tools.o parse.o cmds.o pdf.o -lqpdf -lpaper
ld: library 'qpdf' not found
</code></pre>
<p>This is not because <code>qpdf</code> is not installed on my system (it actually is!). But
the compiler and linker don&rsquo;t know how to <em>find</em> the <code>qpdf</code> library. To fix this, we need to:</p>
<ul>
<li>pass <code>&quot;-I/opt/homebrew/include&quot;</code> to the compiler (to tell it where to find the header files)</li>
<li>pass <code>&quot;-L/opt/homebrew/lib -liconv&quot;</code> to the linker (to tell it where to find library files and to link in <code>iconv</code>)</li>
</ul>
<p>And we can get <code>make</code> to pass those extra parameters to the compiler and linker using environment variables!
To see how this works: inside <code>paperjam</code>&rsquo;s Makefile you can see a bunch of environment variables, like <code>LDLIBS</code> here:</p>
<pre><code>paperjam: $(OBJS)
	$(LD) -o $@ $^ $(LDLIBS)
</code></pre>
<p>Everything you put into the <code>LDLIBS</code> environment variable gets passed to the
linker (<code>ld</code>) as a command line argument.</p>
<h3 id="secret-environment-variable-cppflags">secret environment variable: <code>CPPFLAGS</code></h3>
<p><code>Makefiles</code> sometimes define their own environment variables that they pass to
the compiler/linker, but <code>make</code> also has a bunch of &ldquo;implicit&rdquo; environment
variables which it will automatically pass to the C compiler and linker. There&rsquo;s a <a href="https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html#index-CFLAGS0">full list of implicit environment variables here</a>,
but one of them is <code>CPPFLAGS</code>, which gets automatically passed to the C compiler.</p>
<p>(technically it would be more normal to use <code>CXXFLAGS</code> for this, but this
particular <code>Makefile</code> hardcodes <code>CXXFLAGS</code> so setting <code>CPPFLAGS</code> was the only
way I could find to set the compiler flags without editing the <code>Makefile</code>)</p>
<small>
As an aside: it took me a long time to realize how closely tied to C/C++ `make` is -- I used
to think that `make` was just a general build system (and of course you can use it for
anything!) but it has a lot of affordances for building C/C++ programs that it
doesn't have for building any other kind of program.
</small>
<h3 id="two-ways-to-pass-environment-variables-to-make">two ways to pass environment variables to <code>make</code></h3>
<p>I learned thanks to <a href="https://www.owlfolio.org/">@zwol</a> that there are actually two ways to pass environment variables to <code>make</code>:</p>
<ol>
<li><code>CXXFLAGS=xyz make</code> (the usual way)</li>
<li><code>make CXXFLAGS=xyz</code></li>
</ol>
<p>The difference between them is that <code>make CXXFLAGS=xyz</code> will override the
value of <code>CXXFLAGS</code> set in the <code>Makefile</code> but <code>CXXFLAGS=xyz make</code> won&rsquo;t.</p>
<p>I&rsquo;m not sure which way is the norm but I&rsquo;m going to use the first way in this
post.</p>
<h3 id="how-to-use-cppflags-and-ldlibs-to-fix-this-compiler-error">how to use <code>CPPFLAGS</code> and <code>LDLIBS</code> to fix this compiler error</h3>
<p>Now that we&rsquo;ve talked about how <code>CPPFLAGS</code> and <code>LDLIBS</code> get passed to the
compiler and linker, here&rsquo;s the final incantation that I used to get the
program to build successfully!</p>
<pre><code>CPPFLAGS=&quot;-I/opt/homebrew/include&quot; LDLIBS=&quot;-L/opt/homebrew/lib -liconv&quot; make paperjam
</code></pre>
<p>This passes <code>-I/opt/homebrew/include</code> to the compiler and <code>-L/opt/homebrew/lib -liconv</code> to the linker.</p>
<p>Also I don&rsquo;t want to pretend that I &ldquo;magically&rdquo; knew that those were the right
arguments to pass, figuring them out involved a bunch of confused Googling that I
skipped over in this post. I will say that:</p>
<ul>
<li>the <code>-I</code> compiler flag tells the compiler which directory to find header files in, like <code>/opt/homebrew/include/qpdf/QPDF.hh</code></li>
<li>the <code>-L</code> linker flag tells the linker which directory to find libraries in, like <code>/opt/homebrew/lib/libqpdf.a</code></li>
<li>the <code>-l</code> linker flag tells the linker which libraries to link in, like <code>-liconv</code> means &ldquo;link in the <code>iconv</code> library&rdquo;, or <code>-lm</code> means &ldquo;link <code>math</code>&rdquo;</li>
</ul>
<h3 id="tip-how-to-just-build-1-specific-file-make-filename">tip: how to just build 1 specific file: <code>make $FILENAME</code></h3>
<p>Yesterday I discovered this cool tool called
<a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> which you can use to quickly
open files from the output of <code>ripgrep</code>.</p>
<p><code>qf</code> is in a big directory of various tools, but I only wanted to compile <code>qf</code>.
So I just compiled <code>qf</code>, like this:</p>
<pre><code>make qf
</code></pre>
<p>Basically if you know (or can guess) the output filename of the file you&rsquo;re
trying to build, you can tell <code>make</code> to just build that file by running <code>make $FILENAME</code></p>
<h3 id="tip-you-don-t-need-a-makefile">tip: you don&rsquo;t need a Makefile</h3>
<p>I sometimes write 5-line C programs with no dependencies, and I just learned
that if I have a file called <code>blah.c</code>, I can just compile it like this without creating a <code>Makefile</code>:</p>
<pre><code>make blah
</code></pre>
<p>It gets automaticaly expanded to <code>cc -o blah blah.c</code>, which saves a bit of
typing. I have no idea if I&rsquo;m going to remember this (I might just keep typing
<code>gcc -o blah blah.c</code> anyway) but it seems like a fun trick.</p>
<h3 id="tip-look-at-how-other-packaging-systems-built-the-same-c-program">tip: look at how other packaging systems built the same C program</h3>
<p>If you&rsquo;re having trouble building a C program, maybe other people had problems building it
too! Every Linux distribution has build files for every package that they
build, so even if you can&rsquo;t install packages from that distribution directly,
maybe you can get tips from that Linux distro for how to build the package.
Realizing this (thanks to my friend Dave) was a huge ah-ha moment for me.</p>
<p>For example, <a href="https://github.com/NixOS/nixpkgs/blob/405624e81a9b65378328accb0a11c3e5369e651c/pkgs/by-name/pa/paperjam/package.nix#L35">this line from the nix package for <code>paperjam</code></a> says:</p>
<pre><code>  env.NIX_LDFLAGS = lib.optionalString stdenv.hostPlatform.isDarwin &quot;-liconv&quot;;
</code></pre>
<p>This is basically saying &ldquo;pass the linker flag <code>-liconv</code> to build this on a
Mac&rdquo;, so that&rsquo;s a clue we could use to build it.</p>
<p>That same file also says <code>  env.NIX_CFLAGS_COMPILE = &quot;-DPOINTERHOLDER_TRANSITION=1&quot;;</code>. I&rsquo;m not sure what this means, but when I try
to build the <code>paperjam</code> package I do get an error about something called a
<code>PointerHolder</code>, so I guess that&rsquo;s somehow related to the &ldquo;PointerHolder
transition&rdquo;.</p>
<h3 id="step-5-installing-the-binary">step 5: installing the binary</h3>
<p>Once you&rsquo;ve managed to compile the program, probably you want to install it somewhere!
Some <code>Makefile</code>s have an <code>install</code> target that let you install the tool on your
system with <code>make install</code>. I&rsquo;m always a bit scared of this (where is it going
to put the files? what if I want to uninstall them later?), so if I&rsquo;m compiling
a pretty simple program I&rsquo;ll often just manually copy the binary to install it
instead, like this:</p>
<pre><code>cp qf ~/bin
</code></pre>
<h3 id="step-6-maybe-make-your-own-package">step 6: maybe make your own package!</h3>
<p>Once I figured out how to do all of this, I realized that I could use my new
<code>make</code> knowledge to contribute a <code>paperjam</code> package to Homebrew! Then I could
just <code>brew install paperjam</code> on future systems.</p>
<p>The good thing is that even if the details of how all of the different
packaging systems, they fundamentally all use C compilers and linkers.</p>
<h3 id="it-can-be-useful-to-understand-a-little-about-c-even-if-you-re-not-a-c-programmer">it can be useful to understand a little about C even if you&rsquo;re not a C programmer</h3>
<p>I think all of this is an interesting example of how it can useful to
understand some basics of how C programs work (like &ldquo;they have header files&rdquo;)
even if you&rsquo;re never planning to write a nontrivial C program if your life.</p>
<p>It feels good to have some ability to compile C/C++ programs myself, even
though I&rsquo;m still not totally confident about all of the compiler and linker
flags and I still plan to never learn anything about how autotools works other
than &ldquo;you run <code>./configure</code> to generate the <code>Makefile</code>&rdquo;.</p>
<p>Two things I left out of this post:</p>
<ul>
<li><code>LD_LIBRARY_PATH / DYLD_LIBRARY_PATH</code> (which you use to tell the dynamic
linker at runtime where to find dynamically linked files) because I can&rsquo;t
remember the last time I ran into an <code>LD_LIBRARY_PATH</code> issue and couldn&rsquo;t
find an example.</li>
<li><code>pkg-config</code>, which I think is important but I don&rsquo;t understand yet</li>
</ul>

---

### [Standards for ANSI escape codes](https://jvns.ca/blog/2025/03/07/escape-code-standards/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Today I want to talk about ANSI escape codes.</p>
<p>For a long time I was vaguely aware of ANSI escape codes (&ldquo;that&rsquo;s how you make
text red in the terminal and stuff&rdquo;) but I had no real understanding of where they were
supposed to be defined or whether or not there were standards for them. I just
had a kind of vague &ldquo;there be dragons&rdquo; feeling around them. While learning
about the terminal this year, I&rsquo;ve learned that:</p>
<ol>
<li>ANSI escape codes are responsible for a lot of usability improvements
in the terminal (did you know there&rsquo;s a way to copy to your system clipboard
when SSHed into a remote machine?? It&rsquo;s an escape code called <a href="https://jvns.ca/til/vim-osc52/">OSC 52</a>!)</li>
<li>They aren&rsquo;t completely standardized, and because of that they don&rsquo;t always
work reliably. And because they&rsquo;re also invisible, it&rsquo;s extremely
frustrating to troubleshoot escape code issues.</li>
</ol>
<p>So I wanted to put together a list for myself of some standards that exist
around escape codes, because I want to know if they <em>have</em> to feel unreliable
and frustrating, or if there&rsquo;s a future where we could all rely on them with
more confidence.</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#what-s-an-escape-code">what&rsquo;s an escape code?</a></li>
<li><a href="https://jvns.ca/atom.xml#ecma-48">ECMA-48</a></li>
<li><a href="https://jvns.ca/atom.xml#xterm-control-sequences">xterm control sequences</a></li>
<li><a href="https://jvns.ca/atom.xml#terminfo">terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#should-programs-use-terminfo">should programs use terminfo?</a></li>
<li><a href="https://jvns.ca/atom.xml#is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</a></li>
<li><a href="https://jvns.ca/atom.xml#some-reasons-to-use-terminfo">some reasons to use terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#some-more-documents-standards">some more documents/standards</a></li>
<li><a href="https://jvns.ca/atom.xml#why-i-think-this-is-interesting">why I think this is interesting</a></li>
</ul>
<h3 id="what-s-an-escape-code">what&rsquo;s an escape code?</h3>
<p>Have you ever pressed the left arrow key in your terminal and seen <code>^[[D</code>?
That&rsquo;s an escape code! It&rsquo;s called an &ldquo;escape code&rdquo; because the first character
is the &ldquo;escape&rdquo; character, which is usually written as <code>ESC</code>, <code>\x1b</code>, <code>\E</code>,
<code>\033</code>, or <code>^[</code>.</p>
<p>Escape codes are how your terminal emulator communicates various kinds of
information (colours, mouse movement, etc) with programs running in the
terminal. There are two kind of escape codes:</p>
<ol>
<li><strong>input codes</strong> which your terminal emulator sends for keypresses or mouse
movements that don&rsquo;t fit into Unicode. For example &ldquo;left arrow key&rdquo; is
<code>ESC[D</code>, &ldquo;Ctrl+left arrow&rdquo; might be <code>ESC[1;5D</code>, and clicking the mouse might
be something like <code>ESC[M :3</code>.</li>
<li><strong>output codes</strong> which programs can print out to colour text, move the
cursor around, clear the screen, hide the cursor, copy text to the
clipboard, enable mouse reporting, set the window title, etc.</li>
</ol>
<p>Now let&rsquo;s talk about standards!</p>
<h3 id="ecma-48">ECMA-48</h3>
<p>The first standard I found relating to escape codes was
<a href="https://ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA-48</a>,
which was originally published in 1976.</p>
<p>ECMA-48 does two things:</p>
<ol>
<li>Define some general <em>formats</em> for escape codes (like &ldquo;CSI&rdquo; codes, which are
<code>ESC[</code> + something and &ldquo;OSC&rdquo; codes, which are <code>ESC]</code> + something)</li>
<li>Define some specific escape codes, like how &ldquo;move the cursor to the left&rdquo; is
<code>ESC[D</code>, or &ldquo;turn text red&rdquo; is  <code>ESC[31m</code>. In the spec, the &ldquo;cursor left&rdquo;
one is called <code>CURSOR LEFT</code> and the one for changing colours is called
<code>SELECT GRAPHIC RENDITION</code>.</li>
</ol>
<p>The formats are extensible, so there&rsquo;s room for others to define more escape
codes in the future. Lots of escape codes that are popular today aren&rsquo;t defined
in ECMA-48: for example it&rsquo;s pretty common for terminal applications (like vim,
htop, or tmux) to support using the mouse, but ECMA-48 doesn&rsquo;t define escape
codes for the mouse.</p>
<h3 id="xterm-control-sequences">xterm control sequences</h3>
<p>There are a bunch of escape codes that aren&rsquo;t defined in ECMA-48, for example:</p>
<ul>
<li>enabling mouse reporting (where did you click in your terminal?)</li>
<li>bracketed paste (did you paste that text or type it in?)</li>
<li>OSC 52 (which terminal applications can use to copy text to your system clipboard)</li>
</ul>
<p>I believe (correct me if I&rsquo;m wrong!) that these and some others came from
xterm, are documented in <a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html">XTerm Control Sequences</a>, and have
been widely implemented by other terminal emulators.</p>
<p>This list of &ldquo;what xterm supports&rdquo; is not a standard exactly, but xterm is
extremely influential and so it seems like an important document.</p>
<h3 id="terminfo">terminfo</h3>
<p>In the 80s (and to some extent today, but my understanding is that it was MUCH
more dramatic in the 80s) there was a huge amount of variation in what escape
codes terminals actually supported.</p>
<p>To deal with this, there&rsquo;s a database of escape codes for various terminals
called &ldquo;terminfo&rdquo;.</p>
<p>It looks like the standard for terminfo is called <a href="https://publications.opengroup.org/c243-1">X/Open Curses</a>, though you need to create
an account to view that standard for some reason. It defines the database format as well
as a C library interface (&ldquo;curses&rdquo;) for accessing the database.</p>
<p>For example you can run this bash snippet to see every possible escape code for
&ldquo;clear screen&rdquo; for all of the different terminals your system knows about:</p>
<pre><code>for term in $(toe -a | awk '{print $1}')
do
  echo $term
  infocmp -1 -T &quot;$term&quot; 2&gt;/dev/null | grep 'clear=' | sed 's/clear=//g;s/,//g'
done
</code></pre>
<p>On my system (and probably every system I&rsquo;ve ever used?), the terminfo database is managed by ncurses.</p>
<h3 id="should-programs-use-terminfo">should programs use terminfo?</h3>
<p>I think it&rsquo;s interesting that there are two main approaches that applications
take to handling ANSI escape codes:</p>
<ol>
<li>Use the terminfo database to figure out which escape codes to use, depending
on what&rsquo;s in the <code>TERM</code> environment variable. Fish does this, for example.</li>
<li>Identify a &ldquo;single common set&rdquo; of escape codes which works in &ldquo;enough&rdquo;
terminal emulators and just hardcode those.</li>
</ol>
<p>Some examples of programs/libraries that take approach #2 (&ldquo;don&rsquo;t use terminfo&rdquo;) include:</p>
<ul>
<li><a href="https://github.com/mawww/kakoune/commit/c12699d2e9c2806d6ed184032078d0b84a3370bb">kakoune</a></li>
<li><a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/165258d2f3ae594b50f16c7b50ffb06627476269/src/prompt_toolkit/input/ansi_escape_sequences.py#L5-L8">python-prompt-toolkit</a></li>
<li><a href="https://github.com/antirez/linenoise">linenoise</a></li>
<li><a href="https://github.com/rockorager/libvaxis">libvaxis</a></li>
<li><a href="https://github.com/chalk/chalk">chalk</a></li>
</ul>
<p>I got curious about why folks might be moving away from terminfo and I found
this very interesting and extremely detailed
<a href="https://twoot.site/@bean/113056942625234032">rant about terminfo from one of the fish maintainers</a>, which argues that:</p>
<blockquote>
<p>[the terminfo authors] have done a lot of work that, at the time, was
extremely important and helpful. My point is that it no longer is.</p>
</blockquote>
<p>I&rsquo;m not going to do it justice so I&rsquo;m not going to summarize it, I think it&rsquo;s
worth reading.</p>
<h3 id="is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</h3>
<p>I was just talking about the idea that you can use a &ldquo;common set&rdquo; of escape
codes that will work for most people. But what is that set? Is there any agreement?</p>
<p>I really do not know the answer to this at all, but from doing some reading it
seems like it&rsquo;s some combination of:</p>
<ul>
<li>The codes that the VT100 supported (though some aren&rsquo;t relevant on modern terminals)</li>
<li>what&rsquo;s in ECMA-48 (which I think also has some things that are no longer relevant)</li>
<li>What xterm supports (though I&rsquo;d guess that not everything in there is actually widely supported enough)</li>
</ul>
<p>and maybe ultimately &ldquo;identify the terminal emulators you think your users are
going to use most frequently and test in those&rdquo;, the same way web developers do
when deciding which CSS features are okay to use</p>
<p>I don&rsquo;t think there are any resources like <a href="https://caniuse.com/">Can I use&hellip;?</a> or
<a href="https://web-platform-dx.github.io/web-features/">Baseline</a> for the terminal
though. (in theory terminfo is supposed to be the &ldquo;caniuse&rdquo; for the terminal
but it seems like it often takes 10+ years to add new terminal features when
people invent them which makes it very limited)</p>
<h3 id="some-reasons-to-use-terminfo">some reasons to use terminfo</h3>
<p>I also asked on Mastodon why people found terminfo valuable in 2025 and got a
few reasons that made sense to me:</p>
<ul>
<li>some people expect to be able to use the <code>TERM</code> environment variable to
control how programs behave (for example with <code>TERM=dumb</code>), and there&rsquo;s
no standard for how that should work in a post-terminfo world</li>
<li>even though there&rsquo;s <em>less</em> variation between terminal emulators than
there was in the 80s, there&rsquo;s far from zero variation: there are graphical
terminals, the Linux framebuffer console, the situation you&rsquo;re in when
connecting to a server via its serial console, Emacs shell mode, and probably
more that I&rsquo;m missing</li>
<li>there is no one standard for what the &ldquo;single common set&rdquo; of escape codes
is, and sometimes programs use escape codes which aren&rsquo;t actually widely
supported enough</li>
</ul>
<h3 id="terminfo-user-agent-detection">terminfo &amp; user agent detection</h3>
<p>The way that ncurses uses the <code>TERM</code> environment variable to decide which
escape codes to use reminds me of how webservers used to sometimes use the
browser user agent to decide which version of a website to serve.</p>
<p>It also seems like it&rsquo;s had some of the same results &ndash; the way iTerm2 reports
itself as being &ldquo;xterm-256color&rdquo; feels similar to how Safari&rsquo;s user agent is
&ldquo;Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7_4) AppleWebKit/605.1.15 (KHTML,
like Gecko) Version/18.3 Safari/605.1.15&rdquo;. In both cases the terminal emulator
/ browser ends up changing its user agent to get around user agent detection
that isn&rsquo;t working well.</p>
<p>On the web we ended up deciding that user agent detection was not a good
practice and to instead focus on standardization so we can serve the same
HTML/CSS to all browsers. I don&rsquo;t know if the same approach is the future in
the terminal though &ndash; I think the terminal landscape today is much more
fragmented than the web ever was as well as being much less well funded.</p>
<h3 id="some-more-documents-standards">some more documents/standards</h3>
<p>A few more documents and standards related to escape codes, in no particular order:</p>
<ul>
<li>the <a href="https://man7.org/linux/man-pages/man4/console_codes.4.html">Linux console_codes man page</a> documents
escape codes that Linux supports</li>
<li>how the <a href="https://vt100.net/docs/vt100-ug/chapter3.html">VT 100</a> handles escape codes &amp; control sequences</li>
<li>the <a href="https://sw.kovidgoyal.net/kitty/keyboard-protocol/">kitty keyboard protocol</a></li>
<li><a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda">OSC 8</a> for links in the terminal (and notes on <a href="https://github.com/Alhadis/OSC8-Adoption?tab=readme-ov-file">adoption</a>)</li>
<li>A <a href="https://github.com/tmux/tmux/blob/882fb4d295deb3e4b803eb444915763305114e4f/tools/ansicode.txt">summary of ANSI standards from tmux</a></li>
<li>this <a href="https://iterm2.com/feature-reporting/">terminal features reporting specification from iTerm</a></li>
<li>sixel graphics</li>
</ul>
<h3 id="why-i-think-this-is-interesting">why I think this is interesting</h3>
<p>I sometimes see people saying that the unix terminal is &ldquo;outdated&rdquo;, and since I
love the terminal so much I&rsquo;m always curious about what incremental changes
might make it feel less &ldquo;outdated&rdquo;.</p>
<p>Maybe if we had a clearer standards landscape (like we do on the web!) it would
be easier for terminal emulator developers to build new features and for
authors of terminal applications to more confidently adopt those features so
that we can all benefit from them and have a richer experience in the terminal.</p>
<p>Obviously standardizing ANSI escape codes is not easy (ECMA-48 was first
published almost 50 years ago and we&rsquo;re still not there!). I don&rsquo;t even know
what all of the challenges are. But the situation with HTML/CSS/JS used to be
extremely bad too and now it&rsquo;s MUCH better, so maybe there&rsquo;s hope.</p>

---

### [How to add a directory to your PATH](https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I was talking to a friend about how to add a directory to your PATH today. It&rsquo;s
something that feels &ldquo;obvious&rdquo; to me since I&rsquo;ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn&rsquo;t find something that explained all of the steps &ndash; a lot of them just
said &ldquo;add this to <code>~/.bashrc</code>&rdquo;, but what if you&rsquo;re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?</p>
<p>So I wanted to try to write down some more complete directions and mention some
of the gotchas I&rsquo;ve run into over the years.</p>
<p>Here&rsquo;s a table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-1-what-shell-are-you-using">step 1: what shell are you using?</a></li>
<li><a href="https://jvns.ca/atom.xml#step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-4-edit-your-shell-config">step 4: edit your shell config</a></li>
<li><a href="https://jvns.ca/atom.xml#step-5-restart-your-shell">step 5: restart your shell</a></li>
<li>problems:
<ul>
<li><a href="https://jvns.ca/atom.xml#problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate PATH entries making it harder to debug</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your PATH</a></li>
</ul>
</li>
<li>notes:
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-source">a note on source</a></li>
<li><a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">a note on fish_add_path</a></li>
</ul>
</li>
</ul>
<h3 id="step-1-what-shell-are-you-using">step 1: what shell are you using?</h3>
<p>If you&rsquo;re not sure what shell you&rsquo;re using, here&rsquo;s a way to find out. Run this:</p>
<pre><code>ps -p $$ -o pid,comm=
</code></pre>
<ul>
<li>if you&rsquo;re using <strong>bash</strong>, it&rsquo;ll print out <code>97295 bash</code></li>
<li>if you&rsquo;re using <strong>zsh</strong>, it&rsquo;ll print out <code>97295 zsh</code></li>
<li>if you&rsquo;re using <strong>fish</strong>, it&rsquo;ll print out an error like &ldquo;In fish, please use
$fish_pid&rdquo; (<code>$$</code> isn&rsquo;t valid syntax in fish, but in any case the error
message tells you that you&rsquo;re using fish, which you probably already knew)</li>
</ul>
<p>Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I&rsquo;ll only cover bash, zsh, and fish in these directions.</p>
<h3 id="step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</h3>
<ul>
<li>in zsh, it&rsquo;s probably <code>~/.zshrc</code></li>
<li>in bash, it might be <code>~/.bashrc</code>, but it&rsquo;s complicated, see the note in the next section</li>
<li>in fish, it&rsquo;s probably <code>~/.config/fish/config.fish</code> (you can run <code>echo $__fish_config_dir</code> if you want to be 100% sure)</li>
</ul>
<h3 id="a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</h3>
<p>Bash has three possible config files: <code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.profile</code>.</p>
<p>If you&rsquo;re not sure which one your system is set up to use, I&rsquo;d recommend
testing this way:</p>
<ol>
<li>add <code>echo hi there</code> to your <code>~/.bashrc</code></li>
<li>Restart your terminal</li>
<li>If you see &ldquo;hi there&rdquo;, that means <code>~/.bashrc</code> is being used! Hooray!</li>
<li>Otherwise remove it and try the same thing with <code>~/.bash_profile</code></li>
<li>You can also try <code>~/.profile</code> if the first two options don&rsquo;t work.</li>
</ol>
<p>(there are a lot of <a href="https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html">elaborate flow charts</a> out there that explain how bash
decides which config file to use but IMO it&rsquo;s not worth it to internalize them
and just testing is the fastest way to be sure)</p>
<h3 id="step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</h3>
<p>Let&rsquo;s say that you&rsquo;re trying to install and run a program called <code>http-server</code>
and it doesn&rsquo;t work, like this:</p>
<pre><code>$ npm install -g http-server
$ http-server
bash: http-server: command not found
</code></pre>
<p>How do you find what directory <code>http-server</code> is in? Honestly in general this is
not that easy &ndash; often the answer is something like &ldquo;it depends on how npm is
configured&rdquo;. A few ideas:</p>
<ul>
<li>Often when setting up a new installer (like <code>cargo</code>, <code>npm</code>, <code>homebrew</code>, etc),
when you first set it up it&rsquo;ll print out some directions about how to update
your PATH. So if you&rsquo;re paying attention you can get the directions then.</li>
<li>Sometimes installers will automatically update your shell&rsquo;s config file
to update your <code>PATH</code> for you</li>
<li>Sometimes just Googling &ldquo;where does npm install things?&rdquo; will turn up the
answer</li>
<li>Some tools have a subcommand that tells you where they&rsquo;re configured to
install things, like:
<ul>
<li>Node/npm: <code>npm config get prefix</code> (then append <code>/bin/</code>)</li>
<li>Go: <code>go env GOPATH</code> (then append <code>/bin/</code>)</li>
<li>asdf: <code>asdf info | grep ASDF_DIR</code> (then append <code>/bin/</code> and <code>/shims/</code>)</li>
</ul>
</li>
</ul>
<h3 id="step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</h3>
<p>Once you&rsquo;ve found a directory you think might be the right one, make sure it&rsquo;s
actually correct! For example, I found out that on my machine, <code>http-server</code> is
in <code>~/.npm-global/bin</code>. I can make sure that it&rsquo;s the right directory by trying to
run the program <code>http-server</code> in that directory like this:</p>
<pre><code>$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
</code></pre>
<p>It worked! Now that you know what directory you need to add to your <code>PATH</code>,
let&rsquo;s move to the next step!</p>
<h3 id="step-4-edit-your-shell-config">step 4: edit your shell config</h3>
<p>Now we have the 2 critical pieces of information we need:</p>
<ol>
<li>Which directory you&rsquo;re trying to add to your PATH (like  <code>~/.npm-global/bin/</code>)</li>
<li>Where your shell&rsquo;s config is (like <code>~/.bashrc</code>, <code>~/.zshrc</code>, or <code>~/.config/fish/config.fish</code>)</li>
</ol>
<p>Now what you need to add depends on your shell:</p>
<p><strong>bash instructions:</strong></p>
<p>Open your shell&rsquo;s config file, and add a line like this:</p>
<pre><code>export PATH=$PATH:~/.npm-global/bin/
</code></pre>
<p>(obviously replace <code>~/.npm-global/bin</code> with the actual directory you&rsquo;re trying to add)</p>
<p><strong>zsh instructions:</strong></p>
<p>You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:</p>
<pre><code>path=(
  $path
  ~/.npm-global/bin
)
</code></pre>
<p><strong>fish instructions:</strong></p>
<p>In fish, the syntax is different:</p>
<pre><code>set PATH $PATH ~/.npm-global/bin
</code></pre>
<p>(in fish you can also use <code>fish_add_path</code>, some notes on that <a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">further down</a>)</p>
<h3 id="step-5-restart-your-shell">step 5: restart your shell</h3>
<p>Now, an extremely important step: updating your shell&rsquo;s config won&rsquo;t take
effect if you don&rsquo;t restart it!</p>
<p>Two ways to do this:</p>
<ol>
<li>open a new terminal (or terminal tab), and maybe close the old one so you don&rsquo;t get confused</li>
<li>Run <code>bash</code> to start a new shell (or <code>zsh</code> if you&rsquo;re using zsh, or <code>fish</code> if you&rsquo;re using fish)</li>
</ol>
<p>I&rsquo;ve found that both of these usually work fine.</p>
<p>And you should be done! Try running the program you were trying to run and
hopefully it works now.</p>
<p>If not, here are a couple of problems that you might run into:</p>
<h3 id="problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</h3>
<p>If the wrong <strong>version</strong> of a program is running, you might need to add the
directory to the <em>beginning</em> of your PATH instead of the end.</p>
<p>For example, on my system I have two versions of <code>python3</code> installed, which I
can see by running <code>which -a</code>:</p>
<pre><code>$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
</code></pre>
<p>The one your shell will use is the <strong>first one listed</strong>.</p>
<p>If you want to use the Homebrew version, you need to add that directory
(<code>/opt/homebrew/bin</code>) to the <strong>beginning</strong> of your PATH instead, by putting this in
your shell&rsquo;s config file (it&rsquo;s <code>/opt/homebrew/bin/:$PATH</code> instead of the usual <code>$PATH:/opt/homebrew/bin/</code>)</p>
<pre><code>export PATH=/opt/homebrew/bin/:$PATH
</code></pre>
<p>or in fish:</p>
<pre><code>set PATH ~/.cargo/bin $PATH
</code></pre>
<h3 id="problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</h3>
<p>All of these directions only work if you&rsquo;re running the program <strong>from your
shell</strong>. If you&rsquo;re running the program from an IDE, from a GUI, in a cron job,
or some other way, you&rsquo;ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.</p>
<p><strong>in a cron job</strong></p>
<p>Some options:</p>
<ul>
<li>use the full path to the program you&rsquo;re running, like <code>/home/bork/bin/my-program</code></li>
<li>put the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:&hellip;.). You can get the full PATH you&rsquo;re
using in your shell by running <code>echo &quot;PATH=$PATH&quot;</code>.</li>
</ul>
<p>I&rsquo;m honestly not sure how to handle it in an IDE/GUI because I haven&rsquo;t run into
that in a long time, will add directions here if someone points me in the right
direction.</p>
<h3 id="problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate <code>PATH</code> entries making it harder to debug</h3>
<p>If you edit your path and start a new shell by running <code>bash</code> (or <code>zsh</code>, or
<code>fish</code>), you&rsquo;ll often end up with duplicate <code>PATH</code> entries, because the shell
keeps adding new things to your <code>PATH</code> every time you start your shell.</p>
<p>Personally I don&rsquo;t think I&rsquo;ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what&rsquo;s going on with your <code>PATH</code> if you&rsquo;re trying to understand its contents.</p>
<p>Some ways you could deal with this:</p>
<ol>
<li>If you&rsquo;re debugging your <code>PATH</code>, open a new terminal to do it in so you get
a &ldquo;fresh&rdquo; state. This should avoid the duplication.</li>
<li>Deduplicate your <code>PATH</code> at the end of your shell&rsquo;s config  (for example in
zsh apparently you can do this with <code>typeset -U path</code>)</li>
<li>Check that the directory isn&rsquo;t already in your <code>PATH</code> when adding it (for
example in fish I believe you can do this with <code>fish_add_path --path /some/directory</code>)</li>
</ol>
<p>How to deduplicate your <code>PATH</code> is shell-specific and there isn&rsquo;t always a
built in way to do it so you&rsquo;ll need to look up how to accomplish it in your
shell.</p>
<h3 id="problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your <code>PATH</code></h3>
<p>Here&rsquo;s a situation that&rsquo;s easy to get into in bash or zsh:</p>
<ol>
<li>Run a command (it fails)</li>
<li>Update your <code>PATH</code></li>
<li>Run <code>bash</code> to reload your config</li>
<li>Press the up arrow a couple of times to rerun the failed command (or open a new terminal)</li>
<li>The failed command isn&rsquo;t in your history! Why not?</li>
</ol>
<p>This happens because in bash, by default, history is not saved until you exit
the shell.</p>
<p>Some options for fixing this:</p>
<ul>
<li>Instead of running <code>bash</code> to reload your config, run <code>source ~/.bashrc</code> (or
<code>source ~/.zshrc</code> in zsh). This will reload the config inside your current
session.</li>
<li>Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you&rsquo;re
using bash or zsh, the history options in zsh are a bit complicated and I&rsquo;m
not exactly sure what the best way is)</li>
</ul>
<h3 id="a-note-on-source">a note on <code>source</code></h3>
<p>When you install <code>cargo</code> (Rust&rsquo;s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don&rsquo;t mention a specific
directory at all.</p>
<pre><code>This is usually done by running one of the following (note the leading DOT):

. &quot;$HOME/.cargo/env&quot;        	# For sh/bash/zsh/ash/dash/pdksh
source &quot;$HOME/.cargo/env.fish&quot;  # For fish
</code></pre>
<p>The idea is that you add that line to your shell&rsquo;s config, and their script
automatically sets up your <code>PATH</code> (and potentially other things) for you.</p>
<p>This is pretty common (for example <a href="https://github.com/Homebrew/install/blob/deacfa6a6e62e5f4002baf9e1fac7a96e9aa5d41/install.sh#L1072-L1087">Homebrew</a> suggests you eval <code>brew shellenv</code>), and there are
two ways to approach this:</p>
<ol>
<li>Just do what the tool suggests (like adding <code>. &quot;$HOME/.cargo/env&quot;</code> to your shell&rsquo;s config)</li>
<li>Figure out which directories the script they&rsquo;re telling you to run would add
to your PATH, and then add those manually. Here&rsquo;s how I&rsquo;d do that:
<ul>
<li>Run <code>. &quot;$HOME/.cargo/env&quot;</code> in my shell (or the fish version if using fish)</li>
<li>Run <code>echo &quot;$PATH&quot; | tr ':' '\n' | grep cargo</code> to figure out which directories it added</li>
<li>See that it says <code>/Users/bork/.cargo/bin</code> and shorten that to <code>~/.cargo/bin</code></li>
<li>Add the directory <code>~/.cargo/bin</code> to PATH (with the directions in this post)</li>
</ul>
</li>
</ol>
<p>I don&rsquo;t think there&rsquo;s anything wrong with doing what the tool suggests (it
might be the &ldquo;best way&rdquo;!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I&rsquo;m changing.</p>
<h3 id="a-note-on-fish-add-path">a note on <code>fish_add_path</code></h3>
<p>fish has a handy function called <code>fish_add_path</code> that you can run to add a directory to your <code>PATH</code> like this:</p>
<pre><code>fish_add_path /some/directory
</code></pre>
<p>This is cool (it&rsquo;s such a simple command!) but I&rsquo;ve stopped using it for a couple of reasons:</p>
<ol>
<li>Sometimes <code>fish_add_path</code> will update the <code>PATH</code> for every session in the
future (with a &ldquo;universal variable&rdquo;) and sometimes it will update the <code>PATH</code>
just for the current session and it&rsquo;s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.</li>
<li>If you ever need to <em>remove</em> the directory from your <code>PATH</code> a few weeks or
months later because maybe you made a mistake, it&rsquo;s kind of hard to do
(there are <a href="https://github.com/fish-shell/fish-shell/issues/8604">instructions in this comments of this github issue though</a>).</li>
</ol>
<h3 id="that-s-all">that&rsquo;s all</h3>
<p>Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!</p>

---

### [Some terminal frustrations](https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>A few weeks ago I ran a terminal survey (you can <a href="https://jvns.ca/terminal-survey/results-bsky.html">read the results here</a>) and at the end I asked:</p>
<blockquote>
<p>What‚Äôs the most frustrating thing about using the terminal for you?</p>
</blockquote>
<p>1600 people answered, and I decided to spend a few days categorizing all the
responses. Along the way I learned that classifying qualitative data is not
easy but I gave it my best shot. I ended up building a custom
<a href="https://github.com/jvns/classificator">tool</a> to make it faster to categorize
everything.</p>
<p>As with all of my surveys the methodology isn&rsquo;t particularly scientific. I just
posted the survey to Mastodon and Twitter, ran it for a couple of days, and got
answers from whoever happened to see it and felt like responding.</p>
<p>Here are the top categories of frustrations!</p>
<p>I think it&rsquo;s worth keeping in mind while reading these comments that</p>
<ul>
<li>40% of people answering this survey have been using the terminal for <strong>21+ years</strong></li>
<li>95% of people answering the survey have been using the terminal for at least 4 years</li>
</ul>
<p>These comments aren&rsquo;t coming from total beginners.</p>
<p>Here are the categories of frustrations! The number in brackets is the number
of people with that frustration. I&rsquo;m mostly writing this up for myself because
I&rsquo;m trying to write a zine about the terminal and I wanted to get a sense for
what people are having trouble with.</p>
<h3 id="remembering-syntax-115">remembering syntax (115)</h3>
<p>People talked about struggles remembering:</p>
<ul>
<li>the syntax for CLI tools like awk, jq, sed, etc</li>
<li>the syntax for redirects</li>
<li>keyboard shortcuts for tmux, text editing, etc</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>There are just so many little &ldquo;trivia&rdquo; details to remember for full
functionality. Even after all these years I&rsquo;ll sometimes forget where it&rsquo;s 2
or 1 for stderr, or forget which is which for <code>&gt;</code> and <code>&gt;&gt;</code>.</p>
</blockquote>
<h3 id="switching-terminals-is-hard-91">switching terminals is hard (91)</h3>
<p>People talked about struggling with switching systems (for example home/work
computer or when SSHing) and running into:</p>
<ul>
<li>OS differences in keyboard shortcuts (like Linux vs Mac)</li>
<li>systems which don&rsquo;t have their preferred text editor (&ldquo;no vim&rdquo; or &ldquo;only vim&rdquo;)</li>
<li>different versions of the same command (like Mac OS grep vs GNU grep)</li>
<li>no tab completion</li>
<li>a shell they aren&rsquo;t used to (&ldquo;the subtle differences between zsh and bash&rdquo;)</li>
</ul>
<p>as well as differences inside the same system like pagers being not consistent
with each other (git diff pagers, other pagers).</p>
<p>One example comment:</p>
<blockquote>
<p>I got used to fish and vi mode which are not available when I ssh into
servers, containers.</p>
</blockquote>
<h3 id="color-85">color (85)</h3>
<p>Lots of problems with color, like:</p>
<ul>
<li>programs setting colors that are unreadable with a light background color</li>
<li>finding a colorscheme they like (and getting it to work consistently across different apps)</li>
<li>color not working inside several layers of SSH/tmux/etc</li>
<li>not liking the defaults</li>
<li>not wanting color at all and struggling to turn it off</li>
</ul>
<p>This comment felt relatable to me:</p>
<blockquote>
<p>Getting my terminal theme configured in a reasonable way between the terminal
emulator and fish (I did this years ago and remember it being tedious and
fiddly and now feel like I&rsquo;m locked into my current theme because it works
and I dread touching any of that configuration ever again).</p>
</blockquote>
<h3 id="keyboard-shortcuts-84">keyboard shortcuts (84)</h3>
<p>Half of the comments on keyboard shortcuts were about how on Linux/Windows, the
keyboard shortcut to copy/paste in the terminal is different from in the rest
of the OS.</p>
<p>Some other issues with keyboard shortcuts other than copy/paste:</p>
<ul>
<li>using <code>Ctrl-W</code> in a browser-based terminal and closing the window</li>
<li>the terminal only supports a limited set of keyboard shortcuts (no
<code>Ctrl-Shift-</code>, no <code>Super</code>, no <code>Hyper</code>, lots of <code>ctrl-</code> shortcuts aren&rsquo;t
possible like <code>Ctrl-,</code>)</li>
<li>the OS stopping you from using a terminal keyboard shortcut (like by default
Mac OS uses <code>Ctrl+left arrow</code> for something else)</li>
<li>issues using emacs in the terminal</li>
<li>backspace not working (2)</li>
</ul>
<h3 id="other-copy-and-paste-issues-75">other copy and paste issues (75)</h3>
<p>Aside from &ldquo;the keyboard shortcut for copy and paste is different&rdquo;, there were
a lot of OTHER issues with copy and paste, like:</p>
<ul>
<li>copying over SSH</li>
<li>how tmux and the terminal emulator both do copy/paste in different ways</li>
<li>dealing with many different clipboards (system clipboard, vim clipboard, the
&ldquo;middle click&rdquo; clipboard on Linux, tmux&rsquo;s clipboard, etc) and potentially
synchronizing them</li>
<li>random spaces added when copying from the terminal</li>
<li>pasting multiline commands which automatically get run in a terrifying way</li>
<li>wanting a way to copy text without using the mouse</li>
</ul>
<h3 id="discoverability-55">discoverability (55)</h3>
<p>There were lots of comments about this, which all came down to the same basic
complaint &ndash; it&rsquo;s hard to discover useful tools or features! This comment kind of
summed it all up:</p>
<blockquote>
<p>How difficult it is to learn independently. Most of what I know is an
assorted collection of stuff I&rsquo;ve been told by random people over the years.</p>
</blockquote>
<h3 id="steep-learning-curve-44">steep learning curve (44)</h3>
<p>A lot of comments about it generally having a steep learning curve. A couple of
example comments:</p>
<blockquote>
<p>After 15 years of using it, I‚Äôm not much faster than using it than I was 5 or
maybe even 10 years ago.</p>
</blockquote>
<p>and</p>
<blockquote>
<p>That I know I could make my life easier by learning more about the shortcuts
and commands and configuring the terminal but I don&rsquo;t spend the time because it
feels overwhelming.</p>
</blockquote>
<h3 id="history-42">history  (42)</h3>
<p>Some issues with shell history:</p>
<ul>
<li>history not being shared between terminal tabs (16)</li>
<li>limits that are too short (4)</li>
<li>history not being restored when terminal tabs are restored</li>
<li>losing history because the terminal crashed</li>
<li>not knowing how to search history</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>It wasted a lot of time until I figured it out and still annoys me that
&ldquo;history&rdquo; on zsh has such a small buffer;  I have to type &ldquo;history 0&rdquo; to get
any useful length of history.</p>
</blockquote>
<h3 id="bad-documentation-37">bad documentation (37)</h3>
<p>People talked about:</p>
<ul>
<li>documentation being generally opaque</li>
<li>lack of examples in man pages</li>
<li>programs which don&rsquo;t have man pages</li>
</ul>
<p>Here&rsquo;s a representative comment:</p>
<blockquote>
<p>Finding good examples and docs. Man pages often not enough, have to wade
through stack overflow</p>
</blockquote>
<h3 id="scrollback-36">scrollback (36)</h3>
<p>A few issues with scrollback:</p>
<ul>
<li>programs printing out too much data making you lose scrollback history</li>
<li>resizing the terminal messes up the scrollback</li>
<li>lack of timestamps</li>
<li>GUI programs that you start in the background printing stuff out that gets in
the way of other programs&rsquo; outputs</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>When resizing the terminal (in particular: making it narrower) leads to
broken rewrapping of the scrollback content because the commands formatted
their output based on the terminal window width.</p>
</blockquote>
<h3 id="it-feels-outdated-33">&ldquo;it feels outdated&rdquo; (33)</h3>
<p>Lots of comments about how the terminal feels hampered by legacy decisions and
how users often end up needing to learn implementation details that feel very
esoteric. One example comment:</p>
<blockquote>
<p>Most of the legacy cruft, it would be great to have a green field
implementation of the CLI interface.</p>
</blockquote>
<h3 id="shell-scripting-32">shell scripting (32)</h3>
<p>Lots of complaints about POSIX shell scripting. There&rsquo;s a general feeling that
shell scripting is difficult but also that switching to a different less
standard scripting language (fish, nushell, etc) brings its own problems.</p>
<blockquote>
<p>Shell scripting. My tolerance to ditch a shell script and go to a scripting
language is pretty low. It‚Äôs just too messy and powerful. Screwing up can be
costly so I don‚Äôt even bother.</p>
</blockquote>
<h3 id="more-issues">more issues</h3>
<p>Some more issues that were mentioned at least 10 times:</p>
<ul>
<li>(31) inconsistent command line arguments: is it -h or help or &ndash;help?</li>
<li>(24) keeping dotfiles in sync across different systems</li>
<li>(23) performance (e.g. &ldquo;my shell takes too long to start&rdquo;)</li>
<li>(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)</li>
<li>(17) generally feeling scared/uneasy (&ldquo;The debilitating fear that I‚Äôm going
to do some mysterious Bad Thing with a command and I will have absolutely no
idea how to fix or undo it or even really figure out what happened&rdquo;)</li>
<li>(16) terminfo issues (&ldquo;Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.&rdquo;)</li>
<li>(16) lack of image support (sixel etc)</li>
<li>(15) SSH issues (like having to start over when you lose the SSH connection)</li>
<li>(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)</li>
<li>(15) typos &amp; slow typing</li>
<li>(13) the terminal getting messed up for various reasons (pressing <code>Ctrl-S</code>, <code>cat</code>ing a binary, etc)</li>
<li>(12) quoting/escaping in the shell</li>
<li>(11) various Windows/PowerShell issues</li>
</ul>
<h3 id="n-a-122">n/a (122)</h3>
<p>There were also 122 answers to the effect of &ldquo;nothing really&rdquo; or &ldquo;only that I
can&rsquo;t do EVERYTHING in the terminal&rdquo;</p>
<p>One example comment:</p>
<blockquote>
<p>Think I&rsquo;ve found work arounds for most/all frustrations</p>
</blockquote>
<h3 id="that-s-all">that&rsquo;s all!</h3>
<p>I&rsquo;m not going to make a lot of commentary on these results, but here are a
couple of categories that feel related to me:</p>
<ul>
<li>remembering syntax &amp; history (often the thing you need to remember is something you&rsquo;ve run before!)</li>
<li>discoverability &amp; the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)</li>
<li>&ldquo;switching systems is hard&rdquo; &amp; &ldquo;it feels outdated&rdquo; (tools that haven&rsquo;t really
changed in 30 or 40 years have many problems but they do tend to be always
<em>there</em> no matter what system you&rsquo;re on, which is very useful and makes them
hard to stop using)</li>
</ul>
<p>Trying to categorize all these results in a reasonable way really gave me an
appreciation for social science researchers&rsquo; skills.</p>

---

### [What's involved in getting a "modern" terminal setup?](https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Recently I ran a terminal survey and I asked people what frustrated
them. One person commented:</p>
<blockquote>
<p>There are so many pieces to having a modern terminal experience. I wish it
all came out of the box.</p>
</blockquote>
<p>My immediate reaction was &ldquo;oh, getting a modern terminal experience isn&rsquo;t that
hard, you just need to&hellip;.&rdquo;, but the more I thought about it, the longer the
&ldquo;you just need to&hellip;&rdquo; list got, and I kept thinking about more and more
caveats.</p>
<p>So I thought I would write down some notes about what it means to me personally
to have a &ldquo;modern&rdquo; terminal experience and what I think can make it hard for
people to get there.</p>
<h3 id="what-is-a-modern-terminal-experience">what is a &ldquo;modern terminal experience&rdquo;?</h3>
<p>Here are a few things that are important to me, with which part of the system
is responsible for them:</p>
<ul>
<li><strong>multiline support for copy and paste</strong>: if you paste 3 commands in your shell, it should not immediately run them all! That&rsquo;s scary! (<strong>shell</strong>, <strong>terminal emulator</strong>)</li>
<li><strong>infinite shell history</strong>: if I run a command in my shell, it should be saved forever, not deleted after 500 history entries or whatever. Also I want commands to be saved to the history immediately when I run them, not only when I exit the shell session (<strong>shell</strong>)</li>
<li><strong>a useful prompt</strong>: I can&rsquo;t live without having my <strong>current directory</strong> and <strong>current git branch</strong> in my prompt (<strong>shell</strong>)</li>
<li><strong>24-bit colour</strong>: this is important to me because I find it MUCH easier to theme neovim with 24-bit colour support than in a terminal with only 256 colours (<strong>terminal emulator</strong>)</li>
<li><strong>clipboard integration</strong> between vim and my operating system so that when I copy in Firefox, I can just press <code>p</code> in vim to paste (<strong>text editor</strong>, maybe the OS/terminal emulator too)</li>
<li><strong>good autocomplete</strong>: for example commands like git should have command-specific autocomplete (<strong>shell</strong>)</li>
<li><strong>having colours in <code>ls</code></strong> (<strong>shell config</strong>)</li>
<li><strong>a terminal theme I like</strong>: I spend a lot of time in my terminal, I want it to look nice and I want its theme to match my terminal editor&rsquo;s theme. (<strong>terminal emulator</strong>, <strong>text editor</strong>)</li>
<li><strong>automatic terminal fixing</strong>: If a programs prints out some weird escape
codes that mess up my terminal, I want that to automatically get reset so
that my terminal doesn&rsquo;t get messed up (<strong>shell</strong>)</li>
<li><strong>keybindings</strong>: I want <code>Ctrl+left arrow</code> to work (<strong>shell</strong> or <strong>application</strong>)</li>
<li><strong>being able to use the scroll wheel in programs like <code>less</code></strong>: (<strong>terminal emulator</strong> and <strong>applications</strong>)</li>
</ul>
<p>There are a million other terminal conveniences out there and different people
value different things, but those are the ones that I would be really unhappy
without.</p>
<h3 id="how-i-achieve-a-modern-experience">how I achieve a &ldquo;modern experience&rdquo;</h3>
<p>My basic approach is:</p>
<ol>
<li>use the <code>fish</code> shell. Mostly don&rsquo;t configure it, except to:
<ul>
<li>set the <code>EDITOR</code> environment variable to my favourite terminal editor</li>
<li>alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>use any terminal emulator with 24-bit colour support. In the past I&rsquo;ve used
GNOME Terminal, Terminator, and iTerm, but I&rsquo;m not picky about this. I don&rsquo;t really
configure it other than to choose a font.</li>
<li>use <code>neovim</code>, with a configuration that I&rsquo;ve been very slowly building over the last 9 years or so (the last time I deleted my vim config and started from scratch was 9 years ago)</li>
<li>use the <a href="https://github.com/chriskempson/base16">base16 framework</a> to theme everything</li>
</ol>
<p>A few things that affect my approach:</p>
<ul>
<li>I don&rsquo;t spend a lot of time SSHed into other machines</li>
<li>I&rsquo;d rather use the mouse a little than come up with keyboard-based ways to do everything</li>
<li>I work on a lot of small projects, not one big project</li>
</ul>
<h3 id="some-out-of-the-box-options-for-a-modern-experience">some &ldquo;out of the box&rdquo; options for a &ldquo;modern&rdquo; experience</h3>
<p>What if you want a nice experience, but don&rsquo;t want to spend a lot of time on
configuration? Figuring out how to configure vim in a way that I was satisfied
with really did take me like ten years, which is a long time!</p>
<p>My best ideas for how to get a reasonable terminal experience with minimal
config are:</p>
<ul>
<li>shell: either <code>fish</code> or <code>zsh</code> with <a href="https://ohmyz.sh/">oh-my-zsh</a></li>
<li>terminal emulator: almost anything with 24-bit colour support, for example all of these are popular:
<ul>
<li>linux: GNOME Terminal, Konsole, Terminator, xfce4-terminal</li>
<li>mac: iTerm (Terminal.app doesn&rsquo;t have 256-colour support)</li>
<li>cross-platform: kitty, alacritty, wezterm, or ghostty</li>
</ul>
</li>
<li>shell config:
<ul>
<li>set the <code>EDITOR</code> environment variable to your favourite terminal text
editor</li>
<li>maybe alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>text editor: this is a tough one, maybe <a href="https://micro-editor.github.io/">micro</a> or <a href="https://helix-editor.com/">helix</a>? I haven&rsquo;t used
either of them seriously but they both seem like very cool projects and I
think it&rsquo;s amazing that you can just use all the usual GUI editor commands
(<code>Ctrl-C</code> to copy, <code>Ctrl-V</code> to paste, <code>Ctrl-A</code> to select all) in micro and
they do what you&rsquo;d expect. I would probably try switching to helix except
that retraining my vim muscle memory seems way too hard. Also helix doesn&rsquo;t
have a GUI or plugin system yet.</li>
</ul>
<p>Personally I <strong>wouldn&rsquo;t</strong> use xterm, rxvt, or Terminal.app as a terminal emulator,
because I&rsquo;ve found in the past that they&rsquo;re missing core features (like 24-bit
colour in Terminal.app&rsquo;s case) that make the terminal harder to use for me.</p>
<p>I don&rsquo;t want to pretend that getting a &ldquo;modern&rdquo; terminal experience is easier
than it is though &ndash; I think there are two issues that make it hard. Let&rsquo;s talk
about them!</p>
<h3 id="issue-1-with-getting-to-a-modern-experience-the-shell">issue 1 with getting to a &ldquo;modern&rdquo; experience: the shell</h3>
<p>bash and zsh are by far the two most popular shells, and neither of them
provide a default experience that I would be happy using out of the box, for
example:</p>
<ul>
<li>you need to customize your prompt</li>
<li>they don&rsquo;t come with git completions by default, you have to set them up</li>
<li>by default, bash only stores 500 (!) lines of history and (at least on Mac OS)
zsh is only configured to store 2000 lines, which is still not a lot</li>
<li>I find bash&rsquo;s tab completion very frustrating, if there&rsquo;s more than
one match then you can&rsquo;t tab through them</li>
</ul>
<p>And even though <a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">I love fish</a>, the fact
that it isn&rsquo;t POSIX does make it hard for a lot of folks to make the switch.</p>
<p>Of course it&rsquo;s totally possible to learn how to customize your prompt in bash
or whatever, and it doesn&rsquo;t even need to be that complicated (in bash I&rsquo;d
probably start with something like <code>export PS1='[\u@\h \W$(__git_ps1 &quot; (%s)&quot;)]\$ '</code>, or maybe use <a href="https://starship.rs/">starship</a>).
But each of these &ldquo;not complicated&rdquo; things really does add up and it&rsquo;s
especially tough if you need to keep your config in sync across several
systems.</p>
<p>An extremely popular solution to getting a &ldquo;modern&rdquo; shell experience is
<a href="https://ohmyz.sh/">oh-my-zsh</a>. It seems like a great project and I know a lot
of people use it very happily, but I&rsquo;ve struggled with configuration systems
like that in the past &ndash; it looks like right now the base oh-my-zsh adds about
3000 lines of config, and often I find that having an extra configuration
system makes it harder to debug what&rsquo;s happening when things go wrong. I
personally have a tendency to use the system to add a lot of extra plugins,
make my system slow, get frustrated that it&rsquo;s slow, and then delete it
completely and write a new config from scratch.</p>
<h3 id="issue-2-with-getting-to-a-modern-experience-the-text-editor">issue 2 with getting to a &ldquo;modern&rdquo; experience: the text editor</h3>
<p>In the terminal survey I ran recently, the most popular terminal text editors
by far were <code>vim</code>, <code>emacs</code>, and <code>nano</code>.</p>
<p>I think the main options for terminal text editors are:</p>
<ul>
<li>use vim or emacs and configure it to your liking, you can probably have any
feature you want if you put in the work</li>
<li>use nano and accept that you&rsquo;re going to have a pretty limited experience
(for example I don&rsquo;t think you can select text with the mouse and then &ldquo;cut&rdquo;
it in nano)</li>
<li>use <code>micro</code> or <code>helix</code> which seem to offer a pretty good out-of-the-box
experience, potentially occasionally run into issues with using a less
mainstream text editor</li>
<li>just avoid using a terminal text editor as much as possible, maybe use VSCode, use
VSCode&rsquo;s terminal for all your terminal needs, and mostly never edit files in
the terminal. Or I know a lot of people use <code>code</code> as their <code>EDITOR</code> in the terminal.</li>
</ul>
<h3 id="issue-3-individual-applications">issue 3: individual applications</h3>
<p>The last issue is that sometimes individual programs that I use are kind of
annoying. For example on my Mac OS machine, <code>/usr/bin/sqlite3</code> doesn&rsquo;t support
the <code>Ctrl+Left Arrow</code> keyboard shortcut. Fixing this to get a reasonable
terminal experience in SQLite was a little complicated, I had to:</p>
<ul>
<li>realize why this is happening (Mac OS won&rsquo;t ship GNU tools, and &ldquo;Ctrl-Left arrow&rdquo; support comes from GNU readline)</li>
<li>find a workaround (install sqlite from homebrew, which does have readline support)</li>
<li>adjust my environment (put Homebrew&rsquo;s sqlite3 in my PATH)</li>
</ul>
<p>I find that debugging application-specific issues like this is really not easy
and often it doesn&rsquo;t feel &ldquo;worth it&rdquo; &ndash; often I&rsquo;ll end up just dealing with
various minor inconveniences because I don&rsquo;t want to spend hours investigating
them. The only reason I was even able to figure this one out at all is that
I&rsquo;ve been spending a huge amount of time thinking about the terminal recently.</p>
<p>A big part of having a &ldquo;modern&rdquo; experience using terminal programs is just
using newer terminal programs, for example I can&rsquo;t be bothered to learn a
keyboard shortcut to sort the columns in <code>top</code>, but in <code>htop</code>  I can just click
on a column heading with my mouse to sort it. So I use htop instead! But discovering new more &ldquo;modern&rdquo; command line tools isn&rsquo;t easy (though
I made <a href="https://jvns.ca/blog/2022/04/12/a-list-of-new-ish--command-line-tools/">a list here</a>),
finding ones that I actually like using in practice takes time, and if you&rsquo;re
SSHed into another machine, they won&rsquo;t always be there.</p>
<h3 id="everything-affects-everything-else">everything affects everything else</h3>
<p>Something I find tricky about configuring my terminal to make everything &ldquo;nice&rdquo;
is that changing one seemingly small thing about my workflow can really affect
everything else. For example right now I don&rsquo;t use tmux. But if I needed to use
tmux again (for example because I was doing a lot of work SSHed into another
machine), I&rsquo;d need to think about a few things, like:</p>
<ul>
<li>if I wanted tmux&rsquo;s copy to synchronize with my system clipboard over
SSH, I&rsquo;d need to make sure that my terminal emulator has <a href="https://old.reddit.com/r/vim/comments/k1ydpn/a_guide_on_how_to_copy_text_from_anywhere/">OSC 52 support</a></li>
<li>if I wanted to use iTerm&rsquo;s tmux integration (which makes tmux tabs into iTerm
tabs), I&rsquo;d need to change how I configure colours &ndash; right now I set them
with a <a href="https://github.com/chriskempson/base16-shell/blob/588691ba71b47e75793ed9edfcfaa058326a6f41/scripts/base16-solarized-light.sh">shell script</a> that I run when my shell starts, but that means the
colours get lost when restoring a tmux session.</li>
</ul>
<p>and probably more things I haven&rsquo;t thought of. &ldquo;Using tmux means that I have to
change how I manage my colours&rdquo; sounds unlikely, but that really did happen to
me and I decided &ldquo;well, I don&rsquo;t want to change how I manage colours right now,
so I guess I&rsquo;m not using that feature!&rdquo;.</p>
<p>It&rsquo;s also hard to remember which features I&rsquo;m relying on &ndash; for example maybe
my current terminal <em>does</em> have OSC 52 support and because copying from tmux over SSH
has always Just Worked I don&rsquo;t even realize that that&rsquo;s something I need, and
then it mysteriously stops working when I switch terminals.</p>
<h3 id="change-things-slowly">change things slowly</h3>
<p>Personally even though I think my setup is not <em>that</em> complicated, it&rsquo;s taken
me 20 years to get to this point! Because terminal config changes are so likely
to have unexpected and hard-to-understand consequences, I&rsquo;ve found that if I
change a lot of terminal configuration all at once it makes it much harder to
understand what went wrong if there&rsquo;s a problem, which can be really
disorienting.</p>
<p>So I usually prefer to make pretty small changes, and accept that changes can
might take me a REALLY long time to get used to. For example I switched from
using <code>ls</code> to <a href="https://github.com/eza-community/eza">eza</a> a year or two ago and
while I like it (because <code>eza -l</code> prints human-readable file sizes by default)
I&rsquo;m still not quite sure about it. But also sometimes it&rsquo;s worth it to make a
big change, like I made the switch to fish (from bash) 10 years ago and I&rsquo;m
very happy I did.</p>
<h3 id="getting-a-modern-terminal-is-not-that-easy">getting a &ldquo;modern&rdquo; terminal is not that easy</h3>
<p>Trying to explain how &ldquo;easy&rdquo; it is to configure your terminal really just made
me think that it&rsquo;s kind of hard and that I still sometimes get confused.</p>
<p>I&rsquo;ve found that there&rsquo;s never one perfect way to configure things in the
terminal that will be compatible with every single other thing. I just need to
try stuff, figure out some kind of locally stable state that works for me, and
accept that if I start using a new tool it might disrupt the system and I might
need to rethink things.</p>

---

### ["Rules" that terminal programs follow](https://jvns.ca/blog/2024/11/26/terminal-rules/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Recently I&rsquo;ve been thinking about how everything that happens in the terminal
is some combination of:</p>
<ol>
<li>Your <strong>operating system</strong>&rsquo;s job</li>
<li>Your <strong>shell</strong>&rsquo;s job</li>
<li>Your <strong>terminal emulator</strong>&rsquo;s job</li>
<li>The job of <strong>whatever program you happen to be running</strong> (like <code>top</code> or <code>vim</code> or <code>cat</code>)</li>
</ol>
<p>The first three (your operating system, shell, and terminal emulator) are all kind of
known quantities &ndash; if you&rsquo;re using bash in GNOME Terminal on Linux, you can
more or less reason about how how all of those things interact, and some of
their behaviour is standardized by POSIX.</p>
<p>But the fourth one (&ldquo;whatever program you happen to be running&rdquo;) feels like it
could do ANYTHING. How are you supposed to know how a program is going to
behave?</p>
<p>This post is kind of long so here&rsquo;s a quick table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#programs-behave-surprisingly-consistently">programs behave surprisingly consistently</a></li>
<li><a href="https://jvns.ca/atom.xml#these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</a></li>
<li><a href="https://jvns.ca/atom.xml#it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: <code>Ctrl-W</code> should delete the last word</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</a></li>
<li><a href="https://jvns.ca/atom.xml#these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</a></li>
</ul>
<h3 id="programs-behave-surprisingly-consistently">programs behave surprisingly consistently</h3>
<p>As far as I know, there are no real standards for how programs in the terminal
should behave &ndash; the closest things I know of are:</p>
<ul>
<li>POSIX, which mostly dictates how your terminal emulator / OS / shell should
work together. I think it does specify a few things about how core utilities like
<code>cp</code> should work but AFAIK it doesn&rsquo;t have anything to say about how for
example <code>htop</code> should behave.</li>
<li>these <a href="https://clig.dev/">command line interface guidelines</a></li>
</ul>
<p>But even though there are no standards, in my experience programs in the
terminal behave in a pretty consistent way. So I wanted to write down a list of
&ldquo;rules&rdquo; that in my experience programs mostly follow.</p>
<h3 id="these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</h3>
<p>My goal here isn&rsquo;t to convince authors of terminal programs that they <em>should</em>
follow any of these rules. There are lots of exceptions to these and often
there&rsquo;s a good reason for those exceptions.</p>
<p>But it&rsquo;s very useful for me to know what behaviour to expect from a random new
terminal program that I&rsquo;m using. Instead of &ldquo;uh, programs could do literally
anything&rdquo;, it&rsquo;s &ldquo;ok, here are the basic rules I expect, and then I can keep a
short mental list of exceptions&rdquo;.</p>
<p>So I&rsquo;m just writing down what I&rsquo;ve observed about how programs behave in my 20
years of using the terminal, why I think they behave that way, and some
examples of cases where that rule is &ldquo;broken&rdquo;.</p>
<h3 id="it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</h3>
<p>There are a bunch of common conventions that I think are pretty clearly the
program&rsquo;s responsibility to implement, like:</p>
<ul>
<li>config files should go in <code>~/.BLAHrc</code> or <code>~/.config/BLAH/FILE</code> or <code>/etc/BLAH/</code> or something</li>
<li><code>--help</code> should print help text</li>
<li>programs should print &ldquo;regular&rdquo; output to stdout and errors to stderr</li>
</ul>
<p>But in this post I&rsquo;m going to focus on things that it&rsquo;s not 100% obvious are
the program&rsquo;s responsibility. For example it feels to me like a &ldquo;law of nature&rdquo;
that pressing <code>Ctrl-D</code> should quit a REPL, but programs often
need to explicitly implement support for it &ndash; even though <code>cat</code> doesn&rsquo;t need
to implement <code>Ctrl-D</code> support, <code>ipython</code> <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">does</a>. (more about that in &ldquo;rule 3&rdquo; below)</p>
<p>Understanding which things are the program&rsquo;s responsibility makes it much less
surprising when different programs&rsquo; implementations are slightly different.</p>
<h3 id="rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></h3>
<p>The main reason for this rule is that noninteractive programs will quit by
default on <code>Ctrl-C</code> if they don&rsquo;t set up a <code>SIGINT</code> signal handler, so this is
kind of a &ldquo;you should act like the default&rdquo; rule.</p>
<p>Something that trips a lot of people up is that this doesn&rsquo;t apply to
<strong>interactive</strong> programs like <code>python3</code> or <code>bc</code> or <code>less</code>. This is because in
an interactive program, <code>Ctrl-C</code> has a different job &ndash; if the program is
running an operation (like for example a search in <code>less</code> or some Python code
in <code>python3</code>), then <code>Ctrl-C</code> will interrupt that operation but not stop the
program.</p>
<p>As an example of how this works in an interactive program: here&rsquo;s the code <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/vi.py#L2225">in prompt-toolkit</a> (the library that iPython uses for handling input)
that aborts a search when you press <code>Ctrl-C</code>.</p>
<h3 id="rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></h3>
<p>TUI programs (like <code>less</code> or <code>htop</code>) will usually quit when you press <code>q</code>.</p>
<p>This rule doesn&rsquo;t apply to any program where pressing <code>q</code> to quit wouldn&rsquo;t make
sense, like <code>tmux</code> or text editors.</p>
<h3 id="rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</h3>
<p>REPLs (like <code>python3</code> or <code>ed</code>) will usually quit when you press <code>Ctrl-D</code> on an
empty line. This rule is similar to the <code>Ctrl-C</code> rule &ndash; the reason for this is
that by default if you&rsquo;re running a program (like <code>cat</code>) in &ldquo;cooked mode&rdquo;, then
the operating system will return an <code>EOF</code> when you press <code>Ctrl-D</code> on an empty
line.</p>
<p>Most of the REPLs I use (sqlite3, python3, fish, bash, etc) don&rsquo;t actually use
cooked mode, but they all implement this keyboard shortcut anyway to mimic the
default behaviour.</p>
<p>For example, here&rsquo;s <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">the code in prompt-toolkit</a>
that quits when you press Ctrl-D, and here&rsquo;s <a href="https://github.com/bminor/bash/blob/6794b5478f660256a1023712b5fc169196ed0a22/lib/readline/readline.c#L658-L672">the same code in readline</a>.</p>
<p>I actually thought that this one was a &ldquo;Law of Terminal Physics&rdquo; until very
recently because I&rsquo;ve basically never seen it broken, but you can see that it&rsquo;s
just something that each individual input library has to implement in the links
above.</p>
<p>Someone pointed out that the Erlang REPL does not quit when you press <code>Ctrl-D</code>,
so I guess not every REPL follows this &ldquo;rule&rdquo;.</p>
<h3 id="rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</h3>
<p>Terminal programs rarely use colours other than the base 16 ANSI colours. This
is because if you specify colours with a hex code, it&rsquo;s very likely to clash
with some users&rsquo; background colour. For example if I print out some text as
<code>#EEEEEE</code>, it would be almost invisible on a white background, though it would
look fine on a dark background.</p>
<p>But if you stick to the default 16 base colours, you have a much better chance
that the user has configured those colours in their terminal emulator so that
they work reasonably well with their background color. Another reason to stick
to the default base 16 colours is that it makes less assumptions about what
colours the terminal emulator supports.</p>
<p>The only programs I usually see breaking this &ldquo;rule&rdquo; are text editors, for
example Helix by default will use a purple background which is not a default
ANSI colour. It seems fine for Helix to break this rule since Helix isn&rsquo;t a
&ldquo;core&rdquo; program and I assume any Helix user who doesn&rsquo;t like that colorscheme
will just change the theme.</p>
<h3 id="rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</h3>
<p>Almost every program I use supports <code>readline</code> keybindings if it would make
sense to do so. For example, here are a bunch of different programs and a link
to where they define <code>Ctrl-E</code> to go to the end of the line:</p>
<ul>
<li>ipython (<a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/emacs.py#L72">Ctrl-E defined here</a>)</li>
<li>atuin (<a href="https://github.com/atuinsh/atuin/blob/a67cfc82fe0dc907a01f07a0fd625701e062a33b/crates/atuin/src/command/client/search/interactive.rs#L407">Ctrl-E defined here</a>)</li>
<li>fzf (<a href="https://github.com/junegunn/fzf/blob/bb55045596d6d08445f3c6d320c3ec2b457462d1/src/terminal.go#L611">Ctrl-E defined here</a>)</li>
<li>zsh (<a href="https://github.com/zsh-users/zsh/blob/86d5f24a3d28541f242eb3807379301ea976de87/Src/Zle/zle_bindings.c#L94">Ctrl-E defined here</a>)</li>
<li>fish (<a href="https://github.com/fish-shell/fish-shell/blob/99fa8aaaa7956178973150a03ce4954ab17a197b/share/functions/fish_default_key_bindings.fish#L43">Ctrl-E defined here</a>)</li>
<li>tmux&rsquo;s command prompt (<a href="https://github.com/tmux/tmux/blob/ae8f2208c98e3c2d6e3fe4cad2281dce8fd11f31/key-bindings.c#L490">Ctrl-E defined here</a>)</li>
</ul>
<p>None of those programs actually uses <code>readline</code> directly, they just sort of
mimic emacs/readline keybindings. They don&rsquo;t always mimic them <em>exactly</em>: for
example atuin seems to use <code>Ctrl-A</code> as a prefix, so <code>Ctrl-A</code> doesn&rsquo;t go to the
beginning of the line.</p>
<p>Also all of these programs seem to implement their own internal cut and paste
buffers so you can delete a line with <code>Ctrl-U</code> and then paste it with <code>Ctrl-Y</code>.</p>
<p>The exceptions to this are:</p>
<ul>
<li>some programs (like <code>git</code>, <code>cat</code>, and <code>nc</code>) don&rsquo;t have any line editing support at all (except for backspace, <code>Ctrl-W</code>, and <code>Ctrl-U</code>)</li>
<li>as usual text editors are an exception, every text editor has its own
approach to editing text</li>
</ul>
<p>I wrote more about this &ldquo;what keybindings does a program support?&rdquo; question in
<a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a>.</p>
<h3 id="rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: Ctrl-W should delete the last word</h3>
<p>I&rsquo;ve never seen a program (other than a text editor) where <code>Ctrl-W</code> <em>doesn&rsquo;t</em>
delete the last word. This is similar to the <code>Ctrl-C</code> rule &ndash; by default if a
program is in &ldquo;cooked mode&rdquo;, the OS will delete the last word if you press
<code>Ctrl-W</code>, and delete the whole line if you press <code>Ctrl-U</code>. So usually programs
will imitate that behaviour.</p>
<p>I can&rsquo;t think of any exceptions to this other than text editors but if there
are I&rsquo;d love to hear about them!</p>
<h3 id="rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</h3>
<p>Most programs will disable colours when writing to a pipe. For example:</p>
<ul>
<li><code>rg blah</code> will highlight all occurrences of <code>blah</code> in the output, but if the
output is to a pipe or a file, it&rsquo;ll turn off the highlighting.</li>
<li><code>ls --color=auto</code> will use colour when writing to a terminal, but not when
writing to a pipe</li>
</ul>
<p>Both of those programs will also format their output differently when writing
to the terminal: <code>ls</code> will organize files into columns, and ripgrep will group
matches with headings.</p>
<p>If you want to force the program to use colour (for example because you want to
look at the colour), you can use <code>unbuffer</code> to force the program&rsquo;s output to be
a tty like this:</p>
<pre><code>unbuffer rg blah |  less -R
</code></pre>
<p>I&rsquo;m sure that there are some programs that &ldquo;break&rdquo; this rule but I can&rsquo;t think
of any examples right now. Some programs have an <code>--color</code> flag that you can
use to force colour to be on, in the example above you could also do <code>rg --color=always | less -R</code>.</p>
<h3 id="rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</h3>
<p>Usually if you pass <code>-</code> to a program instead of a filename, it&rsquo;ll read from
stdin or write to stdout (whichever is appropriate). For example, if you want
to format the Python code that&rsquo;s on your clipboard with <code>black</code> and then copy
it, you could run:</p>
<pre><code>pbpaste | black - | pbcopy
</code></pre>
<p>(<code>pbpaste</code> is a Mac program, you can do something similar on Linux with <code>xclip</code>)</p>
<p>My impression is that most programs implement this if it would make sense and I
can&rsquo;t think of any exceptions right now, but I&rsquo;m sure there are many
exceptions.</p>
<h3 id="these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</h3>
<p>These rules took me a long time for me to learn because I had to:</p>
<ol>
<li>learn that the rule applied anywhere at all (&quot;<code>Ctrl-C</code> will exit programs&quot;)</li>
<li>notice some exceptions (&ldquo;okay, <code>Ctrl-C</code> will exit <code>find</code> but not <code>less</code>&rdquo;)</li>
<li>subconsciously figure out what the pattern is (&quot;<code>Ctrl-C</code> will generally quit
noninteractive programs, but in interactive programs it might interrupt the
current operation instead of quitting the program&quot;)</li>
<li>eventually maybe formulate it into an explicit rule that I know</li>
</ol>
<p>A lot of my understanding of the terminal is honestly still in the
&ldquo;subconscious pattern recognition&rdquo; stage. The only reason I&rsquo;ve been taking the
time to make things explicit at all is because I&rsquo;ve been trying to explain how
it works to others. Hopefully writing down these &ldquo;rules&rdquo; explicitly will make
learning some of this stuff a little bit faster for others.</p>

---

### [Why pipes sometimes get "stuck": buffering](https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Here&rsquo;s a niche terminal problem that has bothered me for years but that I never
really understood until a few weeks ago. Let&rsquo;s say you&rsquo;re running this command
to watch for some specific output in a log file:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>If log lines are being added to the file relatively slowly, the result I&rsquo;d see
is&hellip; nothing! It doesn&rsquo;t matter if there were matches in the log file or not,
there just wouldn&rsquo;t be any output.</p>
<p>I internalized this as &ldquo;uh, I guess pipes just get stuck sometimes and don&rsquo;t
show me the output, that&rsquo;s weird&rdquo;, and I&rsquo;d handle it by just
running <code>grep thing1 /some/log/file | grep thing2</code> instead, which would work.</p>
<p>So as I&rsquo;ve been doing a terminal deep dive over the last few months I was
really excited to finally learn exactly why this happens.</p>
<h3 id="why-this-happens-buffering">why this happens: buffering</h3>
<p>The reason why &ldquo;pipes get stuck&rdquo; sometimes is that it&rsquo;s VERY common for
programs to buffer their output before writing it to a pipe or file. So the
pipe is working fine, the problem is that the program never even wrote the data
to the pipe!</p>
<p>This is for performance reasons: writing all output immediately as soon as you
can uses more system calls, so it&rsquo;s more efficient to save up data until you
have 8KB or so of data to write (or until the program exits) and THEN write it
to the pipe.</p>
<p>In this example:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>the problem is that <code>grep thing1</code> is saving up all of its matches until it has
8KB of data to write, which might literally never happen.</p>
<h3 id="programs-don-t-buffer-when-writing-to-a-terminal">programs don&rsquo;t buffer when writing to a terminal</h3>
<p>Part of why I found this so disorienting is that <code>tail -f file | grep thing</code>
will work totally fine, but then when you add the second <code>grep</code>, it stops
working!! The reason for this is that the way <code>grep</code> handles buffering depends
on whether it&rsquo;s writing to a terminal or not.</p>
<p>Here&rsquo;s how <code>grep</code> (and many other programs) decides to buffer its output:</p>
<ul>
<li>Check if stdout is a terminal or not using the <code>isatty</code> function
<ul>
<li>If it&rsquo;s a terminal, use line buffering (print every line immediately as soon as you have it)</li>
<li>Otherwise, use &ldquo;block buffering&rdquo; &ndash; only print data if you have at least 8KB or so of data to print</li>
</ul>
</li>
</ul>
<p>So if <code>grep</code> is writing directly to your terminal then you&rsquo;ll see the line as
soon as it&rsquo;s printed, but if it&rsquo;s writing to a pipe, you won&rsquo;t.</p>
<p>Of course the buffer size isn&rsquo;t always 8KB for every program, it depends on the implementation. For <code>grep</code> the buffering is handled by libc, and libc&rsquo;s buffer size is
defined in the <code>BUFSIZ</code> variable. <a href="https://github.com/bminor/glibc/blob/c69e8cccaff8f2d89cee43202623b33e6ef5d24a/libio/stdio.h#L100">Here&rsquo;s where that&rsquo;s defined in glibc</a>.</p>
<p>(as an aside: &ldquo;programs do not use 8KB output buffers when writing to a
terminal&rdquo; isn&rsquo;t, like, a law of terminal physics, a program COULD use an 8KB
buffer when writing output to a terminal if it wanted, it would just be
extremely weird if it did that, I can&rsquo;t think of any program that behaves that
way)</p>
<h3 id="commands-that-buffer-commands-that-don-t">commands that buffer &amp; commands that don&rsquo;t</h3>
<p>One annoying thing about this buffering behaviour is that you kind of need to
remember which commands buffer their output when writing to a pipe.</p>
<p>Some commands that <strong>don&rsquo;t</strong> buffer their output:</p>
<ul>
<li>tail</li>
<li>cat</li>
<li>tee</li>
</ul>
<p>I think almost everything else will buffer output, especially if it&rsquo;s a command
where you&rsquo;re likely to be using it for batch processing. Here&rsquo;s a list of some
common commands that buffer their output when writing to a pipe, along with the
flag that disables block buffering.</p>
<ul>
<li>grep (<code>--line-buffered</code>)</li>
<li>sed (<code>-u</code>)</li>
<li>awk (there&rsquo;s a <code>fflush()</code> function)</li>
<li>tcpdump (<code>-l</code>)</li>
<li>jq (<code>-u</code>)</li>
<li>tr (<code>-u</code>)</li>
<li>cut (can&rsquo;t disable buffering)</li>
</ul>
<p>Those are all the ones I can think of, lots of unix commands (like <code>sort</code>) may
or may not buffer their output but it doesn&rsquo;t matter because <code>sort</code> can&rsquo;t do
anything until it finishes receiving input anyway.</p>
<p>Also I did my best to test both the Mac OS and GNU versions of these but there
are a lot of variations and I might have made some mistakes.</p>
<h3 id="programming-languages-where-the-default-print-statement-buffers">programming languages where the default &ldquo;print&rdquo; statement buffers</h3>
<p>Also, here are a few programming language where the default print statement
will buffer output when writing to a pipe, and some ways to disable buffering
if you want:</p>
<ul>
<li>C (disable with <code>setvbuf</code>)</li>
<li>Python (disable with <code>python -u</code>, or <code>PYTHONUNBUFFERED=1</code>, or <code>sys.stdout.reconfigure(line_buffering=False)</code>, or <code>print(x, flush=True)</code>)</li>
<li>Ruby (disable with <code>STDOUT.sync = true</code>)</li>
<li>Perl (disable with <code>$| = 1</code>)</li>
</ul>
<p>I assume that these languages are designed this way so that the default print
function will be fast when you&rsquo;re doing batch processing.</p>
<p>Also whether output is buffered or not might depend on how you print, for
example in C++ <code>cout &lt;&lt; &quot;hello\n&quot;</code> buffers when writing to a pipe but <code>cout &lt;&lt; &quot;hello&quot; &lt;&lt; endl</code> will flush its output.</p>
<h3 id="when-you-press-ctrl-c-on-a-pipe-the-contents-of-the-buffer-are-lost">when you press <code>Ctrl-C</code> on a pipe, the contents of the buffer are lost</h3>
<p>Let&rsquo;s say you&rsquo;re running this command as a hacky way to watch for DNS requests
to <code>example.com</code>, and you forgot to pass <code>-l</code> to tcpdump:</p>
<pre><code>sudo tcpdump -ni any port 53 | grep example.com
</code></pre>
<p>When you press <code>Ctrl-C</code>, what happens? In a magical perfect world, what I would
<em>want</em> to happen is for <code>tcpdump</code> to flush its buffer, <code>grep</code> would search for
<code>example.com</code>, and I would see all the output I missed.</p>
<p>But in the real world, what happens is that all the programs get killed and the
output in <code>tcpdump</code>&rsquo;s buffer is lost.</p>
<p>I think this problem is probably unavoidable &ndash; I spent a little time with
<code>strace</code> to see how this works and <code>grep</code> receives the <code>SIGINT</code> before
<code>tcpdump</code> anyway so even if <code>tcpdump</code> tried to flush its buffer <code>grep</code> would
already be dead.</p>
<small>
<p>After a little more investigation, there is a workaround: if you find
<code>tcpdump</code>&rsquo;s PID and <code>kill -TERM $PID</code>, then tcpdump will flush the buffer so
you can see the output. That&rsquo;s kind of a pain but I tested it and it seems to
work.</p>
</small>
<h3 id="redirecting-to-a-file-also-buffers">redirecting to a file also buffers</h3>
<p>It&rsquo;s not just pipes, this will also buffer:</p>
<pre><code>sudo tcpdump -ni any port 53 &gt; output.txt
</code></pre>
<p>Redirecting to a file doesn&rsquo;t have the same &ldquo;<code>Ctrl-C</code> will totally destroy the
contents of the buffer&rdquo; problem though &ndash; in my experience it usually behaves
more like you&rsquo;d want, where the contents of the buffer get written to the file
before the program exits. I&rsquo;m not 100% sure whether this is something you can
always rely on or not.</p>
<h3 id="a-bunch-of-potential-ways-to-avoid-buffering">a bunch of potential ways to avoid buffering</h3>
<p>Okay, let&rsquo;s talk solutions. Let&rsquo;s say you&rsquo;ve run this command:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>I asked people on Mastodon how they would solve this in practice and there were
5 basic approaches. Here they are:</p>
<h4 id="solution-1-run-a-program-that-finishes-quickly">solution 1: run a program that finishes quickly</h4>
<p>Historically my solution to this has been to just avoid the &ldquo;command writing to
pipe slowly&rdquo; situation completely and instead run a program that will finish quickly
like this:</p>
<pre><code>cat /some/log/file | grep thing1 | grep thing2 | tail
</code></pre>
<p>This doesn&rsquo;t do the same thing as the original command but it does mean that
you get to avoid thinking about these weird buffering issues.</p>
<p>(you could also do <code>grep thing1 /some/log/file</code> but I often prefer to use an
&ldquo;unnecessary&rdquo; <code>cat</code>)</p>
<h4 id="solution-2-remember-the-line-buffer-flag-to-grep">solution 2: remember the &ldquo;line buffer&rdquo; flag to grep</h4>
<p>You could remember that grep has a flag to avoid buffering and pass it like this:</p>
<pre><code>tail -f /some/log/file | grep --line-buffered thing1 | grep thing2
</code></pre>
<h4 id="solution-3-use-awk">solution 3: use awk</h4>
<p>Some people said that if they&rsquo;re specifically dealing with a multiple greps
situation, they&rsquo;ll rewrite it to use a single <code>awk</code> instead, like this:</p>
<pre><code>tail -f /some/log/file |  awk '/thing1/ &amp;&amp; /thing2/'
</code></pre>
<p>Or you would write a more complicated <code>grep</code>, like this:</p>
<pre><code>tail -f /some/log/file |  grep -E 'thing1.*thing2'
</code></pre>
<p>(<code>awk</code> also buffers, so for this to work you&rsquo;ll want <code>awk</code> to be the last command in the pipeline)</p>
<h4 id="solution-4-use-stdbuf">solution 4: use <code>stdbuf</code></h4>
<p><code>stdbuf</code> uses LD_PRELOAD to turn off libc&rsquo;s buffering, and you can use it to turn off output buffering like this:</p>
<pre><code>tail -f /some/log/file | stdbuf -o0 grep thing1 | grep thing2
</code></pre>
<p>Like any <code>LD_PRELOAD</code> solution it&rsquo;s a bit unreliable &ndash; it doesn&rsquo;t work on
static binaries, I think won&rsquo;t work if the program isn&rsquo;t using libc&rsquo;s
buffering, and doesn&rsquo;t always work on Mac OS. Harry Marr has a really nice <a href="https://hmarr.com/blog/how-stdbuf-works/">How stdbuf works</a> post.</p>
<h4 id="solution-5-use-unbuffer">solution 5: use <code>unbuffer</code></h4>
<p><code>unbuffer program</code> will force the program&rsquo;s output to be a TTY, which means
that it&rsquo;ll behave the way it normally would on a TTY (less buffering, colour
output, etc). You could use it in this example like this:</p>
<pre><code>tail -f /some/log/file | unbuffer grep thing1 | grep thing2
</code></pre>
<p>Unlike <code>stdbuf</code> it will always work, though it might have unwanted side
effects, for example <code>grep thing1</code>&rsquo;s will also colour matches.</p>
<p>If you want to install unbuffer, it&rsquo;s in the <code>expect</code> package.</p>
<h3 id="that-s-all-the-solutions-i-know-about">that&rsquo;s all the solutions I know about!</h3>
<p>It&rsquo;s a bit hard for me to say which one is &ldquo;best&rdquo;, I think personally I&rsquo;m
mostly likely to use <code>unbuffer</code> because I know it&rsquo;s always going to work.</p>
<p>If I learn about more solutions I&rsquo;ll try to add them to this post.</p>
<h3 id="i-m-not-really-sure-how-often-this-comes-up">I&rsquo;m not really sure how often this comes up</h3>
<p>I think it&rsquo;s not very common for me to have a program that slowly trickles data
into a pipe like this, normally if I&rsquo;m using a pipe a bunch of data gets
written very quickly, processed by everything in the pipeline, and then
everything exits. The only examples I can come up with right now are:</p>
<ul>
<li>tcpdump</li>
<li><code>tail -f</code></li>
<li>watching log files in a different way like with <code>kubectl logs</code></li>
<li>the output of a slow computation</li>
</ul>
<h3 id="what-if-there-were-an-environment-variable-to-disable-buffering">what if there were an environment variable to disable buffering?</h3>
<p>I think it would be cool if there were a standard environment variable to turn
off buffering, like <code>PYTHONUNBUFFERED</code> in Python. I got this idea from a
<a href="https://blog.plover.com/Unix/stdio-buffering.html">couple</a> of <a href="https://blog.plover.com/Unix/stdio-buffering-2.html">blog posts</a> by Mark Dominus
in 2018. Maybe <code>NO_BUFFER</code> like <a href="https://no-color.org/">NO_COLOR</a>?</p>
<p>The design seems tricky to get right; Mark points out that NETBSD has <a href="https://man.netbsd.org/setbuf.3">environment variables called <code>STDBUF</code>, <code>STDBUF1</code>, etc</a> which gives you a
ton of control over buffering but I imagine most developers don&rsquo;t want to
implement many different environment variables to handle a relatively minor
edge case.</p>
<p>I&rsquo;m also curious about whether there are any programs that just automatically
flush their output buffers after some period of time (like 1 second). It feels
like it would be nice in theory but I can&rsquo;t think of any program that does that
so I imagine there are some downsides.</p>
<h3 id="stuff-i-left-out">stuff I left out</h3>
<p>Some things I didn&rsquo;t talk about in this post since these posts have been
getting pretty long recently and seriously does anyone REALLY want to read 3000
words about buffering?</p>
<ul>
<li>the difference between line buffering and having totally unbuffered output</li>
<li>how buffering to stderr is different from buffering to stdout</li>
<li>this post is only about buffering that happens <strong>inside the program</strong>, your
operating system&rsquo;s TTY driver also does a little bit of buffering sometimes</li>
<li>other reasons you might need to flush your output other than &ldquo;you&rsquo;re writing
to a pipe&rdquo;</li>
</ul>

---

### [2025 Recap: so many projects](https://fasterthanli.me/articles/2025-recap)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I‚Äôve been working on so many projects in 2025, I thought it was important for me
to make a recap, if only just to clear my head.</p>

<p>There are many, many, many things to go through and we don‚Äôt have a sponsor
today, so I‚Äôm gonna start right away with facet!</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#facet" id="facet"><h2>facet</h2></a>
<p>facet is a project that I started working on in March of this year ‚Äî that‚Äôs
right, it‚Äôs only been ten months, yet it feels like an eternity.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#in-the-beginning" id="in-the-beginning"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#the-first-golden-age" id="the-first-golden-age"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#disappointment" id="disappointment"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#refocus" id="refocus"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#volte-face" id="volte-face"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#just-in-time-for-the-new-year" id="just-in-time-for-the-new-year"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#dodeca" id="dodeca"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#rapace" id="rapace"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#tracey" id="tracey"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#picante" id="picante"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#pikru" id="pikru"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#aasvg-rs" id="aasvg-rs"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#facet-keeps-growing" id="facet-keeps-growing"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#fs-kitty" id="fs-kitty"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#vixen" id="vixen"></a>






























































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Introducing arborium, a tree-sitter distribution](https://fasterthanli.me/articles/introducing-arborium)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>About two weeks ago I entered a discussion with the docs.rs team about,
basically, why we have to look at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages but they're all white on black. It's sad.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-no-colors@2x~eb49252c206ebe40.jxl" title="" width="1083" /></p><p>When we could be looking at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages. this time it's colored.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-yes-colors@2x~708d0f07e1265747.jxl" title="" width="1083" /></p><p>And of course, as always, there are reasons why things are the way they are.
In an effort to understand those reasons, I opened a GitHub issue which resulted
in a <a href="https://github.com/rust-lang/docs.rs/issues/3040">short but productive</a> discussion.</p>

<p>I walked away discouraged, and then decided to, reasons be damned, attack this
problem from three different angles.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#background" id="background"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#problems" id="problems"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#solutions" id="solutions"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-1-just-include-this-script" id="angle-1-just-include-this-script"></a>

























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-2-it-goes-in-the-rustdoc-hole" id="angle-2-it-goes-in-the-rustdoc-hole"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#angle-3-only-in-the-backend" id="angle-3-only-in-the-backend"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#post-mortem" id="post-mortem"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Does Dioxus spark joy?](https://fasterthanli.me/articles/does-dioxus-spark-joy)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <div class="dialog right">
<div class="dialog-head" title="Amos says:">
  <source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Amos" height="42" src="https://cdn.fasterthanli.me/content/img/reimena/amos-neutral~55a3477398fe0cb1.jxl" width="42" />
</div>
<div class="dialog-text markup-container">
<p>Note: this article is adapted from a presentation I gave at a Rust Paris Meetup ‚Äî that‚Äôs why
it sounds a little different than usual. Enjoy!</p>

</div>
</div><p>Good evening! Tonight, I will attempt to answer the question: Does
<a href="https://github.com/dioxuslabs/dioxus">Dioxus</a> spark joy? Or at the very least,
whimsy.</p>

<p>What‚Äôs Dioxus, you ask? It is first and foremost a name that is quote: ‚Äúlegally
not inspired by any Pok√©mon‚Äù.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="The deoxys pokemon
" class="" height="449" src="https://cdn.fasterthanli.me/content/articles/does-dioxus-spark-joy/deoxys@2x~d76eb1e3eeda1b65.jxl" title="" width="422" /></p><p>Even if the author concedes <a href="https://news.ycombinator.com/item?id=39853385">in a Hacker News comment</a> that the ‚ÄúDeoxys‚Äù Pok√©mon
is, I quote: ‚Äúawesome‚Äù.</p>











<a class="anchor" href="https://fasterthanli.me/index.xml#a-short-and-mostly-wrong-history-of-web-apps" id="a-short-and-mostly-wrong-history-of-web-apps"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#a-practical-example" id="a-practical-example"></a>































<a class="anchor" href="https://fasterthanli.me/index.xml#love-hate" id="love-hate"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#does-dioxus-spark-joy" id="does-dioxus-spark-joy"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#afterword" id="afterword"></a>

---

### [Engineering a Rust optimization quiz](https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are several Rust quizzes online, including one that‚Äôs literally called the
‚ÄúUnfair Rust Quiz‚Äù at <a href="https://this.quiz.is.fckn.gay/">https:&#x2f;&#x2f;this.quiz.is.fckn.gay&#x2f;</a>, but when I was given the
opportunity to record an episode of the <a href="https://sdr-podcast.com/">Self-Directed Research
podcast</a> live on the main stage of <a href="https://eurorust.eu/2025/">EuroRust
2025</a>, I thought I‚Äôd come up with something special.</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Question Misc 6 of the unfair Rust quiz, about drop order. " height="714" src="https://cdn.fasterthanli.me/content/articles/engineering-a-rust-optimization-quiz/unfair-rust-quiz@2x~478a86d3446281d7.jxl" title="The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side. " width="795" /><figcaption><div class="markup-container figure-caption"><p>The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side.</p>
</div></figcaption>
</figure>



<a class="anchor" href="https://fasterthanli.me/index.xml#coming-up-with-the-questions" id="coming-up-with-the-questions"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#nih-syndrome-ppt" id="nih-syndrome-ppt"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#server-side-shenanigans-and-room-codes" id="server-side-shenanigans-and-room-codes"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#deploying-the-beast" id="deploying-the-beast"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#d-2-starting-fights-at-a-paris-meetup" id="d-2-starting-fights-at-a-paris-meetup"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#d-1-panic-mode-and-missing-explanations" id="d-1-panic-mode-and-missing-explanations"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#day-of-github-oauth-and-swipe-gestures" id="day-of-github-oauth-and-swipe-gestures"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#showtime" id="showtime"></a>

---

### [Making our own spectrogram](https://fasterthanli.me/articles/making-our-own-spectrogram)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>A couple months ago <a href="https://fasterthanli.me/articles/the-science-of-loudness">I made a loudness meter</a>
and went way too in-depth into how humans have measured loudness over time.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the fasterthanlime audio meter, with RMS, sample peak, true peak, and various loudness metrics.
" class="" height="512" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/fasterthanlime-audio-meter@2x~d5e6b54e3ade21f5.jxl" title="" width="800" /></p><p>Today we‚Äôre looking at a spectrogram visualization I made, which is a lot more entertaining!</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="" class="" height="632" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/spectrogram-sonata-op25-3@2x~c22af608785d58b2.jxl" title="" width="1100" /></p><p>We‚Äôre going to talk about how to extract frequencies from sound waves, but also
how my spectrogram app is assembled from different Rust crates, how it
handles audio and graphics threads, how it draws the spectrogram etc.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#the-humble-sine-wave" id="the-humble-sine-wave"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#approximating-a-square-wave" id="approximating-a-square-wave"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#a-real-world-sample" id="a-real-world-sample"></a>







<a class="anchor" href="https://fasterthanli.me/index.xml#chunking-and-windowing" id="chunking-and-windowing"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#the-gabor-limit" id="the-gabor-limit"></a>





















<a class="anchor" href="https://fasterthanli.me/index.xml#interpolation" id="interpolation"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#color-mapping" id="color-mapping"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#frequency-mapping" id="frequency-mapping"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#audio-input" id="audio-input"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#drawing-the-ui" id="drawing-the-ui"></a>



















































<a class="anchor" href="https://fasterthanli.me/index.xml#updating-the-texture" id="updating-the-texture"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#profiling-my-program" id="profiling-my-program"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#having-fun" id="having-fun"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#closing-words" id="closing-words"></a>

---

### [crates.io phishing attempt](https://fasterthanli.me/articles/crates-io-phishing-attempt)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>Earlier this week, an <a href="https://fasterthanli.me/articles/color-npm-package-compromised">npm supply chain attack</a>.</p>

<p>It‚Äôs turn for <a href="https://crates.io">crates.io</a>, the main public repository for <a href="https://rust-lang.org">Rust</a>
crates (packages).</p>

<p>The phishing e-mail looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="A phishing e-mail: Important: Breach notification regarding crates.io  Hi, BurntSushi! We recently discovered that an unauthorized actor had compromised the crates.io infrastructure and accessed a limited amount of user information. The attacker's access was revoked, and we are currently reviewing our security posture. We are currently drafting a blog post to outline the timeline and the steps we took to mitigate this. In the meantime, we strongly suggest you to rotate your login info by signing in here to our internal SSO, which is a temporary fix to ensure that the attacker cannot modify any packages published by you. " height="653" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/phishing-email~052f360399d29116.jxl" title="" width="708" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;burntsushi.net&#x2f;post&#x2f;3lynehptw6c2n">Andrew Gallant on BlueSky
</a></cite></figcaption>
</figure><p>And it leads to a GitHub login page that looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A fake GitHub sign-in page. " height="1378" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/github-phish~e78524d35ede5efb.jxl" title="" width="1322" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;github.com&#x2f;rust-lang&#x2f;crates.io&#x2f;discussions&#x2f;11889#discussion-8886064">Barre on GitHub
</a></cite></figcaption>
</figure><p>Several maintainers received it ‚Äî the issue is being discussed <a href="https://github.com/rust-lang/crates.io/discussions/11889">on GitHub</a>.</p>

<p>The <a href="https://www.rust-lang.org/governance/teams/dev-tools#team-crates-io">crates.io team</a> has acknowledged
the attack and said they‚Äôd see if they can do something about it.</p>

---

### [color npm package compromised](https://fasterthanli.me/articles/color-npm-package-compromised)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>On September 8 2025, around 13:00 UTC, someone compromised <a href="https://www.npmjs.com/~qix">Josh Junon‚Äôs npm
account (qix)</a> and started publishing backdoored
versions of his package.</p>

<p>Someone noticed and let Josh know:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Hey. Your npm account seems to have been compromised. 1 hour ago it started posting packages with backdoors to all your popular packages. " height="177" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/charlie-noticed@2x~4a9e74f87760a4af.jxl" title="" width="595" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;charlieeriksen.bsky.social&#x2f;post&#x2f;3lydffcyulc2n">Charlie Eriksen on BlueSky
</a></cite></figcaption>
</figure><p>Josh confirmed he‚Äôd gotten pwned by a fake 2FA (two-factor authentication) reset e-mail:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Yep, I've been pwned. 2FA reset email, looked very legitimate.  Only NPM affected. I've sent an email off to @npmjs.bsky.social  to see if I can get access again.  Sorry everyone, I should have paid more attention. Not like me; have had a stressful week. Will work to get this cleaned up. " height="396" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/josh-fake-2fa@2x~ca37f72d582a4442.jxl" title="" width="592" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;bad-at-computer.bsky.social&#x2f;post&#x2f;3lydioq5swk2y">Josh Junon on BlueSky
</a></cite></figcaption>
</figure><p>The phishing e-mail came from <code>npmsj.help</code> (registered 3 days prior) and claimed
users had to reset their 2FA:</p>







<a class="anchor" href="https://fasterthanli.me/index.xml#the-payload" id="the-payload"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#current-situation" id="current-situation"></a>

---

### [The science of loudness](https://fasterthanli.me/articles/the-science-of-loudness)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>My watch has a ‚ÄúNoise‚Äù app: it shows <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">B</mi></mrow></math>, for decibels.</p>

<p><video alt="A video of my Apple Watch showing me how loud the sound coming from my speakers is.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~d96a722c49fdda5d.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~e53858e76f36614d.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~fb169411c4d8653e.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>My amp has a volume knob, which also shows decibels, although.. negative ones, this time.</p>

<p><video alt="A video of me adjusting my volume on my Cambridge AXR100 amplifier. The
volume goes from -61 to -32 decibels in that video.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~46153fb1ad60f230.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~a8af11cbe6d90598.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~9a7388f3f768f948.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>And finally, my video editing software has a ton of meters ‚Äî which are all in decibel or
decibel-adjacent units.</p>

<p><video alt="A screenshot of DaVinci Resolve, showing various meters: we have Bus 1,
Control Room with TP, Loudness, YouTube (LUFS), then Loudness History
with Integrated, Momentary and Short Term. In the Mixer, each track has
its meter.
" class="" controls="controls" height="1127" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x.thumb" preload="none" title="" width="2002"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~9cc29f2809d1202a.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~da316f2ab0e0aeb5.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~e50f9e76cf48090c.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>How do all these decibels fit together?</p>






<a class="anchor" href="https://fasterthanli.me/index.xml#what-even-is-sound" id="what-even-is-sound"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#under-pressure" id="under-pressure"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#signal-processing" id="signal-processing"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#root-mean-square" id="root-mean-square"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#sample-peak-true-peak" id="sample-peak-true-peak"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#the-loudness-wars" id="the-loudness-wars"></a>











































































<a class="anchor" href="https://fasterthanli.me/index.xml#a-weighting" id="a-weighting"></a>

---

### [Summer fasterthanlime update](https://fasterthanli.me/articles/summer-fasterthanlime-update)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are news!</p>

<div class="tip markup-container">
<div class="tip-header bear-mark">
<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Cool bear" class="bear" height="48" src="https://cdn.fasterthanli.me/content/img/reimena/coolbear-idea~85823bd96ffd0bb6.jxl" width="48" />
Cool Bear's hot tip
</div>
<p>TL;DR: If you‚Äôre a patron or sponsor, check your <a href="https://fasterthanli.me/profile">Profile</a> page to
get detailed explainers of every perk. You‚Äôll need to log in. Duh.</p>

</div><p>Here are all the changes I‚Äôm implementing, summarized as a table:</p>

<div class="responsive-table"><table><thead><td>Before</td><td>After</td></thead><tr><td>üìö Articles remain exclusive for <strong>6 months</strong></td><td>Early access (<strong>couple weeks</strong>) for Silver tier</td></tr><tr><td>üéûÔ∏è No early access for video</td><td><strong>Video early access</strong> on Patreon and website</td></tr></table></div>

<a class="anchor" href="https://fasterthanli.me/index.xml#looking-back" id="looking-back"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#a-discord-server" id="a-discord-server"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#early-access-revamp" id="early-access-revamp"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#dual-rss-feeds" id="dual-rss-feeds"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#bye-ko-fi" id="bye-ko-fi"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#more-casual-posting" id="more-casual-posting"></a>




<a class="anchor" href="https://fasterthanli.me/index.xml#what-about-content-that-was-still-exclusive" id="what-about-content-that-was-still-exclusive"></a>

---

### [All color is best-effort](https://fasterthanli.me/articles/all-color-is-best-effort)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I do not come to you with answers today, but rather some observations and a lot of questions.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#the-weird-glitch" id="the-weird-glitch"><h2>The weird glitch</h2></a>
<p>Recently I was editing some video and I noticed this:</p>

<p>



<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/dvr-circles@2x~3c031f1f42693336.jxl" title="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." width="1966" /></p>

<p>Not what the finger is pointing at ‚Äî the dots.</p>

<p>Here are the separate layers this image is made up of: the background is a stock image
I‚Äôve licensed from Envato Elements:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A picture of a canyon, darker than you‚Äôd expect." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/canyon-background@2x~dcfb5771209ddbd5.jxl" title="A picture of a canyon, darker than you‚Äôd expect." width="1966" /></p>

<p>Because I use it as a background image, I‚Äôve cranked down the exposition in the Color tab:</p>













































































<a class="anchor" href="https://fasterthanli.me/index.xml#playing-with-color-spaces" id="playing-with-color-spaces"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#cie-chromaticity-diagram" id="cie-chromaticity-diagram"></a>


























































<a class="anchor" href="https://fasterthanli.me/index.xml#our-first-transfer-function" id="our-first-transfer-function"></a>






















































<a class="anchor" href="https://fasterthanli.me/index.xml#parade-scope" id="parade-scope"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#more-transfer-functions" id="more-transfer-functions"></a>































































































<a class="anchor" href="https://fasterthanli.me/index.xml#how-white-is-your-white" id="how-white-is-your-white"></a>









































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

## Tech News

### [Giving University Exams in the Age of Chatbots](https://ploum.net/2026-01-19-exam-with-chatbots.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/parmy3/giving_university_exams_age_chatbots">Comments</a></p>

---

### [Unconventional PostgreSQL Optimizations](https://hakibenita.com/postgresql-unconventional-optimizations)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/hrsaz6/unconventional_postgresql">Comments</a></p>

---

### [Why ALTER TABLE is such a problem for SQLite](https://www.sqlite.org/lang_altertable.html#why_alter_table_is_such_a_problem_for_sqlite)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/nsjybj/why_alter_table_is_such_problem_for_sqlite">Comments</a></p>

---

### [Type-safe eval in Grace](https://haskellforall.com/2026/01/typesafe-eval)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/kvviy0/type_safe_eval_grace">Comments</a></p>

---

### [Reliable Signals of Honest Intent](https://zanlib.dev/blog/reliable-signals-of-honest-intent/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/qy6azg/reliable_signals_honest_intent">Comments</a></p>

---

### [A static site generator written in POSIX shell](https://aashvik.com/posts/shell-ssg)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p>A walk through the design, tradeoffs, and questionable <code>sed</code> / <code>eval</code> of the POSIX shell script that builds my blog.</p>
<p><a href="https://lobste.rs/s/fsq2qu/static_site_generator_written_posix">Comments</a></p>

---

### [Prediction: Within 15 years Microsoft will discontinue Windows in favor of a Windows themed Linux distribution](https://gamesbymason.com/blog/2026/microsoft/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/yzsckk/prediction_within_15_years_microsoft">Comments</a></p>

---

### [Where I'm at with AI](https://paulosman.me/2026/01/18/where-im-at-with-ai/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/ezyebi/where_i_m_at_with_ai">Comments</a></p>

---

### [My week with opencode](https://deadsimpletech.com/blog/week_with_opencode)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/oxeeyz/my_week_with_opencode">Comments</a></p>

---


---

## üìö Â¶Ç‰Ωï‰ΩøÁî®

1. ÊµèËßàÊÑüÂÖ¥Ë∂£ÁöÑÊ†áÈ¢ò
2. ÈòÖËØªAIÁîüÊàêÁöÑÊëòË¶ÅÂø´ÈÄü‰∫ÜËß£ÂÜÖÂÆπ
3. ÁÇπÂáªÈìæÊé•Ê∑±ÂÖ•ÈòÖËØª
4. Êúâ‰ª∑ÂÄºÁöÑÂÜÖÂÆπÂèØ‰ª•Êï¥ÁêÜÂà∞ÂØπÂ∫îÁöÑ‰∏ªÈ¢òÁõÆÂΩï

## üîß ÈÖçÁΩÆ

‰øÆÊîπ `config/sources.yaml` ÂèØ‰ª•:
- Ê∑ªÂä†/Âà†Èô§RSSËÆ¢ÈòÖÊ∫ê
- Ë∞ÉÊï¥HackerNewsÊúÄÂ∞èÂàÜÊï∞ÈòàÂÄº
- ÈÖçÁΩÆÂÜÖÂÆπËøáÊª§ÂÖ≥ÈîÆËØç

*Êú¨ÊñáÊ°£Áî± [daily-digestËÑöÊú¨](../scripts/generate_digest.py) Ëá™Âä®ÁîüÊàê*