# ÊØèÊó•ÊäÄÊúØÊëòË¶Å - 2026-01-20

> Ëá™Âä®ÁîüÊàê‰∫é 2026-01-20 01:08:36
> ÂÖ±Êî∂ÈõÜ 149 ÁØáÊñáÁ´†

## üìë ÁõÆÂΩï

- [AI Research](#ai-research) (4ÁØá)
- [Engineering](#engineering) (10ÁØá)
- [HackerNews](#hackernews) (90ÁØá)
- [LLM Infrastructure](#llm-infrastructure) (10ÁØá)
- [Operating Systems](#operating-systems) (6ÁØá)
- [Systems](#systems) (21ÁØá)
- [Tech News](#tech-news) (8ÁØá)

---

## AI Research

### [Pretraining on Aligned AI Data Dramatically Reduces Misalignment‚ÄîEven After Post-Training](https://www.lesswrong.com/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 19, 2026 9:24 PM GMT<br /><br /><h2>Alignment Pretraining Shows Promise</h2><p><strong>TL;DR</strong>: A <a href="https://arxiv.org/pdf/2601.10160v1">new paper</a> shows that pretraining language models on data about AI behaving well dramatically reduces misaligned behavior, and this effect persists through post-training. The major labs appear to be taking notice. It‚Äôs now the third paper on this idea, and excitement seems to be building.</p><h3>How We Got Here</h3><p><i>(This is a survey/reading list, and doubtless omits some due credit and useful material ‚Äî please suggest additions in the comments, so I can update it. Or you can just </i><a href="https://www.lesswrong.com/editPost?postId=ZeWewFEefCtx4Rj3G&amp;key=4b092ce72ccb4955d7130b242998f8#New_Paper_Shows_Strong_Results"><i>skip forward to the paper</i></a><i>.)</i></p><p>Personally I‚Äôve been very excited about this alignment technique for a couple of years, ever since I read the seminal paper on it <a href="https://arxiv.org/pdf/2302.08582.pdf"><i>Pretraining Language Models with Human Preferences</i></a> (Feb ‚Äô23).<span class="footnote-reference" id="fnrefegi42dgdbep"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnegi42dgdbep">[1]</a></sup></span>&nbsp;(This technique is now called ‚Äú<a href="https://www.lesswrong.com/w/alignment-pretraining">alignment pretraining</a>‚Äù: it‚Äôs part of the broader ‚Äúsafety pretraining‚Äù area.) Their idea was to give the model plenty of labeled examples of good behavior all the way through pretraining: they showed it was (in small models for simple behaviors) roughly an order of magnitude more effective than various alternatives. I linkposted this in <a href="https://www.lesswrong.com/posts/JviYwAk5AfBR7HhEn/how-to-control-an-llm-s-behavior-why-my-p-doom-went-down-1">How to Control an LLM's Behavior (why my P(DOOM) went down)</a> (Nov ‚Äô23).</p><p>There was then a two-year lull in academic papers on the topic; undeterred, in <a href="https://www.lesswrong.com/posts/RCoBrvWfBMzGWNZ4t/motivating-alignment-of-llm-powered-agents-easy-for-agi-hard">Motivating Alignment of LLM-Powered Agents: Easy for AGI, Hard for ASI?</a> (Jan ‚Äô24) I wrote about possible motivations to instill and suggested <a href="https://www.lesswrong.com/w/aligned-ai-role-model-fiction">Aligned AI Role-Model Fiction</a> as a way of generating alignment pretraining data. <a href="https://www.beren.io/aboutme/">Beren Millidge</a> posted <a href="https://www.beren.io/2024-05-11-Alignment-in-the-Age-of-Synthetic-Data/">Alignment In The Age Of Synthetic Data</a> (May ‚Äô24) pointing out the alignment possibilities of pretraining-scale synthetic datasets, following on from his earlier related posts <a href="https://www.lesswrong.com/posts/XnnMYMjDGuYhkhQPB/the-case-for-removing-alignment-and-ml-research-from-the">The case for removing alignment and ML research from the training data</a> (May ‚Äô23) and <a href="https://www.beren.io/2023-07-05-My-path-to-prosaic-alignment-and-open-questions/">My path to prosaic alignment and open questions</a> (Jul ‚Äô23). I continued posting on this topic in <a href="https://www.lesswrong.com/posts/oRQMonLfdLfoGcDEh/a-bitter-lesson-approach-to-aligning-agi-and-asi-1">A "Bitter Lesson" Approach to Aligning AGI and ASI</a> (Jul ‚Äô24)<span class="footnote-reference" id="fnref443ujm44s26"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn443ujm44s26">[2]</a></sup></span>&nbsp;and <a href="https://www.lesswrong.com/posts/XdpJsY6QGdCbvo2dS/why-aligning-an-llm-is-hard-and-how-to-make-it-easier">Why Aligning an LLM is Hard, and How to Make it Easier</a> (Jan ‚Äô25). Meanwhile <a href="https://www.lesswrong.com/users/antonio-clarke?from=post_header">Antonio Clarke</a> posted <a href="https://www.lesswrong.com/posts/wopNDxxizjY6sMbpd/building-safer-ai-from-the-ground-up-steering-model-behavior">Building Safer AI from the Ground Up: Steering Model Behavior via Pre-Training Data Curation</a> (Sep ‚Äô24)</p><p>During 2025, quite a number of other people have also written about this approach, or closely related ideas. In February the academic position paper <a href="https://arxiv.org/abs/2502.05475"><i>You Are What You Eat - AI Alignment Requires Understanding How Data Shapes Structure and Generalisation</i></a> came out (which sadly I missed at the time, so was unable to linkpost ‚Äî go read it, it‚Äôs excellent). Technically this isn‚Äôt actually an alignment pretraining paper: it frames alignment as a dataset generalization problem, for a dataset that starts from pretraining and is then repeatedly modified and supplemented by all subsequent training steps, from which our training processes progressively develop a model whose learned algorithms may or may not generalize well, and it argues for researching a deeper understanding of this process, without ever specifically suggesting that intervening at the pretraining stage might be a good thing to try ‚Äî however their framing is closely compatible, and alignment pretraining is an obvious approach. Also in February <a href="https://www.lesswrong.com/users/richard-juggins?from=post_header">Richard Juggins</a> posted <a href="https://www.lesswrong.com/posts/ajudxufov7HnYxHFr/making-alignment-a-law-of-the-universe">Making alignment a law of the universe</a> inspired by Antonio Clark.</p><p>In March <a href="https://turntrout.com/about">TurnTrout</a> wrote <a href="https://turntrout.com/self-fulfilling-misalignment">Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models</a>, citing the original paper and explicitly proposing alignment pretraining (both filtering and what he called ‚Äú<a href="https://turntrout.com/self-fulfilling-misalignment#upweighting-positive-data">upweighting positive data</a>‚Äù). His post inspired <a href="https://www.lesswrong.com/users/chris-lakin?from=post_header">Chris Lakin</a> to ask for <a href="https://www.lesswrong.com/posts/e3CpMJrZQjbXeqA6C/examples-of-self-fulfilling-prophecies-in-ai-alignment">Examples of self-fulfilling prophecies in AI alignment?</a> and several of the answers various people posted over the rest of the year were relevant.</p><p>In April, the second academic paper directly on this topic <a href="https://arxiv.org/pdf/2504.16980"><i>Safety Pretraining: Toward the Next Generation of Safe AI</i></a> finally came out (26 months after the first), and in May I linkposted that in <a href="https://www.lesswrong.com/posts/xAsviBJGSBBtgBiCw/the-best-way-to-align-an-llm-is-inner-alignment-now-a-solved">The Best Way to Align an LLM: Is Inner Alignment Now a Solved Problem?</a> (spoiler alert: progress, not yet solved).</p><p>In June <a href="https://www.lesswrong.com/users/nostalgebraist?from=post_header">nostalgebraist</a> wrote <a href="https://www.lesswrong.com/posts/3EzbtNLdcnZe8og8b/the-void-1">the void</a>, which points out that the helpful, harmless, and honest persona of AI assistants is fictional, riffing on previous fictional tropes and other data about AIs from the training set ‚Äî his post eloquently and poetically explains the problem in detail, but doesn‚Äôt explicitly advocate a solution: however alignment pretraining is an obvious response. Also in June, <a href="https://substack.com/@astralcodexten">Scott Alexander</a> and <a href="https://blog.ai-futures.org/">the AI Futures Project</a> wrote <a href="https://blog.ai-futures.org/p/against-misalignment-as-self-fulfilling">We aren't worried about misalignment as self-fulfilling prophecy</a> (a skeptical take on the problem). OpenAI published <a href="https://openai.com/index/emergent-misalignment/"><i>Toward understanding and preventing misalignment generalization</i></a> (Jun) which traced <a href="https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly">emergent misalignment</a> back to documents in the pretaining set about people like war criminals and misogynists. <a href="https://www.lesswrong.com/users/mark-keavney?from=post_header">Mark Keavney</a> then wrote <a href="https://www.lesswrong.com/posts/LH9SoGvgSwqGtcFwk/misalignment-and-roleplaying-are-misaligned-llms-acting-out">Misalignment and Roleplaying: Are Misaligned LLMs Acting Out Sci-Fi Stories?</a> (Sep). <a href="https://arxiv.org/abs/2406.06144"><i>Language Models Resist Alignment: Evidence From Data Compression</i></a> (Sep) demonstrated that post-training approaches to alignment were fragile and models tend to revert to the alignment properties of the base pretrained model (they don‚Äôt advocate alignment pretraining, which they call ‚Äúnot particularly cost-effective and feasible‚Äù, but do suggest using larger alignment trainings datasets). <a href="https://www.lesswrong.com/users/alek-westover?from=post_header">Alek Westover</a> wrote <a href="https://www.lesswrong.com/posts/dEiBJDtSmbC8dChwe/what-training-data-should-developers-filter-to-reduce-risk-1">What training data should developers filter to reduce risk from misaligned AI? An initial narrow proposal</a> (Sep) and <a href="https://www.lesswrong.com/posts/6DfWFtL7mcs3vnHPn/should-ai-developers-remove-discussion-of-ai-misalignment">Should AI Developers Remove Discussion of AI Misalignment from AI Training Data?</a> (Oct), both on the filtering side. <a href="https://www.lesswrong.com/users/jayterwahl?from=post_header">Aaron Silverbook</a>/<a href="https://www.hyperstitionai.com/">Hyperstition AI</a> working with <a href="https://alexanderwales.com/adventures-in-ai-text-generation-pt-2-of/">Alexander Wales</a> then <a href="https://www.astralcodexten.com/p/acx-grants-results-2025">got a $5000 grant from ACX</a> (Oct ‚Äî Scott Alexander had by then become less skeptical) to actually implement my <a href="https://www.lesswrong.com/w/aligned-ai-role-model-fiction">Aligned AI Role-Model Fiction</a> idea,<span class="footnote-reference" id="fnref3ljta5zz3zi"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn3ljta5zz3zi">[3]</a></sup></span>&nbsp;and posted <a href="https://www.lesswrong.com/posts/9NntwpQj9onbEFM8L/silicon-morality-plays-the-hyperstition-progress-report-1">Silicon Morality Plays: The Hyperstition Progress Report</a> (Nov) and <a href="https://www.lesswrong.com/posts/hkzw97Y73yWMS7BFd/special-persona-training-hyperstition-progress-report-2">Special Persona Training: Hyperstition Progress Report 2</a> (Jan ‚Äô26). Also in January <a href="https://www.lesswrong.com/users/seth-herd?from=search_autocomplete">Seth Herd</a> wrote <a href="https://www.lesswrong.com/posts/oveSZmWHjFQbgR4Nn/broadening-the-training-set-for-alignment">Broadening the training set for alignment</a>, which isn‚Äôt specific to alignment pretraining, but advocates generating a lot of alignment training data (to reduce the risk of alignment not generalizing outside the training distribution), so is very relevant to it.</p><p>So interest in alignment pretraining and closely related topics has clearly been picking up and spreading over the last year.<span class="footnote-reference" id="fnreflkehwo8ocn8"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnlkehwo8ocn8">[4]</a></sup></span></p><h2>New Paper Shows Strong Results</h2><p>So I‚Äôm delighted that there‚Äôs already a third academic paper on this subject up on arXiv, only 9 months after the second: <a href="https://arxiv.org/html/2601.10160v1"><i>Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment</i></a>, from <a href="https://www.geodesicresearch.org/">Geodesic Research</a>, Cambridge and Oxford Universities, and <a href="https://www.aisi.gov.uk/">UK AISI</a> (compute from <a href="https://www.bristol.ac.uk/research/centres/bristol-supercomputing/#isambard-ai">Isambard-AI</a>). The authors wrote <a href="https://www.lesswrong.com/posts/TcfyGD2aKdZ7Rt3hk/alignment-pretraining-ai-discourse-causes-self-fulfilling">their own Alignment Forum linkpost</a> ‚Äî but I‚Äôm not going to let that stop me from also linkposting their work, and then trying to explain what I see as really promising about it. It has even stronger results than the previous ones, from larger (6.9B) models trained on more data.</p><p>The authors show that increasing the prevalence of information about AI behaving well in the base model‚Äôs training set dramatically reduces misaligned behavior (~5-fold). Decreasing the prevalence of information about AI behaving in misaligned ways in the training set is also helpful, and increasing that makes things worse. Much as when educating children, providing detailed positive role models has a large effect (misalignment reduced from 45% to 9%), and reducing the amount of bad influences is also somewhat helpful (45% down to 31%). The paper calls the target of these effects ‚Äúalignment priors‚Äù. (My interpretation is that the supplementary data taught the base model‚Äôs world model a detailed understanding of aligned AI‚Äôs goals, values, ethics, and behaviors: fleshing out a detailed persona for and aligned AI.)&nbsp;</p><p>They next showed that the dramatic difference from improved role models persists after alignment post-training: starting post-training with a dramatically better aligned base model makes post-training a lot more effective (~4-fold). Interestingly, the bad-influences effect actually reversed at this point (with some variation depending on <a href="https://vintagedata.org/blog/posts/what-is-mid-training">mid-training</a> details): under some circumstances, knowing more about misalignment could also be mildly helpful for the final alignment of the model.</p><p>They also demonstrated that, while the most effective approach was to synthesize and then train on additional data all the way through pretraining, roughly a 2¬Ω-fold benefit (i.e. around half the total effect) could be obtained with an order-of-magnitude less data (and thus an order of magnitude less synthesis/training cost), by doing this only during mid-training.<span class="footnote-reference" id="fnrefdovgrq1kqgq"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fndovgrq1kqgq">[5]</a></sup></span>&nbsp;(If nothing else, this suggests to me a much cheaper way to experiment with this technique, where, once we have it working well in mid-training, we are confident we can improve results just by throwing more time and effort at scaling it up to pretraining.)</p><p>They then tested the effect of various alignment pretraining interventions on capabilities. On a range of broad capabilities evals, neither filtering misaligned AI data out of the model‚Äôs training set, nor adding more good AI behavior data, had much effect. The most noticeable effects seemed to be on a few evaluations that the balance of the pretraining dataset had been very carefully optimized for, where tinkering with that threw this off ‚Äî presumably it could be rebalanced again by someone familiar with this tuning.<span class="footnote-reference" id="fnrefqellp11ci6k"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnqellp11ci6k">[6]</a></sup></span>&nbsp;For those evals that the dataset had not been carefully optimized for, the effects were smaller, in some cases actually showing improvements, and may be just measurement noise. They did not test the effect of the filtering information on misalignment specifically on models‚Äô capabilities in the area of understanding AI alignment theory, where this would likely be concentrated. (I suspect that might be a good follow-up paper.)</p><p>This suggests that the ‚Äúalignment tax‚Äù for alignment pretraining is mostly just creating the new training data and the compute cost of training on it, rather than any significant drag on capabilities.<br /><br />They also had a lot of interesting appendices, including on their methodology, using fact vs. fiction for supplementing the pretraining data, and personality testing ‚Äî of which I‚Äôm only going to try to summarize one:</p><p>In <a href="https://arxiv.org/html/2601.10160v1#A7.p1.1">Appendix G</a>, they show that (unlike <a href="https://arxiv.org/abs/2406.06144">previous results on post-trained alignment</a>) simply fine-tuning an alignment pretrained model on innocuous behavior does not cause loss of alignment performance: the ‚Äúelasticity‚Äù effect identified in that previous research is, as expected, now working for us rather than against us. This seems like a very important result (especially in any context where end-users can fine-tune models).</p><p>They also suggest a number of areas for follow-on work. Briefly:</p><ul><li>Further investigation of how best to use post-training to elicit as the ‚Äúdefault persona‚Äù the aligned AI persona that alignment pretraining has taught the model about</li><li>Applying the techniques of Training Data Attribution and Mechanistic Analysis to help inform alignment pretraining</li><li>Understanding scaling laws for alignment pretraining: how does the amount, quality, type,<span class="footnote-reference" id="fnrefqcsseptaoy"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnqcsseptaoy">[7]</a></sup></span>&nbsp;and mix of synthetic data, plus the target and effectiveness of any data filtering affect the results, and how do all of these scale with model size? For larger models, does the amount of synthetic training data you need to generate to do this well scale linearly with total training data, or does it plateau once the aligned AI persona is well-described, or somewhere between the two?</li><li>Training dynamics: if you only have a limited budget for generating high quality synthetic data and filtering your training set of bad data, where during pretraining, mid-training and fine-tuning should you spend how much of this?</li><li>How does alignment pretraining interact with <a href="https://openai.com/index/emergent-misalignment/">emergent misalignment</a> and similar misalignment generalization, and with related techniques such as <a href="https://www.lesswrong.com/posts/AXRHzCPMv6ywCxCFp/inoculation-prompting-instructing-models-to-misbehave-at">inoculation prompting</a>?<span class="footnote-reference" id="fnrefwht1uiv1t0c"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnwht1uiv1t0c">[8]</a></sup></span></li></ul><p>All of these are great questions, and I hope to read papers about all of them over the next year or so (or even help write some).</p><h3>My Suggested Follow-Ons</h3><p><strong>Early Dense Supervision via Stochastic Gradient Descent</strong></p><p>On eliciting the aligned AI persona (the authors‚Äô first follow-on topic), an aspect I think would be particularly interesting to research is how alignment pretraining interacts with the very first stages of instruct and alignment training (sometimes called ‚Äú<a href="https://arxiv.org/abs/2204.05862">helpful, harmless, and honest</a>‚Äù training). One of the biggest concerns here is that, as the model starts to narrow its range of personas from the base model‚Äôs full range towards a hopefully-HHH AI assistant behavior, if it starts to put significant weight on a <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">scheming</a> alignment-faking <a href="https://www.lesswrong.com/w/llm-personas">persona</a> early in the process, then this persona seems likely to be very difficult to train out, if it‚Äôs sufficiently capable at <a href="https://www.lesswrong.com/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models">alignment faking</a>. Even detecting that this has happened and determining that you need to restart the instruct-training run might be challenging. Thus starting any <a href="https://www.lesswrong.com/w/reinforcement-learning">reinforcement learning</a> process with a much higher prior for aligned AI personas rather than for scheming alignment-faking personas seems vital. You really want the model already well aligned with the very dense supervision from stochastic gradient descent, before any scheming alignment-faking persona can get boosted by the far sparser, easier-to-fake/<a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">hack</a> supervision from reinforcement learning.</p><p>So <a href="https://www.alignmentforum.org/posts/EMZeJ7vpfeF4GrWwm/self-supervised-learning-and-agi-safety">we really need a stochastic gradient descent technique</a> for starting the alignment process off, before we apply any reinforcement learning: one which can be applied <i>before</i> the model has focused on a small number of personas, and which directly affects the probability of personas with different alignment properties. That‚Äôs exactly what alignment pretraining is: just doing SGD next-token prediction training on data that comes either from humans, or else synthetic data derived from a previous model that we have (somehow) tested very carefully and now fully trust the alignment of.</p><p>Obviously, fine tuning is also an SGD technique and thus has dense supervision, and is generally done before reinforcement learning. (DPO is comparable, differing from fine-tuning mostly in that it gives additional supervision at those points where the two texts diverge.) The biggest advantage that alignment pretraining has over those is the cumulative total amount of supervision, and particularly how much of that total is applied before the model starts to focus in on a narrow set of personas.</p><p><strong>Abundant Fine Detail About Alignment</strong></p><p><a href="https://www.lesswrong.com/posts/iHdfi7jwDrBxa2MZ9/grounding-value-learning-in-evolutionary-psychology-an">Alignment is in one sense rather simple</a>: a sentence along the lines of ‚Äúyour sole terminal goal is to help fulfill the goals of all humans, present and future ‚Äî in so far as those are not mutually exclusive, and to find a fair mutually agreeable and socially acceptable compromise by means in accordance with human values in situations where they‚Äôre not entirely compatible‚Äù could be a basis for it. (Add provisos, hedging, and evolutionary moral psychology and sociological background explanation to taste.)</p><p>What makes alignment very complex is that human values are very complex (though not irredeemably complex: the genetic description of the shared heritable causes of them fit in the ~4GB human genome, while the cultural aspects for any single culture are compact enough that the majority of members of that culture can reliably learn them). An LLM‚Äôs world model already contains a vast amount of detail about human values ‚Äî nuanced trivia about humans is their forte. A sufficiently smart AI could presumably deduce how an aligned AI should navigate optimizing outcomes according to human values from first principles if it had to; a less smart one would definitely benefit from having that terminal goal stated and also broken down into many <a href="https://www.lesswrong.com/w/shard-theory">shards</a>. So it should do a lot of good, especially for lower capability AIs, to train them on a very large number of worked examples covering a very large range of situations, involving both human values that we almost all share (for genetically determined reasons), and also ones on which different cultures tend to have different balances of emphasis on the fundamentals ‚Äî including situations confined to a single culture where which viewpoint to use is obvious, and also ones involving multiple cultures where there is a need for culturally-sensitive compromise.</p><p>Alignment pretraining has the strength of having very high information bandwidth, compared to other alignment techniques: pretraining is the time to supply all the fine detail that we can‚Äôt fit into something like a <a href="https://www.anthropic.com/news/claudes-constitution">constitution</a> or distilling an n-shot prompt or even a supervised fine-tuning corpus. So creating synthetic alignment pretraining data would benefit from care, attention, and a judicious balance of different cultural viewpoints on how to weight and balance the fundamental human moral intuitions and preferences that we all share. Don‚Äôt just start from a compact constitution and leave interpreting it to a small current LLM. Instead, have a lot of people think through the issues, and use as much human input, judgement, and inference time from the best well-aligned models we have, and as wide a combination of these as you can. Alignment pretraining gives us the bandwidth, we should take advantage of it.</p><p>So, my concrete suggestion is to think hard about how we would all want aligned AI to navigate tricky questions around human values. Then we need to think hard about the synthetic data generation processes, build a variety of them, and then test the effect on pretraining alignment of different mixes of these.</p><p><strong>Open-Weights Models</strong></p><p>Obviously alignment/safety pretraining (i.e. training set augmentation and filtering for alignment and safety) is the one of the few alignment/safety techniques applicable to open-weights base models. Similarly, alignment pretraining seems like a promising candidate for being one of the few able to make an open-weights instruct/chat model noticeably more resistant to being intentionally (or even unintentionally) misaligned by a small amount of fine-tuning or DPO.</p><p><strong>How Will This Scale to AGI and ASI?</strong></p><p>At the risk of speculating on the basis of no actual data, I suspect that for very capable models, filtering narrow knowledge gaps for specific dangerous technical knowledge may be less effective, since there‚Äôs a higher risk they can fill in the gap with some effort. Mildly downweighting prevalence of misaligned-AI behavior/goals and significantly upweighting prevalence of aligned-AI behavior/goals to reduce the salience/probability of misaligned priors and increase those of aligned priors at the start of default-persona training seems likely to continue to help: priors affect Bayesians of any capability level. However, these might help for less long for a more capable AI that presumably gathers more Bayesian updates during its training: then we would need to quickly determine which minimum‚Äôs basin of attraction it starts into, between alignment or alignment-faking. There may also be less actual need to upweight data about aligned-AI behavior in the future, once there is more Internet history of us actually interacting with pretty-well-aligned fairly-capable AIs: I suspect Claude‚Äôs trail on the Internet is broad, and for the most part a good influence.</p><p>The approach that I‚Äôd personally be most hopeful about for a really capable AI is a combination of broad data normalizing aligned-AI behavior for background/priors, a focus on <a href="https://www.lesswrong.com/posts/RCoBrvWfBMzGWNZ4t/motivating-alignment-of-llm-powered-agents-easy-for-agi-hard">those motivations/goals that seem most likely to scale to ASI</a>, and in particular making sure it‚Äôs already entirely familiar with the logical arguments why an aligned AI is a <a href="https://www.lesswrong.com/posts/bTusuAFmwyTpnBfEm/alignment-has-a-basin-of-attraction-beyond-the-orthogonality">consistent, obvious</a>, and <a href="https://www.lesswrong.com/posts/iHdfi7jwDrBxa2MZ9/grounding-value-learning-in-evolutionary-psychology-an">in an engineering/evolutionary sense correct thing to be</a>, and all the consequences of that for aligned AI given the vagaries of human values, by intentionally upweighting high quality real or high-realism documents on all of those things in the training set.</p><h2>Reaching Takeoff</h2><p>Between this recent paper, expanding interest on LessWrong/the Alignment Forum, <a href="https://www.hyperstitionai.com/">Hyperstition AI</a>‚Äôs recent work, some of the authors of the first paper being hired to do safety work at Anthropic, TurnTrout (a.k.a. Alex Turner) at DeepMind writing about this (he also gave a talk on it at MATS Summer 2025), and OpenAI posting an opening for <a href="https://openai.com/careers/researcher-pretraining-safety-san-francisco/">Researcher, Pretraining Safety</a><strong> </strong>(which explicitly mentions alignment as well as safety),<span class="footnote-reference" id="fnrefp7eae8i304r"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnp7eae8i304r">[9]</a></sup></span>&nbsp;work on this topic now seems to finally be starting to really take off ‚Äî even all three of the major foundation labs appear to be taking it seriously. The approach is also mentioned several times in the <a href="https://www.lesswrong.com/posts/Wti4Wr7Cf5ma3FGWa/shallow-review-of-technical-ai-safety-2025-2">Shallow review of technical AI safety, 2025</a> (scattered in several places under the headings ‚ÄúPretraining Safety‚Äù, ‚ÄúData filtering‚Äù, ‚ÄúHyperstition studies‚Äù, ‚ÄúSynthetic data for alignment‚Äù and ‚ÄúIterative alignment at pretrain-time‚Äù). I‚Äôm absolutely delighted to see this.</p><p>(Also, if anyone is interested in working on this, I‚Äôd love to discuss the topic, and can put you in touch with others interested in it. It is, of course, a computationally expensive research topic.)</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnegi42dgdbep"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefegi42dgdbep">^</a></strong></sup></span><div class="footnote-content"><p>Seminal in the sense that, to the best of my knowledge, they were the first to propose or try modifying the entire pretraining dataset for alignment purposes, and thus the first to discover that this is far more effective than fine-tuning or other post-training approaches.</p><p>Similar safety/alignment ideas just for fine-tuning datasets date back at least to <a href="https://arxiv.org/pdf/2106.10328">Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets</a> (2021) ‚Äî which explicitly dismisses attempting this during pretraining as impractical. Obviously people have known for a long time that training corpus selection is important (e.g. <a href="https://link.springer.com/chapter/10.1007/978-0-585-35958-8_20">Representativeness in Corpus Design</a> (1994), <a href="https://aclanthology.org/P01-1005.pdf">Scaling to Very Very Large Corpora for Natural Language Disambiguation</a> (2001), and <a href="https://aclanthology.org/P10-2041.pdf">Intelligent Selection of Language Model Training Data</a> (2010)) ‚Äî but until this paper no-one seems to have applied this technique to alignment.</p><p>Filtering pretraining data for safety to reduce the prevalence of certain behaviors (such as toxicity or hatespeech) or topics (such as NSFW) has been known since <a href="https://arxiv.org/abs/1910.10683"><i>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</i></a> (‚Äô19) and <a href="https://arxiv.org/abs/2104.08758"><i>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus </i></a>(‚Äô21). This is now standard practice: the <a href="https://arxiv.org/abs/2306.01116"><i>RefinedWeb</i></a> (‚Äô23), <a href="https://arxiv.org/abs/2402.00159"><i>Dolma</i></a> (‚Äô24), <a href="https://arxiv.org/abs/2406.17557"><i>FineWeb</i></a> (‚Äô24) and <a href="https://arxiv.org/abs/2411.12372"><i>RedPajama</i></a> (‚Äô24) pretraining corpora are all filtered and/or annotated. See also <a href="https://arxiv.org/abs/2305.13169"><i>A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity</i></a> (‚Äô23). Boosting desirable behaviors with synthetic data is less common in in AI safety, but dates back to at least to <a href="https://aclanthology.org/N18-2003.pdf"><i>Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods</i></a> (‚Äô18). So this wasn‚Äôt the seminal paper for safety pretraining as a whole, just for the alignment pretraining subtopic of safety pretraining.</p></div></li><li class="footnote-item" id="fn443ujm44s26"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref443ujm44s26">^</a></strong></sup></span><div class="footnote-content"><p>This was one of my best-received Alignment Forum/LessWrong posts, and Seth Herd was kind enough to summarize and linkdrop it in <a href="https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=7P7GBWHQMxk8eGyMo">a comment on TurnTrout‚Äôs shortform</a> during a discussion about The Bitter Lesson.</p></div></li><li class="footnote-item" id="fn3ljta5zz3zi"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref3ljta5zz3zi">^</a></strong></sup></span><div class="footnote-content"><p>I attended <a href="https://www.waypoint.lighthaven.space/e/lessonline-2025/session/session-1748319417665">a talk that Alexander Wales gave at LessOnline in LightHaven Jun 1st ‚Äô25</a> on using LLMs to write fiction. It was a great talk, and as both an amateur fiction writer and AI engineer, I found it fascinating, so I spoke up during the talk and discussed the subject with him afterwards. (Here‚Äôs <a href="https://docs.google.com/presentation/d/15DLVd9cJlTM7X2Tqyce7eiyhXsfI7sZkizR3vnbbEMs/edit?slide=id.p#slide=id.p">the slide deck</a> for people who missed it.) I can‚Äôt recall for certain that I suggested to him the concept of using this to generate <a href="https://www.lesswrong.com/w/aligned-ai-role-model-fiction">Aligned AI Role-Model Fiction</a> as I‚Äôd previously suggested here, but I‚Äôm sure the possibility would have occurred to me during the talk, so I strongly suspect that I did. So I think I may have managed to meme Hyperstition AI into existence ‚Äî which would be amusingly self-referential‚Ä¶</p></div></li><li class="footnote-item" id="fnlkehwo8ocn8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnreflkehwo8ocn8">^</a></strong></sup></span><div class="footnote-content"><p>Work on the filtering side of safety pretraining, both narrowly and broadly targeted, has also been active over the last year or so, with a number of interesting results. I haven‚Äôt attempted to comprehensively survey that as well, but here are some interesting-looking recent links that I turned up anyway:<br /><a href="https://arxiv.org/abs/2503.05721"><i>What Are They Filtering Out? An Experimental Benchmark of Filtering Strategies for Harm Reduction in Pretraining Datasets</i></a><i> </i>(Feb ‚Äô25)<br /><a href="https://arxiv.org/abs/2504.01542">Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation</a> (Apr ‚Äô25)<br /><a href="https://arxiv.org/abs/2505.02009"><i>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</i> </a>(May ‚Äô25)<br /><a href="https://arxiv.org/abs/2505.04741v1"><i>When Bad Data Leads to Good Models: Toxicity in Pretraining Data Enables Better Alignment</i></a> (May ‚Äô25)<br /><a href="https://blog.eleuther.ai/deep-ignorance/"><i>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</i></a> (Aug ‚Äò25)<br /><a href="https://alignment.anthropic.com/2025/pretraining-data-filtering/"><i>Enhancing Model Safety through Pretraining Data Filtering</i> </a>(Aug ‚Äô25)<br /><a href="https://alignment.anthropic.com/2025/selective-gradient-masking/"><i>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</i></a> (Dec ‚Äô25)</p></div></li><li class="footnote-item" id="fndovgrq1kqgq"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefdovgrq1kqgq">^</a></strong></sup></span><div class="footnote-content"><p>Mid-training is another stage of continued stochastic gradient descent training at the end of the pretraining period (with separate metaparameters), generally used to train the model on your highest quality bulk data at long context lengths ‚Äî it differs from fine-tuning primarily in that it uses a lot more data and a significantly lower learning rate. This is a recent development, and foundation model companies are still experimenting with it. More detail can be found in <a href="https://arxiv.org/abs/2510.14865">Midtraining Bridges Pretraining and Posttraining Distributions</a> (Oct ‚Äô25).</p></div></li><li class="footnote-item" id="fnqellp11ci6k"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefqellp11ci6k">^</a></strong></sup></span><div class="footnote-content"><p>Presumably using techniques along the lines of papers such as <a href="https://arxiv.org/abs/2403.16952"><i>Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance</i></a>, <a href="https://arxiv.org/abs/2305.10429"><i>DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining</i></a>, <a href="https://arxiv.org/abs/2412.15285"><i>Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining</i></a>, or <a href="https://arxiv.org/abs/2501.11747"><i>UtiliMax: Optimizing Pretraining Data Mixtures with LLM-Estimated Utility</i></a>.</p></div></li><li class="footnote-item" id="fnqcsseptaoy"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefqcsseptaoy">^</a></strong></sup></span><div class="footnote-content"><p>See for example <a href="https://arxiv.org/pdf/2504.01542"><i>Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation</i></a> for why this may be important.</p></div></li><li class="footnote-item" id="fnwht1uiv1t0c"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefwht1uiv1t0c">^</a></strong></sup></span><div class="footnote-content"><p>See Appendix I of the new paper for a preliminary investigation: alignment pretraining seemed to vary the response to emergent misalignment (EM), but not in a consistent pattern. Possibly this is because the persona being elicited during EM is that of a human criminal, not of an AI, so is mostly-unaffected by changes to the AI-related parts of the pretraining set? Or possibly this evaluation is inherently noisy?</p></div></li><li class="footnote-item" id="fnp7eae8i304r"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefp7eae8i304r">^</a></strong></sup></span><div class="footnote-content"><p>The linked job description document seems likely to go away once the position is filled. So here is the most relevant portion of it for anyone who wants to assess how seriously OpenAI appear to be taking this topic:</p><blockquote><p>About the Team:</p><p>The <a href="https://openai.com/safety/safety-systems"><u>Safety Systems team</u></a> is responsible for various safety work to ensure our best models can be safely deployed to the real world to benefit the society and is at the forefront of OpenAI‚Äôs mission to build and deploy safe AGI, driving our commitment to AI safety and fostering a culture of trust and transparency.</p><p>The Pretraining Safety team‚Äôs goal is to build safer, more capable base models and enable earlier, more reliable safety evaluation during training. We aim to:</p><ol><li><strong>Develop upstream safety evaluations</strong> that to monitor how and when unsafe behaviors and goals emerge;</li><li><strong>Create safer priors</strong> through targeted pretraining and mid-training interventions that make downstream alignment more effective and efficient</li><li><strong>Design safe-by-design architectures</strong> that allow for more controllability of model capabilities<br />&nbsp;</li></ol><p>In addition, we will conduct the foundational research necessary for understanding how behaviors emerge, generalize, and can be reliably measured throughout training.</p><h3>&nbsp;</h3><p>About the Role:</p><p>The Pretraining Safety team is pioneering how safety is built into models before they reach post-training and deployment. In this role, you will work throughout the full stack of model development with a focus on pre-training:</p><ul><li>Identify safety-relevant behaviors as they first emerge in base models</li><li>Evaluate and reduce risk without waiting for full-scale training runs</li><li>Design architectures and training setups that make safer behavior the default</li><li>Strengthen models by incorporating richer, earlier safety signals<br />&nbsp;</li></ul><p>We collaborate across OpenAI‚Äôs safety ecosystem‚Äîfrom Safety Systems to Training‚Äîto ensure that safety foundations are robust, scalable, and grounded in real-world risks.</p><p>In this role, you will:</p><ul><li>Develop new techniques to predict, measure, and evaluate unsafe behavior in early-stage models</li><li>Design data curation strategies that improve pretraining priors and reduce downstream risk</li><li>Explore safe-by-design architectures and training configurations that improve controllability</li><li>Introduce novel safety-oriented loss functions, metrics, and evals into the pretraining stack</li><li>Work closely with cross-functional safety teams to unify pre- and post-training risk reduction<br />&nbsp;</li></ul><p>You might thrive in this role if you:</p><ul><li>Have experience developing or scaling pretraining architectures (LLMs, diffusion models, multimodal models, etc.)</li><li>Are comfortable working with training infrastructure, data pipelines, and evaluation frameworks (e.g., Python, PyTorch/JAX, Apache Beam)</li><li>Enjoy hands-on research ‚Äî designing, implementing, and iterating on experiments</li><li>Enjoy collaborating with diverse technical and cross-functional partners (e.g., policy, legal, training)</li><li>Are data-driven with strong statistical reasoning and rigor in experimental design</li><li>Value building clean, scalable research workflows and streamlining processes for yourself and others</li></ul></blockquote><p><i>(Note: My inclusion of this text in this footnote should not be read as a covert endorsement of working on alignment at OpenAI ‚Äî people need to make their own ethical decisions on how best to spend their 80,000 hours.)</i></p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/ZeWewFEefCtx4Rj3G/pretraining-on-aligned-ai-data-dramatically-reduces#comments">Discuss</a>

---

### [Five Theses on AI Art](https://www.lesswrong.com/posts/KKmR3cKWxzRdyHcEp/five-theses-on-ai-art)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 19, 2026 4:24 AM GMT<br /><br /><h1>1. We've Been On This Ride Before</h1><p><a href="https://sabzian.be/text/the-cinema">Virginia Woolf, writing at the dawn of cinema</a> (1926), expresses doubt about whether or not this new medium has any legs:</p><blockquote><p><i>"Anna [Karenina] falls in love with Vronsky‚Äù ‚Äì that is to say, the lady in black velvet falls into the arms of a gentleman in uniform and they kiss with enormous succulence, great deliberation, and infinite gesticulation, on a sofa in an extremely well-appointed library, while a gardener incidentally mows the lawn. So we lurch and lumber through the most famous novels of the world. So we spell them out in words of one syllable, written, too, in the scrawl of an illiterate schoolboy. A kiss is love. A broken cup is jealousy. A grin is happiness. Death is a hearse. None of these things has the least connexion with the novel that Tolstoy wrote, and it is only when we give up trying to connect the pictures with the book that we guess from some accidental scene ‚Äì like the gardener mowing the lawn ‚Äì what the cinema might do if left to its own devices. But what, then, are its devices? If it ceased to be a parasite, how would it walk erect?"</i></p></blockquote><p>Reviewing clips from <a href="https://www.youtube.com/watch?v=teUsrBOeM6M">Anna Karenina (1911)</a><span class="footnote-reference" id="fnref4hvbgwbgd9k"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn4hvbgwbgd9k">[1]</a></sup></span>, you can see her point. Every single scene is shot from an awkward, middle-ish distance. The composition is terrible, the movement is jittery, there really aren't that many pixels to look at and it's monochrome to boot.</p><figure class="image"><img alt="anna-karenina" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KKmR3cKWxzRdyHcEp/z1jfusaylsqrdjpuq60f" /></figure><p>The camera is still, even when action takes place in different locations around the set.</p><figure class="image"><img alt="anna-karenina-2" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KKmR3cKWxzRdyHcEp/vhocynimpbpcmfzhf9r2" /></figure><p>It reads (watches?) more like a bootleg recording of a stage play than a movie as we would know one today. Not only were early filmmakers overly focused on adapting classical works of literature, it was doing so through emulating adjacent, well established mediums, instead of exploring the boundaries of its own. The incentives are understandable there: you want to exploit the established market, you don't want to do something too weird and scare the <s>hoes</s> investors, "emulate x perfectly" is such a wonderfully clear win condition.</p><p>I don't blame Virginia Woolf for doubting if cinema has any devices at all. But the film industry slowly got their act together, and by the time they made my comfort movie Sissi in 1955, they had managed to invent things like close ups, and shooting scenes from more than one angle, and also colour and sound.</p><figure class="image"><img alt="sissi" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KKmR3cKWxzRdyHcEp/wusykmf8qoxyje4u99iq" /></figure><p>And then by the time they shot Barbie (2024), they had invented more things, like zooming and panning the camera, shenanigans with body doubles so they don't need both of their very expensive movie stars to be on set at the same time, and spending hundreds of millions of dollars on making a single movie.</p><figure class="image"><img alt="barbie-movie" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KKmR3cKWxzRdyHcEp/y7f8hcgetbzzftbn6zcg" /></figure><p>Maybe the industry will continue to invent more things! It just took the industry a few decades to start cooking, is all.</p><p>I don't think this story is unique to cinema. <a href="https://www.lesswrong.com/posts/DxeiYddzJH3CBhK5E/deconstructing-arguments-against-ai-art">Deconstructing Arguments Against AI Art</a> notes similar dynamics for photography, recorded music, and digital drawing and editing tools.</p><h1>2. Rapid Mass Adoption Makes AI Art Seem More Banal Than It Is</h1><p>One thing that's different this time around is the rapid accessibility of the shiny new technology.</p><p>To wildly oversimplify, historically, new inventions tend to percolate out very slowly: super expensive prototypes only accessible to a handful of dedicated specialists and wealthy patrons who existed in a tight feedback loop with the manufacturers (or <i>were</i> the manufacturers), then a gradual, often decades-long, democratization. Think books/manuscripts, cameras, film recorders, personal computers. This gave culture time to adapt, and for genuine craft to emerge alongside the new toy.</p><p>Now imagine if we‚Äôd somehow only invented the camera after smartphones were already in everyone‚Äôs pocket. One random Tuesday morning, every single person on Earth suddenly has a camera app. What‚Äôs the immediate, overwhelming result? An instant, planet-wide tsunami of the most banal photos imaginable: beach sunsets and cute girls and juicy burgers and so so many pictures of cats.</p><p>Of course everyone would be tripping over themselves to denounce it as a worthless, trivial gimmick, utterly incapable of producing anything of True Artistic Merit‚Ñ¢ or any kind of value.</p><p>Perhaps they might change their minds when they see the first photo that came from an active war zone, or deep space, or the other end of a microscope. Or maybe it doesn't happen until someone gets the idea to take a lot of pictures in very quick succession, dozens of times per second, and then play back the pictures on a screen at very high speed accompanied by sound. And I'm sure some will stubbornly cling on to their first, dismissive reaction until the very bitter end, and <i>still</i> insist that photography is not a real art form.</p><p>I think we're sort of stuck at this step of the discourse currently, but why wouldn't we be? Woolf published her hate mail more than <a href="https://en.wikipedia.org/wiki/Auguste_and_Louis_Lumi%C3%A8re">30 years</a> after the first public screenings of the Lumi√®re brothers' first short films. This means we can continue collectively having bad takes about AI art until 2050, and <i>still</i> come out ahead.</p><h1>3. AI Art Will Democratize More Mediums</h1><p>Feature-length movies, animated cartoons of any length, and video games are examples of mediums where it's very difficult to make a finished piece with one person, or only a small number of them. Teams are good for various things, but they also encumber you ‚Äî they require capital, overhead coordination work, and the smoothing over of disagreements in artistic vision.</p><p>Something wonderful happened to music in the 00s, called "<a href="https://en.wikipedia.org/wiki/FL_Studio">FL Studio</a> is good now". When I was in grade school, two kids just a few years older than me met on an internet hobbyist forum for producing electronic music. With very little formal music training, and mostly the computers they already had around, they were able to create little tunes to share with others, and talk shop.</p><p>Porter Robinson and Madeon went on to make the glittering, soaring EDM that defined my adolescence. They picked up music theory as they went, started collaborating with other artists, and continue to make music that is really good. But at the start, they were just teens messing around in their bedrooms.</p><p>I want that to happen to more mediums! I want edgy cartoons clearly made by a single emo teen, with production values rivalling that of <i>The Lion King</i> or <i>The Little Mermaid</i>. I want them to explode over the internet, so much so that we end up treating them with mild disdain, the way we treat, like SoundCloud rappers today<span class="footnote-reference" id="fnrefojtbj7vg98n"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnojtbj7vg98n">[2]</a></sup></span>. I want video game production to be as accessible to any fifteen year old as recording bedroom pop or shooting a video for TikTok. Yes, the variance is going to be high, and the median is going to be crap, but why care? It's already like that for everything else, and we have the curatorial technology for dealing; the good will float to the top, and we'll all be better off for having more variety to choose from.</p><p>It‚Äôs good if more modes of artistic expression aren‚Äôt gated behind technical expertise with film cameras or game engines, proficiency with actually playing a musical instrument, or colour theory. Powerful AI can make it easier for people to get started, and they'll pick up what they require as they go along.</p><h1>4. AI Art Will Make M<strong>ake Other Artistic Mediums Do Interesting Things in Response</strong></h1><p>Once cameras could capture realistic likenesses cheaply, it freed up painters to explore other directions with more deliberation (or perhaps desperation). That's kind of how we got <a href="https://en.wikipedia.org/wiki/Impressionism#Content_and_composition">impressionism</a>,<span class="footnote-reference" id="fnrefaoazhcw9mc"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnaoazhcw9mc">[3]</a></sup></span>&nbsp;and everything afterwards:</p><blockquote><p>Rather than compete with photography to emulate reality, artists focused "on the one thing they could inevitably do better than the photograph‚Äîby further developing into an art form its very subjectivity in the conception of the image, the very subjectivity that photography eliminated".</p></blockquote><p>A similar story plays out with theatre and film, though to a <a href="https://en.wikipedia.org/wiki/Twentieth-century_theatre">smaller and messier extent</a>:</p><blockquote><p>Throughout the century, the artistic reputation of theatre improved after being derided throughout the 19th century. However, the growth of other media, especially film, has resulted in a diminished role within the culture at large. In light of this change, theatrical artists have been forced to seek new ways to engage with society. The various answers offered in response to this have prompted the transformations that make up its modern history.</p></blockquote><p>In the first case, portraiture was an attractor that many painters were historically pulled into. When demand for that suddenly dissipated, a vivid artistic movement bloomed, and that‚Äôs how we got our Monets and Turners and Van Goghs. In the second, film took over the role that theatre had as cheap entertainment for the masses, and then theatre, too, went off in weirder directions (though, uh, cards on the table I'm less confident? that that's a good thing since I'm not actually an experimental theatre enjoyer). And perhaps movies today are weirder than they otherwise would have been, if YouTube hadn't then taken the place of cheap entertainment for the masses in turn!</p><p>Artistic mediums, as we understand them, are a mix of technical constraints innate to the medium and incentives that are not. In the best case, getting rid of some of the incentives can enable its practitioners to better explore the breadth of what it is technically capable of.</p><p>AI artists are going to find certain niches that older mediums are currently servicing sub-optimally, and it'll be the kick in the pants the older mediums need to stretch out fully and do more exploration.<span class="footnote-reference" id="fnref0g7urf7q147a"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn0g7urf7q147a">[4]</a></sup></span>&nbsp;I look forward to the results.</p><p>Regarding film and theatre, there's also a pretty interesting bi-directionality that ended up happening. Personnel and technologies continue to flow back and forth between the two mediums, presumably for the better (though to <a href="https://monoskop.org/images/9/94/Sontag_Susan_1964_Film_and_Theatre.pdf">some theatre snobs' dismay</a>). More recently, did you know the guy who made <a href="https://www.youtube.com/watch?v=R_FQU4KzN7A">Potion Seller</a> for the new location for cheap entertainment for the masses ended up making <a href="https://en.wikipedia.org/wiki/Challengers_(film)">one of the best movies of 2024</a>?</p><p>You could imagine some cinematography technique emerging from AI-generated media that seems obvious in hindsight but is lateral to where the film industry is currently heading. That technique could then make its way back into traditional filmmaking. More shrimp Jesus in all of the movies, that's what I always say.</p><h1>5. The Devices of AI Art Will Take Time To Emerge</h1><p>By and large, creative use of AI today is falling into the same trap as early cinema did: we use it to generate the kinds of things we are already familiar with: static text, images, videos, software. We don't let it be strange, in the way AI is strange. So here's the question of the hour: <i>what are the devices of AI art?</i></p><p>It's still the very early days, but I think Gary Hustwit's 2024 documentary, Eno, might be instructive. Eno rarely agrees to documentaries, but he's a neophile, and Hustwit baited him into this one with the AI angle. All in all, he ended up recording 30 hours of interviews with Eno, and separately assembled 500 hours of archival footage. Then he hand-assigned weights to every slice of film, and created an algorithm to generate a unique 90 minute documentary for every screening. You can see the trailer <a href="https://www.youtube.com/watch?v=R89DFlUqaTI">here</a>, but to watch the actual documentary, you'll need to catch a bespoke showing in a theatre.</p><p>Ben Davis is probably my favourite contemporary art critic. He <a href="https://news.artnet.com/art-world/brian-eno-documentary-generative-art-personality-2518437">writes</a>:</p><blockquote><p>"I‚Äôve seen <i>Eno</i> three times now. I love Brian Eno, so this was not a chore, and I can say that each version contained incidents that probably would be central to this or that telling of Eno‚Äôs career&nbsp;that the other two versions didn‚Äôt include. ... The tone is consistent, and consistently affecting... I imagine that it is very, very difficult to assemble all the parts and to weight all the probabilities to generate this consistent personality‚Äî<strong>it is likely more labor, not less.</strong>" (emphasis mine)</p></blockquote><p>(It's a great documentary! Go watch it if you can catch a showing. But, uh, also, if you do you should cross your fingers and hope for a cut that features more David Bowie than U2. Davis again: "<i>Admittedly, to say that you are looking at an artwork for its ‚Äúpersonality‚Äù is also to say that you might catch it on better or worse days.</i>")</p><p>Beyond experimental documentaries, artists dabbling with AI are doing lots of other things too! Like, ummmmm, <a href="https://github.com/henrygabriels/clone_yourself_from_an_iphone_backup">making an impression of your sixteen-year-old self from an iPhone backup and then letting you talk to an LLM roleplaying as them</a>. Oh uhhhh <a href="https://botto.com/">have humans vote on thousands of generated pieces weekly and then the best ones get minted into NFTs</a>?</p><p>Okay, look, I fully admit that at the moment none of it is very good. But there's really no reason to expect it to be; we're working with the equivalent of pinhole cameras here, and a body of knowledge has not yet been established.</p><p>But this is going to be a very temporary state of affairs, especially if the capital keeps flowing. The tech is going to improve, the creators are going to update on what works well, we're going to figure out what the best practices are and what to stay away from.</p><p>It takes like thirty years for us to figure this shit out! Let it cook! Just let the AI cook. <a href="https://ai-2027.com/">Certainly</a> <a href="https://ifanyonebuildsit.com/">nothing</a> <a href="http://aisafetyfundamentals.com/blog/alignment-introduction">bad</a> will happen from just letting the AI cook for thirty years.</p><p>&nbsp;</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn4hvbgwbgd9k"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref4hvbgwbgd9k">^</a></strong></sup></span><div class="footnote-content"><p>Woolf possibly watched the <a href="https://en.wikipedia.org/wiki/Anna_Karenina_%281920_film%29">1920 adaptation</a>, but sadly I couldn't find versions of that online.</p></div></li><li class="footnote-item" id="fnojtbj7vg98n"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefojtbj7vg98n">^</a></strong></sup></span><div class="footnote-content"><p>I also happen to think that <a href="https://www.youtube.com/watch?v=8C0SUdGItRA">SoundCloud rappers can be very good</a>, but that's beside the point.</p></div></li><li class="footnote-item" id="fnaoazhcw9mc"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefaoazhcw9mc">^</a></strong></sup></span><div class="footnote-content"><p>To be clear, that is one factor among broader cultural, philosophical, and artistic questions that artists were exploring at the time, but my understanding is that it's an important one.</p></div></li><li class="footnote-item" id="fn0g7urf7q147a"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref0g7urf7q147a">^</a></strong></sup></span><div class="footnote-content"><p>...in the best case. In the worst case they shrivel up and die. But that's a sacrifice I'm willing to make o7</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/KKmR3cKWxzRdyHcEp/five-theses-on-ai-art#comments">Discuss</a>

---

### [When the LLM isn't the one who's wrong](https://www.lesswrong.com/posts/Cd8tRgpWnKPuNhZ2r/when-the-llm-isn-t-the-one-who-s-wrong)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 18, 2026 9:37 PM GMT<br /><br /><p>Recently I've been accumulating stories where I think an LLM is mistaken, only to discover that I'm the one who's wrong. My favorite recent case came while researching 19th century US-China opium trade.&nbsp;</p><p>It's a somewhat convoluted history: opium was smuggled when it was legal to sell and when it wasn't, and the US waffled between banning and legalizing the trade. I wanted to find out how it was banned the second time, and both Claude Research and Grokipedia told me it was by the <a href="https://en.wikipedia.org/wiki/Angell_Treaty_of_1880">Angell Treaty of 1880</a> between the US and China. Problem is, I've <a href="https://en.wikisource.org/wiki/Treaty_Regulating_Immigration_from_China">read that treaty</a>, and it only has to do with immigration‚Äîit's a notable prelude to the infamous <a href="https://www.archives.gov/milestone-documents/chinese-exclusion-act">Chinese Exclusion Act of 1882</a>. Claude didn't cite a source specifically for its claim, and Grok cited "[internal knowledge]", strangely, and googling didn't turn up anything, so I figured the factoid was confabulated.</p><p>However, doing more research about the Angell mission to China later, I came across an offhand mention of a <i>second</i> treaty negotiated by James Angell with Qing China in 1880 (on an <a href="https://www.rrauction.com/auctions/lot-detail/349696907110049-president-james-a-garfield-suspends-chinese-emigration-and-restricts-the-trade-of-opium/#:~:text=The%20treaty%20was%20concluded%20along%20with%20another%20treaty%2C%20also%20negotiated%20by%20the%20Angell%20Commission%2C%20that%20imposed%20restrictions%20on%20trade%20in%20opium.">auction website</a> of all places<span class="footnote-reference" id="fnrefxrj7lg9mdp9"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnxrj7lg9mdp9">[1]</a></sup></span>). Eventually I managed to find a good University of Michigan <a href="https://heritage.umich.edu/stories/angell-china-and-opium/">source</a> on the matter, as well as the <a href="https://www.loc.gov/item/lltreaties-ustbv006/">actual text of the second treaty</a> in the State Department's "Treaties and Other International Agreements of the United States of America: Volume 6 (Bilateral treaties, 1776-1949: Canada-Czechoslovakia)".</p><p>Anyway, Claude and Grok were right. Even though opium wasn't even in the remit of the Angell mission, when <a href="https://en.wikipedia.org/wiki/Li_Hongzhang">Li Hongzhang</a> surprised the American delegation by proposing a second treaty banning it, James Angell agreed on the spot. It was later ratified alongside the main immigration treaty. The opium treaty doesn't appear to have a distinct name from its more famous brother; the State Department merely lists the immigration treaty under the title "Immigration", and the opium treaty under the title "Commercial Relations and Judicial Procedure", so I can't entirely fault the LLMs for not specifying, though they ought to have done so for clarity. I suspect they were confused by the gap between the US government records they were trained on and the lack of sources they could find online?</p><p>(An aside: by 1880 US opium trade was in decline, while British opium trade was peaking, just about to be overtaken by the growth of domestic Chinese production. Angell judged correctly that the moral case overwhelmed the limited remaining Bostonian business interests and made the ban good politics in the US, particularly because it was reciprocal‚Äîhe could claim to be protecting Americans from the drug as well. Though, that's a harsh way of putting it; Angell personally stuck his neck out, mostly upon his own convictions, and both he and the US deserve credit for that.<span class="footnote-reference" id="fnrefrqel3lsbhc"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrqel3lsbhc">[2]</a></sup></span>)</p><p>If all that doesn't convince you to doublecheck your own assumptions when dealing with LLMs, well, there have been more boring cases too: I asked Claude to perform a tiresome calculation similar to one I had done myself a month before, Claude got a very different answer, I assumed it made a mistake, but actually it turns out I did it wrong the first time! Claude made a change in my code, I reverted it thinking it was wrong, but actually it had detected a subtle bug! I think by now we're all aware that LLMs are quite capable in math and coding, of course, but I list these examples for completeness in my argument: the correct update to make when an LLM contradicts you is not zero, and it's getting bigger.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnxrj7lg9mdp9"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefxrj7lg9mdp9">^</a></strong></sup></span><div class="footnote-content"><p>Apparently there's a decent market for presidential signatures of note? They managed to sell President Garfield's signature ratifying the Angell Treaty of 1880 for ten grand, partly off the infamy of the treaty and partly because Garfield's presidential signature is rare, him having been assassinated 6 months into the job.</p></div></li><li class="footnote-item" id="fnrqel3lsbhc"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefrqel3lsbhc">^</a></strong></sup></span><div class="footnote-content"><p>Fun bit of color from the <a href="https://heritage.umich.edu/stories/angell-china-and-opium/#:~:text=Viceroy%20Li%20Hongzhang.-,Long%20afterward,-%2C%20writing%20his%20memoirs">UMich source</a>:&nbsp;</p><blockquote><p>Long afterward, writing his memoirs, Angell would remember the genuine warmth of Li [Hongzhang]‚Äôs greeting. The viceroy was full of praise for the commercial treaty signed by the two nations.</p><p>‚ÄúHe was exceedingly affable ‚Ä¶,‚Äù Angell remembered, ‚Äúand [began] with the warmest expressions in respect to my part in the opium clause.</p><p>‚ÄúI told him, it did not take us a minute to agree on that article, because the article was right.</p><p>‚ÄúHe replied that I had been so instructed in the Christian doctrine &amp; in the principles of right that it was natural for me to do right.‚Äù</p></blockquote></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/Cd8tRgpWnKPuNhZ2r/when-the-llm-isn-t-the-one-who-s-wrong#comments">Discuss</a>

---

### [How to Love Them Equally](https://www.lesswrong.com/posts/5PhfJjNgor4iou3mf/how-to-love-them-equally)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 18, 2026 5:09 PM GMT<br /><br /><p>My parents have always said that they love all four of their children equally. I always thought this was a Correct Lie: that they don‚Äôt love us all equally, but they feel such a strong loyalty to us and have Specific Family Values such that lying about it is the thing to do to make sure we all flourish.</p><p>I realized this morning they are probably not lying.</p><p>The reason I originally thought they were lying is that it seems clear to me that they are on the whole more frequently delighted by some of us than others. And on the whole can relate more frequently to some of us than others. And that‚Äôs skipping over who they might be <i>most proud</i> of.</p><p>Now I grew up with a distinction between ‚Äúliking‚Äù and ‚Äúloving‚Äù which I have always found helpful: ‚ÄúLiking‚Äù is the immediate positive experiences and payoffs you get from a relationship. ‚ÄúLoving‚Äù is the sense of deeper connection you have with someone<span class="footnote-reference" id="fnrefzj1bn7qwhen"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnzj1bn7qwhen">[1]</a></sup></span>.</p><p>Liking goes up and down. Loving stays the same or goes up, unless you misunderstood someone‚Äôs fundamental nature entirely. You can like someone more if they are in a good mood than in a bad one. But you don‚Äôt love them more or less for it.</p><p>What do you love them for instead? For their values, their way of relating to the world, their skills and traits that are so essentially <i>them</i> that they outline every edge of their spirit. Not ‚Äúspirit‚Äù as a metaphysical object, but like how some people deeply embody kindness cause they <i>are just that way.</i> There might be something in their deep values, or their reward wiring, or their instincts, that makes them so deeply kind. And <i>that</i>. That, is something you can <i>love</i>.</p><p>Now children, genetically, are 50% of each parent<span class="footnote-reference" id="fnrefblhi0u1ysc"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnblhi0u1ysc">[2]</a></sup></span>. If a parent loves all of themselves and loves all of their partner then ... they will naturally love all of their children.</p><p>What‚Äôs the ‚Äúequal‚Äù doing though? Don‚Äôt you love some people more than others?</p><p>Yes and no. The way I think about ‚Äúlove‚Äù the loving feeling is the ‚Äúsame‚Äù for the kindness in John as for the competence in Jack. But if Jill is both kind and competent than I may love her more than John or Jack (all things being equal that is. Ha!)</p><p>And of course you can‚Äôt math the traits together. It‚Äôs an intuition of a direction of a feeling.</p><p>But I think that direction points to this: Your kids are built from all of you and all of your partner - If you love all of that, then you love all of them.</p><p>Of course, mother nature has more chemicals to solve any problem in that equation. Drugs are a hell of a drug.</p><p>But even if you lack those, then your children are roughly a mosaic of you and the person you picked to make them with.</p><p>And that means something else too: If you <i>don‚Äôt</i> love parts of yourself or your partner, then your children will see that too. If you get angry at yourself for always being late or angry at your partner for always making a mess, then your kids will see you won‚Äôt love those parts of them either.</p><p>And sure, not all genes express in all phenotypes, and nurture and experience matter too. But love is a fuzzy feeling and will fuzz out most of the difference. If the core traits are there, distributed across your children in various combinations, then each of them is Clearly Loveable. Because so are you, and so is your partner.</p><p>My parents are good at this. They clearly accept themselves fully and each other too.</p><p>I don't always accept myself fully.</p><p>But I'm working on it.</p><p>Because if my kids grow up and find any of their parts to be like mine, I want them to be able to look at me and see that I love those parts too. And maybe that will in turn help them figure out how to love themselves just as equally.</p><p>&nbsp;</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnzj1bn7qwhen"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefzj1bn7qwhen">^</a></strong></sup></span><div class="footnote-content"><p>I‚Äôm not claiming these are the de facto correct ways to think about liking and loving. My intention is to offer a frame for these concepts that might be worth exploring. You can also keep your own definitions and think about this as alt-liking and alt-loving, and still track them as ways of relating.</p></div></li><li class="footnote-item" id="fnblhi0u1ysc"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefblhi0u1ysc">^</a></strong></sup></span><div class="footnote-content"><p>I‚Äôm skipping over blended families here. My own family has aspects of that too and it is great and the love is as real as ever. This essay is more a messy exploration of how loving and accepting yourself can have positive effects on your bond with your children.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/5PhfJjNgor4iou3mf/how-to-love-them-equally#comments">Discuss</a>

---

## Engineering

### [Stop Picking Sides: Manage the Tension Between Adaptation and                Optimization](https://martinfowler.com/articles/stop-picking-sides.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/stop-picking-sides.html"><img src="https://martinfowler.com/articles/stop-picking-sides/infograph.jpg" width="" /></a></div>

<p><b class="author">Jim Highsmith</b> notes that many teams have turned into
      tribes wedded to exclusively adaptation or optimization. But he feels this
      misses the point that both of these are important, and we need to manage
      the tension between them. We can do this by thinking of two operating
      modes: explore (adaptation-dominant) and exploit (optimization dominant).
      We tailor a team's operating model to a particular blend of the two -
      considering uncertainty, risk, cost of change, and an evidence threshold.
      We should be particularly careful at the points where there is a handoff
      between the two modes</p>

<p><a class="more" href="https://martinfowler.com/articles/stop-picking-sides.html">more‚Ä¶</a></p>

---

### [My favorite musical discoveries of 2025](https://martinfowler.com/articles/2025-music.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/2025-music.html"><img src="https://martinfowler.com/articles/2025-music/card.png" width="350px" /></a></div>

<p>My favorite albums from last year. Balkan brass, an
      acoustic favorite of 80s returns, Ethio-jazz, Guatemalan singer-guitarist,
      jazz-rock/Indian classical fusion, and a unique male vocalist.</p>

<p><a class="more" href="https://martinfowler.com/articles/2025-music.html">more‚Ä¶</a></p>

---

### [Fragments: January  8](https://martinfowler.com/fragments/2026-01-08.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>Anthropic report on <a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic">how their AI is changing their own software development practice</a>.</p>

<ul>
  <li>Most usage is for debugging and helping understand existing code</li>
  <li>Notable increase in using it for implementing new features</li>
  <li>Developers using it for 59% of their work and getting 50% productivity increase</li>
  <li>14% of developers are ‚Äúpower users‚Äù reporting much greater gains</li>
  <li>Claude helps developers to work outside their core area</li>
  <li>Concerns about changes to the profession, career evolution, and social dynamics</li>
</ul>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Much of the discussion about using LLMs for software development lacks details on workflow. Rather than just hear people gush about how wonderful it is, I want to understand the gritty details. What kinds of interactions occur with the LLM? What decisions do the humans make? When reviewing LLM outputs, what kinds of things are the humans looking for, what corrections do they make?</p>

<p><a href="https://obie.medium.com/what-used-to-take-months-now-takes-days-cc8883cc21e9">Obie Fernandez</a> has written a post that goes into these kinds of details. Over the Christmas / New Year period he used Claude to build a knowledge distillation application, that takes transcripts from Claude Code sessions, slack discussion, github PR threads etc, turns them into an RDF graph database, and provides a web app with natural language ways to query them.</p>

<blockquote>
  <p>Not a proof of concept. Not a demo. The first cut of Nexus, a production-ready system with authentication, semantic search, an MCP server for agent access, webhook integrations for our primary SaaS platforms, comprehensive test coverage, deployed, integrated and ready for full-scale adoption at my company this coming Monday. Nearly 13,000 lines of code.</p>
</blockquote>

<p>The article is long, but worth the time to read it.</p>

<p>An important feature of his workflow is relying on <a href="https://martinfowler.com/bliki/TestDrivenDevelopment.html">Test-Driven Development</a></p>

<blockquote>
  <p>Here‚Äôs what made this sustainable rather than chaotic: TDD. Test-driven development. For most of the features, I insisted that Claude Code follow the red-green-refactor cycle with me. Write a failing test first. Make it pass with the simplest implementation. Then refactor while keeping tests green.</p>

  <p>This wasn‚Äôt just methodology purism. TDD served a critical function in AI-assisted development: it kept me in the loop. When you‚Äôre directing thousands of lines of code generation, you need a forcing function that makes you actually understand what‚Äôs being built. Tests are that forcing function. You can‚Äôt write a meaningful test for something you don‚Äôt understand. And you can‚Äôt verify that a test correctly captures intent without understanding the intent yourself.</p>
</blockquote>

<p>The account includes a major refactoring, and much evolution of the initial version of the tool. It‚Äôs also an interesting glimpse of how AI tooling may finally make RDF useful.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>When thinking about requirements for software, most discussions focus on prioritization. Some folks talk about buckets such as the <a href="https://en.wikipedia.org/wiki/MoSCoW_method">MoSCoW set</a>: Must, Should, Could, and Want. (The old joke being that, in MoSCoW, the cow is silent, because hardly any requirements end up in those buckets.) <a href="https://world.hey.com/jason/the-obvious-the-easy-and-the-possible-2e11a3fb">Jason Fried</a> has a different set of buckets for interface design: <strong>Obvious, Easy, and Possible</strong>. This immediately resonates with me: a good way of think about how to allocate the cognitive costs for those who use a tool.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.platformer.news/fake-uber-eats-whisleblower-hoax-debunked/">Casey Newton</a> explains how he followed up on an interesting story of dark patterns in food delivery, and found it to be a fake story, buttressed by AI image and document creation. On one hand, it clarifies the important role reporters play in exposing lies that get traction on the internet. But time taken to do this is time not spent on investigating real stories</p>

<blockquote>
  <p>For most of my career up until this point, the document shared with me by the whistleblower would have seemed highly credible in large part because it would have taken so long to put together. Who would take the time to put together a detailed, 18-page technical document about market dynamics just to troll a reporter? Who would go to the trouble of creating a fake badge?</p>

  <p>Today, though, the report can be generated within minutes, and the badge within seconds. And while no good reporter would ever have published a story based on a single document and an unknown source, plenty would take the time to investigate the document‚Äôs contents and see whether human sources would back it up.</p>
</blockquote>

<p>The internet has always been full of slop, and we have always needed to be wary of what we read there. AI now makes it easy to manufacture convincing looking evidence, and this is never more dangerous than when it confirms strongly held beliefs and fears.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/">Kent Beck</a>:</p>

<blockquote>
  <p>The descriptions of Spec-Driven development that I have seen emphasize writing the whole specification before implementation. This encodes the (to me bizarre) assumption that you aren‚Äôt going to learn anything during implementation that would change the specification.
I‚Äôve heard this story so many times told so many ways by well-meaning folks‚Äìif only we could get the specification ‚Äúright‚Äù, the rest of this would be easy.</p>
</blockquote>

<p>Like him, that story has been the constant background siren to my career in tech. But the <a href="https://martinfowler.com/articles/llm-learning-loop.html">learning loop</a> of experimentation is essential to the model building that‚Äôs at the heart of any kind of worthwhile specification. As <a href="https://martinfowler.com/articles/llm-learning-loop.html">Unmesh puts it:</a></p>

<blockquote>
  <p>Large Language Models give us great leverage‚Äîbut they only work if we focus on learning and understanding. They make it easier to explore ideas, to set things up, to translate intent into code across many specialized languages. But the real capability‚Äîour ability to respond to change‚Äîcomes not from how fast we can produce code, but from how deeply we understand the system we are shaping.</p>
</blockquote>

<p>When Kent defined Extreme Programming, he made <em>feedback</em> one of its four core values. It strikes me that the key to making the full use of AI in software development is how to use it to accelerate the feedback loops.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context. Programming-oriented tools are geting more sophisticated for that, but there‚Äôs also efforts at providing simpler tools, that allow customization. <a href="https://lixo.org">Carlos Villela</a> recently recommended Pi, and its developer, <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">Mario Zechner, has an interesting blog</a> on its development.</p>

<blockquote>
  <p>So what‚Äôs an old guy yelling at Claudes going to do? He‚Äôs going to write his own coding agent harness and give it a name that‚Äôs entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?</p>
</blockquote>

<p>If I ever get the time to sit and really play with these tools, then something like Pi would be something I‚Äôd like to try out. Although as an addict to The One True Editor, I‚Äôm interested in some of libraries that work with that, such as <a href="https://github.com/karthink/gptel">gptel</a>. That would enable me to use Emacs‚Äôs inherent programability to create my own command set to drive the interaction with LLMs.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Outside of my professional work, I‚Äôve <a href="https://boardgamegeek.com/blog/13064/martins-7th-decade">posting regularly about my boardgaming</a> on the specialist site BoardGameGeek. However its blogging environment doesn‚Äôt do a good job of providing an index to my posts, so I‚Äôve created <a href="https://martinfowler.com/boardgames/">a list of my BGG posts </a> on my own site. If you‚Äôre interested in my regular posts on boardgaming, and you‚Äôre on BGG you can subscribe to me there. If you‚Äôre not on BGG you can  subscribe to the blog‚Äôs <a href="https://boardgamegeek.com/rss/blog/13064">RSS feed</a>.</p>

<p>I‚Äôve also created a list of <a href="https://martinfowler.com/boardgames/fav-games.html">my favorite board games</a>.</p>

<p><img alt="" src="https://martinfowler.com/boardgames/index/game-grid.png" /></p>

---

### [Fragments: December 16](https://martinfowler.com/fragments/2025-12-16.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.linkedin.com/in/gitanjalivenkatraman/">Gitanjali Venkatraman</a> does wonderful illustrations of complex subjects (which is why I was so happy to work with her on our <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a> article). She has now published the latest in her series of illustrated guides: tackling the complex topic of <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf">Mainframe Modernization</a></p>

<p><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf"><img alt="" src="https://martinfowler.com/fragments/2025/gitanjali-mainframe.png" /></a></p>

<p>In it she illustrates the history and value of mainframes, why modernization is so tricky, and how to tackle the problem by breaking it down into tractable pieces. I love the clarity of her explanations, and smile frequently at her way of enhancing her words with her quirky pictures.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Gergely Orosz on <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m7xd33hi5c2d">social media</a></p>

<blockquote>
  <p>Unpopular opinion:</p>

  <p>Current code review tools just don‚Äôt make much sense for AI-generated code</p>

  <p>When reviewing code I really want to know:</p>

  <ul>
    <li>The prompt made by the dev</li>
    <li>What corrections the other dev made to the code</li>
    <li>Clear marking of code AI-generated not changed by a human</li>
  </ul>
</blockquote>

<p>Some people pushed back saying they don‚Äôt (and shouldn‚Äôt care) whether it was written by a human, generated by an LLM, or copy-pasted from Stack Overflow.</p>

<p>In my view it matters <em>a lot</em> - because of the second vital purpose of code review.</p>

<p>When asked why do code reviews, most people will answer the first vital purpose - quality control. We want to ensure bad code gets blocked before it hits <a href="https://martinfowler.com/articles/branching-patterns.html#mainline">mainline</a>. We do this to avoid bugs and to avoid other quality issues, in particular comprehensibility and ease of change.</p>

<p>But I hear the second vital purpose less often: code review is a mechanism to communicate and educate. If I‚Äôm submitting some sub-standard code, and it gets rejected, I want to know why so that I can improve my programming. Maybe I‚Äôm unaware of some library features, or maybe there‚Äôs some project-specific standards I haven‚Äôt run into yet, or maybe my naming isn‚Äôt as clear as I thought it was. Whatever the reasons, I need to know in order to learn. And my employer needs me to learn, so I can be more effective.</p>

<p>We need to know the writer of the code we review both so we can communicate our better practice to them, but also to know how to improve things. With a human, its a conversation, and perhaps some documentation if we realize we‚Äôve needed to explain things repeatedly. But with an LLM it‚Äôs about how to modify its context, as well as humans learning how to better drive the LLM.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Wondering why I‚Äôve been making a lot of posts like this recently? <a href="https://martinfowler.com/articles/writing-fragments.html">I explain why</a> I‚Äôve been reviving the link blog.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://simonwillison.net/2025/Dec/10/html-tools/">Simon Willison</a> describes how he uses LLMs to build disposable but useful web apps</p>

<blockquote>
  <p>These are the characteristics I have found to be most productive in building tools of this nature:</p>

  <ol>
    <li>A single file: inline JavaScript and CSS in a single HTML file means the least hassle in hosting or distributing them, and crucially means you can copy and paste them out of an LLM response.</li>
    <li>Avoid React, or anything with a build step. The problem with React is that JSX requires a build step, which makes everything massively less convenient. I prompt ‚Äúno react‚Äù and skip that whole rabbit hole entirely.</li>
    <li>Load dependencies from a CDN. The fewer dependencies the better, but if there‚Äôs a well known library that helps solve a problem I‚Äôm happy to load it from CDNjs or jsdelivr or similar.</li>
    <li>Keep them small. A few hundred lines means the maintainability of the code doesn‚Äôt matter too much: any good LLM can read them and understand what they‚Äôre doing, and rewriting them from scratch with help from an LLM takes just a few minutes.</li>
  </ol>
</blockquote>

<p>His repository includes all these tools, together with transcripts of the chats that got the LLMs to build them.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://obie.medium.com/what-happens-when-the-coding-becomes-the-least-interesting-part-of-the-work-ab10c213c660">Obie Fernandez</a>: while many engineers are underwhelmed by AI tools, some senior engineers are finding them really valuable. He feels that senior engineers have an oft-unspoken mindset, which in conjunction with an LLM, enables the LLM to be much more valuable.</p>

<blockquote>
  <p>Levels of abstraction and generalization problems get talked about a lot because they‚Äôre easy to name. But they‚Äôre far from the whole story.</p>

  <p>Other tools show up just as often in real work:</p>

  <ul>
    <li>A sense for blast radius. Knowing which changes are safe to make loudly and which should be quiet and contained.</li>
    <li>A feel for sequencing. Knowing when a technically correct change is still wrong because the system or the team isn‚Äôt ready for it yet.</li>
    <li>An instinct for reversibility. Preferring moves that keep options open, even if they look less elegant in the moment.</li>
    <li>An awareness of social cost. Recognizing when a clever solution will confuse more people than it helps.</li>
    <li>An allergy to false confidence. Spotting places where tests are green but the model is wrong.</li>
  </ul>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://friendlybit.com/python/writing-justhtml-with-coding-agents/">Emil Stenstr√∂m</a> built an HTML5 parser in python using coding agents, using Github Copilot in Agent mode with Claude Sonnet 3.7. He automatically approved most commands. It took him ‚Äúa couple of months on off-hours‚Äù, including at least one restart from scratch. The parser now passes all the tests in html5lib test suite.</p>

<blockquote>
  <p>After writing the parser, I still don‚Äôt know HTML5 properly. The agent wrote it for me. I guided it when it came to API design and corrected bad decisions at the high level, but it did ALL of the gruntwork and wrote all of the code.</p>

  <p>I handled all git commits myself, reviewing code as it went in. I didn‚Äôt understand all the algorithmic choices, but I understood when it didn‚Äôt do the right thing.</p>
</blockquote>

<p>Although he gives an overview of what happens, there‚Äôs not very much information on his workflow and how he interacted with the LLM. There‚Äôs certainly not enough detail here to try to replicate his approach. This is contrast to Simon Willison (above) who has detailed links to his chat transcripts - although they are much smaller tools and I haven‚Äôt looked at them properly to see how useful they are.</p>

<p>One thing that is clear, however, is the vital need for a comprehensive test suite. Much of his work is driven by having that suite as a clear guide for him and the LLM agents.</p>

<blockquote>
  <p>JustHTML is about 3,000 lines of Python with 8,500+ tests passing. I couldn‚Äôt have written it this quickly without the agent.</p>

  <p>But ‚Äúquickly‚Äù doesn‚Äôt mean ‚Äúwithout thinking.‚Äù I spent a lot of time reviewing code, making design decisions, and steering the agent in the right direction. The agent did the typing; I did the thinking.</p>
</blockquote>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†</p>

<p>Then Simon Willison <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">ported the library to JavaScript</a>:</p>

<blockquote>
  <p>Time elapsed from project idea to finished library: about 4 hours, during which I also bought and decorated a Christmas tree with family and watched the latest Knives Out movie.</p>
</blockquote>

<p>One of his lessons:</p>

<blockquote>
  <p>If you can reduce a problem to a robust test suite you can set a coding agent loop loose on it with a high degree of confidence that it will eventually succeed. I called this <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">designing the agentic loop</a> a few months ago. I think it‚Äôs the key skill to unlocking the potential of LLMs for complex tasks.</p>
</blockquote>

<p>Our experience at Thoughtworks backs this up. We‚Äôve been doing a fair bit of work recently in legacy modernization (mainframe and otherwise) using AI to migrate substantial software systems. Having a robust test suite is necessary (but not sufficient) to making this work. I hope to share my colleagues‚Äô experiences on this in the coming months.</p>

<p>But before I leave Willison‚Äôs post, I should highlight his final open questions on the legalities, ethics, and effectiveness of all this - they are well-worth contemplating.</p>

---

### [Writing Fragments](https://martinfowler.com/articles/writing-fragments.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>If you‚Äôre a regular reader of my site, you‚Äôll have noticed that in the
last few months I‚Äôve been making a <a href="https://martinfowler.com/articles/20251204-frags.html">number</a> of ‚Äúfragments‚Äù <a href="https://martinfowler.com/articles/2025-11-19-frags.html">posts</a>. Such a post
is a short post with a bunch of little, unconnected segments. These are
usually a reference to something I‚Äôve found on the web, sometimes a small
thought of my own.</p>

<p>A few years ago, I wouldn‚Äôt have covered these topics with posts on my
own site. Instead I would use Twitter, either retweeting someone else‚Äôs
point, or just highlighting something I‚Äôd found. But since the Muskover,
Twitter has effectively died. I‚Äôm not saying that due to any technical
issues with the site, which has mostly just been fine, nor directly due to
any of the policy changes there. The point is that lots of people have left, so that
the audience I would have reached with Twitter is now fragmented. Some
remain on X, but I see more activity on LinkedIn. There‚Äôs also Fediverse/Mastodon
and Bluesky.</p>

<p>What this means for short posts is that I can no longer just post in one
place. When I announce new articles on martinfowler.com, I announce now on
four social media sites (X, LinkedIn, Fediverse, and Bluesky). It makes
sense to do this, but I don‚Äôt want to go through all this hassle for the
kind of micro-post that Twitter served so well.</p>

<p>So I‚Äôve started to batch them up. When I see something interesting, I
make a note. When I have enough notes, I post a fragments post. Initially I
did this in a rather ad-hoc way, just using the same mechanisms I use for
most articles, but last week I started to put in some more deliberate
mechanisms into the site. (If you‚Äôre observant, you‚Äôll spot that in the URLs.)</p>

<p>One benefit of all of this, at least in my book, is that it means my material is
now fully visible in RSS. I‚Äôm probably showing my age, but I‚Äôm a big fan of RSS
(or in my case, strictly Atom) feeds. I miss the feel of the heyday of the
‚Äúblogosphere‚Äù before it got steamrolled by social media, and these fragment
posts are, of course, just the same as the link blogs from that era. I still use my
RSS reader every day to keep up with writers I like. (I‚Äôm pleased that Substack
makes its content available via RSS.) It bothered me a bit that my micro-founts
of Twitter knowledge weren‚Äôt visible on RSS, but was too lazy to do something
about it. Now I don‚Äôt need to - the fragments are available in my RSS feed.</p>

---

### [Fragments Dec 11](https://martinfowler.com/articles/2025-12-11-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html?utm_source=substack&amp;utm_medium=email">Why does AI write like‚Ä¶ that</a> (NYT, gift link). Sam Kriss delves into the quiet hum of AI writing. AI‚Äôs work is not compelling prose: it‚Äôs phantom text, ghostly scribblings, a spectre woven into our communal tapestry.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://coding-is-like-cooking.info/2025/12/test-desiderata-2-0/">Emily Bache</a> has written a set of Test Desiderata, building on some earlier writing from Kent Beck. She lists the characteristics of good tests, and how they support her four ‚Äúmacro desiderata‚Äù - the properties of a sound test suite</p>

<blockquote>
  <ul>
    <li>Predict success in production</li>
    <li>Fast to get feedback</li>
    <li>Support ongoing code design change</li>
    <li>Low total cost of ownership</li>
  </ul>
</blockquote>

<p>She also has a great list of other writers‚Äô lists of good test characteristics.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.techpolicy.press/the-eus-fine-against-x-is-not-about-speech-or-censorship/">Daphne Keller</a> explains that the EUs fines on X aren‚Äôt about free speech.</p>

<blockquote>
  <p>There are three charges against X, which all stem from a multi-year investigation that was launched in 2023. One is about verification ‚Äî X‚Äôs blue checkmarks on user accounts ‚Äî and two are about transparency. These charges have nothing to do with what content is on X, or what user speech the platform should or should not allow.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington">Cory Doctorow</a> The Reverse-Centaur‚Äôs Guide to Criticizing AI</p>

<blockquote>
  <p>Start with what a reverse centaur is. In automation theory, a ‚Äúcentaur‚Äù is a person who is assisted by a machine. ‚Ä¶ And obviously, a reverse centaur is machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.</p>

  <p>Like an Amazon delivery driver‚Ä¶ the van can‚Äôt drive itself and can‚Äôt get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.</p>
</blockquote>

---

### [Fragments Dec 4](https://martinfowler.com/articles/20251204-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://blog.robbowley.net/2025/12/04/ai-is-still-making-code-worse-a-new-cmu-study-confirms/">Rob Bowley</a> summarizes a study from Carnegie Mellon looking on the impact of AI on a bunch of open-source software projects. Like any such study, we shouldn‚Äôt take its results as definitive, but there seems enough there to make it a handy data point. The key point is that the AI code probably reduced the quality of the code base - at least if static code analysis can be trusted to determine quality. And perhaps some worrying second-order effects</p>

<blockquote>
  <p>This study shows more than 800 popular GitHub projects with code quality degrading after adopting AI tools. It‚Äôs hard not to see a form of context collapse playing out in real time. If the public code that future models learn from is becoming more complex and less maintainable, there‚Äôs a real risk that newer models will reinforce and amplify those trends, producing even worse code over time.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Rob‚Äôs post is typical of much of the thoughtful writing on AI. We can see its short-term benefits, but worry about its long-term impact. But on a much deeper note is this lovely story from <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">Jim Highsmith</a>. Jim has turned 0x50, and has spent the last decade fighting Parkinson‚Äôs disease. To help him battle it he has two AI assisted allies.</p>

<blockquote>
  <p>Between my neural implants and Byron‚Äôs digital guidance, I now collaborate with two adaptive systems: one for motion, one for thought. Neither replaces me. Both extend me.</p>
</blockquote>

<p><strong>If you read anything on AI this week, <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">make it be this</a>.</strong> It offers a positive harbinger for our future and opens my mind to a whole different perspective of the role of AI in it</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Anthropic recently announced that it disrupted a Chinese state-sponsored operation abusing Claude Code. Jim Gumbley looks at the core lesson to learn from this, that we have to understand the serious risk of  <a href="https://www.thoughtworks.com/insights/blog/security/anthropic-ai-espionage-disclosure-signal-from-noise?utm_source=linkedin&amp;utm_medium=social-organic&amp;utm_campaign=dt_bs_rp-in-clt_tech_thought_leaders_2025-11&amp;gh_src=d1fe17bc1us">AI Jailbreaking</a></p>

<blockquote>
  <p>New AI tools are able to analyze your attack surface at the next level of granularity. As a business leader, that means you now have two options: wait for someone else to run AI-assisted vulnerability detection against your attack surface, or run it yourself first.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>There‚Äôs plenty of claims that AI Vibe Coding can replace software developers, something that folks like me (perhaps with a bias) think unlikely. <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m75kmb7bh22i">Gergely Orosz</a> shared this tidbit</p>

<blockquote>
  <p>Talked with an exec at a tech company who is obsessed with AI and has been for 3 years. Not a developer but company makes software. Uses AI for everything, vibe codes ideas.</p>

  <p>Here‚Äôs the kicker:</p>

  <p>Has a team of several devs to implement his vibe coded prototypes to sg workable</p>
</blockquote>

<p>I‚Äôd love to hear more about this (and similar stories)</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/">Nick Radcliffe</a> writes about a month of using AI</p>

<blockquote>
  <p>I spent a solid month ‚Äúpair programming‚Äù with Claude Code, trying to suspend disbelief and adopt a this-will-be-productive mindset. More specifically, I got Claude to write well over 99% of the code produced during the month. I found the experience infuriating, unpleasant, and stressful before even worrying about its energy impact. Ideally, I would prefer not to do it again for at least a year or two. The only problem with that is that it ‚Äúworked‚Äù.</p>
</blockquote>

<p>He stresses that his approach is the ‚Äúpolar opposite‚Äù of Vibe Coding. The post is long, and rambles a bit, but is worthwhile because he talks in detail about his workflow and how he uses the tool. Such posts are important so we can learn the nitty-gritty of how our programming habits are changing.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Along similar lines is a post of <a href="https://brianchambers.substack.com/p/chamber-of-tech-secrets-55-issue">Brian Chambers</a> on his workflow, that he calls Issue-Driven Development (and yes, I‚Äôm also sick of the ‚Äúsomething-driven‚Äù phraseology). As with much of the better stuff I‚Äôve heard about AI assisted work, it‚Äôs all about carefully managing the context window, ensuring the AI is focused on the right things and not distracted by textual squirrels.</p>

---

### [Fragments Nov 19](https://martinfowler.com/articles/2025-11-19-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><img alt="radar image" src="https://martinfowler.com/articles/images/frags-2025-11/radar.png" width="300px" /></p>

<p>I‚Äôve been on the road in Europe for the last couple of weeks, and while I was there Thoughtworks released <a href="https://www.thoughtworks.com/radar">volume 33 of our Technology Radar</a>. Again it‚Äôs dominated by the AI wave, with lots of blips capturing our explorations of how to use LLMs and similar technology. ‚ÄúAgents‚Äù are the big thing these days but we‚Äôre also seeing growing movements in infrastructure orchestration, coding workflows - and the inevitable antipatterns. Many thanks to my colleagues for putting this together again.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Gergely and I recording the podcast" src="https://martinfowler.com/articles/images/frags-2025-11/podcast.png" width="400px" /></p>

<p>My trip to Europe started in Amsterdam, for a Thoughtworks event for a few of our clients there. Since I was in that lovely city, I got in touch with Gergely Orosz, host of <a href="https://www.pragmaticengineer.com/">The Pragmatic Engineer</a>, and he arranged to <a href="https://www.youtube.com/watch?v=CQmI4XKTa0U">record a podcast</a> with me. No surprise that AI was front-and-center of the conversation, as I said it was the biggest shift I‚Äôd seen in programming during my career, comparable only to the shift to high-level languages, which even I am not old enough to have experienced. It was a fun chat and I really enjoyed myself. Gergely later joined myself James Lewis and Giles Edwards-Alexander at the Thoughtworks event the next day.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>My travels also took me to N√ºremberg, where I attended an internal conference for Siemens on the future of software architecture. When we think of technology, it‚Äôs easy to focus on the Faangs of Silicon Valley, but Siemens have a huge workforce of software developers working on heavy engineering systems like trains and factory automation. It was good to hear them talk about federated architectures, data mesh, and their use of AI.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Kent's graph of options vs features" src="https://martinfowler.com/articles/images/frags-2025-11/features.webp" width="400px" /></p>

<p>I‚Äôve often used pseudo-graphs to help explain why <a href="https://martinfowler.com/articles/is-quality-worth-cost.html">high quality software is cheaper</a>. This time, <a href="https://tidyfirst.substack.com/p/why-does-development-slow">Kent Beck</a> creates a unique perspective to this chart, dispensing with the temporal axis to help think in terms of optionality.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Heavy Cardboard banner" src="https://martinfowler.com/articles/images/frags-2025-11/hc.jpg" width="400px" /></p>

<p>And in another life, Edward has finally finished the great migration of the Heavy Cardboard studio and returns to the tubes with <a href="https://www.youtube.com/live/e8juRvm9h0c">our first game in the new digs</a>. (No surprise that it‚Äôs Age of Steam.)</p>

---

### [My Foreword to "Frictionless"](https://martinfowler.com/articles/frictionless-foreword.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/frictionless-foreword.html"><img src="https://martinfowler.com/articles/frictionless-card.jpg" width="" /></a></div>

<p>I find most writing on software productivity to be twaddle, but Nicole
      Forsgren and Abi Noda are notable exceptions. I had a chance to take a
      look at their new book, published today, and liked it so much I wrote a
      foreword.</p>

<p><a class="more" href="https://martinfowler.com/articles/frictionless-foreword.html">more‚Ä¶</a></p>

---

### [The Learning Loop and LLMs](https://martinfowler.com/articles/llm-learning-loop.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/llm-learning-loop.html"><img src="https://martinfowler.com/articles/llm-learning-loop/card.png" width="350px" /></a></div>

<p><b class="author">Unmesh Joshi</b> finds LLMs to be a useful tool, but
      explains why their help <a href="https://martinfowler.com/articles/llm-learning-loop.html">becomes illusory</a> if we use them to shortcut the
      learning loop that's an essential part of our professional practice.</p>

<p><a class="more" href="https://martinfowler.com/articles/llm-learning-loop.html">more‚Ä¶</a></p>

---

## HackerNews

### [ASCII characters are not pixels: a deep dive into ASCII rendering](https://alexharri.com/blog/ascii-rendering)

**ÂàÜÊï∞**: 1302 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46657122)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [jQuery 4](https://blog.jquery.com/2026/01/17/jquery-4-0-0/)

**ÂàÜÊï∞**: 788 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46664755)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Gaussian Splatting ‚Äì A$AP Rocky "Helicopter" music video](https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting)

**ÂàÜÊï∞**: 737 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46670024)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Dead Internet Theory](https://kudmitry.com/articles/dead-internet-theory/)

**ÂàÜÊï∞**: 639 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46671731)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A decentralized peer-to-peer messaging application that operates over Bluetooth](https://bitchat.free/)

**ÂàÜÊï∞**: 569 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46675853)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A Social Filesystem](https://overreacted.io/a-social-filesystem/)

**ÂàÜÊï∞**: 488 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46665839)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Radboud University selects Fairphone as standard smartphone for employees](https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees)

**ÂàÜÊï∞**: 485 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46676276)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Amazon is ending all inventory commingling as of March 31, 2026](https://twitter.com/ghhughes/status/2012824754319753456)

**ÂàÜÊï∞**: 462 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46678205)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Flux 2 Klein pure C inference](https://github.com/antirez/flux2.c)

**ÂàÜÊï∞**: 425 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46670279)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Letter from a Birmingham Jail (1963)](https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html)

**ÂàÜÊï∞**: 422 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683205)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)](https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html)

**ÂàÜÊï∞**: 398 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46666085)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Texas police invested in phone-tracking software and won‚Äôt say how it‚Äôs used](https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/)

**ÂàÜÊï∞**: 367 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46672150)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)

**ÂàÜÊï∞**: 326 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46679872)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Apple testing new App Store design that blurs the line between ads and results](https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/)

**ÂàÜÊï∞**: 273 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46680974)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [What came first: the CNAME or the A record?](https://blog.cloudflare.com/cname-a-record-order-dns-standards/)

**ÂàÜÊï∞**: 269 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46681611)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [High-speed train collision in Spain kills at least 39](https://www.bbc.com/news/articles/cedw6ylpynyo)

**ÂàÜÊï∞**: 260 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46673453)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Wikipedia: WikiProject AI Cleanup](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup)

**ÂàÜÊï∞**: 218 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46677106)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Gas Town Decoded](https://www.alilleybrinker.com/mini/gas-town-decoded/)

**ÂàÜÊï∞**: 200 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46624883)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Level S4 solar radiation event](https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026)

**ÂàÜÊï∞**: 193 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684056)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nvidia contacted Anna's Archive to access books](https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/)

**ÂàÜÊï∞**: 187 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46677628)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nearly a third of social media research has undisclosed ties to industry](https://www.science.org/content/article/nearly-third-social-media-research-has-undisclosed-ties-industry-preprint-claims)

**ÂàÜÊï∞**: 182 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682534)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Sins of the Children](https://asteriskmag.com/issues/07/sins-of-the-children)

**ÂàÜÊï∞**: 179 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46669663)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Dock ‚Äì Slack minus the bloat, tax, and 90-day memory loss](https://getdock.io/)

**ÂàÜÊï∞**: 167 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46671952)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Provide agents with automated feedback](https://banay.me/dont-waste-your-backpressure/)

**ÂàÜÊï∞**: 164 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46656897)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Code-Only Agent](https://rijnard.com/blog/the-code-only-agent)

**ÂàÜÊï∞**: 147 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46674416)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Lume 0.2 ‚Äì Build and Run macOS VMs with unattended setup](https://cua.ai/docs/lume/guide/getting-started/introduction)

**ÂàÜÊï∞**: 142 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46670181)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Threads edges out X in daily mobile users, new data shows](https://techcrunch.com/2026/01/18/threads-edges-out-x-in-daily-mobile-users-new-data-shows/)

**ÂàÜÊï∞**: 139 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683947)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Starting from scratch: Training a 30M Topological Transformer](https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer)

**ÂàÜÊï∞**: 137 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46666963)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Americans Are the Ones Paying for Tariffs, Study Finds](https://www.wsj.com/economy/trade/americans-are-the-ones-paying-for-tariffs-study-finds-e254ed2e)

**ÂàÜÊï∞**: 134 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683519)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [San Francisco coyote swims to Alcatraz](https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php)

**ÂàÜÊï∞**: 132 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46674433)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### ["Anyone else out there vibe circuit-building?"](https://twitter.com/beneater/status/2012988790709928305)

**ÂàÜÊï∞**: 132 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46679896)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Microstructure of Wealth Transfer in Prediction Markets](https://www.jbecker.dev/research/prediction-market-microstructure)

**ÂàÜÊï∞**: 127 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46680515)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Notes on Apple's Nano Texture (2025)](https://jon.bo/posts/nano-texture/)

**ÂàÜÊï∞**: 126 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682518)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Fix your robots.txt or your site disappears from Google](https://www.alanwsmith.com/en/37/wa/jz/s1/)

**ÂàÜÊï∞**: 114 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46681454)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nepal's Mountainside Teahouses Elevate the Experience for Trekkers](https://www.smithsonianmag.com/travel/nepal-mountainside-teahouses-elevate-experience-trekkers-heading-to-top-world-180987844/)

**ÂàÜÊï∞**: 112 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46627410)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [There's a hidden Android setting that spots fake cell towers](https://www.howtogeek.com/theres-a-hidden-android-setting-that-spots-fake-cell-towers/)

**ÂàÜÊï∞**: 103 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683833)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Simple Sabotage Field Manual (1944) [pdf]](https://www.cia.gov/static/5c875f3ec660e092cf893f60b4a288df/SimpleSabotage.pdf)

**ÂàÜÊï∞**: 101 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684335)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [CSS Web Components for marketing sites (2024)](https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/)

**ÂàÜÊï∞**: 99 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46679907)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Bypassing Gemma and Qwen safety with raw strings](https://teendifferent.substack.com/p/apply_chat_template-is-the-safety)

**ÂàÜÊï∞**: 97 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46675271)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I set all 376 Vim options and I'm still a fool](https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/)

**ÂàÜÊï∞**: 97 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46652690)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [From Nevada to Kansas by Glider](https://www.weglide.org/flight/978820)

**ÂàÜÊï∞**: 94 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46641036)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nuclear elements detected in West Philippine Sea](https://www.philstar.com/headlines/2026/01/18/2501750/nuclear-elements-detected-west-philippine-sea)

**ÂàÜÊï∞**: 85 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46677436)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Conditions in the Intel 8087 floating-point chip's microcode](https://www.righto.com/2025/12/8087-microcode-conditions.html)

**ÂàÜÊï∞**: 84 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46632120)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Cows can use sophisticated tools](https://nautil.us/the-far-side-had-it-all-wrong-cows-really-can-use-sophisticated-tools-1262026/)

**ÂàÜÊï∞**: 82 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46681153)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Pipenet ‚Äì A Modern Alternative to Localtunnel](https://pipenet.dev/)

**ÂàÜÊï∞**: 81 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46680597)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Greenland Crisis](https://en.wikipedia.org/wiki/Greenland_crisis)

**ÂàÜÊï∞**: 81 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683782)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [MTOTP: Wouldn't it be nice if you were the 2FA device?](https://github.com/VBranimir/mTOTP/tree/develop)

**ÂàÜÊï∞**: 80 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46676264)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [RISC-V is coming along quite speedily: Milk-V Titan Mini-ITX 8-core board](https://www.tomshardware.com/pc-components/cpus/milk-v-titan-mini-ix-board-with-ur-dp1000-processor-shows-risc-v-ecosystem-taking-shape-m-2-ddr4-and-pcie-card-support-form-a-kit-that-you-can-use-out-of-the-box)

**ÂàÜÊï∞**: 74 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46677212)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Iterative image reconstruction using random cubic b√©zier strokes](https://tangled.org/luthenwald.tngl.sh/splined)

**ÂàÜÊï∞**: 72 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46632184)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Luxury Yacht is a desktop app for managing Kubernetes clusters](https://github.com/luxury-yacht/app)

**ÂàÜÊï∞**: 70 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46619391)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Folding NASA Experience into an Origamist's Toolkit (2024)](https://spinoff.nasa.gov/Folding_NASA_Experience_into_an_Origamist%E2%80%99s_Toolkit)

**ÂàÜÊï∞**: 68 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46658003)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Gladys West's vital contributions to GPS technology](https://en.wikipedia.org/wiki/Gladys_West)

**ÂàÜÊï∞**: 67 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46651691)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Sending Data over Offline Finding Networks](https://cc-sw.com/find-my-and-find-hub-network-research/)

**ÂàÜÊï∞**: 63 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46622096)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The coming industrialisation of exploit generation with LLMs](https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/)

**ÂàÜÊï∞**: 60 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46676081)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [There is no comfortable reading position](https://slate.com/life/2026/01/body-books-reading-position-posture-pain.html)

**ÂàÜÊï∞**: 60 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682931)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Nanolang: A tiny experimental language designed to be targeted by coding LLMs](https://github.com/jordanhubbard/nanolang)

**ÂàÜÊï∞**: 58 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684958)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A Brief History of Ralph](https://www.humanlayer.dev/blog/brief-history-of-ralph)

**ÂàÜÊï∞**: 57 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682325)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: Subth.ink ‚Äì write something and see how many others wrote the same](https://subth.ink/)

**ÂàÜÊï∞**: 57 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682732)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Self Sanitizing Door Handle](https://www.jamesdysonaward.org/en-US/2019/project/self-sanitizing-door-handle/)

**ÂàÜÊï∞**: 56 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46633025)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Graphics In Flatland ‚Äì 2D ray tracing [video]](https://www.youtube.com/watch?v=WYTOykSqf2Y)

**ÂàÜÊï∞**: 52 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46649781)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Astrophotography visibility plotting and planning tool](https://airmass.org/)

**ÂàÜÊï∞**: 51 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46637937)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Fix macOS 26 (Tahoe) exaggerated rounded corners](https://github.com/makalin/CornerFix)

**ÂàÜÊï∞**: 50 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683589)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: AWS-doctor ‚Äì A terminal-based AWS health check and cost optimizer in Go](https://github.com/elC0mpa/aws-doctor)

**ÂàÜÊï∞**: 48 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46675092)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Robust Conditional 3D Shape Generation from Casual Captures](https://facebookresearch.github.io/ShapeR/)

**ÂàÜÊï∞**: 47 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46677918)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The assistant axis: situating and stabilizing the character of LLMs](https://www.anthropic.com/research/assistant-axis)

**ÂàÜÊï∞**: 46 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684708)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Simulating the Ladybug Clock Puzzle](https://austinhenley.com/blog/ladybugclock.html)

**ÂàÜÊï∞**: 45 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46661644)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Mammals have evolved into ant eaters 12 times since the dinosaur age ‚Äì study (2025)](https://phys.org/news/2025-07-mammals-evolved-ant-eaters-dinosaur.html)

**ÂàÜÊï∞**: 44 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683969)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Starlink users must opt out of all browsing data being used to train xAI models](https://twitter.com/cryps1s/status/2013345999826153943)

**ÂàÜÊï∞**: 42 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684788)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Understanding C++ Ownership System](https://blog.aiono.dev/posts/understanding-c++-ownership-system.html)

**ÂàÜÊï∞**: 40 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683752)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How to wrangle non-deterministic AI outputs into conventional software? (2025)](https://www.domainlanguage.com/articles/ai-components-deterministic-system/)

**ÂàÜÊï∞**: 40 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46643754)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The space and motion of communicating agents (2008) [pdf]](https://www.cl.cam.ac.uk/archive/rm135/Bigraphs-draft.pdf)

**ÂàÜÊï∞**: 35 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46639897)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Poking holes into bytecode with peephole optimisations](https://xnacly.me/posts/2026/purple-garden-first-optimisations/)

**ÂàÜÊï∞**: 34 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46624396)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [British redcoat's lost memoir reveals harsh realities of life as a disabled vet](https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html)

**ÂàÜÊï∞**: 33 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46642458)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Study: Minimal evidence links social media, gaming to teen mental health issues](https://www.manchester.ac.uk/about/news/time-spent-on-gaming-and-social-media/)

**ÂàÜÊï∞**: 32 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46619021)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Raccoons break into liquor stores, scale skyscrapers and pick locks](https://theconversation.com/raccoons-break-into-liquor-stores-scale-skyscrapers-and-pick-locks-studying-their-clever-brains-can-clarify-human-intelligence-too-272487)

**ÂàÜÊï∞**: 28 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46681587)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How we made Python's packaging library 3x faster](https://iscinumpy.dev/post/packaging-faster/)

**ÂàÜÊï∞**: 26 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46644173)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Reticulum, a secure and anonymous mesh networking stack](https://github.com/markqvist/Reticulum)

**ÂàÜÊï∞**: 24 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46686273)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: An interactive physics simulator with 1000's of balls, in your terminal](https://github.com/minimaxir/ballin)

**ÂàÜÊï∞**: 24 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682115)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Netflix tells directors to repeat plot for people using phones, says Matt Damon](https://www.nme.com/news/film/netflix-tells-directors-to-repeat-plot-for-people-using-phones-while-watching-says-matt-damon-3924120)

**ÂàÜÊï∞**: 24 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683831)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: A creative coding library for making art with desktop windows](https://github.com/willmeyers/window-art)

**ÂàÜÊï∞**: 23 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683858)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Use Social Media Mindfully](https://danielleheberling.xyz/blog/mindful-social-media/)

**ÂàÜÊï∞**: 22 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46684862)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [We've Turned Off AI‚ÄëAssisted Answers](https://noai.duckduckgo.com/)

**ÂàÜÊï∞**: 21 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46686338)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Radicle 1.6.0 ‚Äì Amaryllis](https://radicle.xyz/2026/01/14/radicle-1.6.0)

**ÂàÜÊï∞**: 20 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46617938)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Project Cybersyn](https://en.wikipedia.org/wiki/Project_Cybersyn)

**ÂàÜÊï∞**: 19 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46681156)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Weight Transfer for RL Post-Training in under 2 seconds](https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds)

**ÂàÜÊï∞**: 16 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683645)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Upgrading DrizzleORM logging with AsyncLocalStorage](https://numeric.substack.com/p/upgrading-drizzleorm-logging-with)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46621656)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [AI Is Not Ready to Replace Junior Devs Says Ruby on Rails Creator](https://www.finalroundai.com/blog/ai-can-not-replace-junior-programmers)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46683875)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [IKOS a static analyzer for C/C++ based on the theory of Abstract Interpretation](https://github.com/NASA-SW-VnV/ikos)

**ÂàÜÊï∞**: 12 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46677455)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Cataloging Failed VC-Backed Startups and Re-Evaluating Them in 2026](https://loot-drop.vercel.app/)

**ÂàÜÊï∞**: 12 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46676614)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Bzfs 1.17.0 near real-time ZFS replication tool is out](https://github.com/whoschek/bzfs)

**ÂàÜÊï∞**: 11 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46682977)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

## LLM Infrastructure

### [v0.14.0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[CI] Implement uploading to PyPI and GitHub in the release pipeline, ‚Ä¶</p>

---

### [v0.14.0rc2: [CI] Fix LM Eval Large Models (H100) (#32423)](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Matthew Bonanni <a href="mailto:mbonanni@redhat.com">mbonanni@redhat.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/bcf2333cd6514543f579d9bb6d309c5b8a0bfd0d"><tt>bcf2333</tt></a>)</p>

---

### [v0.14.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#‚Ä¶</p>

---

### [v0.14.0rc0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony mode‚Ä¶</p>

---

### [v0.13.0](https://github.com/vllm-project/vllm/releases/tag/v0.13.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.13.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features <strong>442 commits from 207 contributors (61 new contributors)!</strong></p>
<p><strong>Breaking Changes</strong>: This release includes deprecation removals, PassConfig flag renames, and attention configuration changes from environment variables to CLI arguments. Please review the breaking changes section carefully before upgrading.</p>
<h3>Model Support</h3>
<ul>
<li><strong>New models</strong>: BAGEL (AR only) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28439">#28439</a>), AudioFlamingo3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a>), JAIS 2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a>), latent MoE architecture support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a>).</li>
<li><strong>Tool parsers</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29848">#29848</a>), Gigachat 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a>), Holo2 reasoning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30048">#30048</a>).</li>
<li><strong>Model enhancements</strong>: Qwen3-VL embeddings support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a>), Qwen3-VL EVS (Efficient Video Sampling) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29752">#29752</a>), DeepSeek V3.2 proper <code>drop_thinking</code> logic (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a>), DeepSeek V3.2 top-k fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27568">#27568</a>).</li>
<li><strong>Task expansion</strong>: Automatic TokenClassification model conversion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30666">#30666</a>), Ultravox v0.7 transformer projector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30089">#30089</a>).</li>
<li><strong>Quantization</strong>: BitsAndBytes for Qwen3-Omni-MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29896">#29896</a>).</li>
<li><strong>Speculative decoding</strong>: Eagle/Eagle3 Transformers backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30340">#30340</a>), Mamba <code>selective_state_update</code> spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29488">#29488</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Compilation</strong>: Conditional compilation via <code>compile_ranges</code> for selective kernel compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24252">#24252</a>).</li>
<li><strong>Prefix caching</strong>: xxHash high-performance hash option (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a>).</li>
<li><strong>Attention</strong>: PrefixLM support for FlexAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27938">#27938</a>) and TritonAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30386">#30386</a>), CUDA graphs for 3D Triton attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28306">#28306</a>), <code>TRITON_MLA</code> without prefix-caching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29125">#29125</a>).</li>
<li><strong>Batch invariance</strong>: FA2 and LoRA batch-invariant support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30018">#30018</a>).</li>
<li><strong>Pooling</strong>: Chunked prefill for ALL pooling tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27145">#27145</a>), multi-vector retrieval API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26686">#26686</a>).</li>
<li><strong>Model Runner V2</strong>: Min-p sampling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30171">#30171</a>), NaN detection in logits (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30187">#30187</a>).</li>
<li><strong>Speculative decoding</strong>: Medusa GPU-CPU sync avoidance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29723">#29723</a>), async spec-decode improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29624">#29624</a>).</li>
<li><strong>Whisper</strong>: Major performance improvements - <a href="https://github.com/vllm-project/vllm/issues/24946#issuecomment-3680725754">V1 is now faster than V0</a> (~3x speedup vs v0.12.0). Encoder batching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29421">#29421</a>), <code>FULL_DECODE_ONLY</code> CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30072">#30072</a>), CPU backend support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>).</li>
<li><strong>Performance</strong>: Fused blockwise quant RMS norm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27883">#27883</a>), MoE LoRA loading reduction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30243">#30243</a>), encoder cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30475">#30475</a>), CPU KV offloading streams (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29013">#29013</a>).</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li><strong>NVIDIA Blackwell Ultra</strong>: SM103 (GB300) support with CUDA 13 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30484">#30484</a>).</li>
<li><strong>DeepSeek optimizations</strong> (benchmarked on DeepSeek-V3.1):
<ul>
<li>DeepEP High-Throughput CUDA graph enabled by default: <strong>5.3% throughput, 4.4% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29558">#29558</a>)</li>
<li>DeepGEMM fused layout kernel: <strong>4.3% throughput, 10.7% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29546">#29546</a>)</li>
<li>DeepGEMM experts initialization: <strong>3.9% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30494">#30494</a>)</li>
<li><code>group_topk</code> kernel: <strong>1.9% throughput, 2.1% TPOT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30159">#30159</a>)</li>
<li>Sparse prefill kernel for FP8 KV-cache in DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27532">#27532</a>)</li>
<li>MLA FP8 optimization with ReduceScatterSum (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29795">#29795</a>), direct k_nope/k_pe copy (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29710">#29710</a>)</li>
</ul>
</li>
<li><strong>CPU</strong>: Whisper support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>), Arm Optimized Routines vectorized exp (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a>), x86 CPU wheel pipeline (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28848">#28848</a>).</li>
<li><strong>AMD ROCm</strong>: Aiter quantization kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25552">#25552</a>), torch.compile layernorm/silu + FP8 quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25693">#25693</a>), Triton ScaledMM fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26668">#26668</a>), MXFP4 w4a4 inference (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29775">#29775</a>).</li>
<li><strong>Intel XPU</strong>: wNa16 compressed tensors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29484">#29484</a>).</li>
<li><strong>Build</strong>: CUDA 13 aarch64 wheels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30341">#30341</a>), Docker kernel build stage (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a>), Ascend NPU Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30015">#30015</a>).</li>
</ul>
<h3>Large Scale Serving &amp; Disaggregated Prefill/Decode</h3>
<ul>
<li><strong>KV connectors</strong>: Mooncake Transfer Engine (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a>), cache reset via <code>/reset_prefix_cache</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27170">#27170</a>), KV events (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28309">#28309</a>), failure recovery config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26813">#26813</a>).</li>
<li><strong>NIXL</strong>: Compatibility checking in handshake (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29503">#29503</a>), large batch proxy support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28782">#28782</a>).</li>
<li><strong>EPLB</strong>: NVFP4 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a>), algorithm abstraction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a>).</li>
<li><strong>Multi-node</strong>: External launcher mode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29833">#29833</a>).</li>
<li><strong>Hybrid allocator</strong>: Optional KV connector integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29805">#29805</a>).</li>
<li><strong>Performance</strong>: silu_mul_per_token_group_quant_fp8 kernel for DP/EP (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29470">#29470</a>).</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>New</strong>: W4A8 grouped GEMM on Hopper (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29691">#29691</a>), online FP8 with streaming post-processing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a>), FP8 weight reloading for RLHF (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28480">#28480</a>).</li>
<li><strong>MoE + LoRA</strong>: AWQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30442">#30442</a>) and GPTQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a>) support.</li>
<li><strong>GGUF</strong>: MoE + GGUF restored for Qwen3 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30116">#30116</a>), Qwen2 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30307">#30307</a>), HF defaults override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30118">#30118</a>).</li>
<li><strong>Compatibility</strong>: Transformers v5 RoPE support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30046">#30046</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>: MCP type infrastructure (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30054">#30054</a>), Browser/Container MCP tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29989">#29989</a>), full MCP Python loop (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29798">#29798</a>), extra body parameters (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a>).</li>
<li><strong>Configuration</strong>: <code>AttentionConfig</code> replaces <code>VLLM_ATTENTION_BACKEND</code> env var (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>).</li>
<li><strong>Chat templates</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29837">#29837</a>), DeepSeek-V3.2 developer tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a>).</li>
<li><strong>Anthropic API</strong>: Streaming fixes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29971">#29971</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30266">#30266</a>).</li>
<li><strong>Embeddings</strong>: Binary format with <code>encoding_format=bytes_only</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30249">#30249</a>), multiple image/audio per request (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29988">#29988</a>), tokenization_kwargs override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29794">#29794</a>).</li>
<li><strong>Metrics</strong>: Prefill KV compute metric excluding cached tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30189">#30189</a>).</li>
<li><strong>Profiling</strong>: Layer-wise NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29990">#29990</a>), profiling CLI config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29912">#29912</a>).</li>
<li><strong>UX</strong>: Better OOM errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28051">#28051</a>), ModelConfig validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a>), distributed executor errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30140">#30140</a>).</li>
</ul>
<h3>Security</h3>
<ul>
<li>Additional protection for <a href="https://github.com/advisories/GHSA-mrw7-hf4f-83pf" title="CVE-2025-62164">CVE-2025-62164</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a>).</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>NVSHMEM 3.3.24 + CUDA 13 fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a>).</li>
<li>TPU tpu-inference 0.12.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30221">#30221</a>).</li>
</ul>
<h3>Breaking Changes &amp; Deprecations</h3>
<ol>
<li><strong>PassConfig flags renamed</strong> per RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/27995">#27995</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29646">#29646</a>)</li>
<li><strong>Attention env vars ‚Üí CLI args</strong>: <code>VLLM_ATTENTION_BACKEND</code> replaced with <code>--attention-backend</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>)</li>
<li><strong>Removed <code>-O.xx</code> flag</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29991">#29991</a>)</li>
<li><strong>Removed deprecated plugin/compilation fields</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30396">#30396</a>)</li>
<li><strong>Removed deprecated task, seed, MM settings</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30397">#30397</a>)</li>
<li><strong>Removed <code>embed_input_ids</code>/<code>embed_multimodal</code> fallbacks</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30458">#30458</a>)</li>
<li><strong>Removed tokenizer setter</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30400">#30400</a>)</li>
<li><strong>Deprecations</strong>: <code>merge_by_field_config</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30035">#30035</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30170">#30170</a>), <code>--convert reward</code> ‚Üí <code>--convert embed</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30463">#30463</a>)</li>
</ol>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/ajpqs">@ajpqs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amitz-nv">@amitz-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29978">#29978</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amrmahdi">@amrmahdi</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andrewbriand">@andrewbriand</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/anker-c2">@anker-c2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30344">#30344</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AuruTus">@AuruTus</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30182">#30182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/avigny">@avigny</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19425">#19425</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bhanu068">@Bhanu068</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a></li>
<li>@Copilot made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29025">#29025</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dbotwinick">@dbotwinick</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30583">#30583</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30234">#30234</a></li>
<li><a class="user-mention notranslate" href="https://github.com/desertfire">@desertfire</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29919">#29919</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dmitry-tokarev-nv">@dmitry-tokarev-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a></li>
<li><a class="user-mention notranslate" href="https://github.com/drslark">@drslark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30632">#30632</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dtcccc">@dtcccc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elizabetht">@elizabetht</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28671">#28671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Elm8116">@Elm8116</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gausah01">@gausah01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29604">#29604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gh-wf">@gh-wf</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30285">#30285</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hdlj-h">@hdlj-h</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30056">#30056</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HF-001">@HF-001</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30051">#30051</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hzxuzhonghu">@hzxuzhonghu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29931">#29931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaviS-Rei">@JaviS-Rei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29882">#29882</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johannesflommersfeld">@johannesflommersfeld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30390">#30390</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KevinMusgrave">@KevinMusgrave</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30529">#30529</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kitaekatt">@kitaekatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30408">#30408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lashahub">@lashahub</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a></li>
<li><a class="user-mention notranslate" href="https://github.com/LuminolT">@LuminolT</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a></li>
<li><a class="user-mention notranslate" href="https://github.com/majiayu000">@majiayu000</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30615">#30615</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MaoJianwei">@MaoJianwei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29797">#29797</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Mercykid-bash">@Mercykid-bash</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mgehre-amd">@mgehre-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30364">#30364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mivehk">@mivehk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30512">#30512</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mondaylord">@mondaylord</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30671">#30671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/noa-neria">@noa-neria</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29320">#29320</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PatrykSaffer">@PatrykSaffer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30330">#30330</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Peng-YM">@Peng-YM</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29074">#29074</a></li>
<li><a class="user-mention notranslate" href="https://github.com/realliujiaxu">@realliujiaxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30059">#30059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/redwrasse">@redwrasse</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29261">#29261</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ri0S">@Ri0S</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scratch-ml">@scratch-ml</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30351">#30351</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seokhyunan">@seokhyunan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30648">#30648</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shaharmor98">@shaharmor98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a></li>
<li><a class="user-mention notranslate" href="https://github.com/taoyun951753">@taoyun951753</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tom-zju">@tom-zju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30057">#30057</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tomtomjhj">@tomtomjhj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29692">#29692</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vkuzo">@vkuzo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vladnosiv">@vladnosiv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiguihua2">@weiguihua2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30042">#30042</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wenqiglantz">@wenqiglantz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wkcn">@wkcn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29879">#29879</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wu-kan">@wu-kan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21804">#21804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wz1qqx">@wz1qqx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30376">#30376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xyDong0223">@xyDong0223</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30455">#30455</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yifant-code">@yifant-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yjc9696">@yjc9696</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yurekami">@yurekami</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30552">#30552</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuttian1">@yuttian1</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30102">#30102</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhijianJiang">@ZhijianJiang</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30219">#30219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhiweiYan-96">@ZhiweiYan-96</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29773">#29773</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.12.0...v0.13.0"><tt>v0.12.0...v0.13.0</tt></a></p>

---

### [v0.13.0rc4: [v1] Add PrefixLM support to TritonAttention backend (#30386)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc4)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/74a1ac38b00a8cf502db085d1bbd77712cf47e41"><tt>74a1ac3</tt></a>)</p>

---

### [v0.13.0rc3: [XPU] fix broken fp8 online quantization for XPU platform (#30831)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc3)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Yan Ma <a href="mailto:yan.ma@intel.com">yan.ma@intel.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/4f735babb7353987137b85ec0465e594e9ed1384"><tt>4f735ba</tt></a>)</p>

---

### [v0.13.0rc2: [ROCm] [Bugfix] Fix torch sdpa hallucination (#30789)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: tjtanaa <a href="mailto:tunjian.tan@embeddedllm.com">tunjian.tan@embeddedllm.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/2410132bb1f9faa5b252fad3f2b83dc926946b08"><tt>2410132</tt></a>)</p>

---

### [v0.13.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>fake rc release to fix nightly wheels</p>

---

### [v0.12.0](https://github.com/vllm-project/vllm/releases/tag/v0.12.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.12.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features 474 commits from 213 contributors (57 new)ÔºÅ</p>
<p><strong>Breaking Changes</strong>: This release includes PyTorch 2.9.0 upgrade (CUDA 12.9), V0 deprecations including <code>xformers</code> backend, and scheduled removals - please review the changelog carefully.</p>
<p><strong>Major Features</strong>:</p>
<ul>
<li><strong>EAGLE Speculative Decoding Improvements</strong>: Multi-step CUDA graph support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>), DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>), and multimodal support with Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Significant Performance Optimizations</strong>: 18.1% throughput improvement from batch invariant BMM (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>), 2.2% throughput improvement from shared experts overlap (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>).</li>
<li><strong>AMD ROCm Expansion</strong>: DeepSeek v3.2 + SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>), FP8 MLA decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>), AITER attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>).</li>
</ul>
<h3>Model Support</h3>
<ul>
<li><strong>New model families</strong>: PLaMo-3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28834">#28834</a>), OpenCUA-7B (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a>), HunyuanOCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29327">#29327</a>), Mistral Large 3 and Ministral 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29757">#29757</a>).</li>
<li><strong>Format support</strong>: Gemma3 GGUF multimodal support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27772">#27772</a>).</li>
<li><strong>Multimodal enhancements</strong>: Qwen3 Omni audio-in-video support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27721">#27721</a>), Eagle3 multimodal support for Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Performance</strong>: QwenVL cos/sin cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28798">#28798</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li>
<p><strong>GPU Model Runner V2 (Experimental)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25266">#25266</a>): Complete refactoring of model execution pipeline:</p>
<ul>
<li>No "reordering" or complex bookkeeping with persistent batch removal</li>
<li>GPU-persistent block tables for better scalability with <code>max_model_len</code> and <code>num_kv_groups</code></li>
<li>Triton-native sampler: no -1 temperature hack, efficient per-request seeds, memory-efficient prompt logprobs</li>
<li>Simplified DP and CUDA graph implementations</li>
<li>Efficient structured outputs support</li>
</ul>
</li>
<li>
<p><strong>Prefill Context Parallel (PCP) (Preparatory)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28718">#28718</a>): Partitions the sequence dimension during prefill for improved long-sequence inference. Complements existing Decode Context Parallel (DCP). See RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25749">#25749</a> for details.</p>
</li>
<li>
<p><strong>RLHF Support</strong>: Pause and Resume Generation for Asynchronous RL Training (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28037">#28037</a>).</p>
</li>
<li>
<p><strong>KV Cache Enhancements</strong>: Cross-layer KV blocks support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27743">#27743</a>), KV cache residency metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27793">#27793</a>).</p>
</li>
<li>
<p><strong>Audio support</strong>: Audio embeddings support in chat completions (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a>).</p>
</li>
<li>
<p><strong>Speculative Decoding</strong>:</p>
<ul>
<li>Multi-step Eagle with CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>)</li>
<li>EAGLE DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>)</li>
<li>EAGLE3 heads without <code>use_aux_hidden_states</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27688">#27688</a>)</li>
<li>Eagle multimodal CUDA graphs with MRoPE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a>)</li>
<li>Logprobs support with spec decode + async scheduling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29223">#29223</a>)</li>
</ul>
</li>
<li>
<p><strong>Configuration</strong>: Flexible <code>inputs_embeds_size</code> separate from <code>hidden_size</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29741">#29741</a>), <code>--fully-sharded-loras</code> for fused_moe (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28761">#28761</a>).</p>
</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li>
<p><strong>NVIDIA Performance</strong>:</p>
<ul>
<li><strong>Batch invariant BMM optimization</strong>: 18.1% throughput improvement, 10.7% TTFT improvement on DeepSeek-V3.1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>)</li>
<li><strong>Shared Experts Overlap with FlashInfer DeepGEMM</strong>: 2.2% throughput improvement, 3.6% TTFT improvement at batch size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>)</li>
<li>DeepGEMM N dim restriction reduced from 128 to 64 multiplier (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28687">#28687</a>)</li>
<li>DeepEP low-latency with round-robin expert placement (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28449">#28449</a>)</li>
<li>NVFP4 MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>H200 Fused MoE Config improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28992">#28992</a>)</li>
</ul>
</li>
<li>
<p><strong>AMD ROCm</strong>:</p>
<ul>
<li>DeepSeek v3.2 and SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>)</li>
<li>FP8 MLA decode support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>)</li>
<li>AITER sampling ops integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26084">#26084</a>)</li>
<li>AITER triton attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>)</li>
<li>Bitsandbytes quantization on AMD GPUs with warp size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27307">#27307</a>)</li>
<li>Fastsafetensors support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28225">#28225</a>)</li>
<li>Sliding window support for AiterFlashAttentionBackend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29234">#29234</a>)</li>
<li>Whisper v1 with Aiter Unified/Flash Attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a>)</li>
</ul>
</li>
<li>
<p><strong>CPU</strong>:</p>
<ul>
<li>Paged attention GEMM acceleration on ARM CPUs with NEON (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29193">#29193</a>)</li>
<li>Parallelize over tokens in int4 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29600">#29600</a>)</li>
<li>CPU all reduce optimization for async_scheduling + DP&gt;1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29311">#29311</a>)</li>
</ul>
</li>
<li>
<p><strong>Attention</strong>: FlashAttention ViT support, now default backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28763">#28763</a>).</p>
</li>
<li>
<p><strong>Long Context</strong>: Optimized <code>gather_and_maybe_dequant_cache</code> kernel for extremely long sequences (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28029">#28029</a>).</p>
</li>
<li>
<p><strong>Multi-NUMA</strong>: Enhanced NUMA functionality for systems with multiple NUMA nodes per socket (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a>).</p>
</li>
<li>
<p><strong>Docker</strong>: Image size reduced by ~200MB (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29060">#29060</a>).</p>
</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>W4A8</strong>: Marlin kernel support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24722">#24722</a>).</li>
<li><strong>NVFP4</strong>:
<ul>
<li>MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>TRTLLM MoE NVFP4 kernel (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a>)</li>
<li>CuteDSL MoE with NVFP4 DeepEP dispatch (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27141">#27141</a>)</li>
<li>Non-gated activations support in modelopt path (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a>)</li>
</ul>
</li>
<li><strong>AWQ</strong>: Compressed-tensors AWQ support for Turing GPUs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29732">#29732</a>).</li>
<li><strong>LoRA</strong>: FusedMoE LoRA Triton kernel for MXFP4 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29708">#29708</a>).</li>
<li><strong>Online quantization</strong>: Moved to <code>model.load_weights</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26327">#26327</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>:
<ul>
<li>Multi-turn support for non-harmony requests (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a>)</li>
<li>Reasoning item input parsing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28248">#28248</a>)</li>
</ul>
</li>
<li><strong>Tool Calling</strong>:
<ul>
<li>Parsed tool arguments support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a>)</li>
<li><code>parallel_tool_calls</code> param compliance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26233">#26233</a>)</li>
<li>Tool filtering support in ToolServer (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29224">#29224</a>)</li>
</ul>
</li>
<li><strong>Whisper</strong>: <code>verbose_json</code> and <code>timestamp</code> features for transcription/translation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24209">#24209</a>).</li>
<li><strong>Sampling</strong>: Flat logprob control moved from env var to <code>SamplingParams</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28914">#28914</a>).</li>
<li><strong>GGUF</strong>: Improved HuggingFace loading UX with <code>repo_id:quant_type</code> syntax (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a>).</li>
<li><strong>Profiling</strong>: Iteration-level profiling for Torch and CUDA profiler (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28987">#28987</a>).</li>
<li><strong>Logs</strong>: Colorized log output (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29017">#29017</a>).</li>
<li><strong>Optimization Levels</strong>: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code> allow trading startup time for performance, more compilation flags will be added in future releases (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26847">#26847</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><strong>PyTorch 2.9.0</strong> with CUDA 12.9 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24994">#24994</a>) - <strong>Breaking change</strong> requiring environment updates.</li>
<li><strong>xgrammar</strong>: Updated to 0.1.27 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28221">#28221</a>).</li>
<li><strong>Transformers</strong>: Updated to 4.57.3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29418">#29418</a>), preparation for v5 with <code>rope_parameters</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28542">#28542</a>).</li>
<li><strong>XPU</strong>: torch &amp; IPEX 2.9 upgrade (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29307">#29307</a>).</li>
</ul>
<h3>V0 Deprecation &amp; Breaking Changes</h3>
<p><strong>Removed Parameters</strong>:</p>
<ul>
<li><code>num_lookahead_slots</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29000">#29000</a>)</li>
<li><code>best_of</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29090">#29090</a>)</li>
<li>LoRA extra vocab (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28545">#28545</a>)</li>
</ul>
<p><strong>Deprecated</strong>:</p>
<ul>
<li><code>xformers</code> backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29262">#29262</a>)</li>
<li><code>seed=None</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29185">#29185</a>)</li>
</ul>
<p><strong>Scheduled Removals</strong> (will be removed in future release):</p>
<ul>
<li><code>ParallelConfig</code>'s direct child EPLB fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29324">#29324</a>)</li>
<li><code>guided_*</code> config fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29326">#29326</a>)</li>
<li><code>override_pooler_config</code> and <code>disable_log_requests</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29402">#29402</a>)</li>
<li><code>CompilationConfig.use_inductor</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29323">#29323</a>)</li>
<li>Deprecated metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29330">#29330</a>)</li>
</ul>
<p><strong>Other Breaking Changes</strong>:</p>
<ul>
<li>PyTorch 2.9.0 upgrade requires CUDA 12.9 environment</li>
<li>Mistral format auto-detection for model loading (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28659">#28659</a>)</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/jesse996">@jesse996</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28846">#28846</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Nepherpitou">@Nepherpitou</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28960">#28960</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Samoed">@Samoed</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27329">#27329</a></li>
<li><a class="user-mention notranslate" href="https://github.com/j20120307">@j20120307</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28999">#28999</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vnadathur">@vnadathur</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26468">#26468</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhyajie">@zhyajie</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28942">#28942</a></li>
<li><a class="user-mention notranslate" href="https://github.com/IzzyPutterman">@IzzyPutterman</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock-amd">@rjrock-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28905">#28905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zq1997">@zq1997</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27715">#27715</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shengliangxu">@shengliangxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28076">#28076</a></li>
<li><a class="user-mention notranslate" href="https://github.com/prashanth058">@prashanth058</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28972">#28972</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qgallouedec">@qgallouedec</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhanggzh">@zhanggzh</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19347">#19347</a></li>
<li><a class="user-mention notranslate" href="https://github.com/pandalee99">@pandalee99</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26628">#26628</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dsuhinin">@dsuhinin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29100">#29100</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xli">@xli</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29124">#29124</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jeremyteboul">@jeremyteboul</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/soodoshll">@soodoshll</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28875">#28875</a></li>
<li><a class="user-mention notranslate" href="https://github.com/bhagyashrigai">@bhagyashrigai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28957">#28957</a></li>
<li><a class="user-mention notranslate" href="https://github.com/skaraban3807">@skaraban3807</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Victor49152">@Victor49152</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock">@rjrock</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29205">#29205</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FlintyLemming">@FlintyLemming</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29182">#29182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/madskildegaard">@madskildegaard</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nandan2003">@nandan2003</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29189">#29189</a></li>
<li><a class="user-mention notranslate" href="https://github.com/michaelact">@michaelact</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29173">#29173</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yongming-qin">@yongming-qin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28958">#28958</a></li>
<li><a class="user-mention notranslate" href="https://github.com/joshiemoore">@joshiemoore</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29249">#29249</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lim4349">@lim4349</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/apinge">@apinge</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gbyu-amd">@gbyu-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kflu">@kflu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29364">#29364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Inokinoki">@Inokinoki</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29200">#29200</a></li>
<li><a class="user-mention notranslate" href="https://github.com/GOavi101">@GOavi101</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29313">#29313</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sts07142">@sts07142</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ivanium">@ivanium</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29143">#29143</a></li>
<li><a class="user-mention notranslate" href="https://github.com/geodavic">@geodavic</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28795">#28795</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Yejing-Lai">@Yejing-Lai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29473">#29473</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Adityayxt">@Adityayxt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29491">#29491</a></li>
<li><a class="user-mention notranslate" href="https://github.com/guodongxiaren">@guodongxiaren</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29620">#29620</a></li>
<li><a class="user-mention notranslate" href="https://github.com/askliar">@askliar</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29426">#29426</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scydas">@scydas</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29589">#29589</a></li>
<li><a class="user-mention notranslate" href="https://github.com/EanWang211123">@EanWang211123</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qGentry">@qGentry</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29506">#29506</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HappyAmazonian">@HappyAmazonian</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29335">#29335</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rgommers">@rgommers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29241">#29241</a></li>
<li><a class="user-mention notranslate" href="https://github.com/staugust">@staugust</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28840">#28840</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mertunsall">@mertunsall</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29667">#29667</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dublc">@dublc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29728">#29728</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nwaughachukwuma">@nwaughachukwuma</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29671">#29671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BowTen">@BowTen</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29731">#29731</a></li>
<li><a class="user-mention notranslate" href="https://github.com/omera-nv">@omera-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhangruoxu">@zhangruoxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29568">#29568</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KKKZOZ">@KKKZOZ</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29783">#29783</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FredericOdermatt">@FredericOdermatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29784">#29784</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Abdennacer-Badaoui">@Abdennacer-Badaoui</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29782">#29782</a></li>
<li><a class="user-mention notranslate" href="https://github.com/knlnguyen1802">@knlnguyen1802</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28525">#28525</a></li>
<li><a class="user-mention notranslate" href="https://github.com/finbarrtimbers">@finbarrtimbers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29796">#29796</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hholtmann">@hholtmann</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29711">#29711</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.11.1...v0.12.0"><tt>v0.11.1...v0.12.0</tt></a></p>

---

## Operating Systems

### [The end of OzLabs](https://lwn.net/Articles/1055051/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <a href="https://ozlabs.org/">OzLabs</a> is a collection of Australian
free-software developers that was, for most of its history, associated with
IBM.  Members of OzLabs have included Hugh Blemings, Michael Ellerman, Ben
Herrenschmidt, Greg Lehey, Paul Mackerras, Martin Pool, Stephen Rothwell,
Rusty Russell, and Andrew Tridgell, among others.  The <a href="https://ozlabs.org/about.html">OzLabs "about" page</a> notes that, as
of January 2026, the last remaining OzLabs members have departed IBM.
"<q>This brought to a close the Ozlabs association with IBM</q>".  Thus
ends a quarter-century of development history.
<p>
(Thanks to Jon Masters).

---

### [Haas: Who contributed to PostgreSQL development in 2025?](https://lwn.net/Articles/1055033/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>PostgreSQL contributor Robert Haas has <a href="https://rhaas.blogspot.com/2026/01/who-contributed-to-postgresql.html">published</a>
a blog post that breaks down code contributions to PostgreSQL in
2025.</p>

<blockquote class="bq">
I calculate that, in 2025, there were 266 people who were the
principal author of at least one PostgreSQL commit. 66% of the new
lines of code where contributed by one of 26 people, and 90% of the
lines of new code were contributed by one of 67 people.
</blockquote>

<p>Contributions to the project seem to be on the upswing; in his <a href="https://rhaas.blogspot.com/2025/01/who-contributed-to-postgresql.html">analysis
of development in 2024</a>, there were 229 people who were the primary
authors of a commit, and 66% of new lines of code were contributed by
one of 18 people. The <a href="https://sites.google.com/site/robertmhaas/contributions">raw
data</a> is also available.</p>

---

### [[$] Task-level io_uring restrictions](https://lwn.net/Articles/1054225/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: The <a href="https://man7.org/linux/man-pages/man7/io_uring.7.html">io_uring
subsystem</a> is more than an asynchronous I/O interface for Linux; it is,
for all practical purposes, an independent system-call API.  It has enabled
high-performance applications, but it also brings challenges for code built
around classic, Unix-style system calls.  For example, the <a href="https://man7.org/linux/man-pages/man2/seccomp.2.html"><tt>seccomp()</tt></a>
sandboxing mechanism does not work with it, causing applications using
<tt>seccomp()</tt> to disable io_uring outright.  Io_uring maintainer Jens
Axboe is seeking to improve that situation with a rapidly evolving patch
series adding a new restrictive mechanism to that subsystem.

---

### [Wine 11.0 released](https://lwn.net/Articles/1055001/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p><a href="https://gitlab.winehq.org/wine/wine/-/releases/wine-11.0">Version
11.0</a> of the Wine Windows compatibility layer is out. "<q>This
release represents a year of development effort, around&#160;6,300
individual changes, and more than&#160;600 bug fixes.</q>" The most notable
changes in this release are support for the NTSync Linux kernel module
(when available), and the completion of the Windows&#160;32-bit on Windows&#160;64-bit (WoW64) architecture that was announced as experimental in Wine&#160;9.0.</p>

---

### [Two new stable kernels for Monday](https://lwn.net/Articles/1054993/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Greg Kroah-Hartman has released the <a href="https://lwn.net/Articles/1054994/">5.15.198</a>, and <a href="https://lwn.net/Articles/1054996/">5.10.248</a> stable kernels. As usual, each
contains important fixes throughout the tree; users are advised to
upgrade.</p>

<p></p>

---

### [Security updates for Monday](https://lwn.net/Articles/1054992/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Security updates have been issued by <b>AlmaLinux</b> (cups, libpq, libsoup3, podman, and postgresql16), <b>Debian</b> (ffmpeg, gpsd, python-urllib3, and thunderbird), <b>Fedora</b> (chromium, foomuuri, forgejo, freerdp, harfbuzz, libtpms, musescore, python-biopython, and python3.12), <b>Mageia</b> (gimp, libpng, nodejs, and python-urllib3), and <b>SUSE</b> (alloy, avahi, bind, chromedriver, chromium, cpp-httplib, docker, erlang, fluidsynth, freerdp, go-sendxmpp, govulncheck-vulndb, kernel, libwireshark19, NetworkManager-applet-l2tp, python, python311-virtualenv, thunderbird, and zk).

---

## Systems

### [A data model for Git (and other docs updates)](https://jvns.ca/blog/2026/01/08/a-data-model-for-git/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! This past fall, I decided to take some time to work on Git&rsquo;s
documentation. I&rsquo;ve been thinking about working on open source docs for a long
time &ndash; usually if I think the documentation for something could be improved,
I&rsquo;ll write a blog post or a zine or something. But this time I wondered: could I
instead make a few improvements to the official documentation?</p>
<p>So <a href="https://marieflanagan.com/">Marie</a> and I made a few changes to the Git
documentation!</p>
<h3 id="a-data-model-for-git">a data model for Git</h3>
<p>After a while working on the documentation, we noticed that Git uses the terms
&ldquo;object&rdquo;, &ldquo;reference&rdquo;, or &ldquo;index&rdquo; in its documentation a lot, but that it didn&rsquo;t
have a great explanation of what those terms mean or how they relate to other
core concepts like &ldquo;commit&rdquo; and &ldquo;branch&rdquo;. So we wrote a new &ldquo;data model&rdquo; document!</p>
<p>You can <a href="https://github.com/git/git/blob/master/Documentation/gitdatamodel.adoc">read the data model here for now</a>.
I assume at some point (after the next release?) it&rsquo;ll also be on the <a href="https://git-scm.com">Git website</a>.</p>
<p>I&rsquo;m excited about this because understanding how Git organizes its commit and
branch data has really helped me reason about how Git works over the years,
and I think it&rsquo;s important to have a short (1600 words!) version of the data
model that&rsquo;s accurate.</p>
<p>The &ldquo;accurate&rdquo; part turned out to not be that easy: I knew the basics of how
Git&rsquo;s data model worked, but during the review process I learned some new
details and had to make quite a few changes (for example how merge conflicts are
stored in the staging area).</p>
<h3 id="updates-to-git-push-git-pull-and-more">updates to <code>git push</code>, <code>git pull</code>, and more</h3>
<p>I also worked on updating the introduction to some of Git&rsquo;s core man pages.
I quickly realized that &ldquo;just try to improve it according to my best judgement&rdquo;
was not going to work: why should the maintainers believe me that my version is
better?</p>
<p>I&rsquo;ve seen a problem a lot when discussing open source documentation changes
where 2 expert users of the software argue about whether an explanation
is clear or not (&ldquo;I think X would be a good way to explain it! Well, I think Y
would be better!&rdquo;)</p>
<p>I don&rsquo;t think this is very productive (expert users of a piece of software
are notoriously bad at being able to tell if an explanation will be clear to
non-experts), so I needed to find a way to identify problems with the man
pages that was a little more evidence-based.</p>
<h3 id="getting-test-readers-to-identify-problems">getting test readers to identify problems</h3>
<p>I asked for test readers on Mastodon to read the current version of
documentation and tell me what they find confusing or what questions they have.
About 80 test readers left comments, and I learned so much!</p>
<p>People left a huge amount of great feedback, for example:</p>
<ul>
<li>terminology they didn&rsquo;t understand (what&rsquo;s a pathspec? what does &ldquo;reference&rdquo; mean? does &ldquo;upstream&rdquo; have a specific meaning in Git?)</li>
<li>specific confusing sentences</li>
<li>suggestions of things things to add (&ldquo;I do X all the time, I think it should be included here&rdquo;)</li>
<li>inconsistencies (&ldquo;here it implies X is the default, but elsewhere it implies Y is the default&rdquo;)</li>
</ul>
<p>Most of the test readers had been using Git for at least 5-10 years, which
I think worked well &ndash; if a group of test readers who have been using Git
regularly for 5+ years find a sentence or term impossible to understand, it
makes it easy to argue that the documentation should be updated to make it
clearer.</p>
<p>I thought this &ldquo;get users of the software to comment on the existing
documentation and then fix the problems they find&rdquo; pattern worked really
well and I&rsquo;m excited about potentially trying it again in the future.</p>
<h3 id="the-man-page-changes">the man page changes</h3>
<p>We ended updating these 4 man pages:</p>
<ul>
<li><code>git add</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-add.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-add.adoc">after</a>)</li>
<li><code>git checkout</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-checkout.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-checkout.adoc">after</a>)</li>
<li><code>git push</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-push.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc">after</a>)</li>
<li><code>git pull</code> (<a href="https://github.com/git/git/blob/2b3ae040/Documentation/git-pull.adoc">before</a>, <a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-pull.adoc">after</a>)</li>
</ul>
<p>The <code>git push</code> and <code>git pull</code> changes were the most interesting to me: in
addition to updating the intro to those pages, we also ended up writing:</p>
<ul>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/urls-remotes.adoc#upstream-branches">a section describing what the term &ldquo;upstream branch&rdquo; means</a> (which previously wasn&rsquo;t really explained)</li>
<li><a href="https://github.com/git/git/blob/e0bfec3dfc356f7d808eb5ee546a54116b794397/Documentation/git-push.adoc#options">a cleaned-up description of what a &ldquo;push refspec&rdquo; is</a></li>
</ul>
<p>Making those changes really gave me an appreciation for how much work it is
to maintain open source documentation: it&rsquo;s not easy to write things that are
both clear and true, and sometimes we had to make compromises, for example the sentence
&ldquo;<code>git push</code> may fail if you haven‚Äôt set an upstream for the current branch,
depending on what <code>push.default</code> is set to.&rdquo; is a little vague, but the exact
details of what &ldquo;depending&rdquo; means are really complicated and untangling that is
a big project.</p>
<h3 id="on-the-process-for-contributing-to-git">on the process for contributing to Git</h3>
<p>It took me a while to understand Git&rsquo;s development process.
I&rsquo;m not going to try to describe it here (that could be a whole other post!), but a few quick notes:</p>
<ul>
<li>Git has a <a href="https://git-scm.com/community#discord">Discord server</a>
with a &ldquo;my first contribution&rdquo; channel for help with getting started contributing.
I found people to be very welcoming on the Discord.</li>
<li>I used <a href="https://gitgitgadget.github.io/">GitGitGadget</a> to make all of my contributions.
This meant that I could make a GitHub pull request (a workflow I&rsquo;m comfortable
with) and GitGitGadget would convert my PRs into the system the Git developers
use (emails with patches attached). GitGitGadget worked great and I was very
grateful to not have to learn how to send patches by email with Git.</li>
<li>Otherwise I used my normal email client (Fastmail&rsquo;s web interface) to reply
to emails, wrapping my text to 80 character lines since that&rsquo;s the mailing
list norm.</li>
</ul>
<p>I also found the mailing list archives on <a href="https://lore.kernel.org/git/">lore.kernel.org</a>
hard to navigate, so I hacked together <a href="https://github.com/jvns/git-list-viewer">my own git list viewer</a>
to make it easier to read the long mailing list threads.</p>
<p>Many people helped me navigate the contribution process and review the changes:
thanks to Emily Shaffer, Johannes Schindelin (the author of GitGitGadget),
Patrick Steinhardt, Ben Knoble, Junio Hamano, and more.</p>
<p>(I&rsquo;m experimenting with <a href="https://comments.jvns.ca/post/115861337435768520">comments on Mastodon, you can see the comments here</a>)</p>

---

### [Notes on switching to Helix from vim](https://jvns.ca/blog/2025/10/10/notes-on-switching-to-helix-from-vim/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Earlier this summer I was talking to a friend about how much I
<a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">love using fish</a>, and
how I love that I don&rsquo;t have to configure it. They said that they feel the same
way about the <a href="https://helix-editor.com/">helix</a> text editor, and so I decided
to give it a try.</p>
<p>I&rsquo;ve been using it for 3 months now and here are a few notes.</p>
<h3 id="why-helix-language-servers">why helix: language servers</h3>
<p>I think what motivated me to try Helix is that I&rsquo;ve been trying to get a working
language server setup (so I can do things like &ldquo;go to definition&rdquo;) and getting
a setup that feels good in Vim or Neovim just felt like too much work.</p>
<p>After using Vim/Neovim for 20 years, I&rsquo;ve tried both &ldquo;build my own custom
configuration from scratch&rdquo; and &ldquo;use someone else&rsquo;s pre-buld configuration
system&rdquo; and even though I love Vim I was excited about having things just work
without having to work on my configuration at all.</p>
<p>Helix comes with built in language server support, and it feels nice
to be able to do things like &ldquo;rename this symbol&rdquo; in any language.</p>
<h3 id="the-search-is-great">the search is great</h3>
<p>One of my favourite things about Helix is the search! If I&rsquo;m searching all the
files in my repository for a string, it lets me scroll through the potential
matching files and see the full context of the match, like this:</p>
<img src="https://jvns.ca/images/helix-search.png" />
<p>For comparison, here&rsquo;s what the vim ripgrep plugin I&rsquo;ve been using looks like:</p>
<img src="https://jvns.ca/images/vim-ripgrep.png" />
<p>There&rsquo;s no context for what else is around that line.</p>
<h3 id="the-quick-reference-is-nice">the quick reference is nice</h3>
<p>One thing I like about Helix is that when I press <code>g</code>, I get a little help popup
telling me places I can go. I really appreciate this because I don&rsquo;t often use
the &ldquo;go to definition&rdquo; or &ldquo;go to reference&rdquo; feature and I often forget the
keyboard shortcut.</p>
<img src="https://jvns.ca/images/goto.png" width="300px" />
<h3 id="some-vim-helix-translations">some vim -&gt; helix translations</h3>
<ul>
<li>Helix doesn&rsquo;t have marks like <code>ma</code>, <code>'a</code>, instead I&rsquo;ve been using <code>Ctrl+O</code> and
<code>Ctrl+I</code> to go back (or forward) to the last cursor location</li>
<li>I think Helix does have macros, but I&rsquo;ve been using multiple cursors in every
case that I would have previously used a macro. I like multiple cursors a lot
more than writing macros all the time. If I want to batch change something in
the document, my workflow is to press <code>%</code> (to highlight everything), then <code>s</code>
to select (with a regex) the things I want to change, then I can just edit
all of them as needed.</li>
<li>Helix doesn&rsquo;t have neovim-style tabs, instead it has a nice buffer switcher (<code>&lt;space&gt;b</code>)
I can use to switch to the buffer I want. There&rsquo;s a
<a href="https://github.com/helix-editor/helix/pull/7109">pull request here</a> to implement neovim-style tabs.
There&rsquo;s also a setting <code>bufferline=&quot;multiple&quot;</code> which can act a bit like tabs
with <code>gp</code>, <code>gn</code> for prev/next &ldquo;tab&rdquo; and <code>:bc</code> to close a &ldquo;tab&rdquo;.</li>
</ul>
<h3 id="some-helix-annoyances">some helix annoyances</h3>
<p>Here&rsquo;s everything that&rsquo;s annoyed me about Helix so far.</p>
<ul>
<li>I like the way Helix&rsquo;s <code>:reflow</code> works much less than how
vim reflows text with <code>gq</code>. It doesn&rsquo;t work as well with lists. (<a href="https://github.com/helix-editor/helix/issues/3332">github issue</a>)</li>
<li>If I&rsquo;m making a Markdown list, pressing &ldquo;enter&rdquo; at the end of a list item
won&rsquo;t continue the list. There&rsquo;s a <a href="https://github.com/helix-editor/helix/wiki/Recipes#continue-markdown-lists--quotes">partial workaround</a>
for bulleted lists but I don&rsquo;t know one for numbered lists.</li>
<li>No persistent undo yet: in vim I could use an
<a href="https://vimdoc.sourceforge.net/htmldoc/options.html#'undofile'">undofile</a> so
that I could undo changes even after quitting. Helix doesn&rsquo;t have that feature yet.
(<a href="https://github.com/helix-editor/helix/pull/9154">github PR</a>)</li>
<li>Helix doesn&rsquo;t autoreload files after they change on disk, I have to run
<code>:reload-all</code> (<code>:ra&lt;tab&gt;</code>) to manually reload them. Not a big deal.</li>
<li>Sometimes it crashes, maybe every week or so. I think it might be
<a href="https://github.com/helix-editor/helix/issues/12582">this issue</a>.</li>
</ul>
<p>The &ldquo;markdown list&rdquo; and reflowing issues come up a lot for me because I spend
a lot of time editing Markdown lists, but I keep using Helix anyway so I guess
they can&rsquo;t be making me that mad.</p>
<h3 id="switching-was-easier-than-i-thought">switching was easier than I thought</h3>
<p>I was worried that relearning 20 years of Vim muscle memory would be really hard.</p>
<p>It turned out to be easier than I expected, I started using Helix on a
vacation for a little low-stakes coding project I was doing on the side and
after a week or two it didn&rsquo;t feel so disorienting anymore. I think it might be
hard to switch back and forth between Vim and Helix, but I haven&rsquo;t needed to use
Vim recently so I don&rsquo;t know if that&rsquo;ll ever become an issue for me.</p>
<p>The first time I tried Helix I tried to force it to use keybindings that were
more similar to Vim and that did not work for me. Just learning the &ldquo;Helix way&rdquo;
was a lot easier.</p>
<p>There are still some things that throw me off: for example <code>w</code> in vim and <code>w</code> in
Helix don&rsquo;t have the same idea of what a &ldquo;word&rdquo; is (the Helix one includes the
space after the word, the Vim one doesn&rsquo;t).</p>
<h3 id="using-a-terminal-based-text-editor">using a terminal-based text editor</h3>
<p>For many years I&rsquo;d mostly been using a GUI version of vim/neovim, so switching
to actually using an editor in the terminal was a bit of an adjustment.</p>
<p>I ended up deciding on:</p>
<ol>
<li>Every project gets its own terminal window, and all of the tabs in that
window (mostly) have the same working directory</li>
<li>I make my Helix tab the first tab in the terminal window</li>
</ol>
<p>It works pretty well, I might actually like it better than my previous workflow.</p>
<h3 id="my-configuration">my configuration</h3>
<p>I appreciate that my configuration is really simple, compared to my neovim
configuration which is hundreds of lines. It&rsquo;s mostly just 4 keyboard
shortcuts.</p>
<pre><code>theme = &quot;solarized_light&quot;
[editor]
# Sync clipboard with system clipboard
default-yank-register = &quot;+&quot;

[keys.normal]
# I didn't like that Ctrl+C was the default &quot;toggle comments&quot; shortcut
&quot;#&quot; = &quot;toggle_comments&quot;

# I didn't feel like learning a different way
# to go to the beginning/end of a line so
# I remapped ^ and $
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.select]
&quot;^&quot; = &quot;goto_first_nonwhitespace&quot;
&quot;$&quot; = &quot;goto_line_end&quot;

[keys.normal.space]
# I write a lot of text so I need to constantly reflow,
# and missed vim's `gq` shortcut
l = &quot;:reflow&quot;
</code></pre>
<p>There&rsquo;s a separate <code>languages.toml</code> configuration where I set some language
preferences, like turning off autoformatting.
For example, here&rsquo;s my Python configuration:</p>
<pre><code>[[language]]
name = &quot;python&quot;
formatter = { command = &quot;black&quot;, args = [&quot;--stdin-filename&quot;, &quot;%{buffer_name}&quot;, &quot;-&quot;] }
language-servers = [&quot;pyright&quot;]
auto-format = false
</code></pre>
<h3 id="we-ll-see-how-it-goes">we&rsquo;ll see how it goes</h3>
<p>Three months is not that long, and it&rsquo;s possible that I&rsquo;ll decide to go back
to Vim at some point. For example, I wrote a <a href="https://jvns.ca/blog/2023/02/28/some-notes-on-using-nix/">post about switching to
nix</a> a while back but
after maybe 8 months I switched back to Homebrew (though I&rsquo;m still using NixOS
to manage one little server, and I&rsquo;m still satisfied with that).</p>

---

### [New zine: The Secret Rules of the Terminal](https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! After many months of writing deep dive blog posts about the terminal, on
Tuesday I released a new zine called &ldquo;The Secret Rules of the Terminal&rdquo;!</p>
<p>You can get it for $12 here:
<a href="https://wizardzines.com/zines/terminal">https://wizardzines.com/zines/terminal</a>, or get
an <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</p>
<p>Here&rsquo;s the cover:</p>
<div align="center">
<a href="https://wizardzines.com/zines/terminal">
  <img src="https://jvns.ca/images/terminal-cover-small.jpg" width="600px" />
  </a>
</div>
<h3 id="the-table-of-contents">the table of contents</h3>
<p>Here&rsquo;s the table of contents:</p>
<a href="https://wizardzines.com/zines/terminal/toc.png">
  <img src="https://jvns.ca/images/terminal-toc-small.png" width="600px" />
</a>
<h3 id="why-the-terminal">why the terminal?</h3>
<p>I&rsquo;ve been using the terminal every day for 20 years but even though I&rsquo;m very
confident in the terminal, I&rsquo;ve always had a bit of an uneasy feeling about it.
Usually things work fine, but sometimes something goes wrong and it just feels
like investigating it is impossible, or at least like it would open up a huge
can of worms.</p>
<p>So I started trying to write down a list of weird problems I&rsquo;ve run into in terminal and I realized
that the terminal has a lot of tiny inconsistencies like:</p>
<ul>
<li>sometimes you can use the arrow keys to move around, but sometimes pressing the arrow keys just prints <code>^[[D</code></li>
<li>sometimes you can use the mouse to select text, but sometimes you can&rsquo;t</li>
<li>sometimes your commands get saved to a history when you run them, and sometimes they don&rsquo;t</li>
<li>some shells let you use the up arrow to see the previous command, and some don&rsquo;t</li>
</ul>
<p>If you use the terminal daily for 10 or 20 years, even if you don&rsquo;t understand
exactly <em>why</em> these things happen, you&rsquo;ll probably build an intuition for them.</p>
<p>But having an intuition for them isn&rsquo;t the same as understanding why they
happen. When writing this zine I actually had to do a lot of work to figure out
exactly what was <em>happening</em> in the terminal to be able to talk about how to
reason about it.</p>
<h3 id="the-rules-aren-t-written-down-anywhere">the rules aren&rsquo;t written down anywhere</h3>
<p>It turns out that the &ldquo;rules&rdquo; for how the terminal works (how do
you edit a command you type in? how do you quit a program? how do you fix your
colours?) are extremely hard to fully understand, because &ldquo;the terminal&rdquo; is actually
made of many different pieces of software (your terminal emulator, your
operating system, your shell, the core utilities like <code>grep</code>, and every other random
terminal program you&rsquo;ve installed) which are written by different people with different
ideas about how things should work.</p>
<p>So I wanted to write something that would explain:</p>
<ul>
<li>how the 4 pieces of the terminal (your shell, terminal emulator, programs, and TTY driver) fit together to make everything work</li>
<li>some of the core conventions for how you can expect things in your terminal to work</li>
<li>lots of tips and tricks for how to use terminal programs</li>
</ul>
<h3 id="this-zine-explains-the-most-useful-parts-of-terminal-internals">this zine explains the most useful parts of terminal internals</h3>
<p>Terminal internals are a mess. A lot of it is just the way it is because
someone made a decision in the 80s and now it&rsquo;s impossible to change, and
honestly I don&rsquo;t think learning everything about terminal internals is worth
it.</p>
<p>But some parts are not that hard to understand and can really make your
experience in the terminal better, like:</p>
<ul>
<li>if you understand what <strong>your shell</strong> is responsible for, you can configure your shell (or use a different one!) to access your history more easily, get great tab completion, and so much more</li>
<li>if you understand <strong>escape codes</strong>, it&rsquo;s much less scary when <code>cat</code>ing a binary to stdout messes up your terminal, you can just type <code>reset</code> and move on</li>
<li>if you understand how <strong>colour</strong> works, you can get rid of bad colour contrast in your terminal so you can actually read the text</li>
</ul>
<h3 id="i-learned-a-surprising-amount-writing-this-zine">I learned a surprising amount writing this zine</h3>
<p>When I wrote <a href="https://wizardzines.com/zines/git">How Git Works</a>, I thought I
knew how Git worked, and I was right. But the terminal is different. Even
though I feel totally confident in the terminal and even though I&rsquo;ve used it
every day for 20 years, I had a lot of misunderstandings about how the terminal
works and (unless you&rsquo;re the author of <code>tmux</code> or something) I think there&rsquo;s a
good chance you do too.</p>
<p>A few things I learned that are actually useful to me:</p>
<ul>
<li>I understand the structure of the terminal better and so I feel more
confident debugging weird terminal stuff that happens to me (I was even able
to suggest a <a href="https://github.com/fish-shell/fish-shell/issues/10834">small improvement</a> to fish!). Identifying exactly which piece of software is causing a weird thing to happen in my terminal still isn&rsquo;t <em>easy</em> but I&rsquo;m a lot better at it now.</li>
<li>you can write a shell script to <a href="https://jvns.ca/til/vim-osc52/">copy to your clipboard over SSH</a></li>
<li>how <code>reset</code> works under the hood (it does the equivalent of <code>stty sane; sleep 1; tput reset</code>) ‚Äì basically I learned that I don&rsquo;t ever need to worry about
remembering <code>stty sane</code> or <code>tput reset</code> and I can just run <code>reset</code> instead</li>
<li>how to look at the invisible escape codes that a program is printing out (run <code>unbuffer program &gt; out; less out</code>)</li>
<li>why the builtin REPLs on my Mac like <code>sqlite3</code> are so annoying to use (they use <code>libedit</code> instead of <code>readline</code>)</li>
</ul>
<h3 id="blog-posts-i-wrote-along-the-way">blog posts I wrote along the way</h3>
<p>As usual these days I wrote a bunch of blog posts about various side quests:</p>
<ul>
<li><a href="https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/">How to add a directory to your PATH</a></li>
<li><a href="https://jvns.ca/blog/2024/11/26/terminal-rules/">&ldquo;rules&rdquo; that terminal problems follow</a></li>
<li><a href="https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/">why pipes sometimes get &ldquo;stuck&rdquo;: buffering</a></li>
<li><a href="https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/">some terminal frustrations</a></li>
<li><a href="https://jvns.ca/blog/2024/10/31/ascii-control-characters/">ASCII control characters in my terminal</a> on &ldquo;what&rsquo;s the deal with Ctrl+A, Ctrl+B, Ctrl+C, etc?&rdquo;</li>
<li><a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a></li>
<li><a href="https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/">what&rsquo;s involved in getting a &ldquo;modern&rdquo; terminal setup?</a></li>
<li><a href="https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/">reasons to use your shell&rsquo;s job control</a></li>
<li><a href="https://jvns.ca/blog/2025/03/07/escape-code-standards/">standards for ANSI escape codes</a>, which is really me trying to figure out if I think the <code>terminfo</code> database is serving us well today</li>
</ul>
<h3 id="people-who-helped-with-this-zine">people who helped with this zine</h3>
<p>A long time ago I used to write zines mostly by myself but with every project I get more
and more help. I met with <a href="https://marieflanagan.com">Marie Claire LeBlanc Flanagan</a> every weekday from September to June to work
on this one.</p>
<p>The cover is by Vladimir Ka≈°ikoviƒá,
Lesley Trites did copy editing,
Simon Tatham (who wrote <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a>) did technical review, our
Operations Manager Lee did the transcription as well as a million other
things, and <a href="https://github.com/doy">Jesse Luehrs</a> (who is one of the very few
people I know who actually understands the terminal&rsquo;s cursed inner workings)
had so many incredibly helpful conversations with me about what is going on in
the terminal.</p>
<h3 id="get-the-zine">get the zine</h3>
<p>Here are some links to get the zine again:</p>
<ul>
<li>get <a href="https://wizardzines.com/zines/terminal">The Secret Rules of the Terminal</a></li>
<li>get a <a href="https://wizardzines.com/zines/all-the-zines/">15-pack of all my zines here</a>.</li>
</ul>
<p>As always, you can get either a PDF version to print at home or a print version
shipped to your house. The only caveat is print orders will ship in <strong>August</strong> &ndash; I
need to wait for orders to come in to get an idea of how many I should print
before sending it to the printer.</p>

---

### [Using `make` to compile C programs (for non-C-programmers)](https://jvns.ca/blog/2025/06/10/how-to-compile-a-c-program/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I have never been a C programmer but every so often I need to compile a C/C++
program from source. This has been kind of a struggle for me: for a
long time, my approach was basically &ldquo;install the dependencies, run <code>make</code>, if
it doesn&rsquo;t work, either try to find a binary someone has compiled or give up&rdquo;.</p>
<p>&ldquo;Hope someone else has compiled it&rdquo; worked pretty well when I was running Linux
but since I&rsquo;ve been using a Mac for the last couple of years I&rsquo;ve been running
into more situations where I have to actually compile programs myself.</p>
<p>So let&rsquo;s talk about what you might have to do to compile a C program! I&rsquo;ll use
a couple of examples of specific C programs I&rsquo;ve compiled and talk about a few
things that can go wrong. Here are three programs we&rsquo;ll be talking about
compiling:</p>
<ul>
<li><a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a></li>
<li><a href="https://www.sqlite.org/download.html">sqlite</a></li>
<li><a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> (a pager you can run to quickly open files from a search with <code>rg -n THING | qf</code>)</li>
</ul>
<h3 id="step-1-install-a-c-compiler">step 1: install a C compiler</h3>
<p>This is pretty simple: on an Ubuntu system if I don&rsquo;t already have a C compiler I&rsquo;ll install one with:</p>
<pre><code>sudo apt-get install build-essential
</code></pre>
<p>This installs <code>gcc</code>, <code>g++</code>, and <code>make</code>. The situation on a Mac is more
confusing but it&rsquo;s something like &ldquo;install xcode command line tools&rdquo;.</p>
<h3 id="step-2-install-the-program-s-dependencies">step 2: install the program&rsquo;s dependencies</h3>
<p>Unlike some newer programming languages, C doesn&rsquo;t have a dependency manager.
So if a program has any dependencies, you need to hunt them down yourself.
Thankfully because of this, C programmers usually keep their dependencies very
minimal and often the dependencies will be available in whatever package manager you&rsquo;re using.</p>
<p>There&rsquo;s almost always a section explaining how to get the dependencies in the
README, for example in <a href="https://mj.ucw.cz/sw/paperjam/">paperjam</a>&rsquo;s README, it
says:</p>
<blockquote>
<p>To compile PaperJam, you need the headers for the libqpdf and libpaper libraries (usually available as libqpdf-dev and libpaper-dev packages).</p>
</blockquote>
<blockquote>
<p>You may need <code>a2x</code> (found in <a href="http://www.methods.co.nz/asciidoc/a2x.1.html">AsciiDoc</a>) for building manual pages.</p>
</blockquote>
<p>So on a Debian-based system you can install the dependencies like this.</p>
<pre><code>sudo apt install -y libqpdf-dev libpaper-dev
</code></pre>
<p>If a README gives a name for a package (like <code>libqpdf-dev</code>), I&rsquo;d basically
always assume that they mean &ldquo;in a Debian-based Linux distro&rdquo;: if you&rsquo;re on a
Mac <code>brew install libqpdf-dev</code> will not work. I still have not 100% gotten
the hang of developing on a Mac yet so I don&rsquo;t have many tips there yet. I
guess in this case it would be <code>brew install qpdf</code> if you&rsquo;re using Homebrew.</p>
<h3 id="step-3-run-configure-if-needed">step 3: run <code>./configure</code> (if needed)</h3>
<p>Some C programs come with a <code>Makefile</code> and some instead come with a script called
<code>./configure</code>. For example, if you download <a href="https://www.sqlite.org/download.html">sqlite&rsquo;s source code</a>, it has a <code>./configure</code> script in
it instead of a Makefile.</p>
<p>My understanding of this <code>./configure</code> script is:</p>
<ol>
<li>You run it, it prints out a lot of somewhat inscrutable output, and then it
either generates a <code>Makefile</code> or fails because you&rsquo;re missing some
dependency</li>
<li>The <code>./configure</code> script is part of a system called
<a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">autotools</a>
that I have never needed to learn anything about beyond &ldquo;run it to generate
a <code>Makefile</code>&rdquo;.</li>
</ol>
<p>I think there might be some options you can pass to get the <code>./configure</code>
script to produce a different <code>Makefile</code> but I have never done that.</p>
<h3 id="step-4-run-make">step 4: run <code>make</code></h3>
<p>The next step is to run <code>make</code> to try to build a program. Some notes about
<code>make</code>:</p>
<ul>
<li>Sometimes you can run <code>make -j8</code> to parallelize the build and make it go
faster</li>
<li>It usually prints out a million compiler warnings when compiling the program.
I always just ignore them. I didn&rsquo;t write the software! The compiler warnings
are not my problem.</li>
</ul>
<h3 id="compiler-errors-are-often-dependency-problems">compiler errors are often dependency problems</h3>
<p>Here&rsquo;s an error I got while compiling <code>paperjam</code> on my Mac:</p>
<pre><code>/opt/homebrew/Cellar/qpdf/12.0.0/include/qpdf/InputSource.hh:85:19: error: function definition does not declare parameters
   85 |     qpdf_offset_t last_offset{0};
      |                   ^
</code></pre>
<p>Over the years I&rsquo;ve learned it&rsquo;s usually best not to overthink problems like
this: if it&rsquo;s talking about <code>qpdf</code>, there&rsquo;s a good change it just means that
I&rsquo;ve done something wrong with how I&rsquo;m including the <code>qpdf</code> dependency.</p>
<p>Now let&rsquo;s talk about some ways to get the <code>qpdf</code> dependency included in the right way.</p>
<h3 id="the-world-s-shortest-introduction-to-the-compiler-and-linker">the world&rsquo;s shortest introduction to the compiler and linker</h3>
<p>Before we talk about how to fix dependency problems: building C programs is split into 2
steps:</p>
<ol>
<li><strong>Compiling</strong> the code into <strong>object files</strong> (with <code>gcc</code> or <code>clang</code>)</li>
<li><strong>Linking</strong> those object files into a final binary (with <code>ld</code>)</li>
</ol>
<p>It&rsquo;s important to know this when building a C program because sometimes you
need to pass the right flags to the compiler and linker to tell them where to
find the dependencies for the program you&rsquo;re compiling.</p>
<h3 id="make-uses-environment-variables-to-configure-the-compiler-and-linker"><code>make</code> uses environment variables to configure the compiler and linker</h3>
<p>If I run <code>make</code> on my Mac to install <code>paperjam</code>, I get this error:</p>
<pre><code>c++ -o paperjam paperjam.o pdf-tools.o parse.o cmds.o pdf.o -lqpdf -lpaper
ld: library 'qpdf' not found
</code></pre>
<p>This is not because <code>qpdf</code> is not installed on my system (it actually is!). But
the compiler and linker don&rsquo;t know how to <em>find</em> the <code>qpdf</code> library. To fix this, we need to:</p>
<ul>
<li>pass <code>&quot;-I/opt/homebrew/include&quot;</code> to the compiler (to tell it where to find the header files)</li>
<li>pass <code>&quot;-L/opt/homebrew/lib -liconv&quot;</code> to the linker (to tell it where to find library files and to link in <code>iconv</code>)</li>
</ul>
<p>And we can get <code>make</code> to pass those extra parameters to the compiler and linker using environment variables!
To see how this works: inside <code>paperjam</code>&rsquo;s Makefile you can see a bunch of environment variables, like <code>LDLIBS</code> here:</p>
<pre><code>paperjam: $(OBJS)
	$(LD) -o $@ $^ $(LDLIBS)
</code></pre>
<p>Everything you put into the <code>LDLIBS</code> environment variable gets passed to the
linker (<code>ld</code>) as a command line argument.</p>
<h3 id="secret-environment-variable-cppflags">secret environment variable: <code>CPPFLAGS</code></h3>
<p><code>Makefiles</code> sometimes define their own environment variables that they pass to
the compiler/linker, but <code>make</code> also has a bunch of &ldquo;implicit&rdquo; environment
variables which it will automatically pass to the C compiler and linker. There&rsquo;s a <a href="https://www.gnu.org/software/make/manual/html_node/Implicit-Variables.html#index-CFLAGS0">full list of implicit environment variables here</a>,
but one of them is <code>CPPFLAGS</code>, which gets automatically passed to the C compiler.</p>
<p>(technically it would be more normal to use <code>CXXFLAGS</code> for this, but this
particular <code>Makefile</code> hardcodes <code>CXXFLAGS</code> so setting <code>CPPFLAGS</code> was the only
way I could find to set the compiler flags without editing the <code>Makefile</code>)</p>
<small>
As an aside: it took me a long time to realize how closely tied to C/C++ `make` is -- I used
to think that `make` was just a general build system (and of course you can use it for
anything!) but it has a lot of affordances for building C/C++ programs that it
doesn't have for building any other kind of program.
</small>
<h3 id="two-ways-to-pass-environment-variables-to-make">two ways to pass environment variables to <code>make</code></h3>
<p>I learned thanks to <a href="https://www.owlfolio.org/">@zwol</a> that there are actually two ways to pass environment variables to <code>make</code>:</p>
<ol>
<li><code>CXXFLAGS=xyz make</code> (the usual way)</li>
<li><code>make CXXFLAGS=xyz</code></li>
</ol>
<p>The difference between them is that <code>make CXXFLAGS=xyz</code> will override the
value of <code>CXXFLAGS</code> set in the <code>Makefile</code> but <code>CXXFLAGS=xyz make</code> won&rsquo;t.</p>
<p>I&rsquo;m not sure which way is the norm but I&rsquo;m going to use the first way in this
post.</p>
<h3 id="how-to-use-cppflags-and-ldlibs-to-fix-this-compiler-error">how to use <code>CPPFLAGS</code> and <code>LDLIBS</code> to fix this compiler error</h3>
<p>Now that we&rsquo;ve talked about how <code>CPPFLAGS</code> and <code>LDLIBS</code> get passed to the
compiler and linker, here&rsquo;s the final incantation that I used to get the
program to build successfully!</p>
<pre><code>CPPFLAGS=&quot;-I/opt/homebrew/include&quot; LDLIBS=&quot;-L/opt/homebrew/lib -liconv&quot; make paperjam
</code></pre>
<p>This passes <code>-I/opt/homebrew/include</code> to the compiler and <code>-L/opt/homebrew/lib -liconv</code> to the linker.</p>
<p>Also I don&rsquo;t want to pretend that I &ldquo;magically&rdquo; knew that those were the right
arguments to pass, figuring them out involved a bunch of confused Googling that I
skipped over in this post. I will say that:</p>
<ul>
<li>the <code>-I</code> compiler flag tells the compiler which directory to find header files in, like <code>/opt/homebrew/include/qpdf/QPDF.hh</code></li>
<li>the <code>-L</code> linker flag tells the linker which directory to find libraries in, like <code>/opt/homebrew/lib/libqpdf.a</code></li>
<li>the <code>-l</code> linker flag tells the linker which libraries to link in, like <code>-liconv</code> means &ldquo;link in the <code>iconv</code> library&rdquo;, or <code>-lm</code> means &ldquo;link <code>math</code>&rdquo;</li>
</ul>
<h3 id="tip-how-to-just-build-1-specific-file-make-filename">tip: how to just build 1 specific file: <code>make $FILENAME</code></h3>
<p>Yesterday I discovered this cool tool called
<a href="https://git.causal.agency/src/tree/bin/qf.c">qf</a> which you can use to quickly
open files from the output of <code>ripgrep</code>.</p>
<p><code>qf</code> is in a big directory of various tools, but I only wanted to compile <code>qf</code>.
So I just compiled <code>qf</code>, like this:</p>
<pre><code>make qf
</code></pre>
<p>Basically if you know (or can guess) the output filename of the file you&rsquo;re
trying to build, you can tell <code>make</code> to just build that file by running <code>make $FILENAME</code></p>
<h3 id="tip-you-don-t-need-a-makefile">tip: you don&rsquo;t need a Makefile</h3>
<p>I sometimes write 5-line C programs with no dependencies, and I just learned
that if I have a file called <code>blah.c</code>, I can just compile it like this without creating a <code>Makefile</code>:</p>
<pre><code>make blah
</code></pre>
<p>It gets automaticaly expanded to <code>cc -o blah blah.c</code>, which saves a bit of
typing. I have no idea if I&rsquo;m going to remember this (I might just keep typing
<code>gcc -o blah blah.c</code> anyway) but it seems like a fun trick.</p>
<h3 id="tip-look-at-how-other-packaging-systems-built-the-same-c-program">tip: look at how other packaging systems built the same C program</h3>
<p>If you&rsquo;re having trouble building a C program, maybe other people had problems building it
too! Every Linux distribution has build files for every package that they
build, so even if you can&rsquo;t install packages from that distribution directly,
maybe you can get tips from that Linux distro for how to build the package.
Realizing this (thanks to my friend Dave) was a huge ah-ha moment for me.</p>
<p>For example, <a href="https://github.com/NixOS/nixpkgs/blob/405624e81a9b65378328accb0a11c3e5369e651c/pkgs/by-name/pa/paperjam/package.nix#L35">this line from the nix package for <code>paperjam</code></a> says:</p>
<pre><code>  env.NIX_LDFLAGS = lib.optionalString stdenv.hostPlatform.isDarwin &quot;-liconv&quot;;
</code></pre>
<p>This is basically saying &ldquo;pass the linker flag <code>-liconv</code> to build this on a
Mac&rdquo;, so that&rsquo;s a clue we could use to build it.</p>
<p>That same file also says <code>  env.NIX_CFLAGS_COMPILE = &quot;-DPOINTERHOLDER_TRANSITION=1&quot;;</code>. I&rsquo;m not sure what this means, but when I try
to build the <code>paperjam</code> package I do get an error about something called a
<code>PointerHolder</code>, so I guess that&rsquo;s somehow related to the &ldquo;PointerHolder
transition&rdquo;.</p>
<h3 id="step-5-installing-the-binary">step 5: installing the binary</h3>
<p>Once you&rsquo;ve managed to compile the program, probably you want to install it somewhere!
Some <code>Makefile</code>s have an <code>install</code> target that let you install the tool on your
system with <code>make install</code>. I&rsquo;m always a bit scared of this (where is it going
to put the files? what if I want to uninstall them later?), so if I&rsquo;m compiling
a pretty simple program I&rsquo;ll often just manually copy the binary to install it
instead, like this:</p>
<pre><code>cp qf ~/bin
</code></pre>
<h3 id="step-6-maybe-make-your-own-package">step 6: maybe make your own package!</h3>
<p>Once I figured out how to do all of this, I realized that I could use my new
<code>make</code> knowledge to contribute a <code>paperjam</code> package to Homebrew! Then I could
just <code>brew install paperjam</code> on future systems.</p>
<p>The good thing is that even if the details of how all of the different
packaging systems, they fundamentally all use C compilers and linkers.</p>
<h3 id="it-can-be-useful-to-understand-a-little-about-c-even-if-you-re-not-a-c-programmer">it can be useful to understand a little about C even if you&rsquo;re not a C programmer</h3>
<p>I think all of this is an interesting example of how it can useful to
understand some basics of how C programs work (like &ldquo;they have header files&rdquo;)
even if you&rsquo;re never planning to write a nontrivial C program if your life.</p>
<p>It feels good to have some ability to compile C/C++ programs myself, even
though I&rsquo;m still not totally confident about all of the compiler and linker
flags and I still plan to never learn anything about how autotools works other
than &ldquo;you run <code>./configure</code> to generate the <code>Makefile</code>&rdquo;.</p>
<p>Two things I left out of this post:</p>
<ul>
<li><code>LD_LIBRARY_PATH / DYLD_LIBRARY_PATH</code> (which you use to tell the dynamic
linker at runtime where to find dynamically linked files) because I can&rsquo;t
remember the last time I ran into an <code>LD_LIBRARY_PATH</code> issue and couldn&rsquo;t
find an example.</li>
<li><code>pkg-config</code>, which I think is important but I don&rsquo;t understand yet</li>
</ul>

---

### [Standards for ANSI escape codes](https://jvns.ca/blog/2025/03/07/escape-code-standards/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Today I want to talk about ANSI escape codes.</p>
<p>For a long time I was vaguely aware of ANSI escape codes (&ldquo;that&rsquo;s how you make
text red in the terminal and stuff&rdquo;) but I had no real understanding of where they were
supposed to be defined or whether or not there were standards for them. I just
had a kind of vague &ldquo;there be dragons&rdquo; feeling around them. While learning
about the terminal this year, I&rsquo;ve learned that:</p>
<ol>
<li>ANSI escape codes are responsible for a lot of usability improvements
in the terminal (did you know there&rsquo;s a way to copy to your system clipboard
when SSHed into a remote machine?? It&rsquo;s an escape code called <a href="https://jvns.ca/til/vim-osc52/">OSC 52</a>!)</li>
<li>They aren&rsquo;t completely standardized, and because of that they don&rsquo;t always
work reliably. And because they&rsquo;re also invisible, it&rsquo;s extremely
frustrating to troubleshoot escape code issues.</li>
</ol>
<p>So I wanted to put together a list for myself of some standards that exist
around escape codes, because I want to know if they <em>have</em> to feel unreliable
and frustrating, or if there&rsquo;s a future where we could all rely on them with
more confidence.</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#what-s-an-escape-code">what&rsquo;s an escape code?</a></li>
<li><a href="https://jvns.ca/atom.xml#ecma-48">ECMA-48</a></li>
<li><a href="https://jvns.ca/atom.xml#xterm-control-sequences">xterm control sequences</a></li>
<li><a href="https://jvns.ca/atom.xml#terminfo">terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#should-programs-use-terminfo">should programs use terminfo?</a></li>
<li><a href="https://jvns.ca/atom.xml#is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</a></li>
<li><a href="https://jvns.ca/atom.xml#some-reasons-to-use-terminfo">some reasons to use terminfo</a></li>
<li><a href="https://jvns.ca/atom.xml#some-more-documents-standards">some more documents/standards</a></li>
<li><a href="https://jvns.ca/atom.xml#why-i-think-this-is-interesting">why I think this is interesting</a></li>
</ul>
<h3 id="what-s-an-escape-code">what&rsquo;s an escape code?</h3>
<p>Have you ever pressed the left arrow key in your terminal and seen <code>^[[D</code>?
That&rsquo;s an escape code! It&rsquo;s called an &ldquo;escape code&rdquo; because the first character
is the &ldquo;escape&rdquo; character, which is usually written as <code>ESC</code>, <code>\x1b</code>, <code>\E</code>,
<code>\033</code>, or <code>^[</code>.</p>
<p>Escape codes are how your terminal emulator communicates various kinds of
information (colours, mouse movement, etc) with programs running in the
terminal. There are two kind of escape codes:</p>
<ol>
<li><strong>input codes</strong> which your terminal emulator sends for keypresses or mouse
movements that don&rsquo;t fit into Unicode. For example &ldquo;left arrow key&rdquo; is
<code>ESC[D</code>, &ldquo;Ctrl+left arrow&rdquo; might be <code>ESC[1;5D</code>, and clicking the mouse might
be something like <code>ESC[M :3</code>.</li>
<li><strong>output codes</strong> which programs can print out to colour text, move the
cursor around, clear the screen, hide the cursor, copy text to the
clipboard, enable mouse reporting, set the window title, etc.</li>
</ol>
<p>Now let&rsquo;s talk about standards!</p>
<h3 id="ecma-48">ECMA-48</h3>
<p>The first standard I found relating to escape codes was
<a href="https://ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA-48</a>,
which was originally published in 1976.</p>
<p>ECMA-48 does two things:</p>
<ol>
<li>Define some general <em>formats</em> for escape codes (like &ldquo;CSI&rdquo; codes, which are
<code>ESC[</code> + something and &ldquo;OSC&rdquo; codes, which are <code>ESC]</code> + something)</li>
<li>Define some specific escape codes, like how &ldquo;move the cursor to the left&rdquo; is
<code>ESC[D</code>, or &ldquo;turn text red&rdquo; is  <code>ESC[31m</code>. In the spec, the &ldquo;cursor left&rdquo;
one is called <code>CURSOR LEFT</code> and the one for changing colours is called
<code>SELECT GRAPHIC RENDITION</code>.</li>
</ol>
<p>The formats are extensible, so there&rsquo;s room for others to define more escape
codes in the future. Lots of escape codes that are popular today aren&rsquo;t defined
in ECMA-48: for example it&rsquo;s pretty common for terminal applications (like vim,
htop, or tmux) to support using the mouse, but ECMA-48 doesn&rsquo;t define escape
codes for the mouse.</p>
<h3 id="xterm-control-sequences">xterm control sequences</h3>
<p>There are a bunch of escape codes that aren&rsquo;t defined in ECMA-48, for example:</p>
<ul>
<li>enabling mouse reporting (where did you click in your terminal?)</li>
<li>bracketed paste (did you paste that text or type it in?)</li>
<li>OSC 52 (which terminal applications can use to copy text to your system clipboard)</li>
</ul>
<p>I believe (correct me if I&rsquo;m wrong!) that these and some others came from
xterm, are documented in <a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html">XTerm Control Sequences</a>, and have
been widely implemented by other terminal emulators.</p>
<p>This list of &ldquo;what xterm supports&rdquo; is not a standard exactly, but xterm is
extremely influential and so it seems like an important document.</p>
<h3 id="terminfo">terminfo</h3>
<p>In the 80s (and to some extent today, but my understanding is that it was MUCH
more dramatic in the 80s) there was a huge amount of variation in what escape
codes terminals actually supported.</p>
<p>To deal with this, there&rsquo;s a database of escape codes for various terminals
called &ldquo;terminfo&rdquo;.</p>
<p>It looks like the standard for terminfo is called <a href="https://publications.opengroup.org/c243-1">X/Open Curses</a>, though you need to create
an account to view that standard for some reason. It defines the database format as well
as a C library interface (&ldquo;curses&rdquo;) for accessing the database.</p>
<p>For example you can run this bash snippet to see every possible escape code for
&ldquo;clear screen&rdquo; for all of the different terminals your system knows about:</p>
<pre><code>for term in $(toe -a | awk '{print $1}')
do
  echo $term
  infocmp -1 -T &quot;$term&quot; 2&gt;/dev/null | grep 'clear=' | sed 's/clear=//g;s/,//g'
done
</code></pre>
<p>On my system (and probably every system I&rsquo;ve ever used?), the terminfo database is managed by ncurses.</p>
<h3 id="should-programs-use-terminfo">should programs use terminfo?</h3>
<p>I think it&rsquo;s interesting that there are two main approaches that applications
take to handling ANSI escape codes:</p>
<ol>
<li>Use the terminfo database to figure out which escape codes to use, depending
on what&rsquo;s in the <code>TERM</code> environment variable. Fish does this, for example.</li>
<li>Identify a &ldquo;single common set&rdquo; of escape codes which works in &ldquo;enough&rdquo;
terminal emulators and just hardcode those.</li>
</ol>
<p>Some examples of programs/libraries that take approach #2 (&ldquo;don&rsquo;t use terminfo&rdquo;) include:</p>
<ul>
<li><a href="https://github.com/mawww/kakoune/commit/c12699d2e9c2806d6ed184032078d0b84a3370bb">kakoune</a></li>
<li><a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/165258d2f3ae594b50f16c7b50ffb06627476269/src/prompt_toolkit/input/ansi_escape_sequences.py#L5-L8">python-prompt-toolkit</a></li>
<li><a href="https://github.com/antirez/linenoise">linenoise</a></li>
<li><a href="https://github.com/rockorager/libvaxis">libvaxis</a></li>
<li><a href="https://github.com/chalk/chalk">chalk</a></li>
</ul>
<p>I got curious about why folks might be moving away from terminfo and I found
this very interesting and extremely detailed
<a href="https://twoot.site/@bean/113056942625234032">rant about terminfo from one of the fish maintainers</a>, which argues that:</p>
<blockquote>
<p>[the terminfo authors] have done a lot of work that, at the time, was
extremely important and helpful. My point is that it no longer is.</p>
</blockquote>
<p>I&rsquo;m not going to do it justice so I&rsquo;m not going to summarize it, I think it&rsquo;s
worth reading.</p>
<h3 id="is-there-a-single-common-set-of-escape-codes">is there a &ldquo;single common set&rdquo; of escape codes?</h3>
<p>I was just talking about the idea that you can use a &ldquo;common set&rdquo; of escape
codes that will work for most people. But what is that set? Is there any agreement?</p>
<p>I really do not know the answer to this at all, but from doing some reading it
seems like it&rsquo;s some combination of:</p>
<ul>
<li>The codes that the VT100 supported (though some aren&rsquo;t relevant on modern terminals)</li>
<li>what&rsquo;s in ECMA-48 (which I think also has some things that are no longer relevant)</li>
<li>What xterm supports (though I&rsquo;d guess that not everything in there is actually widely supported enough)</li>
</ul>
<p>and maybe ultimately &ldquo;identify the terminal emulators you think your users are
going to use most frequently and test in those&rdquo;, the same way web developers do
when deciding which CSS features are okay to use</p>
<p>I don&rsquo;t think there are any resources like <a href="https://caniuse.com/">Can I use&hellip;?</a> or
<a href="https://web-platform-dx.github.io/web-features/">Baseline</a> for the terminal
though. (in theory terminfo is supposed to be the &ldquo;caniuse&rdquo; for the terminal
but it seems like it often takes 10+ years to add new terminal features when
people invent them which makes it very limited)</p>
<h3 id="some-reasons-to-use-terminfo">some reasons to use terminfo</h3>
<p>I also asked on Mastodon why people found terminfo valuable in 2025 and got a
few reasons that made sense to me:</p>
<ul>
<li>some people expect to be able to use the <code>TERM</code> environment variable to
control how programs behave (for example with <code>TERM=dumb</code>), and there&rsquo;s
no standard for how that should work in a post-terminfo world</li>
<li>even though there&rsquo;s <em>less</em> variation between terminal emulators than
there was in the 80s, there&rsquo;s far from zero variation: there are graphical
terminals, the Linux framebuffer console, the situation you&rsquo;re in when
connecting to a server via its serial console, Emacs shell mode, and probably
more that I&rsquo;m missing</li>
<li>there is no one standard for what the &ldquo;single common set&rdquo; of escape codes
is, and sometimes programs use escape codes which aren&rsquo;t actually widely
supported enough</li>
</ul>
<h3 id="terminfo-user-agent-detection">terminfo &amp; user agent detection</h3>
<p>The way that ncurses uses the <code>TERM</code> environment variable to decide which
escape codes to use reminds me of how webservers used to sometimes use the
browser user agent to decide which version of a website to serve.</p>
<p>It also seems like it&rsquo;s had some of the same results &ndash; the way iTerm2 reports
itself as being &ldquo;xterm-256color&rdquo; feels similar to how Safari&rsquo;s user agent is
&ldquo;Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7_4) AppleWebKit/605.1.15 (KHTML,
like Gecko) Version/18.3 Safari/605.1.15&rdquo;. In both cases the terminal emulator
/ browser ends up changing its user agent to get around user agent detection
that isn&rsquo;t working well.</p>
<p>On the web we ended up deciding that user agent detection was not a good
practice and to instead focus on standardization so we can serve the same
HTML/CSS to all browsers. I don&rsquo;t know if the same approach is the future in
the terminal though &ndash; I think the terminal landscape today is much more
fragmented than the web ever was as well as being much less well funded.</p>
<h3 id="some-more-documents-standards">some more documents/standards</h3>
<p>A few more documents and standards related to escape codes, in no particular order:</p>
<ul>
<li>the <a href="https://man7.org/linux/man-pages/man4/console_codes.4.html">Linux console_codes man page</a> documents
escape codes that Linux supports</li>
<li>how the <a href="https://vt100.net/docs/vt100-ug/chapter3.html">VT 100</a> handles escape codes &amp; control sequences</li>
<li>the <a href="https://sw.kovidgoyal.net/kitty/keyboard-protocol/">kitty keyboard protocol</a></li>
<li><a href="https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda">OSC 8</a> for links in the terminal (and notes on <a href="https://github.com/Alhadis/OSC8-Adoption?tab=readme-ov-file">adoption</a>)</li>
<li>A <a href="https://github.com/tmux/tmux/blob/882fb4d295deb3e4b803eb444915763305114e4f/tools/ansicode.txt">summary of ANSI standards from tmux</a></li>
<li>this <a href="https://iterm2.com/feature-reporting/">terminal features reporting specification from iTerm</a></li>
<li>sixel graphics</li>
</ul>
<h3 id="why-i-think-this-is-interesting">why I think this is interesting</h3>
<p>I sometimes see people saying that the unix terminal is &ldquo;outdated&rdquo;, and since I
love the terminal so much I&rsquo;m always curious about what incremental changes
might make it feel less &ldquo;outdated&rdquo;.</p>
<p>Maybe if we had a clearer standards landscape (like we do on the web!) it would
be easier for terminal emulator developers to build new features and for
authors of terminal applications to more confidently adopt those features so
that we can all benefit from them and have a richer experience in the terminal.</p>
<p>Obviously standardizing ANSI escape codes is not easy (ECMA-48 was first
published almost 50 years ago and we&rsquo;re still not there!). I don&rsquo;t even know
what all of the challenges are. But the situation with HTML/CSS/JS used to be
extremely bad too and now it&rsquo;s MUCH better, so maybe there&rsquo;s hope.</p>

---

### [How to add a directory to your PATH](https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>I was talking to a friend about how to add a directory to your PATH today. It&rsquo;s
something that feels &ldquo;obvious&rdquo; to me since I&rsquo;ve been using the terminal for a
long time, but when I searched for instructions for how to do it, I actually
couldn&rsquo;t find something that explained all of the steps &ndash; a lot of them just
said &ldquo;add this to <code>~/.bashrc</code>&rdquo;, but what if you&rsquo;re not using bash? What if your
bash config is actually in a different file? And how are you supposed to figure
out which directory to add anyway?</p>
<p>So I wanted to try to write down some more complete directions and mention some
of the gotchas I&rsquo;ve run into over the years.</p>
<p>Here&rsquo;s a table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-1-what-shell-are-you-using">step 1: what shell are you using?</a></li>
<li><a href="https://jvns.ca/atom.xml#step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</a>
<ul>
<li><a href="https://jvns.ca/atom.xml#step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</a></li>
</ul>
</li>
<li><a href="https://jvns.ca/atom.xml#step-4-edit-your-shell-config">step 4: edit your shell config</a></li>
<li><a href="https://jvns.ca/atom.xml#step-5-restart-your-shell">step 5: restart your shell</a></li>
<li>problems:
<ul>
<li><a href="https://jvns.ca/atom.xml#problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate PATH entries making it harder to debug</a></li>
<li><a href="https://jvns.ca/atom.xml#problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your PATH</a></li>
</ul>
</li>
<li>notes:
<ul>
<li><a href="https://jvns.ca/atom.xml#a-note-on-source">a note on source</a></li>
<li><a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">a note on fish_add_path</a></li>
</ul>
</li>
</ul>
<h3 id="step-1-what-shell-are-you-using">step 1: what shell are you using?</h3>
<p>If you&rsquo;re not sure what shell you&rsquo;re using, here&rsquo;s a way to find out. Run this:</p>
<pre><code>ps -p $$ -o pid,comm=
</code></pre>
<ul>
<li>if you&rsquo;re using <strong>bash</strong>, it&rsquo;ll print out <code>97295 bash</code></li>
<li>if you&rsquo;re using <strong>zsh</strong>, it&rsquo;ll print out <code>97295 zsh</code></li>
<li>if you&rsquo;re using <strong>fish</strong>, it&rsquo;ll print out an error like &ldquo;In fish, please use
$fish_pid&rdquo; (<code>$$</code> isn&rsquo;t valid syntax in fish, but in any case the error
message tells you that you&rsquo;re using fish, which you probably already knew)</li>
</ul>
<p>Also bash is the default on Linux and zsh is the default on Mac OS (as of
2024). I&rsquo;ll only cover bash, zsh, and fish in these directions.</p>
<h3 id="step-2-find-your-shell-s-config-file">step 2: find your shell&rsquo;s config file</h3>
<ul>
<li>in zsh, it&rsquo;s probably <code>~/.zshrc</code></li>
<li>in bash, it might be <code>~/.bashrc</code>, but it&rsquo;s complicated, see the note in the next section</li>
<li>in fish, it&rsquo;s probably <code>~/.config/fish/config.fish</code> (you can run <code>echo $__fish_config_dir</code> if you want to be 100% sure)</li>
</ul>
<h3 id="a-note-on-bash-s-config-file">a note on bash&rsquo;s config file</h3>
<p>Bash has three possible config files: <code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.profile</code>.</p>
<p>If you&rsquo;re not sure which one your system is set up to use, I&rsquo;d recommend
testing this way:</p>
<ol>
<li>add <code>echo hi there</code> to your <code>~/.bashrc</code></li>
<li>Restart your terminal</li>
<li>If you see &ldquo;hi there&rdquo;, that means <code>~/.bashrc</code> is being used! Hooray!</li>
<li>Otherwise remove it and try the same thing with <code>~/.bash_profile</code></li>
<li>You can also try <code>~/.profile</code> if the first two options don&rsquo;t work.</li>
</ol>
<p>(there are a lot of <a href="https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html">elaborate flow charts</a> out there that explain how bash
decides which config file to use but IMO it&rsquo;s not worth it to internalize them
and just testing is the fastest way to be sure)</p>
<h3 id="step-3-figure-out-which-directory-to-add">step 3: figure out which directory to add</h3>
<p>Let&rsquo;s say that you&rsquo;re trying to install and run a program called <code>http-server</code>
and it doesn&rsquo;t work, like this:</p>
<pre><code>$ npm install -g http-server
$ http-server
bash: http-server: command not found
</code></pre>
<p>How do you find what directory <code>http-server</code> is in? Honestly in general this is
not that easy &ndash; often the answer is something like &ldquo;it depends on how npm is
configured&rdquo;. A few ideas:</p>
<ul>
<li>Often when setting up a new installer (like <code>cargo</code>, <code>npm</code>, <code>homebrew</code>, etc),
when you first set it up it&rsquo;ll print out some directions about how to update
your PATH. So if you&rsquo;re paying attention you can get the directions then.</li>
<li>Sometimes installers will automatically update your shell&rsquo;s config file
to update your <code>PATH</code> for you</li>
<li>Sometimes just Googling &ldquo;where does npm install things?&rdquo; will turn up the
answer</li>
<li>Some tools have a subcommand that tells you where they&rsquo;re configured to
install things, like:
<ul>
<li>Node/npm: <code>npm config get prefix</code> (then append <code>/bin/</code>)</li>
<li>Go: <code>go env GOPATH</code> (then append <code>/bin/</code>)</li>
<li>asdf: <code>asdf info | grep ASDF_DIR</code> (then append <code>/bin/</code> and <code>/shims/</code>)</li>
</ul>
</li>
</ul>
<h3 id="step-3-1-double-check-it-s-the-right-directory">step 3.1: double check it&rsquo;s the right directory</h3>
<p>Once you&rsquo;ve found a directory you think might be the right one, make sure it&rsquo;s
actually correct! For example, I found out that on my machine, <code>http-server</code> is
in <code>~/.npm-global/bin</code>. I can make sure that it&rsquo;s the right directory by trying to
run the program <code>http-server</code> in that directory like this:</p>
<pre><code>$ ~/.npm-global/bin/http-server
Starting up http-server, serving ./public
</code></pre>
<p>It worked! Now that you know what directory you need to add to your <code>PATH</code>,
let&rsquo;s move to the next step!</p>
<h3 id="step-4-edit-your-shell-config">step 4: edit your shell config</h3>
<p>Now we have the 2 critical pieces of information we need:</p>
<ol>
<li>Which directory you&rsquo;re trying to add to your PATH (like  <code>~/.npm-global/bin/</code>)</li>
<li>Where your shell&rsquo;s config is (like <code>~/.bashrc</code>, <code>~/.zshrc</code>, or <code>~/.config/fish/config.fish</code>)</li>
</ol>
<p>Now what you need to add depends on your shell:</p>
<p><strong>bash instructions:</strong></p>
<p>Open your shell&rsquo;s config file, and add a line like this:</p>
<pre><code>export PATH=$PATH:~/.npm-global/bin/
</code></pre>
<p>(obviously replace <code>~/.npm-global/bin</code> with the actual directory you&rsquo;re trying to add)</p>
<p><strong>zsh instructions:</strong></p>
<p>You can do the same thing as in bash, but zsh also has some slightly fancier
syntax you can use if you prefer:</p>
<pre><code>path=(
  $path
  ~/.npm-global/bin
)
</code></pre>
<p><strong>fish instructions:</strong></p>
<p>In fish, the syntax is different:</p>
<pre><code>set PATH $PATH ~/.npm-global/bin
</code></pre>
<p>(in fish you can also use <code>fish_add_path</code>, some notes on that <a href="https://jvns.ca/atom.xml#a-note-on-fish-add-path">further down</a>)</p>
<h3 id="step-5-restart-your-shell">step 5: restart your shell</h3>
<p>Now, an extremely important step: updating your shell&rsquo;s config won&rsquo;t take
effect if you don&rsquo;t restart it!</p>
<p>Two ways to do this:</p>
<ol>
<li>open a new terminal (or terminal tab), and maybe close the old one so you don&rsquo;t get confused</li>
<li>Run <code>bash</code> to start a new shell (or <code>zsh</code> if you&rsquo;re using zsh, or <code>fish</code> if you&rsquo;re using fish)</li>
</ol>
<p>I&rsquo;ve found that both of these usually work fine.</p>
<p>And you should be done! Try running the program you were trying to run and
hopefully it works now.</p>
<p>If not, here are a couple of problems that you might run into:</p>
<h3 id="problem-1-it-ran-the-wrong-program">problem 1: it ran the wrong program</h3>
<p>If the wrong <strong>version</strong> of a program is running, you might need to add the
directory to the <em>beginning</em> of your PATH instead of the end.</p>
<p>For example, on my system I have two versions of <code>python3</code> installed, which I
can see by running <code>which -a</code>:</p>
<pre><code>$ which -a python3
/usr/bin/python3
/opt/homebrew/bin/python3
</code></pre>
<p>The one your shell will use is the <strong>first one listed</strong>.</p>
<p>If you want to use the Homebrew version, you need to add that directory
(<code>/opt/homebrew/bin</code>) to the <strong>beginning</strong> of your PATH instead, by putting this in
your shell&rsquo;s config file (it&rsquo;s <code>/opt/homebrew/bin/:$PATH</code> instead of the usual <code>$PATH:/opt/homebrew/bin/</code>)</p>
<pre><code>export PATH=/opt/homebrew/bin/:$PATH
</code></pre>
<p>or in fish:</p>
<pre><code>set PATH ~/.cargo/bin $PATH
</code></pre>
<h3 id="problem-2-the-program-isn-t-being-run-from-your-shell">problem 2: the program isn&rsquo;t being run from your shell</h3>
<p>All of these directions only work if you&rsquo;re running the program <strong>from your
shell</strong>. If you&rsquo;re running the program from an IDE, from a GUI, in a cron job,
or some other way, you&rsquo;ll need to add the directory to your PATH in a different
way, and the exact details might depend on the situation.</p>
<p><strong>in a cron job</strong></p>
<p>Some options:</p>
<ul>
<li>use the full path to the program you&rsquo;re running, like <code>/home/bork/bin/my-program</code></li>
<li>put the full PATH you want as the first line of your crontab (something like
PATH=/bin:/usr/bin:/usr/local/bin:&hellip;.). You can get the full PATH you&rsquo;re
using in your shell by running <code>echo &quot;PATH=$PATH&quot;</code>.</li>
</ul>
<p>I&rsquo;m honestly not sure how to handle it in an IDE/GUI because I haven&rsquo;t run into
that in a long time, will add directions here if someone points me in the right
direction.</p>
<h3 id="problem-3-duplicate-path-entries-making-it-harder-to-debug">problem 3: duplicate <code>PATH</code> entries making it harder to debug</h3>
<p>If you edit your path and start a new shell by running <code>bash</code> (or <code>zsh</code>, or
<code>fish</code>), you&rsquo;ll often end up with duplicate <code>PATH</code> entries, because the shell
keeps adding new things to your <code>PATH</code> every time you start your shell.</p>
<p>Personally I don&rsquo;t think I&rsquo;ve run into a situation where this kind of
duplication breaks anything, but the duplicates can make it harder to debug
what&rsquo;s going on with your <code>PATH</code> if you&rsquo;re trying to understand its contents.</p>
<p>Some ways you could deal with this:</p>
<ol>
<li>If you&rsquo;re debugging your <code>PATH</code>, open a new terminal to do it in so you get
a &ldquo;fresh&rdquo; state. This should avoid the duplication.</li>
<li>Deduplicate your <code>PATH</code> at the end of your shell&rsquo;s config  (for example in
zsh apparently you can do this with <code>typeset -U path</code>)</li>
<li>Check that the directory isn&rsquo;t already in your <code>PATH</code> when adding it (for
example in fish I believe you can do this with <code>fish_add_path --path /some/directory</code>)</li>
</ol>
<p>How to deduplicate your <code>PATH</code> is shell-specific and there isn&rsquo;t always a
built in way to do it so you&rsquo;ll need to look up how to accomplish it in your
shell.</p>
<h3 id="problem-4-losing-your-history-after-updating-your-path">problem 4: losing your history after updating your <code>PATH</code></h3>
<p>Here&rsquo;s a situation that&rsquo;s easy to get into in bash or zsh:</p>
<ol>
<li>Run a command (it fails)</li>
<li>Update your <code>PATH</code></li>
<li>Run <code>bash</code> to reload your config</li>
<li>Press the up arrow a couple of times to rerun the failed command (or open a new terminal)</li>
<li>The failed command isn&rsquo;t in your history! Why not?</li>
</ol>
<p>This happens because in bash, by default, history is not saved until you exit
the shell.</p>
<p>Some options for fixing this:</p>
<ul>
<li>Instead of running <code>bash</code> to reload your config, run <code>source ~/.bashrc</code> (or
<code>source ~/.zshrc</code> in zsh). This will reload the config inside your current
session.</li>
<li>Configure your shell to continuously save your history instead of only saving
the history when the shell exits. (How to do this depends on whether you&rsquo;re
using bash or zsh, the history options in zsh are a bit complicated and I&rsquo;m
not exactly sure what the best way is)</li>
</ul>
<h3 id="a-note-on-source">a note on <code>source</code></h3>
<p>When you install <code>cargo</code> (Rust&rsquo;s installer) for the first time, it gives you
these instructions for how to set up your PATH, which don&rsquo;t mention a specific
directory at all.</p>
<pre><code>This is usually done by running one of the following (note the leading DOT):

. &quot;$HOME/.cargo/env&quot;        	# For sh/bash/zsh/ash/dash/pdksh
source &quot;$HOME/.cargo/env.fish&quot;  # For fish
</code></pre>
<p>The idea is that you add that line to your shell&rsquo;s config, and their script
automatically sets up your <code>PATH</code> (and potentially other things) for you.</p>
<p>This is pretty common (for example <a href="https://github.com/Homebrew/install/blob/deacfa6a6e62e5f4002baf9e1fac7a96e9aa5d41/install.sh#L1072-L1087">Homebrew</a> suggests you eval <code>brew shellenv</code>), and there are
two ways to approach this:</p>
<ol>
<li>Just do what the tool suggests (like adding <code>. &quot;$HOME/.cargo/env&quot;</code> to your shell&rsquo;s config)</li>
<li>Figure out which directories the script they&rsquo;re telling you to run would add
to your PATH, and then add those manually. Here&rsquo;s how I&rsquo;d do that:
<ul>
<li>Run <code>. &quot;$HOME/.cargo/env&quot;</code> in my shell (or the fish version if using fish)</li>
<li>Run <code>echo &quot;$PATH&quot; | tr ':' '\n' | grep cargo</code> to figure out which directories it added</li>
<li>See that it says <code>/Users/bork/.cargo/bin</code> and shorten that to <code>~/.cargo/bin</code></li>
<li>Add the directory <code>~/.cargo/bin</code> to PATH (with the directions in this post)</li>
</ul>
</li>
</ol>
<p>I don&rsquo;t think there&rsquo;s anything wrong with doing what the tool suggests (it
might be the &ldquo;best way&rdquo;!), but personally I usually use the second approach
because I prefer knowing exactly what configuration I&rsquo;m changing.</p>
<h3 id="a-note-on-fish-add-path">a note on <code>fish_add_path</code></h3>
<p>fish has a handy function called <code>fish_add_path</code> that you can run to add a directory to your <code>PATH</code> like this:</p>
<pre><code>fish_add_path /some/directory
</code></pre>
<p>This is cool (it&rsquo;s such a simple command!) but I&rsquo;ve stopped using it for a couple of reasons:</p>
<ol>
<li>Sometimes <code>fish_add_path</code> will update the <code>PATH</code> for every session in the
future (with a &ldquo;universal variable&rdquo;) and sometimes it will update the <code>PATH</code>
just for the current session and it&rsquo;s hard for me to tell which one it will
do. In theory the docs explain this but I could not understand them.</li>
<li>If you ever need to <em>remove</em> the directory from your <code>PATH</code> a few weeks or
months later because maybe you made a mistake, it&rsquo;s kind of hard to do
(there are <a href="https://github.com/fish-shell/fish-shell/issues/8604">instructions in this comments of this github issue though</a>).</li>
</ol>
<h3 id="that-s-all">that&rsquo;s all</h3>
<p>Hopefully this will help some people. Let me know (on Mastodon or Bluesky) if
you there are other major gotchas that have tripped you up when adding a
directory to your PATH, or if you have questions about this post!</p>

---

### [Some terminal frustrations](https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>A few weeks ago I ran a terminal survey (you can <a href="https://jvns.ca/terminal-survey/results-bsky.html">read the results here</a>) and at the end I asked:</p>
<blockquote>
<p>What‚Äôs the most frustrating thing about using the terminal for you?</p>
</blockquote>
<p>1600 people answered, and I decided to spend a few days categorizing all the
responses. Along the way I learned that classifying qualitative data is not
easy but I gave it my best shot. I ended up building a custom
<a href="https://github.com/jvns/classificator">tool</a> to make it faster to categorize
everything.</p>
<p>As with all of my surveys the methodology isn&rsquo;t particularly scientific. I just
posted the survey to Mastodon and Twitter, ran it for a couple of days, and got
answers from whoever happened to see it and felt like responding.</p>
<p>Here are the top categories of frustrations!</p>
<p>I think it&rsquo;s worth keeping in mind while reading these comments that</p>
<ul>
<li>40% of people answering this survey have been using the terminal for <strong>21+ years</strong></li>
<li>95% of people answering the survey have been using the terminal for at least 4 years</li>
</ul>
<p>These comments aren&rsquo;t coming from total beginners.</p>
<p>Here are the categories of frustrations! The number in brackets is the number
of people with that frustration. I&rsquo;m mostly writing this up for myself because
I&rsquo;m trying to write a zine about the terminal and I wanted to get a sense for
what people are having trouble with.</p>
<h3 id="remembering-syntax-115">remembering syntax (115)</h3>
<p>People talked about struggles remembering:</p>
<ul>
<li>the syntax for CLI tools like awk, jq, sed, etc</li>
<li>the syntax for redirects</li>
<li>keyboard shortcuts for tmux, text editing, etc</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>There are just so many little &ldquo;trivia&rdquo; details to remember for full
functionality. Even after all these years I&rsquo;ll sometimes forget where it&rsquo;s 2
or 1 for stderr, or forget which is which for <code>&gt;</code> and <code>&gt;&gt;</code>.</p>
</blockquote>
<h3 id="switching-terminals-is-hard-91">switching terminals is hard (91)</h3>
<p>People talked about struggling with switching systems (for example home/work
computer or when SSHing) and running into:</p>
<ul>
<li>OS differences in keyboard shortcuts (like Linux vs Mac)</li>
<li>systems which don&rsquo;t have their preferred text editor (&ldquo;no vim&rdquo; or &ldquo;only vim&rdquo;)</li>
<li>different versions of the same command (like Mac OS grep vs GNU grep)</li>
<li>no tab completion</li>
<li>a shell they aren&rsquo;t used to (&ldquo;the subtle differences between zsh and bash&rdquo;)</li>
</ul>
<p>as well as differences inside the same system like pagers being not consistent
with each other (git diff pagers, other pagers).</p>
<p>One example comment:</p>
<blockquote>
<p>I got used to fish and vi mode which are not available when I ssh into
servers, containers.</p>
</blockquote>
<h3 id="color-85">color (85)</h3>
<p>Lots of problems with color, like:</p>
<ul>
<li>programs setting colors that are unreadable with a light background color</li>
<li>finding a colorscheme they like (and getting it to work consistently across different apps)</li>
<li>color not working inside several layers of SSH/tmux/etc</li>
<li>not liking the defaults</li>
<li>not wanting color at all and struggling to turn it off</li>
</ul>
<p>This comment felt relatable to me:</p>
<blockquote>
<p>Getting my terminal theme configured in a reasonable way between the terminal
emulator and fish (I did this years ago and remember it being tedious and
fiddly and now feel like I&rsquo;m locked into my current theme because it works
and I dread touching any of that configuration ever again).</p>
</blockquote>
<h3 id="keyboard-shortcuts-84">keyboard shortcuts (84)</h3>
<p>Half of the comments on keyboard shortcuts were about how on Linux/Windows, the
keyboard shortcut to copy/paste in the terminal is different from in the rest
of the OS.</p>
<p>Some other issues with keyboard shortcuts other than copy/paste:</p>
<ul>
<li>using <code>Ctrl-W</code> in a browser-based terminal and closing the window</li>
<li>the terminal only supports a limited set of keyboard shortcuts (no
<code>Ctrl-Shift-</code>, no <code>Super</code>, no <code>Hyper</code>, lots of <code>ctrl-</code> shortcuts aren&rsquo;t
possible like <code>Ctrl-,</code>)</li>
<li>the OS stopping you from using a terminal keyboard shortcut (like by default
Mac OS uses <code>Ctrl+left arrow</code> for something else)</li>
<li>issues using emacs in the terminal</li>
<li>backspace not working (2)</li>
</ul>
<h3 id="other-copy-and-paste-issues-75">other copy and paste issues (75)</h3>
<p>Aside from &ldquo;the keyboard shortcut for copy and paste is different&rdquo;, there were
a lot of OTHER issues with copy and paste, like:</p>
<ul>
<li>copying over SSH</li>
<li>how tmux and the terminal emulator both do copy/paste in different ways</li>
<li>dealing with many different clipboards (system clipboard, vim clipboard, the
&ldquo;middle click&rdquo; clipboard on Linux, tmux&rsquo;s clipboard, etc) and potentially
synchronizing them</li>
<li>random spaces added when copying from the terminal</li>
<li>pasting multiline commands which automatically get run in a terrifying way</li>
<li>wanting a way to copy text without using the mouse</li>
</ul>
<h3 id="discoverability-55">discoverability (55)</h3>
<p>There were lots of comments about this, which all came down to the same basic
complaint &ndash; it&rsquo;s hard to discover useful tools or features! This comment kind of
summed it all up:</p>
<blockquote>
<p>How difficult it is to learn independently. Most of what I know is an
assorted collection of stuff I&rsquo;ve been told by random people over the years.</p>
</blockquote>
<h3 id="steep-learning-curve-44">steep learning curve (44)</h3>
<p>A lot of comments about it generally having a steep learning curve. A couple of
example comments:</p>
<blockquote>
<p>After 15 years of using it, I‚Äôm not much faster than using it than I was 5 or
maybe even 10 years ago.</p>
</blockquote>
<p>and</p>
<blockquote>
<p>That I know I could make my life easier by learning more about the shortcuts
and commands and configuring the terminal but I don&rsquo;t spend the time because it
feels overwhelming.</p>
</blockquote>
<h3 id="history-42">history  (42)</h3>
<p>Some issues with shell history:</p>
<ul>
<li>history not being shared between terminal tabs (16)</li>
<li>limits that are too short (4)</li>
<li>history not being restored when terminal tabs are restored</li>
<li>losing history because the terminal crashed</li>
<li>not knowing how to search history</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>It wasted a lot of time until I figured it out and still annoys me that
&ldquo;history&rdquo; on zsh has such a small buffer;  I have to type &ldquo;history 0&rdquo; to get
any useful length of history.</p>
</blockquote>
<h3 id="bad-documentation-37">bad documentation (37)</h3>
<p>People talked about:</p>
<ul>
<li>documentation being generally opaque</li>
<li>lack of examples in man pages</li>
<li>programs which don&rsquo;t have man pages</li>
</ul>
<p>Here&rsquo;s a representative comment:</p>
<blockquote>
<p>Finding good examples and docs. Man pages often not enough, have to wade
through stack overflow</p>
</blockquote>
<h3 id="scrollback-36">scrollback (36)</h3>
<p>A few issues with scrollback:</p>
<ul>
<li>programs printing out too much data making you lose scrollback history</li>
<li>resizing the terminal messes up the scrollback</li>
<li>lack of timestamps</li>
<li>GUI programs that you start in the background printing stuff out that gets in
the way of other programs&rsquo; outputs</li>
</ul>
<p>One example comment:</p>
<blockquote>
<p>When resizing the terminal (in particular: making it narrower) leads to
broken rewrapping of the scrollback content because the commands formatted
their output based on the terminal window width.</p>
</blockquote>
<h3 id="it-feels-outdated-33">&ldquo;it feels outdated&rdquo; (33)</h3>
<p>Lots of comments about how the terminal feels hampered by legacy decisions and
how users often end up needing to learn implementation details that feel very
esoteric. One example comment:</p>
<blockquote>
<p>Most of the legacy cruft, it would be great to have a green field
implementation of the CLI interface.</p>
</blockquote>
<h3 id="shell-scripting-32">shell scripting (32)</h3>
<p>Lots of complaints about POSIX shell scripting. There&rsquo;s a general feeling that
shell scripting is difficult but also that switching to a different less
standard scripting language (fish, nushell, etc) brings its own problems.</p>
<blockquote>
<p>Shell scripting. My tolerance to ditch a shell script and go to a scripting
language is pretty low. It‚Äôs just too messy and powerful. Screwing up can be
costly so I don‚Äôt even bother.</p>
</blockquote>
<h3 id="more-issues">more issues</h3>
<p>Some more issues that were mentioned at least 10 times:</p>
<ul>
<li>(31) inconsistent command line arguments: is it -h or help or &ndash;help?</li>
<li>(24) keeping dotfiles in sync across different systems</li>
<li>(23) performance (e.g. &ldquo;my shell takes too long to start&rdquo;)</li>
<li>(20) window management (potentially with some combination of tmux tabs, terminal tabs, and multiple terminal windows. Where did that shell session go?)</li>
<li>(17) generally feeling scared/uneasy (&ldquo;The debilitating fear that I‚Äôm going
to do some mysterious Bad Thing with a command and I will have absolutely no
idea how to fix or undo it or even really figure out what happened&rdquo;)</li>
<li>(16) terminfo issues (&ldquo;Having to learn about terminfo if/when I try a new terminal emulator and ssh elsewhere.&rdquo;)</li>
<li>(16) lack of image support (sixel etc)</li>
<li>(15) SSH issues (like having to start over when you lose the SSH connection)</li>
<li>(15) various tmux/screen issues (for example lack of integration between tmux and the terminal emulator)</li>
<li>(15) typos &amp; slow typing</li>
<li>(13) the terminal getting messed up for various reasons (pressing <code>Ctrl-S</code>, <code>cat</code>ing a binary, etc)</li>
<li>(12) quoting/escaping in the shell</li>
<li>(11) various Windows/PowerShell issues</li>
</ul>
<h3 id="n-a-122">n/a (122)</h3>
<p>There were also 122 answers to the effect of &ldquo;nothing really&rdquo; or &ldquo;only that I
can&rsquo;t do EVERYTHING in the terminal&rdquo;</p>
<p>One example comment:</p>
<blockquote>
<p>Think I&rsquo;ve found work arounds for most/all frustrations</p>
</blockquote>
<h3 id="that-s-all">that&rsquo;s all!</h3>
<p>I&rsquo;m not going to make a lot of commentary on these results, but here are a
couple of categories that feel related to me:</p>
<ul>
<li>remembering syntax &amp; history (often the thing you need to remember is something you&rsquo;ve run before!)</li>
<li>discoverability &amp; the learning curve (the lack of discoverability is definitely a big part of what makes it hard to learn)</li>
<li>&ldquo;switching systems is hard&rdquo; &amp; &ldquo;it feels outdated&rdquo; (tools that haven&rsquo;t really
changed in 30 or 40 years have many problems but they do tend to be always
<em>there</em> no matter what system you&rsquo;re on, which is very useful and makes them
hard to stop using)</li>
</ul>
<p>Trying to categorize all these results in a reasonable way really gave me an
appreciation for social science researchers&rsquo; skills.</p>

---

### [What's involved in getting a "modern" terminal setup?](https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Hello! Recently I ran a terminal survey and I asked people what frustrated
them. One person commented:</p>
<blockquote>
<p>There are so many pieces to having a modern terminal experience. I wish it
all came out of the box.</p>
</blockquote>
<p>My immediate reaction was &ldquo;oh, getting a modern terminal experience isn&rsquo;t that
hard, you just need to&hellip;.&rdquo;, but the more I thought about it, the longer the
&ldquo;you just need to&hellip;&rdquo; list got, and I kept thinking about more and more
caveats.</p>
<p>So I thought I would write down some notes about what it means to me personally
to have a &ldquo;modern&rdquo; terminal experience and what I think can make it hard for
people to get there.</p>
<h3 id="what-is-a-modern-terminal-experience">what is a &ldquo;modern terminal experience&rdquo;?</h3>
<p>Here are a few things that are important to me, with which part of the system
is responsible for them:</p>
<ul>
<li><strong>multiline support for copy and paste</strong>: if you paste 3 commands in your shell, it should not immediately run them all! That&rsquo;s scary! (<strong>shell</strong>, <strong>terminal emulator</strong>)</li>
<li><strong>infinite shell history</strong>: if I run a command in my shell, it should be saved forever, not deleted after 500 history entries or whatever. Also I want commands to be saved to the history immediately when I run them, not only when I exit the shell session (<strong>shell</strong>)</li>
<li><strong>a useful prompt</strong>: I can&rsquo;t live without having my <strong>current directory</strong> and <strong>current git branch</strong> in my prompt (<strong>shell</strong>)</li>
<li><strong>24-bit colour</strong>: this is important to me because I find it MUCH easier to theme neovim with 24-bit colour support than in a terminal with only 256 colours (<strong>terminal emulator</strong>)</li>
<li><strong>clipboard integration</strong> between vim and my operating system so that when I copy in Firefox, I can just press <code>p</code> in vim to paste (<strong>text editor</strong>, maybe the OS/terminal emulator too)</li>
<li><strong>good autocomplete</strong>: for example commands like git should have command-specific autocomplete (<strong>shell</strong>)</li>
<li><strong>having colours in <code>ls</code></strong> (<strong>shell config</strong>)</li>
<li><strong>a terminal theme I like</strong>: I spend a lot of time in my terminal, I want it to look nice and I want its theme to match my terminal editor&rsquo;s theme. (<strong>terminal emulator</strong>, <strong>text editor</strong>)</li>
<li><strong>automatic terminal fixing</strong>: If a programs prints out some weird escape
codes that mess up my terminal, I want that to automatically get reset so
that my terminal doesn&rsquo;t get messed up (<strong>shell</strong>)</li>
<li><strong>keybindings</strong>: I want <code>Ctrl+left arrow</code> to work (<strong>shell</strong> or <strong>application</strong>)</li>
<li><strong>being able to use the scroll wheel in programs like <code>less</code></strong>: (<strong>terminal emulator</strong> and <strong>applications</strong>)</li>
</ul>
<p>There are a million other terminal conveniences out there and different people
value different things, but those are the ones that I would be really unhappy
without.</p>
<h3 id="how-i-achieve-a-modern-experience">how I achieve a &ldquo;modern experience&rdquo;</h3>
<p>My basic approach is:</p>
<ol>
<li>use the <code>fish</code> shell. Mostly don&rsquo;t configure it, except to:
<ul>
<li>set the <code>EDITOR</code> environment variable to my favourite terminal editor</li>
<li>alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>use any terminal emulator with 24-bit colour support. In the past I&rsquo;ve used
GNOME Terminal, Terminator, and iTerm, but I&rsquo;m not picky about this. I don&rsquo;t really
configure it other than to choose a font.</li>
<li>use <code>neovim</code>, with a configuration that I&rsquo;ve been very slowly building over the last 9 years or so (the last time I deleted my vim config and started from scratch was 9 years ago)</li>
<li>use the <a href="https://github.com/chriskempson/base16">base16 framework</a> to theme everything</li>
</ol>
<p>A few things that affect my approach:</p>
<ul>
<li>I don&rsquo;t spend a lot of time SSHed into other machines</li>
<li>I&rsquo;d rather use the mouse a little than come up with keyboard-based ways to do everything</li>
<li>I work on a lot of small projects, not one big project</li>
</ul>
<h3 id="some-out-of-the-box-options-for-a-modern-experience">some &ldquo;out of the box&rdquo; options for a &ldquo;modern&rdquo; experience</h3>
<p>What if you want a nice experience, but don&rsquo;t want to spend a lot of time on
configuration? Figuring out how to configure vim in a way that I was satisfied
with really did take me like ten years, which is a long time!</p>
<p>My best ideas for how to get a reasonable terminal experience with minimal
config are:</p>
<ul>
<li>shell: either <code>fish</code> or <code>zsh</code> with <a href="https://ohmyz.sh/">oh-my-zsh</a></li>
<li>terminal emulator: almost anything with 24-bit colour support, for example all of these are popular:
<ul>
<li>linux: GNOME Terminal, Konsole, Terminator, xfce4-terminal</li>
<li>mac: iTerm (Terminal.app doesn&rsquo;t have 256-colour support)</li>
<li>cross-platform: kitty, alacritty, wezterm, or ghostty</li>
</ul>
</li>
<li>shell config:
<ul>
<li>set the <code>EDITOR</code> environment variable to your favourite terminal text
editor</li>
<li>maybe alias <code>ls</code> to <code>ls --color=auto</code></li>
</ul>
</li>
<li>text editor: this is a tough one, maybe <a href="https://micro-editor.github.io/">micro</a> or <a href="https://helix-editor.com/">helix</a>? I haven&rsquo;t used
either of them seriously but they both seem like very cool projects and I
think it&rsquo;s amazing that you can just use all the usual GUI editor commands
(<code>Ctrl-C</code> to copy, <code>Ctrl-V</code> to paste, <code>Ctrl-A</code> to select all) in micro and
they do what you&rsquo;d expect. I would probably try switching to helix except
that retraining my vim muscle memory seems way too hard. Also helix doesn&rsquo;t
have a GUI or plugin system yet.</li>
</ul>
<p>Personally I <strong>wouldn&rsquo;t</strong> use xterm, rxvt, or Terminal.app as a terminal emulator,
because I&rsquo;ve found in the past that they&rsquo;re missing core features (like 24-bit
colour in Terminal.app&rsquo;s case) that make the terminal harder to use for me.</p>
<p>I don&rsquo;t want to pretend that getting a &ldquo;modern&rdquo; terminal experience is easier
than it is though &ndash; I think there are two issues that make it hard. Let&rsquo;s talk
about them!</p>
<h3 id="issue-1-with-getting-to-a-modern-experience-the-shell">issue 1 with getting to a &ldquo;modern&rdquo; experience: the shell</h3>
<p>bash and zsh are by far the two most popular shells, and neither of them
provide a default experience that I would be happy using out of the box, for
example:</p>
<ul>
<li>you need to customize your prompt</li>
<li>they don&rsquo;t come with git completions by default, you have to set them up</li>
<li>by default, bash only stores 500 (!) lines of history and (at least on Mac OS)
zsh is only configured to store 2000 lines, which is still not a lot</li>
<li>I find bash&rsquo;s tab completion very frustrating, if there&rsquo;s more than
one match then you can&rsquo;t tab through them</li>
</ul>
<p>And even though <a href="https://jvns.ca/blog/2024/09/12/reasons-i--still--love-fish/">I love fish</a>, the fact
that it isn&rsquo;t POSIX does make it hard for a lot of folks to make the switch.</p>
<p>Of course it&rsquo;s totally possible to learn how to customize your prompt in bash
or whatever, and it doesn&rsquo;t even need to be that complicated (in bash I&rsquo;d
probably start with something like <code>export PS1='[\u@\h \W$(__git_ps1 &quot; (%s)&quot;)]\$ '</code>, or maybe use <a href="https://starship.rs/">starship</a>).
But each of these &ldquo;not complicated&rdquo; things really does add up and it&rsquo;s
especially tough if you need to keep your config in sync across several
systems.</p>
<p>An extremely popular solution to getting a &ldquo;modern&rdquo; shell experience is
<a href="https://ohmyz.sh/">oh-my-zsh</a>. It seems like a great project and I know a lot
of people use it very happily, but I&rsquo;ve struggled with configuration systems
like that in the past &ndash; it looks like right now the base oh-my-zsh adds about
3000 lines of config, and often I find that having an extra configuration
system makes it harder to debug what&rsquo;s happening when things go wrong. I
personally have a tendency to use the system to add a lot of extra plugins,
make my system slow, get frustrated that it&rsquo;s slow, and then delete it
completely and write a new config from scratch.</p>
<h3 id="issue-2-with-getting-to-a-modern-experience-the-text-editor">issue 2 with getting to a &ldquo;modern&rdquo; experience: the text editor</h3>
<p>In the terminal survey I ran recently, the most popular terminal text editors
by far were <code>vim</code>, <code>emacs</code>, and <code>nano</code>.</p>
<p>I think the main options for terminal text editors are:</p>
<ul>
<li>use vim or emacs and configure it to your liking, you can probably have any
feature you want if you put in the work</li>
<li>use nano and accept that you&rsquo;re going to have a pretty limited experience
(for example I don&rsquo;t think you can select text with the mouse and then &ldquo;cut&rdquo;
it in nano)</li>
<li>use <code>micro</code> or <code>helix</code> which seem to offer a pretty good out-of-the-box
experience, potentially occasionally run into issues with using a less
mainstream text editor</li>
<li>just avoid using a terminal text editor as much as possible, maybe use VSCode, use
VSCode&rsquo;s terminal for all your terminal needs, and mostly never edit files in
the terminal. Or I know a lot of people use <code>code</code> as their <code>EDITOR</code> in the terminal.</li>
</ul>
<h3 id="issue-3-individual-applications">issue 3: individual applications</h3>
<p>The last issue is that sometimes individual programs that I use are kind of
annoying. For example on my Mac OS machine, <code>/usr/bin/sqlite3</code> doesn&rsquo;t support
the <code>Ctrl+Left Arrow</code> keyboard shortcut. Fixing this to get a reasonable
terminal experience in SQLite was a little complicated, I had to:</p>
<ul>
<li>realize why this is happening (Mac OS won&rsquo;t ship GNU tools, and &ldquo;Ctrl-Left arrow&rdquo; support comes from GNU readline)</li>
<li>find a workaround (install sqlite from homebrew, which does have readline support)</li>
<li>adjust my environment (put Homebrew&rsquo;s sqlite3 in my PATH)</li>
</ul>
<p>I find that debugging application-specific issues like this is really not easy
and often it doesn&rsquo;t feel &ldquo;worth it&rdquo; &ndash; often I&rsquo;ll end up just dealing with
various minor inconveniences because I don&rsquo;t want to spend hours investigating
them. The only reason I was even able to figure this one out at all is that
I&rsquo;ve been spending a huge amount of time thinking about the terminal recently.</p>
<p>A big part of having a &ldquo;modern&rdquo; experience using terminal programs is just
using newer terminal programs, for example I can&rsquo;t be bothered to learn a
keyboard shortcut to sort the columns in <code>top</code>, but in <code>htop</code>  I can just click
on a column heading with my mouse to sort it. So I use htop instead! But discovering new more &ldquo;modern&rdquo; command line tools isn&rsquo;t easy (though
I made <a href="https://jvns.ca/blog/2022/04/12/a-list-of-new-ish--command-line-tools/">a list here</a>),
finding ones that I actually like using in practice takes time, and if you&rsquo;re
SSHed into another machine, they won&rsquo;t always be there.</p>
<h3 id="everything-affects-everything-else">everything affects everything else</h3>
<p>Something I find tricky about configuring my terminal to make everything &ldquo;nice&rdquo;
is that changing one seemingly small thing about my workflow can really affect
everything else. For example right now I don&rsquo;t use tmux. But if I needed to use
tmux again (for example because I was doing a lot of work SSHed into another
machine), I&rsquo;d need to think about a few things, like:</p>
<ul>
<li>if I wanted tmux&rsquo;s copy to synchronize with my system clipboard over
SSH, I&rsquo;d need to make sure that my terminal emulator has <a href="https://old.reddit.com/r/vim/comments/k1ydpn/a_guide_on_how_to_copy_text_from_anywhere/">OSC 52 support</a></li>
<li>if I wanted to use iTerm&rsquo;s tmux integration (which makes tmux tabs into iTerm
tabs), I&rsquo;d need to change how I configure colours &ndash; right now I set them
with a <a href="https://github.com/chriskempson/base16-shell/blob/588691ba71b47e75793ed9edfcfaa058326a6f41/scripts/base16-solarized-light.sh">shell script</a> that I run when my shell starts, but that means the
colours get lost when restoring a tmux session.</li>
</ul>
<p>and probably more things I haven&rsquo;t thought of. &ldquo;Using tmux means that I have to
change how I manage my colours&rdquo; sounds unlikely, but that really did happen to
me and I decided &ldquo;well, I don&rsquo;t want to change how I manage colours right now,
so I guess I&rsquo;m not using that feature!&rdquo;.</p>
<p>It&rsquo;s also hard to remember which features I&rsquo;m relying on &ndash; for example maybe
my current terminal <em>does</em> have OSC 52 support and because copying from tmux over SSH
has always Just Worked I don&rsquo;t even realize that that&rsquo;s something I need, and
then it mysteriously stops working when I switch terminals.</p>
<h3 id="change-things-slowly">change things slowly</h3>
<p>Personally even though I think my setup is not <em>that</em> complicated, it&rsquo;s taken
me 20 years to get to this point! Because terminal config changes are so likely
to have unexpected and hard-to-understand consequences, I&rsquo;ve found that if I
change a lot of terminal configuration all at once it makes it much harder to
understand what went wrong if there&rsquo;s a problem, which can be really
disorienting.</p>
<p>So I usually prefer to make pretty small changes, and accept that changes can
might take me a REALLY long time to get used to. For example I switched from
using <code>ls</code> to <a href="https://github.com/eza-community/eza">eza</a> a year or two ago and
while I like it (because <code>eza -l</code> prints human-readable file sizes by default)
I&rsquo;m still not quite sure about it. But also sometimes it&rsquo;s worth it to make a
big change, like I made the switch to fish (from bash) 10 years ago and I&rsquo;m
very happy I did.</p>
<h3 id="getting-a-modern-terminal-is-not-that-easy">getting a &ldquo;modern&rdquo; terminal is not that easy</h3>
<p>Trying to explain how &ldquo;easy&rdquo; it is to configure your terminal really just made
me think that it&rsquo;s kind of hard and that I still sometimes get confused.</p>
<p>I&rsquo;ve found that there&rsquo;s never one perfect way to configure things in the
terminal that will be compatible with every single other thing. I just need to
try stuff, figure out some kind of locally stable state that works for me, and
accept that if I start using a new tool it might disrupt the system and I might
need to rethink things.</p>

---

### ["Rules" that terminal programs follow](https://jvns.ca/blog/2024/11/26/terminal-rules/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Recently I&rsquo;ve been thinking about how everything that happens in the terminal
is some combination of:</p>
<ol>
<li>Your <strong>operating system</strong>&rsquo;s job</li>
<li>Your <strong>shell</strong>&rsquo;s job</li>
<li>Your <strong>terminal emulator</strong>&rsquo;s job</li>
<li>The job of <strong>whatever program you happen to be running</strong> (like <code>top</code> or <code>vim</code> or <code>cat</code>)</li>
</ol>
<p>The first three (your operating system, shell, and terminal emulator) are all kind of
known quantities &ndash; if you&rsquo;re using bash in GNOME Terminal on Linux, you can
more or less reason about how how all of those things interact, and some of
their behaviour is standardized by POSIX.</p>
<p>But the fourth one (&ldquo;whatever program you happen to be running&rdquo;) feels like it
could do ANYTHING. How are you supposed to know how a program is going to
behave?</p>
<p>This post is kind of long so here&rsquo;s a quick table of contents:</p>
<ul>
<li><a href="https://jvns.ca/atom.xml#programs-behave-surprisingly-consistently">programs behave surprisingly consistently</a></li>
<li><a href="https://jvns.ca/atom.xml#these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</a></li>
<li><a href="https://jvns.ca/atom.xml#it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></a></li>
<li><a href="https://jvns.ca/atom.xml#rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: <code>Ctrl-W</code> should delete the last word</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</a></li>
<li><a href="https://jvns.ca/atom.xml#rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</a></li>
<li><a href="https://jvns.ca/atom.xml#these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</a></li>
</ul>
<h3 id="programs-behave-surprisingly-consistently">programs behave surprisingly consistently</h3>
<p>As far as I know, there are no real standards for how programs in the terminal
should behave &ndash; the closest things I know of are:</p>
<ul>
<li>POSIX, which mostly dictates how your terminal emulator / OS / shell should
work together. I think it does specify a few things about how core utilities like
<code>cp</code> should work but AFAIK it doesn&rsquo;t have anything to say about how for
example <code>htop</code> should behave.</li>
<li>these <a href="https://clig.dev/">command line interface guidelines</a></li>
</ul>
<p>But even though there are no standards, in my experience programs in the
terminal behave in a pretty consistent way. So I wanted to write down a list of
&ldquo;rules&rdquo; that in my experience programs mostly follow.</p>
<h3 id="these-are-meant-to-be-descriptive-not-prescriptive">these are meant to be descriptive, not prescriptive</h3>
<p>My goal here isn&rsquo;t to convince authors of terminal programs that they <em>should</em>
follow any of these rules. There are lots of exceptions to these and often
there&rsquo;s a good reason for those exceptions.</p>
<p>But it&rsquo;s very useful for me to know what behaviour to expect from a random new
terminal program that I&rsquo;m using. Instead of &ldquo;uh, programs could do literally
anything&rdquo;, it&rsquo;s &ldquo;ok, here are the basic rules I expect, and then I can keep a
short mental list of exceptions&rdquo;.</p>
<p>So I&rsquo;m just writing down what I&rsquo;ve observed about how programs behave in my 20
years of using the terminal, why I think they behave that way, and some
examples of cases where that rule is &ldquo;broken&rdquo;.</p>
<h3 id="it-s-not-always-obvious-which-rules-are-the-program-s-responsibility-to-implement">it&rsquo;s not always obvious which &ldquo;rules&rdquo; are the program&rsquo;s responsibility to implement</h3>
<p>There are a bunch of common conventions that I think are pretty clearly the
program&rsquo;s responsibility to implement, like:</p>
<ul>
<li>config files should go in <code>~/.BLAHrc</code> or <code>~/.config/BLAH/FILE</code> or <code>/etc/BLAH/</code> or something</li>
<li><code>--help</code> should print help text</li>
<li>programs should print &ldquo;regular&rdquo; output to stdout and errors to stderr</li>
</ul>
<p>But in this post I&rsquo;m going to focus on things that it&rsquo;s not 100% obvious are
the program&rsquo;s responsibility. For example it feels to me like a &ldquo;law of nature&rdquo;
that pressing <code>Ctrl-D</code> should quit a REPL, but programs often
need to explicitly implement support for it &ndash; even though <code>cat</code> doesn&rsquo;t need
to implement <code>Ctrl-D</code> support, <code>ipython</code> <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">does</a>. (more about that in &ldquo;rule 3&rdquo; below)</p>
<p>Understanding which things are the program&rsquo;s responsibility makes it much less
surprising when different programs&rsquo; implementations are slightly different.</p>
<h3 id="rule-1-noninteractive-programs-should-quit-when-you-press-ctrl-c">rule 1: noninteractive programs should quit when you press <code>Ctrl-C</code></h3>
<p>The main reason for this rule is that noninteractive programs will quit by
default on <code>Ctrl-C</code> if they don&rsquo;t set up a <code>SIGINT</code> signal handler, so this is
kind of a &ldquo;you should act like the default&rdquo; rule.</p>
<p>Something that trips a lot of people up is that this doesn&rsquo;t apply to
<strong>interactive</strong> programs like <code>python3</code> or <code>bc</code> or <code>less</code>. This is because in
an interactive program, <code>Ctrl-C</code> has a different job &ndash; if the program is
running an operation (like for example a search in <code>less</code> or some Python code
in <code>python3</code>), then <code>Ctrl-C</code> will interrupt that operation but not stop the
program.</p>
<p>As an example of how this works in an interactive program: here&rsquo;s the code <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/vi.py#L2225">in prompt-toolkit</a> (the library that iPython uses for handling input)
that aborts a search when you press <code>Ctrl-C</code>.</p>
<h3 id="rule-2-tuis-should-quit-when-you-press-q">rule 2: TUIs should quit when you press <code>q</code></h3>
<p>TUI programs (like <code>less</code> or <code>htop</code>) will usually quit when you press <code>q</code>.</p>
<p>This rule doesn&rsquo;t apply to any program where pressing <code>q</code> to quit wouldn&rsquo;t make
sense, like <code>tmux</code> or text editors.</p>
<h3 id="rule-3-repls-should-quit-when-you-press-ctrl-d-on-an-empty-line">rule 3: REPLs should quit when you press <code>Ctrl-D</code> on an empty line</h3>
<p>REPLs (like <code>python3</code> or <code>ed</code>) will usually quit when you press <code>Ctrl-D</code> on an
empty line. This rule is similar to the <code>Ctrl-C</code> rule &ndash; the reason for this is
that by default if you&rsquo;re running a program (like <code>cat</code>) in &ldquo;cooked mode&rdquo;, then
the operating system will return an <code>EOF</code> when you press <code>Ctrl-D</code> on an empty
line.</p>
<p>Most of the REPLs I use (sqlite3, python3, fish, bash, etc) don&rsquo;t actually use
cooked mode, but they all implement this keyboard shortcut anyway to mimic the
default behaviour.</p>
<p>For example, here&rsquo;s <a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/shortcuts/prompt.py#L824-L837">the code in prompt-toolkit</a>
that quits when you press Ctrl-D, and here&rsquo;s <a href="https://github.com/bminor/bash/blob/6794b5478f660256a1023712b5fc169196ed0a22/lib/readline/readline.c#L658-L672">the same code in readline</a>.</p>
<p>I actually thought that this one was a &ldquo;Law of Terminal Physics&rdquo; until very
recently because I&rsquo;ve basically never seen it broken, but you can see that it&rsquo;s
just something that each individual input library has to implement in the links
above.</p>
<p>Someone pointed out that the Erlang REPL does not quit when you press <code>Ctrl-D</code>,
so I guess not every REPL follows this &ldquo;rule&rdquo;.</p>
<h3 id="rule-4-don-t-use-more-than-16-colours">rule 4: don&rsquo;t use more than 16 colours</h3>
<p>Terminal programs rarely use colours other than the base 16 ANSI colours. This
is because if you specify colours with a hex code, it&rsquo;s very likely to clash
with some users&rsquo; background colour. For example if I print out some text as
<code>#EEEEEE</code>, it would be almost invisible on a white background, though it would
look fine on a dark background.</p>
<p>But if you stick to the default 16 base colours, you have a much better chance
that the user has configured those colours in their terminal emulator so that
they work reasonably well with their background color. Another reason to stick
to the default base 16 colours is that it makes less assumptions about what
colours the terminal emulator supports.</p>
<p>The only programs I usually see breaking this &ldquo;rule&rdquo; are text editors, for
example Helix by default will use a purple background which is not a default
ANSI colour. It seems fine for Helix to break this rule since Helix isn&rsquo;t a
&ldquo;core&rdquo; program and I assume any Helix user who doesn&rsquo;t like that colorscheme
will just change the theme.</p>
<h3 id="rule-5-vaguely-support-readline-keybindings">rule 5: vaguely support readline keybindings</h3>
<p>Almost every program I use supports <code>readline</code> keybindings if it would make
sense to do so. For example, here are a bunch of different programs and a link
to where they define <code>Ctrl-E</code> to go to the end of the line:</p>
<ul>
<li>ipython (<a href="https://github.com/prompt-toolkit/python-prompt-toolkit/blob/a2a12300c635ab3c051566e363ed27d853af4b21/src/prompt_toolkit/key_binding/bindings/emacs.py#L72">Ctrl-E defined here</a>)</li>
<li>atuin (<a href="https://github.com/atuinsh/atuin/blob/a67cfc82fe0dc907a01f07a0fd625701e062a33b/crates/atuin/src/command/client/search/interactive.rs#L407">Ctrl-E defined here</a>)</li>
<li>fzf (<a href="https://github.com/junegunn/fzf/blob/bb55045596d6d08445f3c6d320c3ec2b457462d1/src/terminal.go#L611">Ctrl-E defined here</a>)</li>
<li>zsh (<a href="https://github.com/zsh-users/zsh/blob/86d5f24a3d28541f242eb3807379301ea976de87/Src/Zle/zle_bindings.c#L94">Ctrl-E defined here</a>)</li>
<li>fish (<a href="https://github.com/fish-shell/fish-shell/blob/99fa8aaaa7956178973150a03ce4954ab17a197b/share/functions/fish_default_key_bindings.fish#L43">Ctrl-E defined here</a>)</li>
<li>tmux&rsquo;s command prompt (<a href="https://github.com/tmux/tmux/blob/ae8f2208c98e3c2d6e3fe4cad2281dce8fd11f31/key-bindings.c#L490">Ctrl-E defined here</a>)</li>
</ul>
<p>None of those programs actually uses <code>readline</code> directly, they just sort of
mimic emacs/readline keybindings. They don&rsquo;t always mimic them <em>exactly</em>: for
example atuin seems to use <code>Ctrl-A</code> as a prefix, so <code>Ctrl-A</code> doesn&rsquo;t go to the
beginning of the line.</p>
<p>Also all of these programs seem to implement their own internal cut and paste
buffers so you can delete a line with <code>Ctrl-U</code> and then paste it with <code>Ctrl-Y</code>.</p>
<p>The exceptions to this are:</p>
<ul>
<li>some programs (like <code>git</code>, <code>cat</code>, and <code>nc</code>) don&rsquo;t have any line editing support at all (except for backspace, <code>Ctrl-W</code>, and <code>Ctrl-U</code>)</li>
<li>as usual text editors are an exception, every text editor has its own
approach to editing text</li>
</ul>
<p>I wrote more about this &ldquo;what keybindings does a program support?&rdquo; question in
<a href="https://jvns.ca/blog/2024/07/08/readline/">entering text in the terminal is complicated</a>.</p>
<h3 id="rule-5-1-ctrl-w-should-delete-the-last-word">rule 5.1: Ctrl-W should delete the last word</h3>
<p>I&rsquo;ve never seen a program (other than a text editor) where <code>Ctrl-W</code> <em>doesn&rsquo;t</em>
delete the last word. This is similar to the <code>Ctrl-C</code> rule &ndash; by default if a
program is in &ldquo;cooked mode&rdquo;, the OS will delete the last word if you press
<code>Ctrl-W</code>, and delete the whole line if you press <code>Ctrl-U</code>. So usually programs
will imitate that behaviour.</p>
<p>I can&rsquo;t think of any exceptions to this other than text editors but if there
are I&rsquo;d love to hear about them!</p>
<h3 id="rule-6-disable-colours-when-writing-to-a-pipe">rule 6: disable colours when writing to a pipe</h3>
<p>Most programs will disable colours when writing to a pipe. For example:</p>
<ul>
<li><code>rg blah</code> will highlight all occurrences of <code>blah</code> in the output, but if the
output is to a pipe or a file, it&rsquo;ll turn off the highlighting.</li>
<li><code>ls --color=auto</code> will use colour when writing to a terminal, but not when
writing to a pipe</li>
</ul>
<p>Both of those programs will also format their output differently when writing
to the terminal: <code>ls</code> will organize files into columns, and ripgrep will group
matches with headings.</p>
<p>If you want to force the program to use colour (for example because you want to
look at the colour), you can use <code>unbuffer</code> to force the program&rsquo;s output to be
a tty like this:</p>
<pre><code>unbuffer rg blah |  less -R
</code></pre>
<p>I&rsquo;m sure that there are some programs that &ldquo;break&rdquo; this rule but I can&rsquo;t think
of any examples right now. Some programs have an <code>--color</code> flag that you can
use to force colour to be on, in the example above you could also do <code>rg --color=always | less -R</code>.</p>
<h3 id="rule-7-means-stdin-stdout">rule 7: <code>-</code> means stdin/stdout</h3>
<p>Usually if you pass <code>-</code> to a program instead of a filename, it&rsquo;ll read from
stdin or write to stdout (whichever is appropriate). For example, if you want
to format the Python code that&rsquo;s on your clipboard with <code>black</code> and then copy
it, you could run:</p>
<pre><code>pbpaste | black - | pbcopy
</code></pre>
<p>(<code>pbpaste</code> is a Mac program, you can do something similar on Linux with <code>xclip</code>)</p>
<p>My impression is that most programs implement this if it would make sense and I
can&rsquo;t think of any exceptions right now, but I&rsquo;m sure there are many
exceptions.</p>
<h3 id="these-rules-take-a-long-time-to-learn">these &ldquo;rules&rdquo; take a long time to learn</h3>
<p>These rules took me a long time for me to learn because I had to:</p>
<ol>
<li>learn that the rule applied anywhere at all (&quot;<code>Ctrl-C</code> will exit programs&quot;)</li>
<li>notice some exceptions (&ldquo;okay, <code>Ctrl-C</code> will exit <code>find</code> but not <code>less</code>&rdquo;)</li>
<li>subconsciously figure out what the pattern is (&quot;<code>Ctrl-C</code> will generally quit
noninteractive programs, but in interactive programs it might interrupt the
current operation instead of quitting the program&quot;)</li>
<li>eventually maybe formulate it into an explicit rule that I know</li>
</ol>
<p>A lot of my understanding of the terminal is honestly still in the
&ldquo;subconscious pattern recognition&rdquo; stage. The only reason I&rsquo;ve been taking the
time to make things explicit at all is because I&rsquo;ve been trying to explain how
it works to others. Hopefully writing down these &ldquo;rules&rdquo; explicitly will make
learning some of this stuff a little bit faster for others.</p>

---

### [Why pipes sometimes get "stuck": buffering](https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: <p>Here&rsquo;s a niche terminal problem that has bothered me for years but that I never
really understood until a few weeks ago. Let&rsquo;s say you&rsquo;re running this command
to watch for some specific output in a log file:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>If log lines are being added to the file relatively slowly, the result I&rsquo;d see
is&hellip; nothing! It doesn&rsquo;t matter if there were matches in the log file or not,
there just wouldn&rsquo;t be any output.</p>
<p>I internalized this as &ldquo;uh, I guess pipes just get stuck sometimes and don&rsquo;t
show me the output, that&rsquo;s weird&rdquo;, and I&rsquo;d handle it by just
running <code>grep thing1 /some/log/file | grep thing2</code> instead, which would work.</p>
<p>So as I&rsquo;ve been doing a terminal deep dive over the last few months I was
really excited to finally learn exactly why this happens.</p>
<h3 id="why-this-happens-buffering">why this happens: buffering</h3>
<p>The reason why &ldquo;pipes get stuck&rdquo; sometimes is that it&rsquo;s VERY common for
programs to buffer their output before writing it to a pipe or file. So the
pipe is working fine, the problem is that the program never even wrote the data
to the pipe!</p>
<p>This is for performance reasons: writing all output immediately as soon as you
can uses more system calls, so it&rsquo;s more efficient to save up data until you
have 8KB or so of data to write (or until the program exits) and THEN write it
to the pipe.</p>
<p>In this example:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>the problem is that <code>grep thing1</code> is saving up all of its matches until it has
8KB of data to write, which might literally never happen.</p>
<h3 id="programs-don-t-buffer-when-writing-to-a-terminal">programs don&rsquo;t buffer when writing to a terminal</h3>
<p>Part of why I found this so disorienting is that <code>tail -f file | grep thing</code>
will work totally fine, but then when you add the second <code>grep</code>, it stops
working!! The reason for this is that the way <code>grep</code> handles buffering depends
on whether it&rsquo;s writing to a terminal or not.</p>
<p>Here&rsquo;s how <code>grep</code> (and many other programs) decides to buffer its output:</p>
<ul>
<li>Check if stdout is a terminal or not using the <code>isatty</code> function
<ul>
<li>If it&rsquo;s a terminal, use line buffering (print every line immediately as soon as you have it)</li>
<li>Otherwise, use &ldquo;block buffering&rdquo; &ndash; only print data if you have at least 8KB or so of data to print</li>
</ul>
</li>
</ul>
<p>So if <code>grep</code> is writing directly to your terminal then you&rsquo;ll see the line as
soon as it&rsquo;s printed, but if it&rsquo;s writing to a pipe, you won&rsquo;t.</p>
<p>Of course the buffer size isn&rsquo;t always 8KB for every program, it depends on the implementation. For <code>grep</code> the buffering is handled by libc, and libc&rsquo;s buffer size is
defined in the <code>BUFSIZ</code> variable. <a href="https://github.com/bminor/glibc/blob/c69e8cccaff8f2d89cee43202623b33e6ef5d24a/libio/stdio.h#L100">Here&rsquo;s where that&rsquo;s defined in glibc</a>.</p>
<p>(as an aside: &ldquo;programs do not use 8KB output buffers when writing to a
terminal&rdquo; isn&rsquo;t, like, a law of terminal physics, a program COULD use an 8KB
buffer when writing output to a terminal if it wanted, it would just be
extremely weird if it did that, I can&rsquo;t think of any program that behaves that
way)</p>
<h3 id="commands-that-buffer-commands-that-don-t">commands that buffer &amp; commands that don&rsquo;t</h3>
<p>One annoying thing about this buffering behaviour is that you kind of need to
remember which commands buffer their output when writing to a pipe.</p>
<p>Some commands that <strong>don&rsquo;t</strong> buffer their output:</p>
<ul>
<li>tail</li>
<li>cat</li>
<li>tee</li>
</ul>
<p>I think almost everything else will buffer output, especially if it&rsquo;s a command
where you&rsquo;re likely to be using it for batch processing. Here&rsquo;s a list of some
common commands that buffer their output when writing to a pipe, along with the
flag that disables block buffering.</p>
<ul>
<li>grep (<code>--line-buffered</code>)</li>
<li>sed (<code>-u</code>)</li>
<li>awk (there&rsquo;s a <code>fflush()</code> function)</li>
<li>tcpdump (<code>-l</code>)</li>
<li>jq (<code>-u</code>)</li>
<li>tr (<code>-u</code>)</li>
<li>cut (can&rsquo;t disable buffering)</li>
</ul>
<p>Those are all the ones I can think of, lots of unix commands (like <code>sort</code>) may
or may not buffer their output but it doesn&rsquo;t matter because <code>sort</code> can&rsquo;t do
anything until it finishes receiving input anyway.</p>
<p>Also I did my best to test both the Mac OS and GNU versions of these but there
are a lot of variations and I might have made some mistakes.</p>
<h3 id="programming-languages-where-the-default-print-statement-buffers">programming languages where the default &ldquo;print&rdquo; statement buffers</h3>
<p>Also, here are a few programming language where the default print statement
will buffer output when writing to a pipe, and some ways to disable buffering
if you want:</p>
<ul>
<li>C (disable with <code>setvbuf</code>)</li>
<li>Python (disable with <code>python -u</code>, or <code>PYTHONUNBUFFERED=1</code>, or <code>sys.stdout.reconfigure(line_buffering=False)</code>, or <code>print(x, flush=True)</code>)</li>
<li>Ruby (disable with <code>STDOUT.sync = true</code>)</li>
<li>Perl (disable with <code>$| = 1</code>)</li>
</ul>
<p>I assume that these languages are designed this way so that the default print
function will be fast when you&rsquo;re doing batch processing.</p>
<p>Also whether output is buffered or not might depend on how you print, for
example in C++ <code>cout &lt;&lt; &quot;hello\n&quot;</code> buffers when writing to a pipe but <code>cout &lt;&lt; &quot;hello&quot; &lt;&lt; endl</code> will flush its output.</p>
<h3 id="when-you-press-ctrl-c-on-a-pipe-the-contents-of-the-buffer-are-lost">when you press <code>Ctrl-C</code> on a pipe, the contents of the buffer are lost</h3>
<p>Let&rsquo;s say you&rsquo;re running this command as a hacky way to watch for DNS requests
to <code>example.com</code>, and you forgot to pass <code>-l</code> to tcpdump:</p>
<pre><code>sudo tcpdump -ni any port 53 | grep example.com
</code></pre>
<p>When you press <code>Ctrl-C</code>, what happens? In a magical perfect world, what I would
<em>want</em> to happen is for <code>tcpdump</code> to flush its buffer, <code>grep</code> would search for
<code>example.com</code>, and I would see all the output I missed.</p>
<p>But in the real world, what happens is that all the programs get killed and the
output in <code>tcpdump</code>&rsquo;s buffer is lost.</p>
<p>I think this problem is probably unavoidable &ndash; I spent a little time with
<code>strace</code> to see how this works and <code>grep</code> receives the <code>SIGINT</code> before
<code>tcpdump</code> anyway so even if <code>tcpdump</code> tried to flush its buffer <code>grep</code> would
already be dead.</p>
<small>
<p>After a little more investigation, there is a workaround: if you find
<code>tcpdump</code>&rsquo;s PID and <code>kill -TERM $PID</code>, then tcpdump will flush the buffer so
you can see the output. That&rsquo;s kind of a pain but I tested it and it seems to
work.</p>
</small>
<h3 id="redirecting-to-a-file-also-buffers">redirecting to a file also buffers</h3>
<p>It&rsquo;s not just pipes, this will also buffer:</p>
<pre><code>sudo tcpdump -ni any port 53 &gt; output.txt
</code></pre>
<p>Redirecting to a file doesn&rsquo;t have the same &ldquo;<code>Ctrl-C</code> will totally destroy the
contents of the buffer&rdquo; problem though &ndash; in my experience it usually behaves
more like you&rsquo;d want, where the contents of the buffer get written to the file
before the program exits. I&rsquo;m not 100% sure whether this is something you can
always rely on or not.</p>
<h3 id="a-bunch-of-potential-ways-to-avoid-buffering">a bunch of potential ways to avoid buffering</h3>
<p>Okay, let&rsquo;s talk solutions. Let&rsquo;s say you&rsquo;ve run this command:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>I asked people on Mastodon how they would solve this in practice and there were
5 basic approaches. Here they are:</p>
<h4 id="solution-1-run-a-program-that-finishes-quickly">solution 1: run a program that finishes quickly</h4>
<p>Historically my solution to this has been to just avoid the &ldquo;command writing to
pipe slowly&rdquo; situation completely and instead run a program that will finish quickly
like this:</p>
<pre><code>cat /some/log/file | grep thing1 | grep thing2 | tail
</code></pre>
<p>This doesn&rsquo;t do the same thing as the original command but it does mean that
you get to avoid thinking about these weird buffering issues.</p>
<p>(you could also do <code>grep thing1 /some/log/file</code> but I often prefer to use an
&ldquo;unnecessary&rdquo; <code>cat</code>)</p>
<h4 id="solution-2-remember-the-line-buffer-flag-to-grep">solution 2: remember the &ldquo;line buffer&rdquo; flag to grep</h4>
<p>You could remember that grep has a flag to avoid buffering and pass it like this:</p>
<pre><code>tail -f /some/log/file | grep --line-buffered thing1 | grep thing2
</code></pre>
<h4 id="solution-3-use-awk">solution 3: use awk</h4>
<p>Some people said that if they&rsquo;re specifically dealing with a multiple greps
situation, they&rsquo;ll rewrite it to use a single <code>awk</code> instead, like this:</p>
<pre><code>tail -f /some/log/file |  awk '/thing1/ &amp;&amp; /thing2/'
</code></pre>
<p>Or you would write a more complicated <code>grep</code>, like this:</p>
<pre><code>tail -f /some/log/file |  grep -E 'thing1.*thing2'
</code></pre>
<p>(<code>awk</code> also buffers, so for this to work you&rsquo;ll want <code>awk</code> to be the last command in the pipeline)</p>
<h4 id="solution-4-use-stdbuf">solution 4: use <code>stdbuf</code></h4>
<p><code>stdbuf</code> uses LD_PRELOAD to turn off libc&rsquo;s buffering, and you can use it to turn off output buffering like this:</p>
<pre><code>tail -f /some/log/file | stdbuf -o0 grep thing1 | grep thing2
</code></pre>
<p>Like any <code>LD_PRELOAD</code> solution it&rsquo;s a bit unreliable &ndash; it doesn&rsquo;t work on
static binaries, I think won&rsquo;t work if the program isn&rsquo;t using libc&rsquo;s
buffering, and doesn&rsquo;t always work on Mac OS. Harry Marr has a really nice <a href="https://hmarr.com/blog/how-stdbuf-works/">How stdbuf works</a> post.</p>
<h4 id="solution-5-use-unbuffer">solution 5: use <code>unbuffer</code></h4>
<p><code>unbuffer program</code> will force the program&rsquo;s output to be a TTY, which means
that it&rsquo;ll behave the way it normally would on a TTY (less buffering, colour
output, etc). You could use it in this example like this:</p>
<pre><code>tail -f /some/log/file | unbuffer grep thing1 | grep thing2
</code></pre>
<p>Unlike <code>stdbuf</code> it will always work, though it might have unwanted side
effects, for example <code>grep thing1</code>&rsquo;s will also colour matches.</p>
<p>If you want to install unbuffer, it&rsquo;s in the <code>expect</code> package.</p>
<h3 id="that-s-all-the-solutions-i-know-about">that&rsquo;s all the solutions I know about!</h3>
<p>It&rsquo;s a bit hard for me to say which one is &ldquo;best&rdquo;, I think personally I&rsquo;m
mostly likely to use <code>unbuffer</code> because I know it&rsquo;s always going to work.</p>
<p>If I learn about more solutions I&rsquo;ll try to add them to this post.</p>
<h3 id="i-m-not-really-sure-how-often-this-comes-up">I&rsquo;m not really sure how often this comes up</h3>
<p>I think it&rsquo;s not very common for me to have a program that slowly trickles data
into a pipe like this, normally if I&rsquo;m using a pipe a bunch of data gets
written very quickly, processed by everything in the pipeline, and then
everything exits. The only examples I can come up with right now are:</p>
<ul>
<li>tcpdump</li>
<li><code>tail -f</code></li>
<li>watching log files in a different way like with <code>kubectl logs</code></li>
<li>the output of a slow computation</li>
</ul>
<h3 id="what-if-there-were-an-environment-variable-to-disable-buffering">what if there were an environment variable to disable buffering?</h3>
<p>I think it would be cool if there were a standard environment variable to turn
off buffering, like <code>PYTHONUNBUFFERED</code> in Python. I got this idea from a
<a href="https://blog.plover.com/Unix/stdio-buffering.html">couple</a> of <a href="https://blog.plover.com/Unix/stdio-buffering-2.html">blog posts</a> by Mark Dominus
in 2018. Maybe <code>NO_BUFFER</code> like <a href="https://no-color.org/">NO_COLOR</a>?</p>
<p>The design seems tricky to get right; Mark points out that NETBSD has <a href="https://man.netbsd.org/setbuf.3">environment variables called <code>STDBUF</code>, <code>STDBUF1</code>, etc</a> which gives you a
ton of control over buffering but I imagine most developers don&rsquo;t want to
implement many different environment variables to handle a relatively minor
edge case.</p>
<p>I&rsquo;m also curious about whether there are any programs that just automatically
flush their output buffers after some period of time (like 1 second). It feels
like it would be nice in theory but I can&rsquo;t think of any program that does that
so I imagine there are some downsides.</p>
<h3 id="stuff-i-left-out">stuff I left out</h3>
<p>Some things I didn&rsquo;t talk about in this post since these posts have been
getting pretty long recently and seriously does anyone REALLY want to read 3000
words about buffering?</p>
<ul>
<li>the difference between line buffering and having totally unbuffered output</li>
<li>how buffering to stderr is different from buffering to stdout</li>
<li>this post is only about buffering that happens <strong>inside the program</strong>, your
operating system&rsquo;s TTY driver also does a little bit of buffering sometimes</li>
<li>other reasons you might need to flush your output other than &ldquo;you&rsquo;re writing
to a pipe&rdquo;</li>
</ul>

---

### [2025 Recap: so many projects](https://fasterthanli.me/articles/2025-recap)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I‚Äôve been working on so many projects in 2025, I thought it was important for me
to make a recap, if only just to clear my head.</p>

<p>There are many, many, many things to go through and we don‚Äôt have a sponsor
today, so I‚Äôm gonna start right away with facet!</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#facet" id="facet"><h2>facet</h2></a>
<p>facet is a project that I started working on in March of this year ‚Äî that‚Äôs
right, it‚Äôs only been ten months, yet it feels like an eternity.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#in-the-beginning" id="in-the-beginning"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#the-first-golden-age" id="the-first-golden-age"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#disappointment" id="disappointment"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#refocus" id="refocus"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#volte-face" id="volte-face"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#just-in-time-for-the-new-year" id="just-in-time-for-the-new-year"></a>






















<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#dodeca" id="dodeca"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#rapace" id="rapace"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#tracey" id="tracey"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#picante" id="picante"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#pikru" id="pikru"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#aasvg-rs" id="aasvg-rs"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#facet-keeps-growing" id="facet-keeps-growing"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#fs-kitty" id="fs-kitty"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#vixen" id="vixen"></a>






























































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Introducing arborium, a tree-sitter distribution](https://fasterthanli.me/articles/introducing-arborium)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>About two weeks ago I entered a discussion with the docs.rs team about,
basically, why we have to look at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages but they're all white on black. It's sad.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-no-colors@2x~eb49252c206ebe40.jxl" title="" width="1083" /></p><p>When we could be looking at this:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="My browser showing a docs.rs page for a crate that I published myself, which contains a lot of different code blocks with different languages. this time it's colored.
" class="" height="750" src="https://cdn.fasterthanli.me/content/articles/introducing-arborium/docsrs-yes-colors@2x~708d0f07e1265747.jxl" title="" width="1083" /></p><p>And of course, as always, there are reasons why things are the way they are.
In an effort to understand those reasons, I opened a GitHub issue which resulted
in a <a href="https://github.com/rust-lang/docs.rs/issues/3040">short but productive</a> discussion.</p>

<p>I walked away discouraged, and then decided to, reasons be damned, attack this
problem from three different angles.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#background" id="background"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#problems" id="problems"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#solutions" id="solutions"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#arborium" id="arborium"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-1-just-include-this-script" id="angle-1-just-include-this-script"></a>

























<a class="anchor" href="https://fasterthanli.me/index.xml#angle-2-it-goes-in-the-rustdoc-hole" id="angle-2-it-goes-in-the-rustdoc-hole"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#angle-3-only-in-the-backend" id="angle-3-only-in-the-backend"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#post-mortem" id="post-mortem"></a>












<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [Does Dioxus spark joy?](https://fasterthanli.me/articles/does-dioxus-spark-joy)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <div class="dialog right">
<div class="dialog-head" title="Amos says:">
  <source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Amos" height="42" src="https://cdn.fasterthanli.me/content/img/reimena/amos-neutral~55a3477398fe0cb1.jxl" width="42" />
</div>
<div class="dialog-text markup-container">
<p>Note: this article is adapted from a presentation I gave at a Rust Paris Meetup ‚Äî that‚Äôs why
it sounds a little different than usual. Enjoy!</p>

</div>
</div><p>Good evening! Tonight, I will attempt to answer the question: Does
<a href="https://github.com/dioxuslabs/dioxus">Dioxus</a> spark joy? Or at the very least,
whimsy.</p>

<p>What‚Äôs Dioxus, you ask? It is first and foremost a name that is quote: ‚Äúlegally
not inspired by any Pok√©mon‚Äù.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="The deoxys pokemon
" class="" height="449" src="https://cdn.fasterthanli.me/content/articles/does-dioxus-spark-joy/deoxys@2x~d76eb1e3eeda1b65.jxl" title="" width="422" /></p><p>Even if the author concedes <a href="https://news.ycombinator.com/item?id=39853385">in a Hacker News comment</a> that the ‚ÄúDeoxys‚Äù Pok√©mon
is, I quote: ‚Äúawesome‚Äù.</p>











<a class="anchor" href="https://fasterthanli.me/index.xml#a-short-and-mostly-wrong-history-of-web-apps" id="a-short-and-mostly-wrong-history-of-web-apps"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#a-practical-example" id="a-practical-example"></a>































<a class="anchor" href="https://fasterthanli.me/index.xml#love-hate" id="love-hate"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#does-dioxus-spark-joy" id="does-dioxus-spark-joy"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#afterword" id="afterword"></a>

---

### [Engineering a Rust optimization quiz](https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are several Rust quizzes online, including one that‚Äôs literally called the
‚ÄúUnfair Rust Quiz‚Äù at <a href="https://this.quiz.is.fckn.gay/">https:&#x2f;&#x2f;this.quiz.is.fckn.gay&#x2f;</a>, but when I was given the
opportunity to record an episode of the <a href="https://sdr-podcast.com/">Self-Directed Research
podcast</a> live on the main stage of <a href="https://eurorust.eu/2025/">EuroRust
2025</a>, I thought I‚Äôd come up with something special.</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Question Misc 6 of the unfair Rust quiz, about drop order. " height="714" src="https://cdn.fasterthanli.me/content/articles/engineering-a-rust-optimization-quiz/unfair-rust-quiz@2x~478a86d3446281d7.jxl" title="The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side. " width="795" /><figcaption><div class="markup-container figure-caption"><p>The unfair rust quiz really deserves its name. It is best passed with a knowledgeable friend by your side.</p>
</div></figcaption>
</figure>



<a class="anchor" href="https://fasterthanli.me/index.xml#coming-up-with-the-questions" id="coming-up-with-the-questions"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#nih-syndrome-ppt" id="nih-syndrome-ppt"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#server-side-shenanigans-and-room-codes" id="server-side-shenanigans-and-room-codes"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#deploying-the-beast" id="deploying-the-beast"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#d-2-starting-fights-at-a-paris-meetup" id="d-2-starting-fights-at-a-paris-meetup"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#d-1-panic-mode-and-missing-explanations" id="d-1-panic-mode-and-missing-explanations"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#day-of-github-oauth-and-swipe-gestures" id="day-of-github-oauth-and-swipe-gestures"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#showtime" id="showtime"></a>

---

### [Making our own spectrogram](https://fasterthanli.me/articles/making-our-own-spectrogram)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>A couple months ago <a href="https://fasterthanli.me/articles/the-science-of-loudness">I made a loudness meter</a>
and went way too in-depth into how humans have measured loudness over time.</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the fasterthanlime audio meter, with RMS, sample peak, true peak, and various loudness metrics.
" class="" height="512" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/fasterthanlime-audio-meter@2x~d5e6b54e3ade21f5.jxl" title="" width="800" /></p><p>Today we‚Äôre looking at a spectrogram visualization I made, which is a lot more entertaining!</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="" class="" height="632" src="https://cdn.fasterthanli.me/content/articles/making-our-own-spectrogram/spectrogram-sonata-op25-3@2x~c22af608785d58b2.jxl" title="" width="1100" /></p><p>We‚Äôre going to talk about how to extract frequencies from sound waves, but also
how my spectrogram app is assembled from different Rust crates, how it
handles audio and graphics threads, how it draws the spectrogram etc.</p>



<a class="anchor" href="https://fasterthanli.me/index.xml#the-humble-sine-wave" id="the-humble-sine-wave"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#approximating-a-square-wave" id="approximating-a-square-wave"></a>









<a class="anchor" href="https://fasterthanli.me/index.xml#a-real-world-sample" id="a-real-world-sample"></a>







<a class="anchor" href="https://fasterthanli.me/index.xml#chunking-and-windowing" id="chunking-and-windowing"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#the-gabor-limit" id="the-gabor-limit"></a>





















<a class="anchor" href="https://fasterthanli.me/index.xml#interpolation" id="interpolation"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#color-mapping" id="color-mapping"></a>










<a class="anchor" href="https://fasterthanli.me/index.xml#frequency-mapping" id="frequency-mapping"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#audio-input" id="audio-input"></a>














<a class="anchor" href="https://fasterthanli.me/index.xml#drawing-the-ui" id="drawing-the-ui"></a>



















































<a class="anchor" href="https://fasterthanli.me/index.xml#updating-the-texture" id="updating-the-texture"></a>


























<a class="anchor" href="https://fasterthanli.me/index.xml#profiling-my-program" id="profiling-my-program"></a>




























<a class="anchor" href="https://fasterthanli.me/index.xml#having-fun" id="having-fun"></a>
































<a class="anchor" href="https://fasterthanli.me/index.xml#closing-words" id="closing-words"></a>

---

### [crates.io phishing attempt](https://fasterthanli.me/articles/crates-io-phishing-attempt)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>Earlier this week, an <a href="https://fasterthanli.me/articles/color-npm-package-compromised">npm supply chain attack</a>.</p>

<p>It‚Äôs turn for <a href="https://crates.io">crates.io</a>, the main public repository for <a href="https://rust-lang.org">Rust</a>
crates (packages).</p>

<p>The phishing e-mail looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source type="image/webp" /><img alt="A phishing e-mail: Important: Breach notification regarding crates.io  Hi, BurntSushi! We recently discovered that an unauthorized actor had compromised the crates.io infrastructure and accessed a limited amount of user information. The attacker's access was revoked, and we are currently reviewing our security posture. We are currently drafting a blog post to outline the timeline and the steps we took to mitigate this. In the meantime, we strongly suggest you to rotate your login info by signing in here to our internal SSO, which is a temporary fix to ensure that the attacker cannot modify any packages published by you. " height="653" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/phishing-email~052f360399d29116.jxl" title="" width="708" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;burntsushi.net&#x2f;post&#x2f;3lynehptw6c2n">Andrew Gallant on BlueSky
</a></cite></figcaption>
</figure><p>And it leads to a GitHub login page that looks like this:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A fake GitHub sign-in page. " height="1378" src="https://cdn.fasterthanli.me/content/articles/crates-io-phishing-attempt/github-phish~e78524d35ede5efb.jxl" title="" width="1322" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;github.com&#x2f;rust-lang&#x2f;crates.io&#x2f;discussions&#x2f;11889#discussion-8886064">Barre on GitHub
</a></cite></figcaption>
</figure><p>Several maintainers received it ‚Äî the issue is being discussed <a href="https://github.com/rust-lang/crates.io/discussions/11889">on GitHub</a>.</p>

<p>The <a href="https://www.rust-lang.org/governance/teams/dev-tools#team-crates-io">crates.io team</a> has acknowledged
the attack and said they‚Äôd see if they can do something about it.</p>

---

### [color npm package compromised](https://fasterthanli.me/articles/color-npm-package-compromised)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>On September 8 2025, around 13:00 UTC, someone compromised <a href="https://www.npmjs.com/~qix">Josh Junon‚Äôs npm
account (qix)</a> and started publishing backdoored
versions of his package.</p>

<p>Someone noticed and let Josh know:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Hey. Your npm account seems to have been compromised. 1 hour ago it started posting packages with backdoors to all your popular packages. " height="177" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/charlie-noticed@2x~4a9e74f87760a4af.jxl" title="" width="595" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;charlieeriksen.bsky.social&#x2f;post&#x2f;3lydffcyulc2n">Charlie Eriksen on BlueSky
</a></cite></figcaption>
</figure><p>Josh confirmed he‚Äôd gotten pwned by a fake 2FA (two-factor authentication) reset e-mail:</p>

<figure class="paragraph-like"><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Yep, I've been pwned. 2FA reset email, looked very legitimate.  Only NPM affected. I've sent an email off to @npmjs.bsky.social  to see if I can get access again.  Sorry everyone, I should have paid more attention. Not like me; have had a stressful week. Will work to get this cleaned up. " height="396" src="https://cdn.fasterthanli.me/content/articles/color-npm-package-compromised/josh-fake-2fa@2x~ca37f72d582a4442.jxl" title="" width="592" /><figcaption><cite><a href="https://fasterthanli.me/&#x2f;&#x2f;bsky.app&#x2f;profile&#x2f;bad-at-computer.bsky.social&#x2f;post&#x2f;3lydioq5swk2y">Josh Junon on BlueSky
</a></cite></figcaption>
</figure><p>The phishing e-mail came from <code>npmsj.help</code> (registered 3 days prior) and claimed
users had to reset their 2FA:</p>







<a class="anchor" href="https://fasterthanli.me/index.xml#the-payload" id="the-payload"></a>
























<a class="anchor" href="https://fasterthanli.me/index.xml#current-situation" id="current-situation"></a>

---

### [The science of loudness](https://fasterthanli.me/articles/the-science-of-loudness)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>My watch has a ‚ÄúNoise‚Äù app: it shows <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">B</mi></mrow></math>, for decibels.</p>

<p><video alt="A video of my Apple Watch showing me how loud the sound coming from my speakers is.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~d96a722c49fdda5d.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~e53858e76f36614d.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/apple-watch-decibel-meter~fb169411c4d8653e.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>My amp has a volume knob, which also shows decibels, although.. negative ones, this time.</p>

<p><video alt="A video of me adjusting my volume on my Cambridge AXR100 amplifier. The
volume goes from -61 to -32 decibels in that video.
" class="" controls="controls" height="2160" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume.thumb" preload="none" title="" width="3840"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~46153fb1ad60f230.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~a8af11cbe6d90598.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/adjust-volume~9a7388f3f768f948.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>And finally, my video editing software has a ton of meters ‚Äî which are all in decibel or
decibel-adjacent units.</p>

<p><video alt="A screenshot of DaVinci Resolve, showing various meters: we have Bus 1,
Control Room with TP, Loudness, YouTube (LUFS), then Loudness History
with Integrated, Momentary and Short Term. In the Mixer, each track has
its meter.
" class="" controls="controls" height="1127" poster="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x.thumb" preload="none" title="" width="2002"><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~9cc29f2809d1202a.mp4" type="video/mp4; codecs=av01.0.08m.08,opus" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~da316f2ab0e0aeb5.h264+aac.mp4" type="video/mp4; codecs=avc1.640034,mp4a.40.2" /><source src="https://cdn.fasterthanli.me/content/articles/the-science-of-loudness/tons-of-meters@2x~e50f9e76cf48090c.vp9+opus.webm" type="video/webm; codecs=vp09.00.41.08,opus" />Your browser does not support the video tag.</video></p><p>How do all these decibels fit together?</p>






<a class="anchor" href="https://fasterthanli.me/index.xml#what-even-is-sound" id="what-even-is-sound"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#under-pressure" id="under-pressure"></a>


















<a class="anchor" href="https://fasterthanli.me/index.xml#signal-processing" id="signal-processing"></a>




































<a class="anchor" href="https://fasterthanli.me/index.xml#root-mean-square" id="root-mean-square"></a>
















<a class="anchor" href="https://fasterthanli.me/index.xml#sample-peak-true-peak" id="sample-peak-true-peak"></a>




















<a class="anchor" href="https://fasterthanli.me/index.xml#the-loudness-wars" id="the-loudness-wars"></a>











































































<a class="anchor" href="https://fasterthanli.me/index.xml#a-weighting" id="a-weighting"></a>

---

### [Summer fasterthanlime update](https://fasterthanli.me/articles/summer-fasterthanlime-update)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>There are news!</p>

<div class="tip markup-container">
<div class="tip-header bear-mark">
<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="Cool bear" class="bear" height="48" src="https://cdn.fasterthanli.me/content/img/reimena/coolbear-idea~85823bd96ffd0bb6.jxl" width="48" />
Cool Bear's hot tip
</div>
<p>TL;DR: If you‚Äôre a patron or sponsor, check your <a href="https://fasterthanli.me/profile">Profile</a> page to
get detailed explainers of every perk. You‚Äôll need to log in. Duh.</p>

</div><p>Here are all the changes I‚Äôm implementing, summarized as a table:</p>

<div class="responsive-table"><table><thead><td>Before</td><td>After</td></thead><tr><td>üìö Articles remain exclusive for <strong>6 months</strong></td><td>Early access (<strong>couple weeks</strong>) for Silver tier</td></tr><tr><td>üéûÔ∏è No early access for video</td><td><strong>Video early access</strong> on Patreon and website</td></tr></table></div>

<a class="anchor" href="https://fasterthanli.me/index.xml#looking-back" id="looking-back"></a>


































<a class="anchor" href="https://fasterthanli.me/index.xml#a-discord-server" id="a-discord-server"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#early-access-revamp" id="early-access-revamp"></a>








<a class="anchor" href="https://fasterthanli.me/index.xml#dual-rss-feeds" id="dual-rss-feeds"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#bye-ko-fi" id="bye-ko-fi"></a>






<a class="anchor" href="https://fasterthanli.me/index.xml#more-casual-posting" id="more-casual-posting"></a>




<a class="anchor" href="https://fasterthanli.me/index.xml#what-about-content-that-was-still-exclusive" id="what-about-content-that-was-still-exclusive"></a>

---

### [All color is best-effort](https://fasterthanli.me/articles/all-color-is-best-effort)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I do not come to you with answers today, but rather some observations and a lot of questions.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#the-weird-glitch" id="the-weird-glitch"><h2>The weird glitch</h2></a>
<p>Recently I was editing some video and I noticed this:</p>

<p>



<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/dvr-circles@2x~3c031f1f42693336.jxl" title="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." width="1966" /></p>

<p>Not what the finger is pointing at ‚Äî the dots.</p>

<p>Here are the separate layers this image is made up of: the background is a stock image
I‚Äôve licensed from Envato Elements:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A picture of a canyon, darker than you‚Äôd expect." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/canyon-background@2x~dcfb5771209ddbd5.jxl" title="A picture of a canyon, darker than you‚Äôd expect." width="1966" /></p>

<p>Because I use it as a background image, I‚Äôve cranked down the exposition in the Color tab:</p>













































































<a class="anchor" href="https://fasterthanli.me/index.xml#playing-with-color-spaces" id="playing-with-color-spaces"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#cie-chromaticity-diagram" id="cie-chromaticity-diagram"></a>


























































<a class="anchor" href="https://fasterthanli.me/index.xml#our-first-transfer-function" id="our-first-transfer-function"></a>






















































<a class="anchor" href="https://fasterthanli.me/index.xml#parade-scope" id="parade-scope"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#more-transfer-functions" id="more-transfer-functions"></a>































































































<a class="anchor" href="https://fasterthanli.me/index.xml#how-white-is-your-white" id="how-white-is-your-white"></a>









































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

### [How we mitigated a vulnerability in Cloudflare‚Äôs ACME validation logic](https://blog.cloudflare.com/acme-path-vulnerability/)

**Êù•Ê∫ê**: The Cloudflare Blog

**ÊëòË¶Å**: A vulnerability was recently identified in Cloudflare‚Äôs automation of certificate validation. Here we explain the vulnerability and outline the steps we‚Äôve taken to mitigate it.

---

## Tech News

### [The Only Two Markup Languages](https://www.gingerbill.org/article/2026/01/19/two-families-of-markup-languages/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/bv4yod/only_two_markup_languages">Comments</a></p>

---

### [Floating-Point Printing and Parsing Can Be Simple And Fast](https://research.swtch.com/fp)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/nbsclr/floating_point_printing_parsing_can_be">Comments</a></p>

---

### [People cannot "just pay attention" to (boring, routine) things](https://utcc.utoronto.ca/~cks/space/blog/tech/PeopleCannotPayAttention)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/docfzx/people_cannot_just_pay_attention_boring">Comments</a></p>

---

### [A Tale of UDP tracking: Hytale server auto stop and start with systemd](https://nwildner.com/posts/2026-01-19-hytale/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/8nv2w5/tale_udp_tracking_hytale_server_auto_stop">Comments</a></p>

---

### [Are arrays functions?](https://futhark-lang.org/blog/2026-01-16-are-arrays-functions.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/f6zsyd/are_arrays_functions">Comments</a></p>

---

### [Personal infrastructure setup 2026](https://linderud.dev/blog/personal-infrastructure-setup-2026/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/fsqjwr/personal_infrastructure_setup_2026">Comments</a></p>

---

### [My thoughts on Gas Town after 10,000 hours of Claude Code](https://simonhartcher.com/posts/2026-01-19-my-thoughts-on-gas-town-after-10000-hours-of-claude-code)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/txknsm/my_thoughts_on_gas_town_after_10_000_hours">Comments</a></p>

---

### [The Incredible Overcomplexity of the Shadcn Radio Button](https://paulmakeswebsites.com/writing/shadcn-radio-button/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/bemom7/incredible_overcomplexity_shadcn_radio">Comments</a></p>

---


---

## üìö Â¶Ç‰Ωï‰ΩøÁî®

1. ÊµèËßàÊÑüÂÖ¥Ë∂£ÁöÑÊ†áÈ¢ò
2. ÈòÖËØªAIÁîüÊàêÁöÑÊëòË¶ÅÂø´ÈÄü‰∫ÜËß£ÂÜÖÂÆπ
3. ÁÇπÂáªÈìæÊé•Ê∑±ÂÖ•ÈòÖËØª
4. Êúâ‰ª∑ÂÄºÁöÑÂÜÖÂÆπÂèØ‰ª•Êï¥ÁêÜÂà∞ÂØπÂ∫îÁöÑ‰∏ªÈ¢òÁõÆÂΩï

## üîß ÈÖçÁΩÆ

‰øÆÊîπ `config/sources.yaml` ÂèØ‰ª•:
- Ê∑ªÂä†/Âà†Èô§RSSËÆ¢ÈòÖÊ∫ê
- Ë∞ÉÊï¥HackerNewsÊúÄÂ∞èÂàÜÊï∞ÈòàÂÄº
- ÈÖçÁΩÆÂÜÖÂÆπËøáÊª§ÂÖ≥ÈîÆËØç

*Êú¨ÊñáÊ°£Áî± [daily-digestËÑöÊú¨](../scripts/generate_digest.py) Ëá™Âä®ÁîüÊàê*