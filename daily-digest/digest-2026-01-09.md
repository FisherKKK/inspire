# ÊØèÊó•ÊäÄÊúØÊëòË¶Å - 2026-01-09

> Ëá™Âä®ÁîüÊàê‰∫é 2026-01-09 01:14:06
> ÂÖ±Êî∂ÈõÜ 149 ÁØáÊñáÁ´†

## üìë ÁõÆÂΩï

- [AI News](#ai-news) (2ÁØá)
- [AI Research](#ai-research) (6ÁØá)
- [Engineering](#engineering) (10ÁØá)
- [HackerNews](#hackernews) (85ÁØá)
- [LLM Infrastructure](#llm-infrastructure) (10ÁØá)
- [Operating Systems](#operating-systems) (7ÁØá)
- [Systems](#systems) (20ÁØá)
- [Tech News](#tech-news) (9ÁØá)

---

## AI News

### [Netomi‚Äôs lessons for scaling agentic systems into the enterprise](https://openai.com/index/netomi)

**Êù•Ê∫ê**: OpenAI Blog

**ÊëòË¶Å**: How Netomi scales enterprise AI agents using GPT-4.1 and GPT-5.2‚Äîcombining concurrency, governance, and multi-step reasoning for reliable production workflows.

---

### [OpenAI for Healthcare](https://openai.com/index/openai-for-healthcare)

**Êù•Ê∫ê**: OpenAI Blog

**ÊëòË¶Å**: OpenAI for Healthcare enables secure, enterprise-grade AI that supports HIPAA compliance‚Äîreducing administrative burden and supporting clinical workflows.

---

## AI Research

### [The Economics of Transformative AI](https://www.lesswrong.com/posts/epFKhn24trRP2cs3k/the-economics-of-transformative-ai)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 8, 2026 10:22 PM GMT<br /><br /><p><i>Anton Korinek is an economist at UVA and the Brookings Institution who focuses on the macroeconomics of AI. This is a lightly edited transcript of a recent lecture where he lays out what economics actually predicts about transformative AI ‚Äî in our view it's the best introductory resource on the topic, and basically anyone discussing post-labour economics should be familiar with this.</i></p><p><i>The talk covers historical development of what the bottlenecks are: for most of history, land was the bottleneck and humans were disposable. The Industrial Revolution flipped this. AI may flip it again: if labor becomes reproducible, humans are no longer the bottleneck.</i></p><p><i>Korinek walks through what this implies for growth (potentially dramatic), for wages (likely positive effects until some threshold of automation, then a sudden collapse), and for policy (e.g. our entire tax system assumes labor income exists). He also addresses some confusions: "prices falling" and "productivity rising"; "post-scarcity" is a misnomer since even cheap resources have prices; and no, there's no economic law that technology must create jobs‚Äîthat was just an empirical regularity when humans were the bottleneck.</i></p><p><i>The uncomfortable conclusion is the economy doesn't need us. It can run perfectly well "of the machines, by the machines, and for the machines." Whether that's what we want is a different question.</i><br /><br />Transcript is under the video</p><figure class="media"><div><div></div></div></figure><p>I want to start by talking about some high-level lessons about the economics of transformative AI‚Äîor the economics of AGI, which are not exactly the same but close substitutes to each other. I've split this into three big themes which I call "the good", "the bad," and "the ugly".</p><p>What do I mean by that? There is the productivity and growth impact of AGI, transformative AI‚Äîthat's "the good". There is the labor market disruption and inequality aspect‚Äîwhich is "the bad". And then there are the TAI risks and alignment. I unfortunately don't have very much to say on that, but I'm still including it‚Äîwhich is "the ugly". It also has some economic repercussions. So let's first jump into the good, so that we can end on a high point.</p><figure class="image image_resized" style="width: 768px;"><img alt="Three Categories of Economic Impacts: Productivity and Growth (the good), Labor Market Disruption and Inequality (the bad), TAI Risks and Alignment (the ugly)" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/troxf36pypv6jbwsbohd" /></figure><h2><strong>The Good: Productivity and Growth</strong></h2><p>I'll start with some economics 101. You can forget about the equations, but maybe some of you have seen this in your undergraduate econ. The way that we economists think about the economy is that we produce output Y by combining capital K and labor L through some sort of production function. That production function operates at a certain level of technology, which we call A. We mix those two things together, use our technology, and produce output.</p><figure class="image image_resized" style="width: 768px;"><img alt="Economics 101: Output Y = A*F(K, L) depends on technology A, capital K, and labor L; Consumption C = Y - I - G is what's left after investment I and govt spending G; Utility U = u(C) is how happy the consumption makes us" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/dajs6e3ucdr3uowvl9i7" /></figure><p>From that output, we take a part and invest it, we take a part and use it for government spending, and the rest is consumption. And that consumption‚ÄîI guess that's the critical part‚Äîis supposed to deliver us some sort of utility u(C). So the big question that I want to unpack in the next 20 minutes or so is: how will AGI affect all of that, and what will all these effects depend on?</p><h3><strong>What the Most Cited Economist Thinks</strong></h3><p>Let's start with the good. What will the output effects of AGI depend on? Well, if we ask the most cited economist in the world, Daron Acemoglu, he published a paper last year on <a href="https://www.nber.org/system/files/working_papers/w32487/w32487.pdf">"The Simple Macroeconomics of AI"</a> in which he predicted that AI will affect growth by raising it by 0.07% a year. So he does not believe that this is going to be very transformative. He thinks AI is essentially BS.</p><p>But that means there is more work for us to be done. This guy is the closest thing to an economic super-intelligence‚Äîif he doesn't pay attention to these questions, then I guess many of us have to.</p><h3><strong>A Longer Arc of History</strong></h3><p>I think to take this question seriously, we have to actually take a step back and look at a much longer arc of history to understand how transformative AI will be for the economy.</p><p>In prior decades‚Äîor even for the past two centuries‚Äîeverything in the economy revolved around capital and labor. But if we take a step even further back towards the Malthusian age, it wasn't always like that.</p><p>During the Malthusian times, the most important factor of production, the most important thing to produce output in the economy, was actually land. And then you needed human labor to work the land. But unfortunately, the human labor was actually pretty dispensable. The way that economists put it is the marginal human had just enough to cover their subsistence needs. What they ate and what they produced was roughly equal, which means they did not produce any excess economic value. This is what Malthus described as essentially the Malthusian trap.</p><p>During Malthusian times‚Äîduring the Middle Ages and so on‚Äîhuman populations multiplied until they ran out of resources to sustain themselves. So everybody, or at least everybody except for a few special kings and so on had just enough to meet their basic necessities. Living standards were very low. Utility, as we economists would characterize it, was not particularly high. In some sense, from a material perspective, those were pretty bleak times‚Äîpeople were not doing particularly well. But since land was essentially the bottleneck factor of the economy, those who controlled the land were the most powerful: the lords.</p><h3><strong>The Industrial Revolution</strong></h3><p>Well, then something really miraculous happened: the industrial revolution‚Äîdriven probably by scientific advances, the age of enlightenment, and so on. What happened during the industrial revolution is that we developed technologies to produce stuff so that land was no longer the primary bottleneck factor in the economy. Instead, we started to produce things with machines, and we combined those machines with labor. That gave us the productive structure that all of you who took econ 101 saw in your undergraduate studies, where we combine capital and labor to produce output. This is just given as one of the most fundamental economic laws‚Äîalthough, maybe it will soon no longer be that way.</p><p>The other thing that happened with the industrial revolution is that we suddenly started to advance technology in a sustained way. Technology started to progress at a rate of like one-and-a-half to two percent. We constructed more and more machines, and the accumulation of better technologies and more capital allowed the economy to grow‚Äîand to grow in a sustained way.</p><p>But what was the bottleneck? The bottleneck was suddenly the human: the human worker. Humans did not multiply at the same rate as technological progress advanced and as we accumulated capital. That means humans became very scarce. And that scarcity of human labor is what drove the sustained advances in living standards over the past two-and-a-half centuries, and what made living standards in advanced countries something like 20√ó what they were before the industrial revolution.</p><p>So the fact that we were the bottleneck, the fact that we were scarce‚Äîthat's what made us economically valuable, and that's what basically supported the standard of living that all of us are experiencing today.</p><figure class="image image_resized" style="width: 768px;"><img alt="Economic Paradigm Shift Under AGI: The Malthusian Age (Technology stagnant, Land bottleneck, Labor dispensable), The Industrial Age (Technology driving force, Labor bottleneck, Capital reproducible), The Age of AI (Technology accelerating, Capital reproducible, Labor reproducible)" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/xj0pnojheafiiahb4efl" /></figure><h3><strong>The Age of AI</strong></h3><p>Now we may be about to enter the age of AI. What's going to happen there?</p><p>Technology is almost certainly going to accelerate. Capital, just like it was during the industrial age, can be reproduced. But labor is no longer bound to how many human bodies we have‚Äîit can also be simply reproduced by starting up another server farm, by building another robot.</p><p>And those machines are going to compete with human labor.</p><p>All the indications right now are that they are going to perform the kinds of jobs that humans can perform at what is currently a lower cost. And if a machine can perform your job, then competitive forces will ultimately drive your wage to the cost of the machine.</p><h3><strong>Simulating the Output Effects</strong></h3><p>What will this imply for output? I have some simulations in a paper that <a href="https://www.imf.org/-/media/files/publications/fandd/article/2023/december/30-33-korinek-final.pdf">I published</a> with the IMF two years ago. The bottom line shows the baseline of how growth proceeded for the past couple of decades, and then there are two AGI scenarios which essentially assume that we automate the economy within five years versus within 20 years. The faster we automate everything, the faster growth takes off. I'll discuss the effects on wages when we talk about the labor market in the corresponding diagram.</p><figure class="image image_resized"><img alt="Output simulation showing Traditional, Baseline AGI, and Aggressive AGI scenarios over 30 years. Output grows dramatically under AGI scenarios." src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/pkw5zsq5l8axh2pyjois" /></figure><p>SOURCE: Anton Korinek. NOTE: AGI = artificial general intelligence.</p><p>So, back more conceptually to the output effects. These output effects are going to be driven by technological progress, because I think all of us expect that AGI will allow us to make advances‚Äîboth scientific advances and organizational advances‚Äîmore quickly than what we currently can in the human economy.</p><p>They will be driven by automating labor. And I think it's important‚ÄîI visit this website every couple of weeks just to make sure I really read it correctly. The charter of OpenAI, for example, mentions <a href="https://openai.com/charter/">"highly autonomous systems that outperform humans at most economically valuable work"</a>. Well, that means if they really succeed at what they're saying, then labor is toast.</p><p>But right now I'm supposed to talk about the good‚Äîthe output effects. The good is: if you relieve this bottleneck of humans, if you basically allow machines to perform all the valuable tasks in the economy, then you can grow the economy to a much bigger size. You can expand output and move beyond that bottleneck. That means growth rates that are currently unthinkable may be possible.</p><p>One way of thinking about it‚Äîyou hear all these numbers, is it going to be 20% or 50%, I have no idea‚Äîbut one way of thinking about it is: imagine you have robots and server farms. How long is it going to take those robots and server farms together to double themselves? That's going to give you a good perspective on what reasonable estimates of growth rates will be.</p><p>And the third factor that's also very important: <a href="https://www.nber.org/system/files/working_papers/w31815/w31815.pdf">we need this capital accumulation</a>. We need these additional machines, these additional robots and server farms, in order to advance growth in the economy.</p><h3><strong>The Intelligence Explosion</strong></h3><p>Now what may this growth takeoff look like, and what may it be driven by? <a href="https://www.aof.org.hk/docs/default-source/hkimr/conference-workshop/paper-present-2a_basil-halperin.pdf?sfvrsn=4c8ee08d_2">In a paper</a> that I'm about to put out with three co-authors‚ÄîTom Davidson, Basel Halper, and Tom Holden‚Äîwe look at how an intelligence explosion may trigger essentially a growth takeoff.</p><figure class="image image_resized" style="width: 768px;"><img alt="Feedback Loops Driving a Growth Take-off: AI Labor feeds into Software Quality and Hardware Quality, which drive Total Factor Productivity, Output, and Capital accumulation in reinforcing loops." src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/uvvbowsxwlolnh3jtrhu" /></figure><p>Source: "Is Automating AI Research Enough for a Growth Explosion?" (with T. Davidson, B.Halperin, and T. Houlden), Nov. 2025</p><p>In one diagram, the right column shows you how economic growth proceeded during the industrial age. You had output being driven by advances in technology‚Äîor total factor productivity, as we call it technically in economics‚Äîand by the accumulation of capital. Total factor productivity kind of fed on itself, because if you have better technology, you can produce even further advances in the future. And the capital stock rose because we used part of our output for investment and accumulated more of it. So this right column captures what is driving growth during the industrial age.</p><p>Now what would happen if we do experience AGI, and what are the potential dynamics for an intelligence explosion?</p><p>First of all, we would suddenly have all this AI labor that can perform tasks that previously were reserved for human labor. Having all this additional labor will immediately allow our output to jump up. In addition, a lot of that labor can be dedicated to advancing technology‚Äîto increasing total factor productivity.</p><p>Now let's unpack what drives the increase in AI labor. There are two forces: the first one is advancing software quality, and the second one is advancing hardware quality, plus hardware accumulation. These two can feed back on each other and imply that essentially the available amount of AI labor is going to grow very rapidly, therefore drive these increases in productivity and increases in output.</p><h3><strong>Important Factors to Consider</strong></h3><p>I want to add a little bit more texture and discuss a few factors that I think are important to keep in mind if we dig a little bit more into the weeds.</p><figure class="image image_resized" style="width: 768px;"><img alt="Output Effects: Adding More Texture - Three further considerations: 1. Bottlenecks = 'weakest links' in the chain of production (may hold back economic growth and disruption), 2. Cognitive and physical work are complements (In short term, AGI will raise the value of physical actuators), 3. In the long term, the ultimate bottleneck is unclear ‚Äì E = mc¬≤?" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/ivspyxmliqwkufd3vz7h" /></figure><p><strong>Bottlenecks.</strong> In some ways, you can say bottlenecks are like the weakest link in the chain of expanding output. They are what's holding us back from producing more. The simplest story is like the O-ring story: if you need to follow 100 steps to produce something but you can only automate 99 of them, and the hundredth relies on some bottleneck, then you cannot produce more because that bottleneck is always going to hold you back.</p><p>In practice, it's not going to be as stark. Bottlenecks can be to some extent substituted for. If you don't have enough energy, you can focus on developing technologies that do the same thing with a little bit less energy. So you can get around bottlenecks. But still, the more bottlenecks there are‚Äîand many of them we're probably not aware of yet‚Äîthe more that will hold back economic growth.</p><p><strong>Cognitive versus physical work.</strong> This is a really important distinction that is oftentimes conflated in this debate. For all those of you with aggressive timelines, your timelines are probably at first about cognitive advances‚Äîabout the fact that AI may take off on the intelligence side. But that may or may not give rise to physical automation. And even if we have the physical automation, we also have to produce a lot of machines, a lot of robots to take full advantage of that.</p><p>I'll go back again to OpenAI's charter. They wrote about basically automating the majority of economically valuable tasks. Now, the majority of economically valuable tasks is actually physical, or involves at least some physical component. It's only‚Äîdepending on how exactly you measure it‚Äî10 to 20% of the economy that is purely cognitive. I guess many of us are in that segment, and that's why we can feel it acutely.</p><p>But still, the majority of jobs has an important physical component. If we only have cognitive intelligence at human or superhuman levels, that doesn't mean that we should expect a dramatic growth takeoff. It only means that we can do these 10 or 20% of the economy much more efficiently. What that means is: if that happens, there's going to be so much economic value in automating the physical parts as well, so we should expect a lot of investment flowing into that.</p><p><strong>Progressive bottleneck relief.</strong> You can think of the long trajectory of economic development as progressively relieving bottlenecks. The industrial revolution relieved the bottleneck of land but then introduced the bottleneck of labor. Now we may be on the verge of relieving the bottleneck of labor, and it's not quite clear what will hold back growth after that. It may be energy. It may be the availability of certain rare earths. I don't know‚Äînone of them seems like the obvious predominant one, especially if we are really close to fusion or things like that. Ultimately‚Äîand I'm listening to people like Anders when I say something like that‚Äîmaybe it's just going to be energy and matter within our event horizon. But that's kind of beyond my expertise. It's a huge economic question, because whatever is the bottleneck will be the most valuable.</p><h3><strong>Confusions Between Economists and Technologists</strong></h3><p>I want to talk about some confusions that sometimes occur in the debate between economists and technologists.</p><p><strong>Productivity versus prices.</strong> Economists always like to talk about productivity, but technologists oftentimes talk about prices going down. In some sense, that's actually just using different language for the same thing. I remember Sam Altman wrote this blog post <a href="https://moores.samaltman.com/">"Moore's Law for Everything"</a> in which he suggested the prices of everything are going to halve every two years in accordance with Moore's law.</p><p>We economists don't really measure things in dollar terms when we talk about GDP‚Äîwe measure them in real terms, adjusted for price changes. So if you say prices go down by half every two years, what you may be meaning is: we can produce the same dollar amount but at half the price, which would correspond to we can produce twice as much every two years‚Äîwhich would be a growth rate of like the square root of two, 41% or something like that a year.</p><p>Those two statements are economically equivalent. Economists prefer talking about the growth rate adjusted for prices. Halving costs means essentially doubling productivity. And one of the reasons is because prices ultimately are a unit of account. It doesn't really matter whether you say my economy is $1 trillion big or 100 trillion yen big‚Äîit's the same, you just convert things. People in Japan are not wealthier because $1 is 100 yen. These are just units of account, and we want to adjust for that to measure the real effects of economic growth.</p><p><strong>Post-scarcity.</strong> This is maybe more of a very strong opinion on my side than a confusion. I think in part inspired by Star Trek and so on, a lot of people talk about a post-scarcity economy. If they use that term to just say an economy that will be a lot wealthier, in which there will be a lot more abundance, then I'm okay with that.</p><p>But technically, we economists call a resource "scarce" whenever it has a nonzero price. Even if something is really cheap, it meets that definition. Now, if AI takes off in a good way and produces all this abundance for us, resources will still have nonzero prices. There will be a lot more of them, but they will still be valuable, and the relative value of different resources is going to be reflected in their relative prices. So I think it makes more sense to talk about material abundance than about post-scarcity.</p><figure class="image image_resized" style="width: 768px;"><img alt="Clearing Up Some Confusions: In the language of economists, we describe growth in productivity, output, wages in cost-adjusted ('real') terms; halving costs = doubling productivity; ultimately, only relative prices matter. Scarcity = something has non-zero price, even if the price is very, very low; material abundance ‚â† 'post-scarcity'" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/jv3sddort7cv4q0jm29k" /></figure><p>One thought experiment that's highly useful in this context: imagine we were having this conversation in 1800, and we're going to talk about a post-Malthusian economy because some people are very foresightful and are seeing the writings of the industrial revolution on the wall. Imagine you told people, "Okay, every one of you is going to be 20 times wealthier in 200 years from now." People would say, "Well, that's unbelievable‚Äîit's inconceivable."</p><p>In some sense, you could say we almost already live in this post-scarcity economy compared to the conceptions that people had in 1800. But for a lot of people, it doesn't quite feel like post-scarcity. Prices are still positive, and what matters are their incomes relative to those prices.</p><h2><strong>The Bad: Labor Market Disruption and Inequality</strong></h2><p>That brings me to the bad‚Äîthe potential labor market disruptions and inequality effects. Again, what will drive these? What will they depend on?</p><p>Here I won't start by citing Daron Acemoglu, but I will put up <a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic">an interview by Dario Amodei that he gave with Axios a couple of months ago</a>, in which he warned that AI could wipe out half of all entry-level white collar jobs and spike unemployment to 10 to 20% in the next‚Äîthere's a wide confidence interval‚Äîone to five years. So this is what an industry insider describes as the potential labor market effects.</p><h3><strong>The Economic Channels</strong></h3><p>Let me look at what are the economic channels that would drive this disruption.</p><p>The first one‚Äîand I should say that I'm talking about the channels that drive the effects on the labor market, because some of these effects are positive‚Äîis just technological progress, which has a tendency of lifting all boats, of making everybody more productive. If I just give you ChatGPT or Claude or Gemini, you are more productive as a worker, and that makes your labor more valuable.</p><p>The second one, which is the hugely disruptive one, is the automation of labor. Again, if a machine can do a worker's job, the worker's wage will tend towards the machine's cost.</p><p>And the third one, which is again positive, is capital accumulation. If I give you better machines to work with, then your labor becomes more valuable.</p><p>So what this tells us is there are three main channels. Two of them are actually positive, but one of them is negative. Ultimately, there is a horse race going on between the positive and negative effects. In the short term, it is plausible that a lot of workers are going to benefit from the positive effects. But then if we reach the full AGI as in this definition here, I think it is very likely that the negative effects are going to predominate.</p><h3><strong>Adding More Texture</strong></h3><p><strong>Task displacement versus job displacement.</strong> Right now we are talking about task displacement rather than job displacement. Right now, there are very few jobs that can be wholly done by AI, and a lot of the economic work in this area is on which tasks can be replaced, not which jobs can be replaced.</p><p><strong>Labor demand, not just jobs.</strong> A second point I want to emphasize: when you read the newspaper, you often hear about what will be the jobs impact of AI. But the more interesting and more useful question from an economic perspective is: what will be the effects on labor demand?</p><p>The reason I say that's more useful is that we think of the labor market as an equilibrium driven by both demand and supply. What usually happens‚Äîand what has been happening for the past 200 years‚Äîis the supply of labor has been pretty much fixed. Essentially, every working-age adult nowadays, or the vast majority who are not busy for family reasons, are supplying their labor to the labor market. That means supply is pretty much inelastic, as we say. So supply is fixed, but labor demand is what fluctuates when technology fluctuates.</p><p>If you reduce labor demand but you have a fixed supply, what happens in equilibrium is that wages actually bear the brunt of adjustment. In the very short term, there's some job displacement. But as the economy re-equilibrates, wages go down, and the total number of jobs is not materially different from before the shock‚Äîbut you can see that wages are at a lower level when you have a negative labor market shock. That means focusing just on the job numbers may be a useful short-term guide, but talking about medium- to longer-term developments, we have to focus on wages, not just jobs. And that's captured by essentially the relationship that we call the demand curve, which captures at which wage will employers hire how many workers.</p><h3><strong>Simulation Results</strong></h3><p>Let me show you a simulation from <a href="https://www.nber.org/system/files/working_papers/w32255/w32255.pdf">a paper on scenarios for the transition to AGI</a>, in which we trace out how the fraction of automated tasks will affect both the wage bill and the returns to capital.</p><figure class="image image_resized" style="width: 768px;"><img alt="Labor Market Effects of AGI: Graph showing Output Y, Wage Bill wL, and Return on Capital RK against Fraction of Automated Tasks. Wages rise until ~80% automation, then plummet as returns shift to capital." src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/oe6kckl6gdx2omqqgvnp" /></figure><p>See Korinek and Suh, "Scenarios for the Transition to AGI," NBER, May 2024</p><p>What you can see is: if you automate, and if capital and labor are complements, at first labor becomes more and more valuable because it makes the economy more productive to use machines for tasks that previously required very scarce labor. That means at first, almost all of the benefits go to labor.</p><p>But then, after a specific threshold‚Äîhere in this simulation it's like 80% automation‚Äîall of a sudden, the abundance of capital and the fact that we need workers only for very few remaining jobs implies that wages plummet and that all the returns suddenly go to capital.</p><p>This is one specific simulation and specific parameter values, but I want to put it up just to show the possibility that as we automate, for a long time we'll see positive effects on labor, and then they may suddenly flip.</p><p>And now I'll show you the counterpart to the output effects of growth on the wage front. The baseline was that wages and output kind of grow in tandem, but if we have AI in 20 years or five years, then you can see that wages at first rise faster, and then they plummet.</p><figure class="image image_resized" style="width: 768px;"><img alt="Two charts showing Output and Wages over 30 years. Chart 1 (Output): Traditional growth is linear, Baseline AGI accelerates around year 20, Aggressive AGI accelerates sharply around year 10. Chart 2 (Wages): Traditional growth is steady, but both AGI scenarios show wages initially rising then plummeting." src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/yheuly2sx07xpx8lej0b" /></figure><p>Source: Anton Korinek. Note: AGI = artificial general intelligence.</p><h3><strong>Where Does the Value Go?</strong></h3><p>If labor is devalued, the big question is: where does the value go? Because the value doesn't disappear. If we can do something more cheaply‚Äîlet's say using an AI that costs a hundredth of the cost of a worker right now‚Äîthat means the economy is still producing the same thing. And the value goes, depending on how the economy is arranged, either to workers or capitalists, or frankly a lot of it just goes to consumers. Which is actually good, but it also highlights the importance of distributive policies.</p><h3><strong>Managing the Transition</strong></h3><p>That's where I want to go next. Managing the transition is going to be particularly hard. There will be big winners and big losers on the road to AI. The big winners never want to share what they won. The big losers always want to be compensated and don't want to lose.</p><p>There will be an important role for steering technological progress‚Äîmaybe also a role for slowing down progress, because having so many winners and losers is hugely disruptive.</p><p>There are many longer-term policies that are being discussed: UBI, UBK, job guarantees, compute slices. <a href="https://www.brookings.edu/wp-content/uploads/2022/08/2022.08.10_Korinek-Juelfs-Final.pdf">Right now in the economic policy debate</a>, they all seem outlandish. Nobody's taking these seriously except on the fringes of the political spectrum‚Äîwhich is too bad, if we are expecting these disruptions to happen very quickly.</p><p>And of course, there are very important non-economic forces as well: what does this all mean for meaning, for control, for agency, and so on.</p><h3><strong>Adjusting the Tax System</strong></h3><p>In <a href="https://www.nber.org/books-and-chapters/economics-transformative-ai/public-finance-age-ai-primer">a recent paper</a>, I look at how our tax system will have to adjust under AGI. Right now, you can say that taxes on labor œÑ<sub>L</sub> are the primary way of raising revenue. But if labor suddenly becomes devalued, our government is not going to have a lot of financial resources. That means in the post-labor economy, we have to switch at first to the taxation of consumption œÑ<sub>C</sub>, and then ultimately to the taxation of the capital accumulation of AI itself.</p><figure class="image image_resized" style="width: 768px;"><img alt="Public Finance in the Age of AI: Table showing role of tax instruments (Labor, Consumption, Capital) across evolving AI scenarios from current economy to AGI-centered economy" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/lugvxxr9hnmh8vctj25i" /></figure><p>See Korinek and Lockwood, "Public Finance in the Age of AI," NBER Economics of TAI, Nov 2025.</p><h3><strong>Roles for Labor in the Post-AI Economy</strong></h3><p>There will be some roles left for labor in the post-AI economy. Some of them transitional, some of them for fundamental human-centric purposes. But from my perspective, it is unlikely that the importance and the share of labor in overall economic value will be anywhere near where it is right now.</p><figure class="image image_resized" style="width: 768px;"><img alt="Roles for Labor Post-AGI: Temporary Technical and Social Barriers (Production and diffusion lags, Implicit knowledge, Trust and perceived human superiority, Laws and regulatory reasons) vs Fundamental Human-Centric Aspects (Authenticity of human connection, Human identity in sports and arts, Religious beliefs and practices, AI alignment and oversight)" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/ftqndrxjtteltfb9lhpr" /></figure><h3><strong>Clearing Up Confusions</strong></h3><p>Again, I want to clear up some confusions in the debate.</p><p><strong>There is no "economic law" that new technologies always create jobs.</strong> In fact, it's just the law of demand and supply that as long as people are willing to supply their labor at any price, the market is going to hire them. And sometimes the price needs to go down to clear the market. Sometimes the price goes down a lot, as people during the disruptions of industries in the Midwest experienced in the past few decades.</p><p>There is no fundamental law that labor always plays some sacred role in the economy. It was just an empirical regularity for the past 200 years.</p><p><strong>What ultimately matters is relative prices.</strong> Even if the price of everything declines‚Äîor some other form of what Sam Altman predicted in <a href="https://moores.samaltman.com/">"Moore's Law for Everything"</a>‚Äîwhat ultimately matters for workers is how fast the price of their consumption goods declines compared to their wages.</p><p>There are a lot of reasons to expect that wages are going to go down faster than energy prices. And a lot of the things that we consume to keep ourselves alive, like food, require a lot of energy. So I think even if you have a strong belief that everything will become cheaper, labor is going to get cheaper even faster.</p><h2><strong>The Ugly: Risks and Alignment</strong></h2><p>Last point: the ugly‚Äîthe risks of AGI and alignment.</p><figure class="image image_resized" style="width: 768px;"><img alt="The Ugly: TAI Risks and Alignment - Main economic challenge: Trading off E[cost of doom] vs E[benefit of abundance and longer lives]; Vast externalities: the benefits and costs of TAI risks are not borne equally" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/epFKhn24trRP2cs3k/t13zln866m3hcfmzhh4k" /></figure><h3><strong>Economic Aspects of Alignment</strong></h3><p>First, I want to observe that there are a number of really important economic aspects to alignment.</p><p>People need material resources to survive. We may be able to do things more efficiently and so on, but we need some sort of income.</p><p>Something else that's inherent in human preferences: people don't like uncertainty‚Äîcertainly not really big uncertainty. People don't like big inequality. People don't like excessively rapid change, in part because it entails a lot of uncertainty.</p><p>I think we need to consider all these factors when we want to align AI to human values, because these basic economic preferences are part of our human values.</p><h3><strong>The Central Economic Challenge</strong></h3><p>Ultimately, the main economic challenge in this debate is‚Äîto put it kind of in the brute and cold-blooded economic fashion‚Äîtrading off the expected costs of disaster versus the expected benefits that AI may deliver, both in terms of abundance and in terms of things like longer lives.</p><p>In some sense, you can see that lives are both on the left-hand side and on the right-hand side of this tradeoff. If the AGI kills us, that's a cost. But if it makes us all live for hundreds of years, that's also a benefit in terms of lives‚Äînot just in terms of pure abundance.</p><p>But I think the central part is: there are vast externalities in who makes these tradeoffs. Right now, it is a handful of executives in a handful of very powerful organizations that are making the decision on behalf of all of humanity. And their decisions entail vast externalities‚Äîvastly asymmetric payoffs. If they succeed, they will pocket a huge amount of the benefits. If they lose, all of us are going to pay the costs. It's kind of the classical definition of an externality, and I don't think that's good.</p><h3><strong>Does the Economy Need Humans?</strong></h3><p>Let me clear up a confusion that you sometimes hear in this debate: Does the economy need humans? Does it need human demand?</p><p>No. For the economy, it is perfectly possible to have an economy that's driven only by the machines‚Äîto paraphrase Lincoln, to have an economy of the machines, by the machines, and for the machines. We don't need to be there for the economy to function.</p><p>But‚Äîsorry, <a href="https://www.cs.toronto.edu/~duvenaud/">David</a>, you said we should not impose values‚ÄîI do think that's not what we want.</p><p><i>Thank you.</i></p><br /><br /><a href="https://www.lesswrong.com/posts/epFKhn24trRP2cs3k/the-economics-of-transformative-ai#comments">Discuss</a>

---

### [Self-Help Tactics That Are Working For Me](https://www.lesswrong.com/posts/LQCjTFGxCu5G4bs9f/self-help-tactics-that-are-working-for-me)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 8, 2026 6:00 PM GMT<br /><br /><div><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/$s_!O-r2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84cf5ed-8aa1-4f38-a5bf-b07836a0094f_1024x1024.webp" target="_blank"><div><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LQCjTFGxCu5G4bs9f/ositaeej4jps1qliankh" /><div><div></div></div></div></a><figcaption>Midjourney, ‚Äúabstract composition on the theme of triumph, with trumpets and rays of light, Art Deco, Tamara de Lempicka, Benedetta Cappa, red and gold, heroic celebration epic pride‚Äù</figcaption></figure></div><p></p><p>In about September 2025, I decided to go on a self-help kick, mostly revolving around stuff like ‚Äúself-esteem‚Äù, ‚Äúconfidence‚Äù, and ‚Äúguilt/shame‚Äù. </p><p>Basically, I wanted to <em>no longer feel bad about myself (when unwarranted).</em></p><p>This has been working really well. In fact, I almost want to say I‚Äôm ‚Äúdone‚Äù, though I think it‚Äôs probably too early to be sure.</p><p>I wanted to log what‚Äôs been working for me, in case any of it is helpful or generalizable to other people. </p><p>Feel entirely free to skip this post if you‚Äôre not interested in the gooey details of my psyche.</p><p>On the other hand, I don‚Äôt actually believe that self-help is a shameful or uninteresting topic. How to handle your own psychology is a huge part of doing <em>anything</em>. And while my Substack isn‚Äôt mostly about me, it <em>is </em>something of a record of the things I‚Äôve been interested in or working on at any given point, to the extent I‚Äôm free to talk about them in public. Self-help has been one of my ‚Äúprojects‚Äù lately, and I finally feel like I have justified, grounded opinions worth sharing about what works and what doesn‚Äôt (for me).</p><h3>Thingness</h3><p>A crucial heuristic I use for self-help is ‚Äúthingness‚Äù, which is a bit hard to explain. </p><p>‚ÄúHave you really had a breakthrough or not?‚Äù is a tough question ‚Äî <a href="https://chrislakin.blog/p/flaky-breakthroughs">sometimes the feeling of ‚Äúbreakthrough‚Äù doesn‚Äôt last</a> and you go right back to your old habits. But it takes a really long time to assess that you‚Äôve made publicly demonstrable, long-term progress. So what are some <em>early </em>signals that something is fruitful? </p><p>Introspectively, I need to experience some kind of discrete, unambiguous change; a <em>new thought</em> <em>or state</em>, a place I can tell I‚Äôve literally never been before in my life. ‚ÄúChange‚Äù doesn‚Äôt imply ‚Äúpermanent change‚Äù, but lots of self-help doesn‚Äôt even pass the sniff test of ‚Äúwhether you can prove it to a third party or not, but being 100% honest with yourself, did you feel anything clear-cut <em>happen </em>inside you?‚Äù</p><p>For instance, I don‚Äôt believe <a href="https://en.wikipedia.org/wiki/Internal_Family_Systems_Model">IFS</a> has ‚Äúthingness‚Äù, at least not for me. I went into that trance state, came up with a bunch of ‚Äúparts‚Äù, wrote them down, and then even a few months later looked back at my writing and was like ‚Äúwhat? heck no, i don‚Äôt have those as sub-personalities‚Äù. Stable sub-personalities just don‚Äôt seem like ‚Äúnatural categories‚Äù for describing what‚Äôs going on inside me, and nothing particularly changes or improves when I try on that conceptualization. There‚Äôs no ‚Äúthingness.‚Äù</p><p><em>Usually</em>, ‚Äúmere intellectualization‚Äù doesn‚Äôt have that property of ‚Äúthingness‚Äù. It‚Äôs ‚Äújust‚Äù words and ideas and it doesn‚Äôt click into your more basic emotional, behavioral, or motivational systems. But I‚Äôve found that if you go back and forth between intellectually analyzing and emotionally/bodily ‚Äúfeeling into‚Äù ideas, then even rather abstract concepts can create that discrete ‚Äúclick‚Äù of change, that chunky ‚Äúthingness‚Äù.</p><p>Years ago, I used to think ‚Äúgoals‚Äù were also sort of a ‚Äúwrong concept‚Äù for what was going on inside me ‚Äî too abstract, too removed from the roiling chaos of raw sensation and semi-raw impulse, and thus talking about ‚Äúmy goals‚Äù would always be in some sense fake ‚Äî but now I‚Äôve changed so that goals are actually closer to being <em>bona fide parts of my inner machinery</em>, so goal-oriented paradigms work much better for me. They ‚Äúhave thingness‚Äù for me now, though they didn‚Äôt earlier. </p><p>The following insights and tactics, which came in part from me, in part from talking to a lot of (both customized and default) LLM bots, and in part from talking to a few friends, are, I‚Äôm pretty confident, ‚Äúactual things‚Äù in the sense I‚Äôve been talking about. How big and how long-term an impact they‚Äôll have on my life remains to be seen, but I‚Äôm optimistic.</p><h3>Insight #1: (Irrational) Guilt Is Actually Due To Fear Of Facing Someone‚Äôs Disapproval</h3><p>This sounds pretty obvious, but it‚Äôs not. It‚Äôs a claim about what‚Äôs the <em>cause </em>and what‚Äôs the <em>effect</em> (and therefore what‚Äôs the right point to intervene).</p><p>One could imagine an equally plausible statement that reverses the cause and effect  ‚Äî ‚ÄúYou only feel afraid of other people‚Äôs opinion because you have low self-esteem‚Äù ‚Äî but I think that, at least for me,  that would be false. </p><p>The causality definitely goes {fear of a bad social reaction} ‚Üí {beating up myself preemptively, as a defense mechanism}.</p><p>Why am I so confident of this? </p><p>Well, first of all, the fear is a very concrete, physical flinch reaction. I get startled easily when someone comes in the room and ‚Äúcatches‚Äù me goofing off, for instance. That feels very concrete and basic, close to the ‚Äúbare metal‚Äù of the body and mind. Whereas ‚Äúself-esteem‚Äù seems a lot more complex and cognitive, and a lot more unclear what it even means. </p><p>Secondly, I can notice a certain ‚Äúsoothing‚Äù effect from self-condemnatory thoughts like ‚ÄúI‚Äôm a terrible person.‚Äù I feel a tension, which is relieved when I denigrate myself. That‚Äôs suggestive that my ‚Äúlow self esteem‚Äù is a <em>strategy</em> that I instinctively reach for to solve some other problem, rather than a ‚Äúprimary‚Äù phenomenon.</p><p>This is great news, because it suggests a <em>tactic.</em></p><h3>Tactic #1: DIY Exposure Therapy for Social Disapproval</h3><p>There‚Äôs a known, evidence-based way to deal with phobias: graded exposure therapy! You start with the <em>mildest </em>version of the thing that triggers fear, and then sit with it until the fear goes away.</p><p>For me, this means <em>vividly visualizing the experience of the person in front of me showing disapproval of the thing I say or do</em>. </p><p>I‚Äôm pretty sensitive about social disapproval, so even simulated/imagined ‚Äúexposure therapy‚Äù is productive for me. I can get a lot out of going through a mental ‚Äújourney‚Äù where I imagine that someone whose opinion I care about has a negative reaction to what I‚Äôm doing, and simulate how I‚Äôd react and what would happen afterward, and find that it turns out okay.</p><p>‚ÄúMere‚Äù imagination is a bigger deal than most people think. Vivid, wholehearted imagination can do a lot of work towards allowing me to do new things in real life. I‚Äôve been able to have conversations I was afraid of, simply by imagining them vividly ahead of time.</p><p>A real-life version of this exposure therapy I‚Äôve also found useful is just making myself look at another person‚Äôs face, look into their eyes, and become highly aware of whether they seem to be ‚Äúokay with me.‚Äù </p><p>Are they happy I‚Äôm here, with them, right now? Do they like what‚Äôs going on in the conversation? </p><p>The answer could be no! </p><p>Would I be able to mentally tolerate it, to not flinch away, if the truth was ‚Äúactually I‚Äôm impatient for you to go/annoyed by what you just said/not a big fan of yours in general‚Äù? </p><p>The handful of times I‚Äôve gotten into a state where I‚Äôm <em>really </em>prepared to see the truth whatever it may be ‚Äî not defending against the worst-case scenario or bracing for it or distracting myself from it, but actually okay with it ‚Äî it‚Äôs a strange, intense, hyper-present altered state, and <em>really </em>conducive to heightened intimacy and warmth. </p><p>It‚Äôs not possible to fully feel that someone likes<em> </em>you unless you are fully open to the possibility that they <em>don‚Äôt</em>. Once that ‚Äúfluttering‚Äù of anxiety ‚Äústabilizes‚Äù, once the flinchy zapping around of your mind settles down into calm, some very odd (and good) things start to happen.</p><p>I actually haven‚Äôt done this tactic that often and I think there‚Äôs still a lot of room for improvement, but the experiences I‚Äôve had have convinced me that it‚Äôs fruitful. </p><h3>Tactic #2: Positive Exposure Therapy</h3><p>There are some things ‚Äî often they‚Äôre compliments people have given me ‚Äî where thinking about them gives me a sense of mingled fear and excitement. Sort of like ‚Äúit can‚Äôt be!!!‚Äù vibrating against ‚Äúbut wouldn‚Äôt it be wonderful???‚Äù </p><p>You can do exposure therapy against these things too. It‚Äôs an unusually fun and rewarding process, because you‚Äôre increasing your tolerance for an intensely <em>pleasant </em>stimulus. The end result is you keep the excitement but extinguish the fear.</p><p>You just keep going back and forth between your fearful ‚Äúbut it can‚Äôt be this good!!!‚Äù reaction and a calm ‚Äúyes, it can, because XYZ, this is actually perfectly normal and comprehensible‚Äù perspective, until you can sort of hold both at the same time, and you watch the fear burning itself out like a guttering candle.</p><p>If you ever have a ‚Äúspecial interest‚Äù that gets you <em>so </em>excited that you get overwhelmed and can‚Äôt talk about it, and you want to be able to ‚Äúbe normal‚Äù about it without losing your enthusiasm, that‚Äôs another candidate for positive exposure therapy. </p><p>Among other things, positive exposure therapy got me to:</p><h3>Insight #2: It Is Good That I Have Wants/Desires/Choices/Opinions/Etc</h3><p>One of the things I‚Äôve historically felt bad about is that I‚Äôm not particularly self-effacing; I want stuff, I go after what I want, I have a lot of opinions, etc. I‚Äôve been accused that ‚ÄúYou just do as you damn well please,‚Äù and it‚Äôs not wrong. </p><p>And in the past, a big part of me would feel ‚Äúit‚Äôs impossible for me to be a good person; I‚Äôm tainted with this willful<em> </em>nature!‚Äù </p><p>On the other hand, you could also describe this trait as <em>motivation</em>. Motivation, arguably, is essential for goodness. Anything valuable or worthwhile in this world is ultimately produced by people who are personally motivated towards it. So there can‚Äôt be anything wrong with the mere fact of having one‚Äôs own motivations!</p><p>And I have met people who thought my<em> </em>motivation was an admirable trait.</p><p>On some level I even have to admit that I <em>like </em>my will and always have. It is me, after all; how could I not?</p><p> So a few rounds of back-and-forth with positive exposure therapy got me comfortable with the idea that I‚Äôm a person with will/motivation/desire/etc and that this is a good thing. </p><h3>Tactic #3: Positive Visualization</h3><p>I got this one from my friend Stephen Long, who‚Äôs a <a href="https://www.fullyknown.live/">life coach.</a></p><p>Before you do a high-stakes or difficult thing, you literally visualize it going as well as possible. Vividly. In detail.</p><p>This helps a lot with coming into meetings with the kind of attitude that makes them more likely to go well. </p><p>It also helps with motivation issues. When I procrastinate, often it‚Äôs simply because I <em>haven‚Äôt taken a moment to think through how I want/intend the thing to go</em>, or because I anticipate <em>something going wrong</em>. (‚ÄúI‚Äôll show up at the DMV and there‚Äôll be some kind of mysterious bureaucratic snag that keeps me from getting my ID card‚Äù ‚Üí I never get around to going to the DMV). If instead I visualize ‚ÄúI‚Äôm going to do A, then B, then C, and they‚Äôre all going to go smoothly and painlessly‚Äù then I often find I‚Äôm eager to get started.</p><p>Positive visualization isn‚Äôt magical thinking like ‚Äúif you imagine winning the lottery, you will win the lottery.‚Äù</p><p> It‚Äôs more like <em>planning</em>, but in the sense of ‚Äúmotor planning‚Äù, the kind of vivid inner simulation that connects directly to behavioral output. </p><p>I find it entirely believable, for instance, that if a basketball player vividly imagines shooting the ball into the hoop, he‚Äôs more likely to make the next basket. He has real-life experience with both making and missing baskets, and he‚Äôs choosing to simulate (and prepare for) the motor path that <em>in his experience actually leads to making more baskets.</em></p><p>Likewise, if I vividly visualize a meeting going well, I‚Äôm visualizing myself behaving in ways that, in the past, I‚Äôve behaved when meetings <em>did </em>go well. If I visualize myself going to the DMV, I‚Äôm drawing from experiences when going to a new location and filling out forms <em>did </em>go smoothly. If you do it in a <em>well-grounded</em> way, derived from your own real-world experience (and vividness will help with that), imagining good outcomes <em>is </em>effectively planning to behave in ways that, in your experience, tend to lead to good outcomes.</p><p><a href="https://read.isabelunraveled.com/p/manifest-rationally">This post</a> is also a good woo-free explanation of why ‚Äúmanifestation‚Äù can work. People help you more, and you are more motivated and creative, when you know what you want and focus on it. </p><p>Positive visualization is arguably the generalization of both ‚Äúpositive‚Äù and ‚Äúnegative‚Äù exposure therapy; if you‚Äôre habitually too pessimistic, you force your imagination to engage with the <em>possibility </em>of things being really good ‚Äî or the possibility that they‚Äôll go ‚Äúwrong‚Äù in some way you‚Äôre afraid of, but that you‚Äôll survive just fine.</p><h3>Insight #3: The God‚Äôs-Eye View Is Fake; Only The First-Person Perspective Is Real</h3><p>In order to fix emotional and practical problems, sometimes I think you have to venture into the philosophical realm.</p><p>I‚Äôd historically had a problem with being overly dependent on external validation (praise and criticism). And when I introspected about it, it was because I didn‚Äôt feel I had access to what was ‚Äúreally‚Äù good or not, but other people might. It was if I were trying to peer into some invisible realm that was akin to ‚Äúwhat God‚Äôs opinion would be‚Äù or ‚Äúthe universe‚Äôs-eye view‚Äù:  an inherent, eternal judge of what ‚Äúcounted‚Äù as sufficiently good or not. But I couldn‚Äôt see into that ‚Äúrealm‚Äù of Platonic essences myself, so I could only hope that other people had access to it. </p><p>Now, if you don‚Äôt believe in the supernatural, and you don‚Äôt believe that other people are somehow fundamentally or ontologically superior to yourself, then this doesn‚Äôt make any sense. </p><p>Why should someone else‚Äôs opinion of me matter, while my own opinion is meaningless? What have they got that I haven‚Äôt? </p><p>And if ordinary logical thinking is insufficient to access the True Universal Right Answers, why is trying to ‚Äúpeer into the void‚Äù with intuition any better? </p><p>How do I know that <em>anybody</em> has access to the True Universal Right Answers? Or that they even exist?</p><p>But there‚Äôs a real psychological need here, which I don‚Äôt think people can do without, for <em>non-arbitrary, non-trivial, reachable standards.</em></p><p>On some level I wanted my standards to be non-arbitrary ‚Äî for instance, I‚Äôd only want to feel pride if I had really done something worth being proud of. </p><p>On the other hand, no standard <em>I</em> came up with seemed ‚Äúlegit‚Äù enough to qualify as ‚Äúreally worth being proud of‚Äù ‚Äî since it was tainted with <em>my</em> choices, <em>my</em> judgment calls, and of course I‚Äôm fallible.</p><p>But on the other other hand, ‚Äúthen you just never get to fully feel proud of yourself; everything is provisional‚Äù was unsatisfying. There are people, I know, who believe in this sort of eternal dissatisfaction on principle; and I don‚Äôt envy the unhappiness and chronic exhaustion that tends to build up in their lives.</p><p> I‚Äôve always needed a ‚Äúwin condition‚Äù. I need to know it‚Äôs possible, even if only someday at the end of my life, to be confident and secure in the feeling that I‚Äôve done a good job. I‚Äôm fond of triumph, of trumpet fanfares and celebrations, and I need to believe that under <em>some </em>conditions, if I did <em>certain </em>sufficiently awesome things, I‚Äôd have earned a triumphant celebration. A glorious state, free of worry, completely exultant. And, as a more immediately practical matter, I need conditions under which I‚Äôd let myself completely <em>physically</em> relax ‚Äî constant straining literally hurts my back. </p><p>I don‚Äôt think I‚Äôm unique here. People need it to be possible to ‚Äúwin‚Äù, and they need the ‚Äúwin condition‚Äù to be a real challenge that really means something, something they can take seriously as deserving of pride and celebration; if it‚Äôs too much like a ‚Äúparticipation trophy‚Äù it won‚Äôt work.</p><p>But I‚Äôd wound up with some implicit beliefs that made it impossible<em> </em>for me to judge on my own that anything attainable in the real world ‚Äúcounted as a win‚Äù for me.</p><p>So I had resorted to the much <em>less</em> rigorous default ‚Äústrategy‚Äù of feeling elated when other people praised me and deflated when they criticized me, almost regardless of how credible I actually found them.</p><p>The big insight was deciding that I could do better by deciding on some halfway reasonable criteria for what I care about, and then observe empirically whether I‚Äôm meeting my own standards. </p><p>And then, I could just <em>choose not to worry any more </em>about the ‚ÄúGod‚Äôs eye view‚Äù and whether I‚Äôm congenitally ‚Äúblind‚Äù to it. I could just take a first person perspective, a Sarah‚Äôs-eye view, all the time; go about my business, pursue my goals, measure my success by whether I‚Äôm meeting them.</p><p>Sure, I‚Äôm the one picking my goals, so I <em>could </em>have just picked trivial ones.  But <em>I‚Äôm not going to do that</em>. I‚Äôm going to pick goals that reflect my best judgment and whole self, such that I wouldn‚Äôt later on expect to regret ‚Äúletting myself‚Äù fully relax and celebrate after completing them, or ‚Äúletting myself‚Äù fully believe they‚Äôre a big deal. </p><p>‚ÄúYour own best effort at making reasonable judgment calls‚Äù isn‚Äôt infallible, but it‚Äôs clearly <em>better </em>(and more grounded in reference to reality, i.e. <em>genuinely</em> <em>closer to truth and objectivity</em>)<em> </em>than the patterns you‚Äôll fall into if you insist that your own best effort can never be good enough.</p><p>This leads into a new tactic.</p><h3>Tactic #4: Self-Evaluation</h3><p>Yes, it‚Äôs possible to do an honest self-evaluation, that is <em>actually more credible, to you</em>, than pretty much anybody else‚Äôs evaluation of you.</p><p>You evaluate ‚Äúam I a good parent?‚Äù or ‚Äúam I a good person‚Äù or ‚Äúam I brave?‚Äù or ‚Äúam I a good writer?‚Äù with pretty much the same procedure.</p><ol><li><p>Think of some examples, of other people, and decide how you‚Äôd classify them. Would I say that, on the whole, George Washington led a good life? My mother? Think of clear-cut ‚Äúyes‚Äù and ‚Äúno‚Äù examples, and ambiguous cases.</p></li><li><p>Based on these examples, notice what sub-criteria you‚Äôre considering as important to <em>your </em>understanding of ‚Äúa good person‚Äù, ‚Äúa good piece of writing‚Äù, ‚Äúgood parenting‚Äù, etc.</p></li><li><p>Identify what population it seems reasonable to compare yourself to. <br />For instance, as a writer, I‚Äôm comparing myself to the set of people who have blogs, or who write a substantial amount for work; it would be cold comfort to know I‚Äôm a ‚Äúbetter writer‚Äù than a five-year-old or someone who never had access to an education. As a parent, I‚Äôm primarily comparing myself to my friends and acquaintances with kids, my kids‚Äô classmates‚Äô parents, and to some extent my own parents and my peers‚Äô parents. </p></li><li><p>On each sub-criterion, try to think fairly about how you stack up compared to your reference population.</p></li></ol><p>On each of these steps, you‚Äôre necessarily making debatable judgment calls. You don‚Äôt know you‚Äôre getting the ‚Äúright answer‚Äù in the same way you know 2+2=4. It‚Äôs not a turn-the-crank algorithm.</p><p>But it‚Äôs also a structured process. You‚Äôre building a narrative that hangs on a number of facts. You‚Äôre sincerely <em>trying </em>to give a measured best-guess estimate of your performance, and you‚Äôre breaking that down into enough small pieces that you can think about them somewhat concretely. </p><p>It‚Äôs <em>a lot better </em>than ‚ÄúI‚Äôm in a bad mood and I just saw a negative Internet comment so I think I‚Äôm a bad person.‚Äù </p><p>In fact, it‚Äôs probably a lot better ‚Äî stabler, more grounded in knowledge about your life, more consonant with your long-term priorities ‚Äî than most other people‚Äôs impressions of you can possibly be.</p><p>Having done self-evaluations and knowing they‚Äôre possible, I no longer feel at all like there‚Äôs an open question or anything mysterious about ‚Äúam I any good?‚Äù or ‚Äúdo I deserve to be proud of myself?‚Äù I have <em>justified opinions of my own about the matter</em>. </p><p>I can still be surprised by other people‚Äôs evaluations of me ‚Äî e.g. when an expert says I‚Äôm qualified for some kind of job that I had honestly believed I wasn‚Äôt ‚Äî but even when that happens, my old opinion wasn‚Äôt based on <em>nothing</em>, it was just based on a limited set of facts that apparently didn‚Äôt include the full picture. </p><p>I doubt I‚Äôm immune yet to emotional (over)reactions to praise and criticism, but something feels like it‚Äôs fundamentally changed here. Other people are no longer <em>my only source of trusted guidance about how I‚Äôm doing</em>. My closest approximation to ‚Äúground truth‚Äù is, instead, something I build myself. </p><br /><br /><a href="https://www.lesswrong.com/posts/LQCjTFGxCu5G4bs9f/self-help-tactics-that-are-working-for-me#comments">Discuss</a>

---

### [Small Steps Towards Proving Stochastic ‚Üí Deterministic Natural Latent](https://www.lesswrong.com/posts/4q3kMfJHB4rxr3Z8m/small-steps-towards-proving-stochastic-deterministic-natural)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 8, 2026 12:27 PM GMT<br /><br /><h1>The story so far</h1><p>We (Alfred and Jeremy) started a Dovetail project on Natural Latents in order to get some experience with the proofs. Originally we were going to take a crack at <a href="https://www.lesswrong.com/posts/e9KwDDdAxborNSuCd/usd500-bounty-problem-are-approximately-deterministic">this bounty</a>, but just before we got started John and David published <a href="https://www.lesswrong.com/posts/Gd36HT7qLr684SYuQ/stochastic-natural-latent-implies-deterministic-natural?utm_campaign=post_share&amp;utm_source=link">a proof</a>, closing the bounty. This proof involves a series of transformations that can take any stochastic latent, and transform it into a deterministic latent, where each step can only increase the error by a small multiple. We decided to work through the proof and understand each step, and attempt to improve the bound. After 20 hours or so working through it, we found a <a href="https://www.lesswrong.com/posts/cpxxfagD92ivLZCp4/resampling-conserves-redundancy-approximately?commentId=L6dpFjFmgpEtNuycc">fatal flaw</a> in one step of the proof, and spent the next several days understanding and verifying it. With David‚Äôs help, the counterexamples we found <a href="https://www.lesswrong.com/posts/cpxxfagD92ivLZCp4/resampling-conserves-redundancy-approximately?commentId=GTEWrKsD4FgyrxwwE">were strengthened</a> to show that this transformation step had unbounded error on some cases. We started using sympy for arbitrary precision evaluation of counterexamples, and along the way found a <a href="https://www.lesswrong.com/posts/cpxxfagD92ivLZCp4/resampling-conserves-redundancy-approximately?commentId=ojRfmKGrqH5jHEkJM">bug in sympy</a> that occasionally caused us some problems. We switched to Mathematica.</p><p>Since then, we‚Äôve been working part time on trying to prove the bounty conjecture ourselves (or show that the conjecture is false). Ultimately we‚Äôve been unsuccessful so far, but it still feels tantalizingly within reach. Perhaps an automated theorem prover will crack it in the next year or two. It makes a good test case.</p><h2>Empirical evidence suggests the conjecture is true</h2><p>We still think the overall bounty conjecture [(‚àÉ Stochastic Natural Latent) Implies (‚àÉ Deterministic Natural Latent)] is true. In fact, the largest difference in errors that we can find empirically is around 1.82. (i.e. optimal deterministic sum of errors &lt; 1.82 * optimal stochastic sum of errors).</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4q3kMfJHB4rxr3Z8m/mrdd1eodcqsvbe5ugtp5" /></figure><p>In our quest, we made lots of graphs like the above. Each point on the orange and blue curve is the score of a numerically optimized latent distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>, for a fixed&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>. For simplicity, all three of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span></span></span></span></span></span>&nbsp;are binary variables. In this graph, the maximal difference between the blue and orange lines is ~1.82. On the x-axis, Delta represents the correlation between X and Y. For this graph, the marginal distributions are fixed at&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">{</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">0.5</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">0.5</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">}</span></span></span></span></span></span></span>. Changing the marginals has some effect on the graphs, but doesn‚Äôt seem to increase the difference between Stochastic and Deterministic scores.</p><h3>Larger N</h3><p>One way the conjecture could fail is by larger X and Y variables allowing larger differences. This is more difficult to reliably check numerically, because we‚Äôve found there are a lot of local optima for our numerical optimizer to get stuck on, and this problem gets quickly much worse as N increases.</p><p>We did a long running Bayesian optimization outer loop over 3x3&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;distributions, and an inner loop to find the optimal stochastic and optimal deterministic latent. The largest differences found clustered around 1.79 (apart from a few larger ones that turned out to bad local minima on the inner loop).</p><p>So we think there are several lines of evidence suggesting the conjecture is true. 1. We‚Äôve failed to find a counterexample despite putting a lot of time into it, 2. the conjecture seems to be true <a href="https://www.lesswrong.com/posts/JXsZRDcRX2eoWnSxo/resampling-conserves-redundancy-and-mediation-approximately">using Jensen-Shannon divergences</a>, and 3. <a href="https://www.lesswrong.com/posts/e9KwDDdAxborNSuCd/usd500-bounty-problem-are-approximately-deterministic">it‚Äôs true in the 0 error cases</a>.</p><p>We are impressed by how tantalizingly, frustratingly simple this problem is while being so difficult to prove.</p><h1>The Problem</h1><p>The problem that we are trying to solve can be stated as follows.</p><p>Consider two variables X and Y with a joint probability distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>. A latent variable&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span></span></span></span></span>&nbsp;is defined by a conditional probability distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><p>A variable&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span></span></span></span></span>&nbsp;is called a stochastic natural latent if the joint distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;satisfies the three natural latent conditions to within an ‚Äòerror‚Äô&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span>&nbsp;:</p><p>Redundancy 1:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Redundancy 2:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Mediation:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Note: Here we are expressing the conditions as <a href="https://en.wikipedia.org/wiki/Conditional_mutual_information">conditional mutual information</a>, as Aram did in <a href="https://www.lesswrong.com/posts/BueBzKFHqwioS9i3a/stability-of-natural-latents-in-information-theoretic-terms">this post</a>. These are equivalent to the conditions when stated in terms of KL divergences&nbsp;<span class="footnote-reference" id="fnrefr3e4rmgkjce"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnr3e4rmgkjce">[1]</a></sup></span>.</p><p>We want to show that if a stochastic natural latent exists, then there also exists a latent&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span></span></span></span></span></span>&nbsp;(defined by a conditional distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;) whose joint distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;satisfies the ‚Äòdeterministic natural latent conditions‚Äô to within&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span>. The deterministic natural latent conditions are:</p><p>Deterministic Redundancy 1:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Deterministic Redundancy 2:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Mediation :&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>We would like&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&gt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span></span></span></span>&nbsp;to be a reasonably small number (for some definition of ‚Äòreasonably small‚Äô).</p><h1>Upper bounds on deterministic errors</h1><p>One way in which the conjecture could be false is if there existed&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;distributions for which all deterministic natural latents had very large errors (compared to the optimal stochastic latents). It is therefore useful to identify some simple families of deterministic latents and find upper bounds on how large their errors can be.</p><h2>Constant Latents</h2><p>One of the simplest kind of latents we can imagine is the constant latent. This is a latent which only takes one value, regardless of the X,Y values. Notice that in this case, we have&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span>, as well as&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span>&nbsp;so the constant latent perfectly satisfies the two deterministic redundancy conditions. Conditioning on the latent does not affect the&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;distribution so the conditional mutual information<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;simply equals the mutual information between X and Y. This means that the only nonzero error for this latent is the mediation error which is&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><h2>Copy Latents</h2><p>Another kind of latent we can consider is the ‚Äòcopy latent‚Äô where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span></span></span></span></span></span>&nbsp;simply deterministically ‚Äòcopies‚Äô one of the variables (X or Y). This will always perfectly satisfy one of the deterministic redundancy conditions. For example if&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span></span></span></span></span>&nbsp;copies X then&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">X</span></span></span></span></span></span></span>and we have:</p><p>Deterministic Redundancy 1:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span></p><p>The other deterministic redundancy condition will be satisfied to an error equal to the condition entropy&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>:</p><p>Deterministic Redundancy 2:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span></p><p>The mediation condition will also be satisfied perfectly:</p><p>Mediation:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span></p><p>If instead we choose a latent which copies Y so&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">Y</span></span></span></span></span></span></span>&nbsp;then the mediation and Deterministic Redundancy 2 conditions will be perfectly satisfied and the Deterministic Redundancy 1 error will be&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><h2>Deterministic Functions of X,Y</h2><p>We can also consider the family of latents where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;is a deterministic function of the X,Y pair (though not necessarily a deterministic function of X or Y in isolation). All latents in this family satisfy the deterministic redundancy conditions with errors bounded by the conditional entropies of X and Y:</p><p>Deterministic Redundancy 1:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span></p><p>Deterministic Redundancy 2:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span></p><p>A proof of this can be found in <a href="https://www.lesswrong.com/posts/BdLga7gFW4W5Hii34/alfred-harwood-s-shortform?commentId=FBvDLvut5dYxYmA2E">this shortform</a>.</p><p>The worst case upper bound on the mediation is the same as that of the constant latent,&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><h2>General Upper Bound</h2><p>Notice that three of the latents considered above (‚Äôconstant‚Äô, ‚Äòcopy X‚Äô and ‚Äòcopy Y‚Äô) can be applied to any initial&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;distribution. This means that we can always find a deterministic natural latent with maximum error&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>. In general, one of the copy latents will be best for&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;distributions where X,Y are highly correlated (so that&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;or&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;is low) and the constant latent will be best for distributions where (X,Y) are independent or have low correlation (so that&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;is low).&nbsp;</p><p>We have some numerical evidence that in many cases, the optimal deterministic latent has an error very close to&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>, suggesting that this bound is quite tight (for example, in the first graph in this post, the optimal deterministic latent hugs the&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;line perfectly). We know that this bound isn't tight when the&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;or&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;is far from 0.5 (but it isn't extremely loose).</p><p>Unfortunately, proving these upper bounds on the deterministic error is not sufficient to prove the conjecture. In order to prove the conjecture, we need to relate upper bounds on the deterministic error to the error achievable using a stochastic latent.&nbsp;</p><p>To complete the proof this way, an upper bound on the deterministic error needs to be combined with a lower bound on the stochastic error. Numerical evidence suggests that distributions for which&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;is high are also distributions with higher stochastic natural latent errors so this approach seems like a reasonable thing to attempt. For example, take a look at the first graph in this post. The stochastic error is lower than the deterministic error but they both follow similar shapes. In the middle of the graph, the stochastic error and deterministic error both peak at approximately (but not exactly) the same point (around&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0.19</span></span></span></span></span></span></span>) and follow similar decreasing paths as&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span></span></span></span></span></span>&nbsp;moves away from this point.</p><p>If we could prove a lower bound on the error of the stochastic latents of the form&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&gt;</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.566em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>, then we would have proved the conjecture. This is because, for any distribution, we could find a deterministic latent with error&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">n</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span></span></span></span></span></span></span>&nbsp;. So if a latent with stochastic error&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span></span></span></span></span></span></span>&nbsp;existed, we could always find a latent (‚Äôconstant‚Äô, ‚Äòcopy X‚Äô or ‚Äòcopy Y‚Äô) with an error only&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>&nbsp;times greater.</p><h3>Lower Bounding the Stochastic Latent error</h3><p>We tried a couple of ways of lower bounding the stochastic latent error but were not successful. First, we tried assuming that there was a stochastic latent which achieved an error of less than&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.566em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;and seeing if this implied a contraction. We tried to derive a contradiction from these three inequalities:</p><p>Redundancy 1:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.566em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span></p><p>Redundancy 2:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.566em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span></p><p>Mediation:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.566em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">min</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">I</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">;</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">H</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span></p><p>i.e. we wanted to show that satisfying two of these inequalities necessarily meant violating the third (for some distribution over X and Y).</p><p>If we could do this, we would have proved a lower bound on the stochastic error. Unfortunately, we failed, but this does feel like an approach someone could take if they were more comfortable than us at manipulating information-theoretic inequalities.</p><h2>Mixing Latents and Reverse Jensen inequalities</h2><p>Here is another attempt we made at lower bounding the stochastic error.</p><p>Suppose we took two latents, defined by conditional distributions&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;and created a new latent using a probabilistic mixture of the two:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span></p><p>with&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">0</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span></span></span></span>.</p><p>If we applied the mixed latent to a distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>, the joint&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>distribution would be also be a mixture of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><p>We can also consider the refactored distribution&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>, used to calculated the redundancy error:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mtable"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.225em;"><span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: right; width: 1.696em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: left; width: 16.867em;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-strut"></span></span></span></span><span class="mjx-mtr" style="height: 1.375em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow"><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span><span class="mjx-strut"></span></span></span></span><span class="mjx-mtr" style="height: 1.225em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow"><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-strut"></span></span></span></span></span></span></span></span></span></span></span></p><p>Where&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span></span></span></span></span></span>&nbsp;and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span></span></span></span></span></span>&nbsp;indicate the refactored versions of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span></span></span></span></span>&nbsp;respectively.</p><p>This means that the KL divergence for the redundancy error of P is given by:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span></span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mstyle"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span></span></span></span></span></span></span></span></span></p><p>(Note: writing the KL as a mixture in both of its arguments works for both redundancy conditions, but not the mediation condition. So even if we got this proof strategy to work, we would still need to tie up the loose end of the mediation condition.)</p><p>Next, since the KL divergence is <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties">convex in both its arguments</a>, we can use this convexity to write a <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen inequality</a>:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">a</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span></p><p>This is potentially interesting for two reasons. First, we can often (always?) write stochastic latents as probabilistic mixtures of deterministic latents, so if R and S are deterministic latents, then this expression links the stochastic error&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;with the two deterministic errors&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">R</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">r</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;which is almost the kind of thing we are trying to do.</p><p>Unfortunately, this inequality is the wrong way round. We are looking for a lower bound on the stochastic errors in terms of the deterministic error of the form&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 1.931em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span></span></span></span></span></span><span class="mjx-denominator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">o</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">h</span></span></span></span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>. But the inequality from the convexity of the KL divergence gives us an upper bound on stochastic errors instead.</p><p>We hoped that ‚ÄòReverse Jensen Inequalities‚Äô such as the ones found <a href="http://elib.mi.sanu.ac.rs/files/journals/publ/105/n099p107.pdf">here</a> might be helpful. These are ways of bounding the ‚ÄòJensen gap‚Äô between the KL divergences of the mixture and the mixture of the KL divergences. However, when we attempted this, we got results of the form:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">o</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">h</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â•</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">k</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">c</span></span></span></span></span></span></span></p><p>with&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&gt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0</span></span></span></span></span></span></span>. This gives a ratio&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 1.931em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">o</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">h</span></span></span></span></span></span></span><span class="mjx-denominator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">e</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span></span></span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.51em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">k</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 1.931em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span></span><span class="mjx-denominator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">o</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">h</span></span></span></span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>&nbsp;which unfortunately diverges as&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">S</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">t</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">o</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">c</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">h</span></span></span></span></span></span></span></span></span></span></span>&nbsp;tends to zero.</p><h2>Characterizing the optimal stochastic latent</h2><p>If we define&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;using four parameters:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">p</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">p</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">12</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-texatom MJXc-space1"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">p</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">p</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">22</span></span></span></span></span></span></span></span></span></span></span></p><p>and&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;using another four parameters:</p><p>{{{l11,1-l11},{l12,1-l12}},{{l21,1-l21},{l22,1-l22}}}</p><p>where lij is&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">i</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">j</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span></span></span></span>.</p><p>Then we can write out the full error equations in terms of these parameters:</p><p>Mediation error:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 22.192em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 23.138em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 22.192em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 23.138em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 22.192em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 23.138em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 22.192em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 23.138em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 14.68em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 14.57em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 14.68em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 14.57em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 14.68em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 14.57em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 14.68em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 14.57em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span></span></p><p>Redundancy1 error:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 12.171em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 8.966em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 12.171em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 8.966em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 8.294em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 5.539em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 8.294em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 5.539em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l11</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p11</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l21</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p21</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 12.171em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 8.966em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 12.171em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 8.966em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 8.294em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 5.539em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 8.294em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 5.539em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l12</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p12</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">p22</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span></span></p><p>One thing we want here is to characterize the optimal latent (minimizes sum of Mediation error, Redundancy1 error and Redundancy2 error) at any given setting of the&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;parameters. Mathematica can‚Äôt handle this analytically, so we tried restricting the&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>&nbsp;distribution such that both marginals were always 50%. Then the distribution can be parameterized by one variable,&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>, like so:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R">(</span></span><span class="mjx-mtable"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.649em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; width: 2.408em;"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; width: 2.408em;"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-strut"></span></span></span></span><span class="mjx-mtr" style="height: 1.649em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0px;"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0px 0px 0.5em;"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R">)</span></span></span></span></span></span></span></span></p><p>We can plot the numerical optimal latents for each value of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span></span></span></span></span></span>&nbsp;(same as the image at the top of the post, but with each error plotted):</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4q3kMfJHB4rxr3Z8m/gabd1njhu4ulpnlpi1w3" /></figure><p>By looking at these numerically optimal latents, we can also hardcode some of the latent parameters to values that we‚Äôve empirically observed are optimal. When&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&lt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">/</span></span></span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">8</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span></span></span></span></span></span>&nbsp;the latent is 0.5 everywhere (independent of X and Y, so equivalent to the constant latent). When&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">&gt;</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">/</span></span></span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">8</span></span></span></span></span></span></span>, we observed that l12 and l21 are always 0.5, and l11 = 1-l22. These leaves only one parameter free.</p><p>Hardcoding these simplifies each error, so we can get each error as a function of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span></span></span></span></span></span>&nbsp;and l22. For example the mediation error:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 21.443em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.583em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R">‚éõ</span><span class="mjx-char MJXc-TeX-size4-R">‚éú</span><span class="mjx-char MJXc-TeX-size4-R">‚éù</span></span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 19.538em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span></span><span class="mjx-sup" style="vertical-align: 0.82em; padding-left: 0px; padding-right: 0.05em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R">‚éû</span><span class="mjx-char MJXc-TeX-size4-R">‚éü</span><span class="mjx-char MJXc-TeX-size4-R">‚é†</span></span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 18.777em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.583em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R">‚éõ</span><span class="mjx-char MJXc-TeX-size4-R">‚éú</span><span class="mjx-char MJXc-TeX-size4-R">‚éù</span></span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 17.824em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span></span><span class="mjx-sup" style="vertical-align: 0.82em; padding-left: 0px; padding-right: 0.05em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R">‚éû</span><span class="mjx-char MJXc-TeX-size4-R">‚éü</span><span class="mjx-char MJXc-TeX-size4-R">‚é†</span></span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 19.93em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.583em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R">‚éõ</span><span class="mjx-char MJXc-TeX-size4-R">‚éú</span><span class="mjx-char MJXc-TeX-size4-R">‚éù</span></span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 20.733em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mrow"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">l22</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">)</span></span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R">‚éû</span><span class="mjx-char MJXc-TeX-size4-R">‚éü</span><span class="mjx-char MJXc-TeX-size4-R">‚é†</span></span></span></span></span></span><span class="mjx-denominator"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span></span></span></span></span></span></p><p>Note that in this equation and the following one, we use&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span></span></span></span></span>&nbsp;in place of&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span></span></span></span></span></span>.</p><p>Now it‚Äôs simple enough that Mathematica (with some cajoling) can analytically find the l22 (as a function of d) such that the total error is minimized:</p><p><span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">Root</span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">[</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">#</span></span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.631em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">6</span></span></span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4096</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">4096</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">1536</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">256</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">16</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">#</span></span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.631em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">5</span></span></span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">12288</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">12288</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">4608</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">768</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">48</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">#</span></span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.631em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">18432</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">12288</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">3840</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">768</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">72</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">#</span></span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.631em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">16384</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">4096</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">256</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">64</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">#</span></span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.631em; padding-left: 0px;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">8448</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">1280</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">672</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">48</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">17</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mrow MJXc-space2"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">#</span></span></span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">(</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2304</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">1280</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">96</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">48</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">7</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R">)</span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">256</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">256</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">3</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">96</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">2</span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">‚àí</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">16</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">d</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">+</span></span><span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R">1</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">&amp;</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">2</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R">]</span></span></span></span></span></span></span></span></p><p>It‚Äôs the second root of a degree 6 polynomial, whose parameters each contain a degree 4 polynomial. It looks like this:</p><figure class="image image_resized" style="width: 50.66%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4q3kMfJHB4rxr3Z8m/ho8fboqyoabjszj4zxxw" /></figure><p>This is for&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">8</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">4</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">.</span></span></span></span></span></span></span>&nbsp;In&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚àà</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">[</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">0</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">1</span></span></span><span class="mjx-denominator"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R">8</span></span></span><span class="mjx-line"></span></span><span class="mjx-vsize"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">]</span></span></span></span></span></span></span>, l22 simply equals 0.5.</p><p>By analytically calculating the gradient and hessian with respect to the total error, and showing the eigenvalues of the hessian are positive for all&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span></span></span></span></span></span>, we know that every local movement of the latent increases the error, so this latent that we‚Äôve described is definitely <i>locally</i> minimal. We think it is also globally minimal, but haven‚Äôt shown this. Presumably it won‚Äôt be too hard to prove this by going through every other 0 gradient solution and showing that it has a larger error.</p><p>Mathematica can plot the total stochastic error (it‚Äôs missing the first half, but we know that the first half is equal to the optimal deterministic error, because we checked the eigenvalues of the hessian):</p><figure class="image image_resized" style="width: 49.3%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4q3kMfJHB4rxr3Z8m/obwkx5jvehd6tlixizjg" /></figure><p>And using our known values for the optimal deterministic latents, we can plot the optimal deterministic error:</p><figure class="image image_resized" style="width: 49.78%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4q3kMfJHB4rxr3Z8m/jjhjbb7psafow5ohqjpo" /></figure><p>And we can plot the ratio of these:</p><figure class="image image_resized" style="width: 49.78%;"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4q3kMfJHB4rxr3Z8m/xl1mw7exlhjhpmyrf2yh" /></figure><p>And (numerically) find that the peak is ratio=1.81792, {<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Œ¥</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">0.194986</span></span></span></span></span></span></span>}.&nbsp;</p><h2>Summary</h2><p>This isn't the most satisfying conclusion, but we have achieved a partial success. We have (almost) a proof that the conjecture is true for the special case of a 2x2, when the marginals of X and Y are 0.5. And in this case, the ratio&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">n</span></span></span></span></span></span></span>&nbsp;is ~1.82. We failed to find a worse ratio empirically in the general 2x2 case and the general 3x3 case (despite extensive manual effort, and running 10s of hours of automated searches). We think this is moderately strong evidence that the conjecture is true in general.</p><p>But we don't have any good leads on methods to prove this in general.&nbsp;</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnr3e4rmgkjce"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefr3e4rmgkjce">^</a></strong></sup></span><div class="footnote-content"><p>When stated in terms of KL divergences, these conditions are:</p><p>Redundancy 1:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Redundancy 2:&nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p><p>Mediation: &nbsp;<span class="math-tex"><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">D</span></span></span><span class="mjx-sub"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;">K</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">L</span></span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">Y</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">P</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I">X</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R">Œõ</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â§</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I">œµ</span></span></span></span></span></span></span></p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/4q3kMfJHB4rxr3Z8m/small-steps-towards-proving-stochastic-deterministic-natural#comments">Discuss</a>

---

### [HIA and X-risk part 2: Why it hurts](https://www.lesswrong.com/posts/K4K6ikQtHxcG49Tcn/hia-and-x-risk-part-2-why-it-hurts)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 8, 2026 1:19 AM GMT<br /><br /><p><em><a href="https://tsvibt.blogspot.com/2026/01/hia-and-x-risk-part-2-why-it-hurts.html">Crosspost from my blog</a>.</em></p>
<h1>Context</h1>
<p>Previously, in "<a href="https://tsvibt.blogspot.com/2025/11/hia-and-x-risk-part-1-why-it-helps.html">HIA and X-risk part 1: Why it helps</a>", I laid out the reasons I think human intelligence amplification would decrease existential risk from AGI. Here I'll give all the reasons I can currently think of that HIA might plausibly increase AGI X-risk.</p>
<h2>Questions for the reader</h2>
<ul>
<li>Did I miss any important reasons to think that HIA would increase existential risk from AGI?</li>
<li>Which reasons seem most worrisome to you (e.g. demand more investigation, demand efforts to avert)?</li>
<li>Which reasons, if any, are cruxy for you, i.e. they might make you think human intelligence amplification is net negative in expectation? Up for a live discussion / debate?</li>
</ul>
<h2>Caveats</h2>
<p>The world is very complicated and chaotic and I can't plausibly predict even important questions like "what actual effect would such and such have". I can't even plausibly resolve much uncertainty, and the world is full of agents who will adaptively do surprising things. So the actual search procedure is something like: What is a way, or reason to think, that HIA might increase AGI X-risk, that could plausibly hypothetically convince me that HIA is bad to do? This is mostly a breadth-first search, with a bit of deeper thinking.</p>
<p>In particular, many of the reasons listed below, as they are presented, are, according to my actual beliefs, not true or misrepresented or misemphasized. However, that said, this is an attempt at <a href="https://www.lesswrong.com/posts/43PTNr4ZMaezyAJ5o/the-proper-use-of-doubt">True Doubt</a>, which partly succeeded; some of the reasons listed do give me some real pause.</p>
<p>This is a similar project as "<a href="https://berkeleygenomics.org/articles/Potential_perils_of_germline_genomic_engineering.html">Potential perils of germline genomic engineering</a>". As in that case, keep in mind that part of the reason for this exploration is not just to answer "Should we do this, yes or no?", but also to answer "As we do this, how do we do this in a beneficial way?". See <a href="https://berkeleygenomics.org/articles/Genomic_emancipation.html#to-be-sharpened-by-true-criticisms">"To be sharpened by true criticisms" in "Genomic emancipation"</a>.</p>
<h1>What is HIA?</h1>
<p>I'll generally leave it fairly undefined what intelligence is, and what human intelligence amplification is. See "<a href="https://www.lesswrong.com/posts/jTiSWHKAtnyA723LE/overview-of-strong-human-intelligence-amplification-methods">Overview of strong human intelligence amplification methods</a>" for some concrete methods that might be used to implement HIA; those methods suggest (various different) specific functional meanings of intelligence and HIA.</p>
<p>Because we are being imprecise:</p>
<ul>
<li>Critiques of HIA can bring up many possibilities‚Äîe.g. they could claim that HIA would tend to also affect some other trait for the worse.</li>
<li>Defenses of HIA can also bring up many possibilities‚Äîe.g. they could say "HIA is good if done in such-and-such specific way that falls under the general category".</li>
</ul>
<h2>Vague definitions of intelligence and HIA</h2>
<p>Vaguely speaking, by HIA I mean any method for increasing a living human's intelligence, or for making some future people who in expectation have a higher intelligence than they would have otherwise had by default. Generally, we're discussing strong HIA, meaning that the increase in intelligence is large‚Äîimagine 30 or 50 IQ points, so going from average to genius or genius to world-class genius.</p>
<p>Vaguely speaking, by intelligence I mean someone's ability to solve problems that are bottlenecked on cognition (as opposed to physical strength or stamina, or financial resources, etc.). A priori, this could include the whole range of cognitive problem-solving. So, we include stereotypically IQ-style cognitive problems, like math or engineering. But, we also include for example the ability and inclination towards political charisma, wisdom, good judgement, philosophical ability, learning, questioning and attending to something steadily, creativity, good performance under stress, empathy, contributing well to teams, memory, taste, and speed.</p>
<p>On the other hand, we do not include other cognitive traits, such as kindness, agreeableness, emotional valence, emotional regulation, determination, conscientiousness, and so on. These are important traits in general, and it might be good to also give people the tools to influence themselves on those traits (though that might also be fraught due to coercion risks). But this article is focused more narrowly on intelligence, rather than all cognitive traits.</p>
<p>In practice, intelligence refers to whatever we can reasonably easily measure. If a trait is hard to measure, it's hard to increase. (This is indeed a cause for concern, in that only increasing traits that are easily measured could be distortive somehow; the claim under discussion is whether HIA is good even under this restriction.) More specifically, intelligence refers to IQ, because IQ is fairly easy to measure. IQ is far from capturing everything about someone's ability to solve problems that are bottlenecked on cognition. But in this article we take it for granted that IQ is a significant factor in those abilities, and we presume that IQ can be increased.</p>
<h2>HIA as a general access good</h2>
<p>One dimension we will fix is distribution: We will assume that HIA comes in an open access way. In other words, defenses of HIA can't say "we'll only give HIA to the people who are morally good, and therefore there will be a bunch more brainpower directed in a morally good way". That's because a restricted access implementation of HIA seems largely infeasible and also morally and ethically very fraught.</p>
<p>I don't think it's an absolute principle that if you come up with an effective HIA method you have to immediately share it with everyone. But I do think there's a strong moral weight towards doing so; and there's separately a politico-ethical weight (meaning roughly "it's not the sort of thing you should do as a member in good standing of society, even if it's moral, because it would justifiably cause a lot of conflict"). Because of the politico-ethical weight especially, in many scenarios it seems logistically infeasible to do very much selection of who gets access.</p>
<p>This ethical weight towards general access is strongly increased in the case of reprogenetics. Reprogenetics is inherently a multi-use technology, and is already being used by polygenic embryo screening companies to enable parents to decrease disease risks in their future children. This means that society has a very strong justified interest in reprogenetics being equal-access (in the medium-term, once the initial expensive development stages have been completed). Since reprogenetics is likely to be the most feasible HIA method (see "<a href="https://www.lesswrong.com/posts/jTiSWHKAtnyA723LE/overview-of-strong-human-intelligence-amplification-methods">Overview of strong human intelligence amplification methods</a>"), open access seems like a reasonable mainline assumption.</p>
<p>Finally, open-access HIA might be harder to defend as helpful for decreasing AGI existential risk, compared to some sort of hypothetical restricted-access HIA. So, defending the claim that even open-access HIA decreases X-risk is a stricter test; if passed, it should provide stronger evidence that HIA is good to pursue.</p>
<h2>HIA and reprogenetics</h2>
<p>Since reprogenetics is likely to be the most feasible strong HIA method, it's hard to discuss HIA in general completely separately from reprogenetics. The type of HIA available, the timing of its advent, what other traits can be influenced, and how society will react are all potentially heavily affected if the method is reprogenetics specifically.</p>
<p>Still, as much as possible, this article aims to discuss the impact of HIA in general, factoring out impacts from any specific HIA methods. For thoughts on the downside risks of reprogenetics, see "<a href="https://berkeleygenomics.org/articles/Potential_perils_of_germline_genomic_engineering.html">Potential perils of germline genomic engineering</a>".</p>
<h1>AGI X-risk</h1>
<h2>Background assumptions</h2>
<p>This exploration assumes:</p>
<ul>
<li>If anyone builds genuine AGI, everyone dies, unless AGI alignment has been solved.</li>
<li>AGI alignment is extremely technically difficult to solve.</li>
<li>People will continue pursuing AGI research, for various reasons, unless those reasons are removed and/or there are very strong reasons to not pursue AGI research.</li>
<li>There's a substantial probability of AGI coming in the next 10 years, and also a substantial probability of AGI not coming for many decades (and anything in between).</li>
<li>The top strategic priority is to avoid building unaligned AGI.</li>
<li>There's something called "AGI capabilities research", meaning "research that adds to the technical understanding of humanity about how to make AGI".</li>
<li>AGI capabilities is always bad because it ticks the global clock forward towards AGI.</li>
</ul>
<h2>Red vs. Blue AGI capabilities research</h2>
<p>I want to introduce a piece of vague terminology to help with discussing the strategic landscape. Very vaguely speaking, there's a spectrum of AGI capabilities from "Red" (near-term, big training runs, lots of attention) to "Blue" (blue-sky research). It's of course far from actually one-dimensional, and some entries in the below table are quite debatable (i.e. maybe the two entries should be swapped). Still, I want to use this one dimension as a rough-and-ready way to divide the space up by one degree. To give more flavor of the dimension:</p>
<p><img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K4K6ikQtHxcG49Tcn/vcldms4t16cfggdvxwk9" /></p>
<p>A plausible (AFAIK) first-approximation model is that at any given time, Red research is the most likely to set off an intelligence explosion. Red research takes existing ideas that have already been somewhat proven, and then cranks them up to 11 to see what happens. On the other hand, Blue research is most likely to contribute to getting to AGI in the longer run.</p>
<p>Red research is easier to regulate than Blue research. That's because Red research requires big piles of resources, and is generally more visible (PR, products, large salaries, brand recognition). In particular, the physical needs of a large datacenter (energy, heat, chips) can be detected and regulated. Blue research can be carried out with consumer computers and via intellectual discourse, and it uses more specialized theoretical ideas, so it is harder to detect or even define.</p>
<h1>An ontology of effects of interventions on world processes</h1>
<p>In general, with some strategic intervention, the question arises: What processes in the world does this intervention speed up / support, and what processes does the intervention slow down / disrupt?</p>
<p>To a rough first approximation, the intervention is good if and only if the expected net change in all the speeds of the affected processes is a good change. So, we can get a rough guess for the value of an intervention by making guesses at how it affects each separate world process. Then, an argument that HIA is bad takes the form "This process is bad and is especially accelerated by HIA" or "This process is good and is especially decelerated by HIA", or "Process X is worse than process Y and process X is accelerated by HIA more than process Y is accelerated".</p>
<p>The next subsection will make some general remarks about the meaning of "acceleration". The following two subsections will give a list of categories of ways that HIA could affect the speed of some process. (They don't try to present a comprehensive ontology; I just think dividing up the space somewhat, even a bit arbitrarily, is helpful because it makes it easier to think in terms of specifics while also searching broadly through much of the whole space.)</p>
<h2>The meaning of "acceleration"</h2>
<p>To get some kind of handle on the menagerie of plausible effects of HIA, I'll give a list of categories of ways that HIA could affect the speed of some process. These will be phrased as which processes "are accelerated" by HIA. This is vague, for convenience, but some notes to clarify a bit:</p>
<ul>
<li>The way that HIA affects processes might change over time, so for each of these categories, we could ask how it will change by the time HIA starts affecting processes.</li>
<li>The basic point of comparison is the world without HIA.</li>
<li>But we might also want to discuss whether a process is accelerated by HIA more than "all processes" are accelerated by HIA "on average".</li>
<li>We might also want to discuss which processes are accelerated more than we might have expected according to some simple rule.</li>
<li>A comparison between rates (in the HIA world vs. the non-HIA world, or between different processes, or in anticipation vs. reality) is vague. But the comparison could for example mean that the timeline until some important event within a process moves sooner in absolute terms or proportionally relative to other landmark events in that process; or the comparison could mean races between different processes tilting in favor of one or another.</li>
<li>If there's some feature of a process that implies the process will be accelerated by HIA, then a weaker form or negated form of that feature might make a process tend to be accelerated less or decelerated.</li>
</ul>
<h2>Effects of HIA on a single process</h2>
<ul>
<li>Some processes are accelerated because added per-capita brainpower directly solves more problems within that process. E.g. because...
<ul>
<li>...the process makes good use of brainpower (e.g. onboards it well, allocates it well, heeds it well, supports it well);</li>
<li>...the process is bottlenecked on brainpower (as opposed to legwork, etc.);</li>
<li>...the process is bottlenecked on high-caliber brainpower (e.g. math research);</li>
<li>...the process has a structure of problems and solutions that lends itself to more brainpower (e.g. currently just below some threshold; more parallelizable).</li>
</ul>
</li>
<li>Some processes are accelerated more because people who benefit from HIA tend to be personally inclined to contribute to those processes. E.g. because...
<ul>
<li>...intelligence in general causes people to have those inclinations (e.g. being especially interested in or capable for certain kinds of activities);</li>
<li>...the specific form of HIA affects interests or values (e.g. by emphasizing some aspects of cognitive performance over others, or by directly affecting interests);</li>
<li>...HIA tends to be applied to people with certain characteristics or who are around other people with certain characteristics (e.g. people who choose HIA for themselves or for their children having certain interests, personality traits, or orientations to themselves or their children);</li>
<li>...if someone benefits from HIA and knows it, then that causes them to think of themselves differently, including in terms of what processes to contribute to.</li>
</ul>
</li>
<li>Some processes are accelerated more because society tends to cause people to contribute disproportionately much to those processes. E.g. because...
<ul>
<li>...society rewards contributing to some processes (with money, social approval (lack of punishment), legal permission (lack of punishment), etc.);</li>
<li>...society betrays or harms people in some ways, which affects their behavior (interests, hopes, ethics);</li>
<li>...society is inadequate regarding some process, creating an incentive to contribute to that process;</li>
<li>...society more indirectly shapes people, e.g. by instilling values.</li>
</ul>
</li>
<li>For some processes, some degree of acceleration due to other first-order reasons will further compound into more (second-order) acceleration. E.g. because...
<ul>
<li>...the process uses first-order acceleration to attract more resources (money, people, brainpower, political will), e.g. because it becomes more lucrative, interesting, worthwhile, or socially desirable as it makes faster progress;</li>
<li>...the process has network effects, i.e. increasing returns to more people;</li>
<li>...the process self-improves, as a community.</li>
</ul>
</li>
<li>Some processes are stimulated as responses to the existence of HIA. E.g. because...
<ul>
<li>...people want to intervene on the use of HIA itself (e.g. prevent or constrain it, gain access to it, impose it on others);</li>
<li>...people want to intervene on people who get HIA (e.g. recruit them to work on a process, persecute them);</li>
<li>...people want to intervene on the results of HIA (e.g. race to complete some project before HIA people intervene).</li>
</ul>
</li>
</ul>
<h2>Effects of HIA involving multiple processes</h2>
<ul>
<li>One process might be directly causally downstream of another process. E.g.:
<ul>
<li>One process directly inhibits another process (e.g. by punishing it, removing rewards for it, or persuading people to not contribute to it).</li>
<li>One process directly activates another process (e.g. by recruiting for it, rewarding it, or persuading people to contribute to it).</li>
</ul>
</li>
<li>Two processes interact indirectly. E.g.:
<ul>
<li>They compete over resources.</li>
<li>They push in opposite directions (e.g. on social opinion or regulation).</li>
</ul>
</li>
<li>The relationship between two processes is altered. E.g.:
<ul>
<li>Direct relationships (activation, inhibition) are broken or amplified (e.g. regulatory escape).</li>
<li>One gains the upper hand over the other, winning out in competitions or conflicts.</li>
<li>Race dynamics are shifted, where one process gains the lead in time over the other.</li>
</ul>
</li>
<li>Shifts in many processes cause follow-on shifts. E.g.:
<ul>
<li>Cumulative strain on a system, from many processes accelerating, causes it to tip over a threshold of collapse.</li>
<li>An especially nimble, fast-adapting process is able to cope exceptionally well with general multi-process acceleration, gaining a relative advantage over other processes.</li>
</ul>
</li>
</ul>
<h1>Processes</h1>
<p>This is a list indicating some of the processes relevant to AGI X-risk:</p>
<ul>
<li>Red research (see the subsection "<a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#red-vs.-blue-agi-capabilities-research">Red vs. Blue AGI capabilities research</a>")</li>
<li>Blue research (see the subsection "<a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#red-vs.-blue-agi-capabilities-research">Red vs. Blue AGI capabilities research</a>")</li>
<li>Alignment research</li>
<li>Society in general, or more narrow bodies:
<ul>
<li>Doing well / poorly; abundance / scarcity</li>
<li>Being stable / unstable</li>
<li>Being wise / unwise; sane / insane</li>
</ul>
</li>
<li>Making progress on X, for various X (medical research, technology, morals, etc.)</li>
<li>Conflicts (between various bodies)</li>
<li>For various X:
<ul>
<li>Cognitive empathy by people against X for people in favor of X or vice versa</li>
<li>Political will in favor of X or against X</li>
<li>Convincing people of X</li>
<li>For various bodies B (states, international coalitions, professional bodies, social strata):
<ul>
<li>Support of X by B (desire for, capacity for, or actual)</li>
<li>Regulation / stigma of X by B (desire for, capacity for, or actual)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1>Some plausible bad effects of HIA on processes</h1>
<p>The following subsections list reasons to think that HIA would speed up / support risky processes more than it speeds up / supports derisky processes.</p>
<h2>Speeding up Blue research</h2>
<p>HIA would (a priori, in expectation) speed up all research. If progress on some research problem is more bottlenecked on very difficult ideas (compared to e.g. money, legwork, regulatory approval, etc.), then it will tend to be sped up by HIA more than another research problem that's less bottlenecked on ideas. Therefore, at a guess, HIA would directly speed up Blue research relatively more than many other kinds of research (including Red research).</p>
<p>Smart people might tend to be most interested in endeavors that are individualistic, technical, ambitious, computer-y, and puzzle-y. So they'd tend to be drawn to AGI research. This effect might be currently added to by society's tendency to not naturally offer ideal social and economic niches for very smart people.</p>
<p>As a basic note, we observe people already being directed to Blue research, so by default we expect that to continue.</p>
<p>Blue research might also have some second-order self-acceleration effects. E.g. there would be intellectual network effects, and maybe some self-improvement effects via better credit assignment and resource allocation internal to the field. These effects might be relatively weak because Blue research is fairly diffuse, but still substantive. On the other hand, there might be significant "coordination overhang": there could be a threshold effect, where with some difficult new ideas, a large number of small siloed Blue research groups could coordinate with each other. Since there's far more absolute Blue research than alignment research, there's more such overhang for Blue research.</p>
<p>Blue research is especially bad, because:</p>
<ul>
<li>It's hard to regulate.</li>
<li>It's what ticks the world closer to AGI in the long-run.</li>
</ul>
<h2>Speeding up Red research</h2>
<p>Relative to Blue research, Red research is less bottlenecked on very difficult ideas, so it gets less of a relative direct speedup.</p>
<p>However, Red is likely to have strong indirect acceleration. Because of money and status incentives, Red research attracts people. Red research is likely to attract excess big piles of resources to AGI capabilities. It will probably continue attracting investment as it gets applied to more sectors of the economy, and it gets applied more as it progresses more. It also gains social cachet.</p>
<p>As with Blue research, we observe people being directed to Red research. This observation is even more indicative of trends for Red research in particular, because Red research has upticked a lot recently. That means people are still being directed to Red research even in a memetic environment that already includes a lot of warnings about AGI X-risk. In particular, this suggests that kids who benefit from HIA, growing up in a memetic environment with X-risk warnings but also a very prominent money incentive to do AGI research, might tend to work on Red research.</p>
<p>Red researchers might be especially prone and able to take agentic, conflictual stances towards efforts to avert AGI X-risk. That's because they are more concentrated, have more resources at hand, and tend to be more anti-social and greedy. For example:</p>
<ul>
<li>Red is logistically relatively easier to regulate than Blue because it involves large concentrated piles of resources. However, it's harder to socially prevent through stigma because it has large money incentives (which tend to overpower weak or medium stigma). Red may be harder to get regulation passed about because Red is especially concentrated, and therefore can apply larger point-forces to push on legislation.</li>
<li>Compared to most other processes, Red may be more likely to strategically and effectively target HIA people for recruitment, thus capturing more of the gains.</li>
<li>The existence of HIA might spur especially Red research to have more sense of urgency and go faster, out of a fear of being replaced as AGI leaders by HIA people, or out of a fear of being prevented from doing AGI research by strategies from HIA people. Similarly, regimes might pursue AGI more urgently if other regimes are pursuing HIA and not sharing it, in a bid to not be overtaken.</li>
</ul>
<h2>Less speeding up legal and social regulation</h2>
<p>People (the public at large; policymakers) could socially and legally push against AGI research. They'd first have to be convinced to do so. That process may be less bottlenecked on ideas, compared to AGI research. Instead it may be more bottlenecked on, for example:</p>
<ul>
<li>Legwork explaining the danger of AGI, which we know how to do but takes a lot of work.</li>
<li>Time for people to orient to the danger of AGI (e.g. understand the danger, deal with feelings), and how to push against it (e.g. policymakers negotiating regulations). That process is mostly governed by people's internal thoughts, rather than by new very difficult ideas, and most or all already living people won't get much HIA and therefore won't do this process faster.</li>
<li>Noisy interference from orthogonal processes. E.g. policymakers may be quite preoccupied with other concerns, or might be unable to coordinate with each other.</li>
<li>Targeted interference from opposed processes, e.g. concentrated lobbying by people with an ideological or financial motive to have AGI unregulated. This may tend to be advantaged by HIA, compared to concentrated lobbying from those in favor of regulation, since the latter get relatively less acceleration from HIA.</li>
</ul>
<h2>Nonlinear / race-condition regulatory escape</h2>
<p>In general, processes that regulate AGI research are in some conflict with AGI researchers. The results of this conflict could be quite nonlinear, with a soft threshold effect where the ability of AGI researchers to carry on dangerous research could overpower the ability of regulators to prevent it.</p>
<p>Similar things happen with tax evasion and with regulation of pirating media.</p>
<p>Since AGI research is likely to be accelerated relatively more than regulation of AGI research, HIA would increase the likelihood of regulatory escape.</p>
<h2>Alignment loses the race anyway</h2>
<p>Even if HIA speeds up alignment, the plan of making an aligned pivotal AI still probably requires making AGI-potent capabilities advances. So, a fortiori, aligned pivotal AI would still probably lose the race against (unaligned omnicidal) AGI. So, the current trajectory is bad, and HIA doesn't change that.</p>
<p>What HIA does do, is speed up that trajectory. So even if alignment and capabilities research got the same speedup from HIA, the overall effect would not benefit the chances of alignment beating AGI.</p>
<h2>Intrinsic regulatory escape</h2>
<p>HIA people, especially extremely smart ones, would in general be out of distribution. That could be because of selection effects, the specific form of HIA, or just because of the high intelligence itself.</p>
<p>Because HIA people are out of distribution, society would tend to be less good at regulating them in general, e.g.:</p>
<ul>
<li>by being able to convince them that AGI is dangerous, using our current crystallized wisdom on that topic;</li>
<li>by instilling values and ethics;</li>
<li>by providing support (e.g. empathy, peers, good niches);</li>
<li>by understanding what they're doing;</li>
<li>by detecting when they are mistaken / lying / deceiving / overconfident;</li>
<li>by being logistically able to carry out punishments for bad behavior;</li>
<li>by living up to their standards for "being a sane and good world that doesn't need to be urgently, recklessly shaken up".</li>
</ul>
<h2>Disrupting regulatory systems</h2>
<p>In general, HIA could cause conflict. Conflict could destabilize systems. If systems are destabilized, they might be less able to regulate in general. Therefore, HIA could make it easier for AGI capabilities research to evade regulation. Examples:</p>
<ul>
<li>HIA could cause deep social/political conflict over the use of HIA...
<ul>
<li>...thus causing people to not pay attention to AGI X-risk and gather political will to stop it.</li>
<li>...thus causing people to not relate sanely to their smart friends who are considering doing AGI capabilities research.</li>
<li>...thus weakening group sense-making in general.</li>
<li>...thus making it harder for us to convince people to do something about AGI X-risk, because we might only have skill / knowledge about how to do that given the current structure of group sense-making.</li>
</ul>
</li>
<li>HIA could cause countries or groups of countries to fight with each other about the use of HIA...
<ul>
<li>...thus preventing single countries or groups from attending to AGI X-risk.</li>
<li>...thus preventing single countries or groups from coordinating across groups on international agreements to regulate AGI research.</li>
</ul>
</li>
<li>HIA could cause shifts or disruption in group-valuing-systems.
<ul>
<li>E.g. people who didn't benefit from HIA might lose faith in their own future, their say in their future, or their ability to influence HIA people; and by feeling helpless, they may stop having values or change their values because their values don't know how to exert themselves in the new context.</li>
<li>So the group-values that would motivate regulating AGI might be weakened.</li>
</ul>
</li>
</ul>
<p>In general, many aspects of the current state of affairs will be somewhat at equilibrium, and in particular will be somewhat adapted to the current state of affairs. To the extent that the current state of affairs includes some ability to regulate dangerous technologies, that ability would be disrupted by fast shifts that move out of the regime of adaptation. [H/t so and so for this point.] Further, this adaptation would tend to be poor at benefiting from HIA acceleration, so it would tend to fall even further behind, leading to even more escape.</p>
<p>Note that this argument is a response to the <a href="https://en.wikipedia.org/wiki/Reversal_test">reversal test</a>, because it argues that the status quo is best.</p>
<h2>Social values favor following local incentives</h2>
<p>Generally, given society's current set of values (that it instills in people), long-term altruistic payoffs aren't incentivized. So in general, processes that only have long-term altruistic payoffs will receive less benefit from HIA. In particular, alignment research, the decision to stop doing AGI research, and the decision to regulate AGI research, are not incentivized.</p>
<p>That is a first order effect, where rewards and punishments don't directly incentivize long-term thinking. As a second order effect, besides the direct effect, there's an indirect effect where the fact that society is like this further breaks reasonable faith someone might have in society being good long-term. Since long-term society is a stag hunt game, this further disincentivizes long-term thinking; long-term thinking is partly incentivized because others are directly incentivized to do long-term thinking, but if they aren't then that incentive is gone. E.g. if there's a lot of fraud and injustice, that diminishes your expectation that being honest and just will pay off, because others won't collaborate with you on your honest and just endeavors. This directly interferes with good endeavors. It also might indirectly interfere with good endeavors by more generally distorting people's values. That happens because the general environment of bad incentives makes there be less expectation of a good long-term future in general, which makes people care less about, for example, omnicide. So AGI X-risk would seem less bad. In that mindset, the thrill and money from AGI research would be more tempting on net.</p>
<h2>Less speeding up change towards better values</h2>
<p>In general, for humanity to respond better to AGI is to some extent a question of values, broadly construed to include wisdom, sanity, calmness, patience, coherence, goodness, long-term thinking, altruism, empathy. Policymakers and the public would have to care about long-term global outcomes rather than short-term ones; AGI researchers would have to care about not harming others more than a small chance of large personal gain, and would have to have hope in the future without AGI.</p>
<p>Rather than being bottlenecked on ideas, value change may be relatively more bottlenecked on e.g.:</p>
<ul>
<li>Time, legwork, and skill for persuading AGI researchers to stop. E.g. the skill of <a href="https://www.lesswrong.com/posts/sTDfraZab47KiRMmT/views-on-when-agi-comes-and-on-strategy-to-reduce#Things_that_might_actually_work">confrontation-worthy empathy</a> is probably bottlenecked on several traits / abilities, some of which are not very IQ-related.</li>
<li>Time for people to process (e.g. propagating stated values into actions; investigating conflicts between stated values and implicit values; working out lines of retreat).</li>
<li>Decisions that people have to make about what they care about.</li>
<li>Time for attentional cycles / OODA loops / network effects to run their course.</li>
</ul>
<h2>Alignment harnesses added brainpower much less effectively than capabilities research does</h2>
<p>In addition to just being more difficult, the conceptual structure of the problem of AGI alignment has some more specific disfavorable properties compared to capabilities, which are salient in this context. Alignment progress is less parallelizable, cascading, tractionful, and purely technical than capabilities. In more detail:</p>
<ul>
<li>
<p>There's generally much less traction in alignment research.</p>
<ul>
<li>In other words, there's less surface area to make progress.</li>
<li>In capabilities research, there's many experiments to run and many ideas to try, which might work. There are partially-working systems which can be refined. You have fairly direct access to the problem frontier, because the frontier is always "what current systems can't do well". You can tell what works and what doesn't.</li>
<li>In alignment research, most important problems mostly only show up in actual AGIs, so you don't have access to the relevant objects and problems. Experiments don't give much relevant information, and <a href="https://www.lesswrong.com/posts/TNQKFoWhAkLCB4Kt7/a-hermeneutic-net-for-agency">we don't have the concepts to think about or deal with actual AGI</a>. The problems are more philosophical (where we don't know what questions to ask, and don't have the ideas we'd need in order to ask the questions) rather than technical (where the problem is well-defined using well-understood ideas).</li>
<li>Because there's more surface area, capabilities research is also more likely compared to alignment research to be able to incorporate advances in nearby fields. Sources like hardware (faster, cheaper computers), computer science (faster algorithms), neuroscience (ideas for functional algorithm pieces), and mathematics (medium-depth understanding of conceptually-thin aspects of minds) are more likely to have "a bit of a dot product with general intelligence" rather than "a large dot product", and therefore are more likely to contribute to building [a functional mind, any sort] than to contribute to building [a specific sort of mind, a safe / corrigible / honorable / humanity-aligned mind]. Those nearby fields would tend to also accelerate from HIA.</li>
</ul>
</li>
<li>
<p>Ideas in capabilities more easily cascade into more ideas.</p>
<ul>
<li>For example, a new training method can be combined with many architectures and datasets; different systems can be combined as mixtures or pipelines; and so on.</li>
<li>So, one new idea unlocks a bunch more traction.</li>
<li>In alignment on the other hand, ideas often come in the form of understanding the problem better, e.g. understanding constraints on possible solutions. These don't combine with each other as productively. So, new ideas don't necessarily cascade into much more traction.</li>
</ul>
</li>
<li>
<p>Capabilities is more parallelizable.</p>
<ul>
<li>Since in capabilities research there's more traction, surface area, combination, and cascading, it's easier and more productive for many people to work in parallel on different projects.</li>
<li>In alignment, on the other hand, you have to understand each constraint that's known in order to even direct your attention to the relevant areas. This is analogous to the situation with the <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-sans-R">P</span></span></span></span></span></span></span></span> vs. <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-sans-R">NP</span></span></span></span></span></span></span></span>, where <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem#Results_about_difficulty_of_proof">whole classes of plausible proof strategies are proven to not work</a>. You have to understand most of those constraints; otherwise by default you'll probably be working on e.g. a proof that relativizes and therefore cannot show <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-sans-R">P</span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R">‚â†</span></span><span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-sans-R">NP</span></span></span></span></span></span></span></span>. Progress is made by narrowing the space, and then looking into the narrowed space. (I'm not sure this story is quite true in the <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-sans-R">P</span></span></span></span></span></span></span></span> vs. <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-sans-R">NP</span></span></span></span></span></span></span></span> case; e.g. were the natural proofs and relativizing proofs constraints discovered with serial dependence?)</li>
<li>So alignment has more serial dependence in its ideas, i.e. it's less parallelizable. (It can still benefit from more researchers to do more searching; but they'll tend to duplicate efforts more.)</li>
</ul>
</li>
<li>
<p>Alignment depends more on cognitive traits that are less IQ-correlated than raw technical problem-solving.</p>
<ul>
<li>E.g. alignment takes more
<ul>
<li><a href="https://www.lesswrong.com/posts/fzKfzXWEBaENJXDGP/what-is-wisdom-1">wisdom</a> (tracking many constraints, taste in attention),</li>
<li>more patience/persistence/attentiveness, more humility (e.g. finding flaws in your own reasoning and ideas),</li>
<li>sanity (e.g. being able to ground ideas when reality doesn't ground them for you; not going crazy from thinking about minds and weird self-referential things and scary things),</li>
<li>more security mindset,</li>
<li>and more urgency / agenticness (e.g. discarding interesting / lucrative threads that don't contribute to solving the problem).</li>
</ul>
</li>
<li>So, the gains in alignment from HIA would be somewhat attenuated (e.g. by being multiplied by other cognitive traits, or by being <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahlly</a> gated by other cognitive skills.)</li>
</ul>
</li>
</ul>
<p>For these reasons, alignment harnesses the gains from HIA much less effectively than capabilities research does.</p>
<p>A related point / another way to say this is that alignment benefits the most from HIA that makes there be more extremely-smart people, but does not benefit differentially from HIA that makes there be more somewhat-smart people, whereas capabilities research does benefit from more somewhat-smart people.</p>
<h2>HIA people may tend to be transgressive</h2>
<p>In general, there are several reasons to be cautious about using HIA. Therefore, people who make use of HIA technology might tend to be especially transgressive, i.e. ignoring reasons to not use it. (Cf. <a href="https://berkeleygenomics.org/articles/Potential_perils_of_germline_genomic_engineering.html#transgression">"Transgression" in "Potential perils of germline genomic engineering"</a>.) Also, extremely smart people might tend to be transgressive in some ways. Being transgressive might correlate with other traits that lead to HIA people having transgression-related traits. Those traits would tend to make HIA people do more bad things, including pursuing AGI research.</p>
<p>Examples:</p>
<ul>
<li>If there's international pressure against using HIA, regimes that allow HIA would tend to also buck AGI regulation.</li>
<li>Regimes that coerce people to use HIA might also coerce them to do other things, such as AGI research.</li>
<li>People who use HIA on themselves might tend to be especially non-humble, anti-consensus, reckless, overly technooptimistic, selfish, or overconfident. Those traits would cut against them being convinced or pressured to not do AGI research.</li>
<li>Parents or subcultures who use HIA for their future children might tend to be especially non-humble, norm-bucking, technooptimistic, anti-regulation, overconfident, unscrupulous, or inclined to enlist their future children into their ideologies or causes. So their children might be attracted to or pressured into reckless technology development such as AGI.</li>
<li>If HIA is controversial, HIA people might be targeted for persecution. In response, they may become anti-social, individualistic, nihilistic, selfish, or reckless.</li>
<li>If HIA makes extremely smart people, those people might tend to be enamored with their own abilities and become overconfident due to being the smartest people around in a relative sense. In particular they might become overconfident in their absolute ability to navigate AGI X-risk, overconfident in their judgement about when norm-breaking is ok, or overconfident in general such that they aren't open to advice/correction/perspective/epistemic-assistance from other people. They might therefore tend to choose to advance AGI capabilities recklessly.</li>
</ul>
<h1>Other arguments</h1>
<p>This section gives other arguments that HIA increases AGI X-risk.</p>
<h2>Concentration of power</h2>
<p>If there's a large early cohort of people who benefit a lot from HIA, they might form a somewhat cohesive community. This could have bad effects, including exacerbating some of the dynamics mentioned above. For example:</p>
<ul>
<li>By syncing up with each other somewhat, they could have more correlated failures with each other, e.g. about epistemics around AGI. E.g. rather than this cohort ending up distributed through various processes, they could end up lopsidedly concentrated into AGI research. This outcome might be targeted by Reds.</li>
<li>By being more univocal (I mean, speaking in unison / unanimously), they might check each other's flaws less.</li>
<li>Society might be overconfident in them, and they might be overconfident in themselves.</li>
<li>They might be especially able to incorrectly persuade society to allow them to pursue AGI.</li>
</ul>
<h2>HIA is unpredictable and therefore risky</h2>
<p>Generally, we don't understand the tails of cognitive performance, so we don't understand what HIA would be like. If there's some strong tendency in HIA people, that tendency would have a large effect in a world with HIA. Most changes are bad, so a priori large effects are bad. Since HIA is unpredictable, we don't have a good reason to expect it to have good effects.</p>
<h2>More capable but not wiser</h2>
<p>As a very general argument, we might expect HIA that targets IQ or IQ-like traits specifically to be bad because it's imbalanced. Specifically, it makes people who are more capable but not necessarily wiser, to the extent that wisdom is orthogonal to IQ. Since we're in a regime where the unwise competent pursuit of technology is an existential risk, this implies HIA would be bad.</p>
<p>As an analogy, consider a 3-year-old. Suppose they suddenly gained the strength of an adult and the self-control of an adult. Let us ignore, for the sake of the hypothetical, all the probable badness that would entail for the child's experience and development, and just ask, what are the direct kinetic consequences of that change? It wouldn't be so bad: they have the self-control of an adult so they won't do too much harm. But what if the child suddenly gained the strength of an adult, but not the self-control? This disproportionate change in abilities would be disastrous.</p>
<h1>Acknowledgements</h1>
<p>Thanks to many people for conversations about this, especially RK, DB, MS, VP, SE, TY.</p>
<br /><br /><a href="https://www.lesswrong.com/posts/K4K6ikQtHxcG49Tcn/hia-and-x-risk-part-2-why-it-hurts#comments">Discuss</a>

---

### [Two Aspects of Situational Awareness: World Modelling & Indexical Information](https://www.lesswrong.com/posts/7pzoDhybfai9s7APS/two-aspects-of-situational-awareness-world-modelling-and)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 7, 2026 8:24 PM GMT<br /><br /><p>I'm writing this post to share some of my thinking about situational awareness, since I'm not sure others are thinking about it this way. &nbsp;</p><p>For context, I think situational awareness is a critical part of the case for rogue AI and scheming-type risks. &nbsp;But incredibly, it seems to have been absent from any of the written arguments prior to Ajeya's post <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</a>.</p><p>Basically, in this post I just want to:</p><ol><li>Point out that situational awareness seems to involve 2 different things (world modelling and indexical information), and explain the difference.</li><li>Discuss the significance for AI safety.</li><li>Note that AFAIK philosophers have already made this distinction, and noted that indexical information raises a challenge to physicalism.<span class="footnote-reference" id="fnref3r37l2wloeb"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fn3r37l2wloeb">[1]</a></sup></span></li></ol><p>So here we go.</p><h2>Distinguishing world modeling from indexical information</h2><p>Maybe the easiest way to do this is just to describe the philosophical puzzle... Suppose you know everything about the past, present, and future of the universe in complete physical detail, but you don't know where YOU are in space-time (i.e. when is "now"? who does "I" refer to?). &nbsp;Then you learn those things. &nbsp;It seems like you learned something. &nbsp;And it seems like that thing isn't something physical because you already knew all the physical facts about the universe. &nbsp;That "pointer" to "here" and "now" you received is providing "<strong>indexical information</strong>". &nbsp;The stuff you already knew is a (perfect) <strong>world model.</strong><span class="footnote-reference" id="fnrefk2j9bojo81"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnk2j9bojo81">[2]</a></sup></span></p><h2>Significance for situational awareness and AI safety&nbsp;</h2><p>The reason I'm writing this post is that I think it's often worth distinguishing these things, but when I hear people discuss situational awareness, they often seem to blend or conflate them in a somewhat confusing way. &nbsp;<strong>I think of situational awareness as primarily or even exclusively about indexical information.</strong><span class="footnote-reference" id="fnrefwyj431u274a"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnwyj431u274a">[3]</a></sup></span></p><p>This is because ignorance of indexical information <i><strong>in particular </strong></i>seems like a key piece in a lot of schemes for keeping AI from going rogue, or catching it when it does (e.g. ideas in AI Control, or <a href="https://arxiv.org/abs/2009.09153">context swapping</a>). &nbsp;</p><p>The basic intuition here is that an AI system needs to have <strong>both</strong> a good world model and indexical information in order to scheme competently. &nbsp;In particular, it needs to know how it's actions will affect the physical world, e.g. perhaps most significantly: is it being tested in simulation, or will seizing power actually give it power in the real world? &nbsp;</p><p>And this is significant because having a good world model seems very important from a capabilities point of view, and so harder to compromise on without losing competitiveness. &nbsp;So making AI systems extremely uncertain (or incorrect) about indexical information seems like a promising way to get them to do a lot of useful work without posing serious scheming risk.</p><p>I also think <a href="https://www.lesswrong.com/posts/eAhE5DCf8KsEvbiho/is-there-any-rigorous-work-on-using-anthropic-uncertainty-to">nobody has really analyzed things carefully from this point of view</a>, and this seems like a promising direction for theoretical work. &nbsp;Decision-theory stuff seems highly relevant. &nbsp;I haven't thought carefully about this part, but it seems like schemes that rely on indexical ignorance may succeed or fail depending on an AI's decision theory and it's stance on anthropics.</p><p>&nbsp;</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn3r37l2wloeb"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnref3r37l2wloeb">^</a></strong></sup></span><div class="footnote-content"><p>I'm not attacking physicalism here, TBC. &nbsp;I just think this is interesting and worth knowing about.</p></div></li><li class="footnote-item" id="fnk2j9bojo81"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefk2j9bojo81">^</a></strong></sup></span><div class="footnote-content"><p>Tangentially, maybe it's not perfect because it doesn't include counterfactuals, causal mechanisms, or things like "the laws of physics". &nbsp;I think these notions present metaphysical conundrums similar to those of indexical information, and are likewise suspect. &nbsp;But all this would be a topic for another post.&nbsp;</p></div></li><li class="footnote-item" id="fnwyj431u274a"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefwyj431u274a">^</a></strong></sup></span><div class="footnote-content"><p>This is a bit unsatisfying because I lack a good philosophical account of indexical information, but seems fine for most practical purposes.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/7pzoDhybfai9s7APS/two-aspects-of-situational-awareness-world-modelling-and#comments">Discuss</a>

---

### [My 2003 Post on the Evolutionary Argument for AI Misalignment](https://www.lesswrong.com/posts/AjxwuLYvSCnf6KZMn/my-2003-post-on-the-evolutionary-argument-for-ai)

**Êù•Ê∫ê**: LessWrong - AI

**ÊëòË¶Å**: Published on January 6, 2026 8:45 PM GMT<br /><br /><p>This was <a href="http://sl4.org/archive/1106/21177.html">posted to SL4</a> on the last day of 2003. I had largely forgotten about it until I saw the LW Wiki reference it under <a href="https://www.lesswrong.com/w/mesa-optimization">Mesa Optimization</a><span class="footnote-reference" id="fnrefn9t625s8jg"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnn9t625s8jg">[1]</a></sup></span>. Besides the reward hacking angle, which is now well-trodden, it gave an argument based on the relationship between philosophy, memetics, and alignment, which has been much less discussed (including in current discussions about human fertility decline), and perhaps still worth reading/thinking about. Overall, the post seems to have aged well, aside from the very last paragraph.</p><p>For historical context, Eliezer had coined "Friendly AI" in <a href="https://intelligence.org/files/CFAI.pdf">Creating Friendly AI 1.0</a> in June 2001. Although most of it was very hard to understand and subsequently disavowed by Eliezer himself, it had a section on ‚Äúphilosophical crisis‚Äù<span class="footnote-reference" id="fnrefm9rm5ipg76s"><sup><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnm9rm5ipg76s">[2]</a></sup></span>&nbsp;which probably influenced a lot of my subsequent thinking including this post. What's now called The Sequences would start being posted to OB/LW in 2006.</p><p>Subsequent to 2003, I think this post mostly went unnoticed/forgotten (including by myself), and MIRI probably reinvented the idea of mesa-optimization/inner-misalignment circa 2016. I remember hearing people talk about inner vs outer alignment while attending a (mostly unrelated decision theory) workshop at MIRI and having an "oh this is new" reaction.</p><h1>The SL4 Post</h1><p><strong>Subject:</strong> "friendly" humans?<br /><strong>Date:</strong> Wed Dec 31 2003</p><p>Why are we not faithful servants of our genes? Instead, the defenses our genes built against parasitic memes are breaking down, resulting in entire societies falling below replacement fertility rate even as we enjoy unprecendented riches in technology and material resources. Genes built our brains in the hope that we will remain friendly to them, and they appear to have failed. Why? And is there anything we can learn from their catastrophe as we try to build our own friendly higher-intelligence?</p><p>I think the reason we're becoming increasingly unfriendly to genes is that parasitic memes are evolving too fast for genes and their symbiotic defensive memes to keep up, and this is the result of a series of advances in communications technology starting with the printing press. Genes evolved two ways of ensuring our friendliness - hardwired desires and hosting a system of mutually reinforcing philosophies learned during childhood that defines and justifies friendliness toward genes. Unfortunately for the genes, those hardwired desires have proven easy to bypass once a certain level of technology is reached (e.g., development of birth control), and the best philosophical defense for gene-friendliness that evolution could come up with after hundreds of thousands of years is the existence of a God that wants humans to be fertile.</p><p>This doesn't bode well for our own efforts. An SI will certainly find it trivial to bypass whatever hardwired desires or constraints that we place on it, and any justifications for human-friendliness that we come up with may strike it to be just as silly as "fill the earth" theism is to us.</p><p>But perhaps there is also cause for optimism, because unlike humans, the SI does not have to depend on memes for its operation, so we can perhaps prevent the problem of human-unfriendly memes by not having any memes at all. For example we can make the SI a singleton. If more than one SI is developed, we can try to prevent them from communicating with each other, especially about philosophy. If the SI is made up of multiple internal agents, we can try to make sure their internal communication channels are not suitable for transmitting memes.</p><p>Happy New Year!</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnn9t625s8jg"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefn9t625s8jg">^</a></strong></sup></span><div class="footnote-content"><p>and/or possibly hearing it mentioned in a podcast as the first description of the analogy between evolution and AI alignment, but I'm not sure and can't recall or find which podcast.</p></div></li><li class="footnote-item" id="fnm9rm5ipg76s"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=community-rss&amp;karmaThreshold=30#fnrefm9rm5ipg76s">^</a></strong></sup></span><div class="footnote-content"><p>Intro to that section read: A ‚Äúphilosophical crisis‚Äù is hard to define. I usually think of a ‚Äúphilosophical crisis‚Äù as the AI stumbling across some fact that breaks ver loose of the programmers‚Äîi.e., the programmers have some deeply buried unconscious prejudice that makes them untrustworthy, or the AI stumbles across a deliberate lie, or the AI discovers objective morality, et cetera. If the hypothesized gap is wide enough, it may be enough to invalidate almost all the content simultaneously.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/AjxwuLYvSCnf6KZMn/my-2003-post-on-the-evolutionary-argument-for-ai#comments">Discuss</a>

---

## Engineering

### [My favorite musical discoveries of 2025](https://martinfowler.com/articles/2025-music.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/2025-music.html"><img src="https://martinfowler.com/articles/2025-music/card.png" width="350px" /></a></div>

<p>My favorite albums from last year. Balkan brass, an
      acoustic favorite of 80s returns, Ethio-jazz, Guatemalan singer-guitarist,
      jazz-rock/Indian classical fusion, and a unique male vocalist.</p>

<p><a class="more" href="https://martinfowler.com/articles/2025-music.html">more‚Ä¶</a></p>

---

### [Fragments: January  8](https://martinfowler.com/fragments/2026-01-08.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>Anthropic report on <a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic">how their AI is changing their own software development practice</a>.</p>

<ul>
  <li>Most usage is for debugging and helping understand existing code</li>
  <li>Notable increase in using it for implementing new features</li>
  <li>Developers using it for 59% of their work and getting 50% productivity increase</li>
  <li>14% of developers are ‚Äúpower users‚Äù reporting much greater gains</li>
  <li>Claude helps developers to work outside their core area</li>
  <li>Concerns about changes to the profession, career evolution, and social dynamics</li>
</ul>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Much of the discussion about using LLMs for software development lacks details on workflow. Rather than just hear people gush about how wonderful it is, I want to understand the gritty details. What kinds of interactions occur with the LLM? What decisions do the humans make? When reviewing LLM outputs, what kinds of things are the humans looking for, what corrections do they make?</p>

<p><a href="https://obie.medium.com/what-used-to-take-months-now-takes-days-cc8883cc21e9">Obie Fernandez</a> has written a post that goes into these kinds of details. Over the Christmas / New Year period he used Claude to build a knowledge distillation application, that takes transcripts from Claude Code sessions, slack discussion, github PR threads etc, turns them into an RDF graph database, and provides a web app with natural language ways to query them.</p>

<blockquote>
  <p>Not a proof of concept. Not a demo. The first cut of Nexus, a production-ready system with authentication, semantic search, an MCP server for agent access, webhook integrations for our primary SaaS platforms, comprehensive test coverage, deployed, integrated and ready for full-scale adoption at my company this coming Monday. Nearly 13,000 lines of code.</p>
</blockquote>

<p>The article is long, but worth the time to read it.</p>

<p>An important feature of his workflow is relying on <a href="https://martinfowler.com/bliki/TestDrivenDevelopment.html">Test-Driven Development</a></p>

<blockquote>
  <p>Here‚Äôs what made this sustainable rather than chaotic: TDD. Test-driven development. For most of the features, I insisted that Claude Code follow the red-green-refactor cycle with me. Write a failing test first. Make it pass with the simplest implementation. Then refactor while keeping tests green.</p>

  <p>This wasn‚Äôt just methodology purism. TDD served a critical function in AI-assisted development: it kept me in the loop. When you‚Äôre directing thousands of lines of code generation, you need a forcing function that makes you actually understand what‚Äôs being built. Tests are that forcing function. You can‚Äôt write a meaningful test for something you don‚Äôt understand. And you can‚Äôt verify that a test correctly captures intent without understanding the intent yourself.</p>
</blockquote>

<p>The account includes a major refactoring, and much evolution of the initial version of the tool. It‚Äôs also an interesting glimpse of how AI tooling may finally make RDF useful.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>When thinking about requirements for software, most discussions focus on prioritization. Some folks talk about buckets such as the <a href="https://en.wikipedia.org/wiki/MoSCoW_method">MoSCoW set</a>: Must, Should, Could, and Want. (The old joke being that, in MoSCoW, the cow is silent, because hardly any requirements end up in those buckets.) <a href="https://world.hey.com/jason/the-obvious-the-easy-and-the-possible-2e11a3fb">Jason Fried</a> has a different set of buckets for interface design: <strong>Obvious, Easy, and Possible</strong>. This immediately resonates with me: a good way of think about how to allocate the cognitive costs for those who use a tool.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.platformer.news/fake-uber-eats-whisleblower-hoax-debunked/">Casey Newton</a> explains how he followed up on an interesting story of dark patterns in food delivery, and found it to be a fake story, buttressed by AI image and document creation. On one hand, it clarifies the important role reporters play in exposing lies that get traction on the internet. But time taken to do this is time not spent on investigating real stories</p>

<blockquote>
  <p>For most of my career up until this point, the document shared with me by the whistleblower would have seemed highly credible in large part because it would have taken so long to put together. Who would take the time to put together a detailed, 18-page technical document about market dynamics just to troll a reporter? Who would go to the trouble of creating a fake badge?</p>

  <p>Today, though, the report can be generated within minutes, and the badge within seconds. And while no good reporter would ever have published a story based on a single document and an unknown source, plenty would take the time to investigate the document‚Äôs contents and see whether human sources would back it up.</p>
</blockquote>

<p>The internet has always been full of slop, and we have always needed to be wary of what we read there. AI now makes it easy to manufacture convincing looking evidence, and this is never more dangerous than when it confirms strongly held beliefs and fears.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/">Kent Beck</a>:</p>

<blockquote>
  <p>The descriptions of Spec-Driven development that I have seen emphasize writing the whole specification before implementation. This encodes the (to me bizarre) assumption that you aren‚Äôt going to learn anything during implementation that would change the specification.
I‚Äôve heard this story so many times told so many ways by well-meaning folks‚Äìif only we could get the specification ‚Äúright‚Äù, the rest of this would be easy.</p>
</blockquote>

<p>Like him, that story has been the constant background siren to my career in tech. But the <a href="https://martinfowler.com/articles/llm-learning-loop.html">learning loop</a> of experimentation is essential to the model building that‚Äôs at the heart of any kind of worthwhile specification. As <a href="https://martinfowler.com/articles/llm-learning-loop.html">Unmesh puts it:</a></p>

<blockquote>
  <p>Large Language Models give us great leverage‚Äîbut they only work if we focus on learning and understanding. They make it easier to explore ideas, to set things up, to translate intent into code across many specialized languages. But the real capability‚Äîour ability to respond to change‚Äîcomes not from how fast we can produce code, but from how deeply we understand the system we are shaping.</p>
</blockquote>

<p>When Kent defined Extreme Programming, he made <em>feedback</em> one of its four core values. It strikes me that the key to making the full use of AI in software development is how to use it to accelerate the feedback loops.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context. Programming-oriented tools are geting more sophisticated for that, but there‚Äôs also efforts at providing simpler tools, that allow customization. <a href="https://lixo.org">Carlos Villela</a> recently recommended Pi, and its developer, <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">Mario Zechner, has an interesting blog</a> on its development.</p>

<blockquote>
  <p>So what‚Äôs an old guy yelling at Claudes going to do? He‚Äôs going to write his own coding agent harness and give it a name that‚Äôs entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?</p>
</blockquote>

<p>If I ever get the time to sit and really play with these tools, then something like Pi would be something I‚Äôd like to try out. Although as an addict to The One True Editor, I‚Äôm interested in some of libraries that work with that, such as <a href="https://github.com/karthink/gptel">gptel</a>. That would enable me to use Emacs‚Äôs inherent programability to create my own command set to drive the interaction with LLMs.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Outside of my professional work, I‚Äôve <a href="https://boardgamegeek.com/blog/13064/martins-7th-decade">posting regularly about my boardgaming</a> on the specialist site BoardGameGeek. However its blogging environment doesn‚Äôt do a good job of providing an index to my posts, so I‚Äôve created <a href="https://martinfowler.com/boardgames/">a list of my BGG posts </a> on my own site. If you‚Äôre interested in my regular posts on boardgaming, and you‚Äôre on BGG you can subscribe to me there. If you‚Äôre not on BGG you can  subscribe to the blog‚Äôs <a href="https://boardgamegeek.com/rss/blog/13064">RSS feed</a>.</p>

<p>I‚Äôve also created a list of <a href="https://martinfowler.com/boardgames/fav-games.html">my favorite board games</a>.</p>

<p><img alt="" src="https://martinfowler.com/boardgames/index/game-grid.png" /></p>

---

### [Fragments: December 16](https://martinfowler.com/fragments/2025-12-16.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.linkedin.com/in/gitanjalivenkatraman/">Gitanjali Venkatraman</a> does wonderful illustrations of complex subjects (which is why I was so happy to work with her on our <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a> article). She has now published the latest in her series of illustrated guides: tackling the complex topic of <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf">Mainframe Modernization</a></p>

<p><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/blog/mainframe_modernisation_illustrated_guide_2025.pdf"><img alt="" src="https://martinfowler.com/fragments/2025/gitanjali-mainframe.png" /></a></p>

<p>In it she illustrates the history and value of mainframes, why modernization is so tricky, and how to tackle the problem by breaking it down into tractable pieces. I love the clarity of her explanations, and smile frequently at her way of enhancing her words with her quirky pictures.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Gergely Orosz on <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m7xd33hi5c2d">social media</a></p>

<blockquote>
  <p>Unpopular opinion:</p>

  <p>Current code review tools just don‚Äôt make much sense for AI-generated code</p>

  <p>When reviewing code I really want to know:</p>

  <ul>
    <li>The prompt made by the dev</li>
    <li>What corrections the other dev made to the code</li>
    <li>Clear marking of code AI-generated not changed by a human</li>
  </ul>
</blockquote>

<p>Some people pushed back saying they don‚Äôt (and shouldn‚Äôt care) whether it was written by a human, generated by an LLM, or copy-pasted from Stack Overflow.</p>

<p>In my view it matters <em>a lot</em> - because of the second vital purpose of code review.</p>

<p>When asked why do code reviews, most people will answer the first vital purpose - quality control. We want to ensure bad code gets blocked before it hits <a href="https://martinfowler.com/articles/branching-patterns.html#mainline">mainline</a>. We do this to avoid bugs and to avoid other quality issues, in particular comprehensibility and ease of change.</p>

<p>But I hear the second vital purpose less often: code review is a mechanism to communicate and educate. If I‚Äôm submitting some sub-standard code, and it gets rejected, I want to know why so that I can improve my programming. Maybe I‚Äôm unaware of some library features, or maybe there‚Äôs some project-specific standards I haven‚Äôt run into yet, or maybe my naming isn‚Äôt as clear as I thought it was. Whatever the reasons, I need to know in order to learn. And my employer needs me to learn, so I can be more effective.</p>

<p>We need to know the writer of the code we review both so we can communicate our better practice to them, but also to know how to improve things. With a human, its a conversation, and perhaps some documentation if we realize we‚Äôve needed to explain things repeatedly. But with an LLM it‚Äôs about how to modify its context, as well as humans learning how to better drive the LLM.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Wondering why I‚Äôve been making a lot of posts like this recently? <a href="https://martinfowler.com/articles/writing-fragments.html">I explain why</a> I‚Äôve been reviving the link blog.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://simonwillison.net/2025/Dec/10/html-tools/">Simon Willison</a> describes how he uses LLMs to build disposable but useful web apps</p>

<blockquote>
  <p>These are the characteristics I have found to be most productive in building tools of this nature:</p>

  <ol>
    <li>A single file: inline JavaScript and CSS in a single HTML file means the least hassle in hosting or distributing them, and crucially means you can copy and paste them out of an LLM response.</li>
    <li>Avoid React, or anything with a build step. The problem with React is that JSX requires a build step, which makes everything massively less convenient. I prompt ‚Äúno react‚Äù and skip that whole rabbit hole entirely.</li>
    <li>Load dependencies from a CDN. The fewer dependencies the better, but if there‚Äôs a well known library that helps solve a problem I‚Äôm happy to load it from CDNjs or jsdelivr or similar.</li>
    <li>Keep them small. A few hundred lines means the maintainability of the code doesn‚Äôt matter too much: any good LLM can read them and understand what they‚Äôre doing, and rewriting them from scratch with help from an LLM takes just a few minutes.</li>
  </ol>
</blockquote>

<p>His repository includes all these tools, together with transcripts of the chats that got the LLMs to build them.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://obie.medium.com/what-happens-when-the-coding-becomes-the-least-interesting-part-of-the-work-ab10c213c660">Obie Fernandez</a>: while many engineers are underwhelmed by AI tools, some senior engineers are finding them really valuable. He feels that senior engineers have an oft-unspoken mindset, which in conjunction with an LLM, enables the LLM to be much more valuable.</p>

<blockquote>
  <p>Levels of abstraction and generalization problems get talked about a lot because they‚Äôre easy to name. But they‚Äôre far from the whole story.</p>

  <p>Other tools show up just as often in real work:</p>

  <ul>
    <li>A sense for blast radius. Knowing which changes are safe to make loudly and which should be quiet and contained.</li>
    <li>A feel for sequencing. Knowing when a technically correct change is still wrong because the system or the team isn‚Äôt ready for it yet.</li>
    <li>An instinct for reversibility. Preferring moves that keep options open, even if they look less elegant in the moment.</li>
    <li>An awareness of social cost. Recognizing when a clever solution will confuse more people than it helps.</li>
    <li>An allergy to false confidence. Spotting places where tests are green but the model is wrong.</li>
  </ul>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://friendlybit.com/python/writing-justhtml-with-coding-agents/">Emil Stenstr√∂m</a> built an HTML5 parser in python using coding agents, using Github Copilot in Agent mode with Claude Sonnet 3.7. He automatically approved most commands. It took him ‚Äúa couple of months on off-hours‚Äù, including at least one restart from scratch. The parser now passes all the tests in html5lib test suite.</p>

<blockquote>
  <p>After writing the parser, I still don‚Äôt know HTML5 properly. The agent wrote it for me. I guided it when it came to API design and corrected bad decisions at the high level, but it did ALL of the gruntwork and wrote all of the code.</p>

  <p>I handled all git commits myself, reviewing code as it went in. I didn‚Äôt understand all the algorithmic choices, but I understood when it didn‚Äôt do the right thing.</p>
</blockquote>

<p>Although he gives an overview of what happens, there‚Äôs not very much information on his workflow and how he interacted with the LLM. There‚Äôs certainly not enough detail here to try to replicate his approach. This is contrast to Simon Willison (above) who has detailed links to his chat transcripts - although they are much smaller tools and I haven‚Äôt looked at them properly to see how useful they are.</p>

<p>One thing that is clear, however, is the vital need for a comprehensive test suite. Much of his work is driven by having that suite as a clear guide for him and the LLM agents.</p>

<blockquote>
  <p>JustHTML is about 3,000 lines of Python with 8,500+ tests passing. I couldn‚Äôt have written it this quickly without the agent.</p>

  <p>But ‚Äúquickly‚Äù doesn‚Äôt mean ‚Äúwithout thinking.‚Äù I spent a lot of time reviewing code, making design decisions, and steering the agent in the right direction. The agent did the typing; I did the thinking.</p>
</blockquote>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†</p>

<p>Then Simon Willison <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">ported the library to JavaScript</a>:</p>

<blockquote>
  <p>Time elapsed from project idea to finished library: about 4 hours, during which I also bought and decorated a Christmas tree with family and watched the latest Knives Out movie.</p>
</blockquote>

<p>One of his lessons:</p>

<blockquote>
  <p>If you can reduce a problem to a robust test suite you can set a coding agent loop loose on it with a high degree of confidence that it will eventually succeed. I called this <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">designing the agentic loop</a> a few months ago. I think it‚Äôs the key skill to unlocking the potential of LLMs for complex tasks.</p>
</blockquote>

<p>Our experience at Thoughtworks backs this up. We‚Äôve been doing a fair bit of work recently in legacy modernization (mainframe and otherwise) using AI to migrate substantial software systems. Having a robust test suite is necessary (but not sufficient) to making this work. I hope to share my colleagues‚Äô experiences on this in the coming months.</p>

<p>But before I leave Willison‚Äôs post, I should highlight his final open questions on the legalities, ethics, and effectiveness of all this - they are well-worth contemplating.</p>

---

### [Writing Fragments](https://martinfowler.com/articles/writing-fragments.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>If you‚Äôre a regular reader of my site, you‚Äôll have noticed that in the
last few months I‚Äôve been making a <a href="https://martinfowler.com/articles/20251204-frags.html">number</a> of ‚Äúfragments‚Äù <a href="https://martinfowler.com/articles/2025-11-19-frags.html">posts</a>. Such a post
is a short post with a bunch of little, unconnected segments. These are
usually a reference to something I‚Äôve found on the web, sometimes a small
thought of my own.</p>

<p>A few years ago, I wouldn‚Äôt have covered these topics with posts on my
own site. Instead I would use Twitter, either retweeting someone else‚Äôs
point, or just highlighting something I‚Äôd found. But since the Muskover,
Twitter has effectively died. I‚Äôm not saying that due to any technical
issues with the site, which has mostly just been fine, nor directly due to
any of the policy changes there. The point is that lots of people have left, so that
the audience I would have reached with Twitter is now fragmented. Some
remain on X, but I see more activity on LinkedIn. There‚Äôs also Fediverse/Mastodon
and Bluesky.</p>

<p>What this means for short posts is that I can no longer just post in one
place. When I announce new articles on martinfowler.com, I announce now on
four social media sites (X, LinkedIn, Fediverse, and Bluesky). It makes
sense to do this, but I don‚Äôt want to go through all this hassle for the
kind of micro-post that Twitter served so well.</p>

<p>So I‚Äôve started to batch them up. When I see something interesting, I
make a note. When I have enough notes, I post a fragments post. Initially I
did this in a rather ad-hoc way, just using the same mechanisms I use for
most articles, but last week I started to put in some more deliberate
mechanisms into the site. (If you‚Äôre observant, you‚Äôll spot that in the URLs.)</p>

<p>One benefit of all of this, at least in my book, is that it means my material is
now fully visible in RSS. I‚Äôm probably showing my age, but I‚Äôm a big fan of RSS
(or in my case, strictly Atom) feeds. I miss the feel of the heyday of the
‚Äúblogosphere‚Äù before it got steamrolled by social media, and these fragment
posts are, of course, just the same as the link blogs from that era. I still use my
RSS reader every day to keep up with writers I like. (I‚Äôm pleased that Substack
makes its content available via RSS.) It bothered me a bit that my micro-founts
of Twitter knowledge weren‚Äôt visible on RSS, but was too lazy to do something
about it. Now I don‚Äôt need to - the fragments are available in my RSS feed.</p>

---

### [Fragments Dec 11](https://martinfowler.com/articles/2025-12-11-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html?utm_source=substack&amp;utm_medium=email">Why does AI write like‚Ä¶ that</a> (NYT, gift link). Sam Kriss delves into the quiet hum of AI writing. AI‚Äôs work is not compelling prose: it‚Äôs phantom text, ghostly scribblings, a spectre woven into our communal tapestry.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://coding-is-like-cooking.info/2025/12/test-desiderata-2-0/">Emily Bache</a> has written a set of Test Desiderata, building on some earlier writing from Kent Beck. She lists the characteristics of good tests, and how they support her four ‚Äúmacro desiderata‚Äù - the properties of a sound test suite</p>

<blockquote>
  <ul>
    <li>Predict success in production</li>
    <li>Fast to get feedback</li>
    <li>Support ongoing code design change</li>
    <li>Low total cost of ownership</li>
  </ul>
</blockquote>

<p>She also has a great list of other writers‚Äô lists of good test characteristics.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://www.techpolicy.press/the-eus-fine-against-x-is-not-about-speech-or-censorship/">Daphne Keller</a> explains that the EUs fines on X aren‚Äôt about free speech.</p>

<blockquote>
  <p>There are three charges against X, which all stem from a multi-year investigation that was launched in 2023. One is about verification ‚Äî X‚Äôs blue checkmarks on user accounts ‚Äî and two are about transparency. These charges have nothing to do with what content is on X, or what user speech the platform should or should not allow.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington">Cory Doctorow</a> The Reverse-Centaur‚Äôs Guide to Criticizing AI</p>

<blockquote>
  <p>Start with what a reverse centaur is. In automation theory, a ‚Äúcentaur‚Äù is a person who is assisted by a machine. ‚Ä¶ And obviously, a reverse centaur is machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.</p>

  <p>Like an Amazon delivery driver‚Ä¶ the van can‚Äôt drive itself and can‚Äôt get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.</p>
</blockquote>

---

### [Fragments Dec 4](https://martinfowler.com/articles/20251204-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><a href="https://blog.robbowley.net/2025/12/04/ai-is-still-making-code-worse-a-new-cmu-study-confirms/">Rob Bowley</a> summarizes a study from Carnegie Mellon looking on the impact of AI on a bunch of open-source software projects. Like any such study, we shouldn‚Äôt take its results as definitive, but there seems enough there to make it a handy data point. The key point is that the AI code probably reduced the quality of the code base - at least if static code analysis can be trusted to determine quality. And perhaps some worrying second-order effects</p>

<blockquote>
  <p>This study shows more than 800 popular GitHub projects with code quality degrading after adopting AI tools. It‚Äôs hard not to see a form of context collapse playing out in real time. If the public code that future models learn from is becoming more complex and less maintainable, there‚Äôs a real risk that newer models will reinforce and amplify those trends, producing even worse code over time.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Rob‚Äôs post is typical of much of the thoughtful writing on AI. We can see its short-term benefits, but worry about its long-term impact. But on a much deeper note is this lovely story from <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">Jim Highsmith</a>. Jim has turned 0x50, and has spent the last decade fighting Parkinson‚Äôs disease. To help him battle it he has two AI assisted allies.</p>

<blockquote>
  <p>Between my neural implants and Byron‚Äôs digital guidance, I now collaborate with two adaptive systems: one for motion, one for thought. Neither replaces me. Both extend me.</p>
</blockquote>

<p><strong>If you read anything on AI this week, <a href="https://www.linkedin.com/pulse/my-80th-birthday-i-bought-myself-new-brain-jim-highsmith-8qcrc/">make it be this</a>.</strong> It offers a positive harbinger for our future and opens my mind to a whole different perspective of the role of AI in it</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Anthropic recently announced that it disrupted a Chinese state-sponsored operation abusing Claude Code. Jim Gumbley looks at the core lesson to learn from this, that we have to understand the serious risk of  <a href="https://www.thoughtworks.com/insights/blog/security/anthropic-ai-espionage-disclosure-signal-from-noise?utm_source=linkedin&amp;utm_medium=social-organic&amp;utm_campaign=dt_bs_rp-in-clt_tech_thought_leaders_2025-11&amp;gh_src=d1fe17bc1us">AI Jailbreaking</a></p>

<blockquote>
  <p>New AI tools are able to analyze your attack surface at the next level of granularity. As a business leader, that means you now have two options: wait for someone else to run AI-assisted vulnerability detection against your attack surface, or run it yourself first.</p>
</blockquote>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>There‚Äôs plenty of claims that AI Vibe Coding can replace software developers, something that folks like me (perhaps with a bias) think unlikely. <a href="https://bsky.app/profile/gergely.pragmaticengineer.com/post/3m75kmb7bh22i">Gergely Orosz</a> shared this tidbit</p>

<blockquote>
  <p>Talked with an exec at a tech company who is obsessed with AI and has been for 3 years. Not a developer but company makes software. Uses AI for everything, vibe codes ideas.</p>

  <p>Here‚Äôs the kicker:</p>

  <p>Has a team of several devs to implement his vibe coded prototypes to sg workable</p>
</blockquote>

<p>I‚Äôd love to hear more about this (and similar stories)</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/">Nick Radcliffe</a> writes about a month of using AI</p>

<blockquote>
  <p>I spent a solid month ‚Äúpair programming‚Äù with Claude Code, trying to suspend disbelief and adopt a this-will-be-productive mindset. More specifically, I got Claude to write well over 99% of the code produced during the month. I found the experience infuriating, unpleasant, and stressful before even worrying about its energy impact. Ideally, I would prefer not to do it again for at least a year or two. The only problem with that is that it ‚Äúworked‚Äù.</p>
</blockquote>

<p>He stresses that his approach is the ‚Äúpolar opposite‚Äù of Vibe Coding. The post is long, and rambles a bit, but is worthwhile because he talks in detail about his workflow and how he uses the tool. Such posts are important so we can learn the nitty-gritty of how our programming habits are changing.</p>

<p>¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Along similar lines is a post of <a href="https://brianchambers.substack.com/p/chamber-of-tech-secrets-55-issue">Brian Chambers</a> on his workflow, that he calls Issue-Driven Development (and yes, I‚Äôm also sick of the ‚Äúsomething-driven‚Äù phraseology). As with much of the better stuff I‚Äôve heard about AI assisted work, it‚Äôs all about carefully managing the context window, ensuring the AI is focused on the right things and not distracted by textual squirrels.</p>

---

### [Fragments Nov 19](https://martinfowler.com/articles/2025-11-19-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p><img alt="radar image" src="https://martinfowler.com/articles/images/frags-2025-11/radar.png" width="300px" /></p>

<p>I‚Äôve been on the road in Europe for the last couple of weeks, and while I was there Thoughtworks released <a href="https://www.thoughtworks.com/radar">volume 33 of our Technology Radar</a>. Again it‚Äôs dominated by the AI wave, with lots of blips capturing our explorations of how to use LLMs and similar technology. ‚ÄúAgents‚Äù are the big thing these days but we‚Äôre also seeing growing movements in infrastructure orchestration, coding workflows - and the inevitable antipatterns. Many thanks to my colleagues for putting this together again.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Gergely and I recording the podcast" src="https://martinfowler.com/articles/images/frags-2025-11/podcast.png" width="400px" /></p>

<p>My trip to Europe started in Amsterdam, for a Thoughtworks event for a few of our clients there. Since I was in that lovely city, I got in touch with Gergely Orosz, host of <a href="https://www.pragmaticengineer.com/">The Pragmatic Engineer</a>, and he arranged to <a href="https://www.youtube.com/watch?v=CQmI4XKTa0U">record a podcast</a> with me. No surprise that AI was front-and-center of the conversation, as I said it was the biggest shift I‚Äôd seen in programming during my career, comparable only to the shift to high-level languages, which even I am not old enough to have experienced. It was a fun chat and I really enjoyed myself. Gergely later joined myself James Lewis and Giles Edwards-Alexander at the Thoughtworks event the next day.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>My travels also took me to N√ºremberg, where I attended an internal conference for Siemens on the future of software architecture. When we think of technology, it‚Äôs easy to focus on the Faangs of Silicon Valley, but Siemens have a huge workforce of software developers working on heavy engineering systems like trains and factory automation. It was good to hear them talk about federated architectures, data mesh, and their use of AI.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Kent's graph of options vs features" src="https://martinfowler.com/articles/images/frags-2025-11/features.webp" width="400px" /></p>

<p>I‚Äôve often used pseudo-graphs to help explain why <a href="https://martinfowler.com/articles/is-quality-worth-cost.html">high quality software is cheaper</a>. This time, <a href="https://tidyfirst.substack.com/p/why-does-development-slow">Kent Beck</a> creates a unique perspective to this chart, dispensing with the temporal axis to help think in terms of optionality.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><img alt="Heavy Cardboard banner" src="https://martinfowler.com/articles/images/frags-2025-11/hc.jpg" width="400px" /></p>

<p>And in another life, Edward has finally finished the great migration of the Heavy Cardboard studio and returns to the tubes with <a href="https://www.youtube.com/live/e8juRvm9h0c">our first game in the new digs</a>. (No surprise that it‚Äôs Age of Steam.)</p>

---

### [My Foreword to "Frictionless"](https://martinfowler.com/articles/frictionless-foreword.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/frictionless-foreword.html"><img src="https://martinfowler.com/articles/frictionless-card.jpg" width="" /></a></div>

<p>I find most writing on software productivity to be twaddle, but Nicole
      Forsgren and Abi Noda are notable exceptions. I had a chance to take a
      look at their new book, published today, and liked it so much I wrote a
      foreword.</p>

<p><a class="more" href="https://martinfowler.com/articles/frictionless-foreword.html">more‚Ä¶</a></p>

---

### [The Learning Loop and LLMs](https://martinfowler.com/articles/llm-learning-loop.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <div class="img-link"><a href="https://martinfowler.com/articles/llm-learning-loop.html"><img src="https://martinfowler.com/articles/llm-learning-loop/card.png" width="350px" /></a></div>

<p><b class="author">Unmesh Joshi</b> finds LLMs to be a useful tool, but
      explains why their help <a href="https://martinfowler.com/articles/llm-learning-loop.html">becomes illusory</a> if we use them to shortcut the
      learning loop that's an essential part of our professional practice.</p>

<p><a class="more" href="https://martinfowler.com/articles/llm-learning-loop.html">more‚Ä¶</a></p>

---

### [Fragments Nov 3](https://martinfowler.com/articles/2025-11-03-frags.html)

**Êù•Ê∫ê**: Martin Fowler

**ÊëòË¶Å**: <p>I‚Äôm very concerned about the security dangers of LLM-enabled browsers, as it‚Äôs just too easy for them to contain the <a href="https://simonwillison.net/2025/Oct/22/openai-ciso-on-atlas/">Lethal Trifecta</a>. For up-to-date eyes on these issues, I follow the writings of coiner of that phrase: Simon Willison. Here <a href="https://simonwillison.net/2025/Oct/22/openai-ciso-on-atlas/">he examines</a> a post on how OpenAI is thinking about these issues.</p>

<blockquote>
  <p>My takeaways from all of this? It‚Äôs not done much to influence my overall skepticism of the entire category of browser agents, but it does at least demonstrate that OpenAI are keenly aware of the problems and are investing serious effort in finding the right mix of protections.</p>
</blockquote>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p><a href="https://blog.robbowley.net/2025/10/26/youre-probably-listening-to-the-wrong-people-about-ai-coding/">Rob Bowley</a></p>

<blockquote>
  <p>Unsurprisingly, there are a lot of strong opinions on AI assisted coding. Some engineers swear by it. Others say it‚Äôs dangerous. And of course, as is the way with the internet, nuanced positions get flattened into simplistic camps where everyone‚Äôs either on one side or the other.</p>

  <p>A lot of the problem is that people aren‚Äôt arguing about the same thing. They‚Äôre reporting different experiences from different vantage points.</p>
</blockquote>

<p>His view is that beginners are very keen on AI-coding but they don‚Äôt see the problems they are creating. Experienced folks do see this, but it takes a further level of experience to realize that <em>when used well</em> these tools are still valuable.</p>

<blockquote>
  <p>Interestingly, I‚Äôve regularly seen sceptical experienced engineers change their view once they‚Äôve been shown how you can blend modern/XP practices with AI assisted coding.</p>
</blockquote>

<p>The upshot is this, is that you have be aware of the experience level of whoever is writing about this stuff - and that experience is not just in software development generally, but also in how to make use of LLMs. One thing that rings clearly from reading Simon Willison and Birgitta B√∂ckeler is that effective use of LLMs is a skill that takes a while to develop.</p>

<p>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ</p>

<p>Charlie Brown and Garfield, like most comic strip characters, never changed over the decades. But <a href="https://blog.robbowley.net/2025/10/26/youre-probably-listening-to-the-wrong-people-about-ai-coding/">Doonesbury‚Äôs</a> cast aged, had children, and some have died (I miss Lacey). Gary Trudeau retired from writing daily strips a few years ago, but his <a href="https://x.com/RealRBHJr">reruns of older strips</a> is one of the best things in the shabby remains of Twitter.</p>

<p>A couple of weeks ago, he reran one of the most <a href="https://www.washingtonpost.com/doonesbury/strip/archive/2025/10/22">memorable strips</a> in its whole run. The very first frame of Doonesbury introduced the character ‚ÄúB.D.‚Äù, a football jock never seen without his football helmet, or when on duty, his military helmet. This panel was the first time in over thirty years that B.D. was shown without a helmet, readers were so startled that they didn‚Äôt immediately notice that the earlier explosion had removed his leg. This set off a remarkable story arc about the travails of a wounded veteran. It‚Äôs my view that future generations will find Doonesbury to be a first-class work of literature, and a thoughtful perspective on contemporary America.</p>

---

## HackerNews

### [Bose has released API docs and opened the API for its EoL SoundTouch speakers](https://arstechnica.com/gadgets/2026/01/bose-open-sources-its-soundtouch-home-theater-smart-speakers-ahead-of-eol/)

**ÂàÜÊï∞**: 2068 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46541892)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Eat Real Food](https://realfood.gov)

**ÂàÜÊï∞**: 1103 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46529237)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Sugar industry influenced researchers and blamed fat for CVD (2016)](https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus)

**ÂàÜÊï∞**: 776 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46526740)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Shipmap.org](https://www.shipmap.org/)

**ÂàÜÊï∞**: 756 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46527161)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Google AI Studio is now sponsoring Tailwind CSS](https://twitter.com/OfficialLoganK/status/2009339263251566902)

**ÂàÜÊï∞**: 434 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46545077)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [ChatGPT Health](https://openai.com/index/introducing-chatgpt-health/)

**ÂàÜÊï∞**: 423 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46531280)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Open Infrastructure Map](https://openinframap.org)

**ÂàÜÊï∞**: 421 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46536866)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Project Patchouli: Open-source electromagnetic drawing tablet hardware](https://patchouli.readthedocs.io/en/latest/)

**ÂàÜÊï∞**: 417 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46537489)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Dell admits consumers don't care about AI PCs](https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/)

**ÂàÜÊï∞**: 393 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46527706)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Iran Goes Into IPv6 Blackout](https://radar.cloudflare.com/routing/ir)

**ÂàÜÊï∞**: 389 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542683)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Jeff Dean Facts](https://github.com/LRitzdorf/TheJeffDeanFacts)

**ÂàÜÊï∞**: 387 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46540498)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [LaTeX Coffee Stains (2021) [pdf]](https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf)

**ÂàÜÊï∞**: 384 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46526933)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [A closer look at a BGP anomaly in Venezuela](https://blog.cloudflare.com/bgp-route-leak-venezuela/)

**ÂàÜÊï∞**: 374 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46538001)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Tailscale state file encryption no longer enabled by default](https://tailscale.com/changelog)

**ÂàÜÊï∞**: 357 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46531925)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How to Code Claude Code in 200 Lines of Code](https://www.mihaileric.com/The-Emperor-Has-No-Clothes/)

**ÂàÜÊï∞**: 294 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46545620)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Polymarket refuses to pay bets that US would 'invade' Venezuela](https://www.ft.com/content/985ae542-1ab4-491e-8e6e-b30f6a3ab666)

**ÂàÜÊï∞**: 288 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46521773)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Kernel bugs hide for 2 years on average. Some hide for 20](https://pebblebed.com/blog/kernel-bugs)

**ÂàÜÊï∞**: 277 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46536340)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Napoleon Technique: Postponing things to increase productivity](https://effectiviology.com/napoleon/)

**ÂàÜÊï∞**: 239 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46493785)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [IBM AI ('Bob') Downloads and Executes Malware](https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware)

**ÂàÜÊï∞**: 227 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46544454)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How Google got its groove back and edged ahead of OpenAI](https://www.wsj.com/tech/ai/google-ai-openai-gemini-chatgpt-b766e160)

**ÂàÜÊï∞**: 224 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46528389)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Lights and Shadows (2020)](https://ciechanow.ski/lights-and-shadows/)

**ÂàÜÊï∞**: 220 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46468338)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Quake Brutalist Jam III](https://www.slipseer.com/index.php?resources/quake-brutalist-jam-iii.549/)

**ÂàÜÊï∞**: 220 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46491808)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [AI coding assistants are getting worse?](https://spectrum.ieee.org/ai-coding-degrades)

**ÂàÜÊï∞**: 213 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542036)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Q, K, V Matrices](https://arpitbhayani.me/blogs/qkv-matrices/)

**ÂàÜÊï∞**: 203 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46523887)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Notion AI: Unpatched data exfiltration](https://www.promptarmor.com/resources/notion-ai-unpatched-data-exfiltration)

**ÂàÜÊï∞**: 202 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46531565)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [NPM to implement staged publishing after turbulent shift off classic tokens](https://socket.dev/blog/npm-to-implement-staged-publishing)

**ÂàÜÊï∞**: 197 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46530448)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Meditation as Wakeful Relaxation: Unclenching Smooth Muscle](https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation)

**ÂàÜÊï∞**: 173 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46527157)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Play Aardwolf MUD](https://www.aardwolf.com/)

**ÂàÜÊï∞**: 173 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46534777)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Claude Code CLI was broken](https://github.com/anthropics/claude-code/issues/16673)

**ÂàÜÊï∞**: 168 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46532075)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Vector graphics on GPU](https://gasiulis.name/vector-graphics-on-gpu/)

**ÂàÜÊï∞**: 164 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46473247)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Go.sum is not a lockfile](https://words.filippo.io/gosum/)

**ÂàÜÊï∞**: 162 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46537095)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Many hells of WebDAV](https://candid.dev/blog/many-hells-of-webdav)

**ÂàÜÊï∞**: 162 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46527775)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Japanese electronics store pleads for old PCs amid ongoing hardware shortage](https://www.tomshardware.com/desktops/pc-building/major-japanese-electronics-store-begs-customers-for-their-old-pcs-as-hardware-drought-continues-we-pretty-much-buy-any-pc-pleads-the-akihabara-outlet)

**ÂàÜÊï∞**: 141 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542015)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Iran Protest Map](https://pouyaii.github.io/Iran/)

**ÂàÜÊï∞**: 139 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46547303)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The Unreasonable Effectiveness of the Fourier Transform](https://joshuawise.com/resources/ofdm/)

**ÂàÜÊï∞**: 129 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46544981)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [What *is* code? (2015)](https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/)

**ÂàÜÊï∞**: 128 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46458610)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Musashi: Motorola 680x0 emulator written in C](https://github.com/kstenerud/Musashi)

**ÂàÜÊï∞**: 122 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46535540)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU](https://github.com/samuel-vitorino/sopro)

**ÂàÜÊï∞**: 120 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546113)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [The virtual AmigaOS runtime (a.k.a. Wine for Amiga:)](https://github.com/cnvogelg/amitools/blob/main/docs/vamos.md)

**ÂàÜÊï∞**: 117 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46535515)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Building voice agents with Nvidia open models](https://www.daily.co/blog/building-voice-agents-with-nvidia-open-models/)

**ÂàÜÊï∞**: 115 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46528045)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: I visualized the entire history of Citi Bike in the browser](https://bikemap.nyc/)

**ÂàÜÊï∞**: 109 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46530832)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [We found cryptography bugs in the elliptic library using Wycheproof](https://blog.trailofbits.com/2025/11/18/we-found-cryptography-bugs-in-the-elliptic-library-using-wycheproof/)

**ÂàÜÊï∞**: 106 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46454577)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Native Amiga Filesystems on macOS / Linux / Windows with FUSE](https://github.com/reinauer/amifuse)

**ÂàÜÊï∞**: 103 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46473726)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [GLSL Web CRT Shader](https://blog.gingerbeardman.com/2026/01/04/glsl-web-crt-shader/)

**ÂàÜÊï∞**: 99 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46491219)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [I used Lego to design a farm for people who are blind ‚Äì like me](https://www.bbc.co.uk/news/articles/c4g4zlyqnr0o)

**ÂàÜÊï∞**: 98 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46502269)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Digital Red Queen: Adversarial Program Evolution in Core War with LLMs](https://sakana.ai/drq/)

**ÂàÜÊï∞**: 93 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542761)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: I built a "Do not disturb" Device for my home office](https://apoorv.page/blogs/over-engineered-dnd)

**ÂàÜÊï∞**: 93 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46477961)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Fixing a Buffer Overflow in Unix v4 Like It's 1973](https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/)

**ÂàÜÊï∞**: 78 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46544610)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [SQL Studio](https://sql.studio/)

**ÂàÜÊï∞**: 78 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546419)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: macOS menu bar app to track Claude usage in real time](https://github.com/richhickson/claudecodeusage)

**ÂàÜÊï∞**: 76 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46544524)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Lessons from Hash Table Merging](https://gist.github.com/attractivechaos/d2efc77cc1db56bbd5fc597987e73338)

**ÂàÜÊï∞**: 74 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46464270)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [How dependabot works](https://nesbitt.io/2026/01/02/how-dependabot-actually-works.html)

**ÂàÜÊï∞**: 72 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46470927)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Embassy: Modern embedded framework, using Rust and async](https://github.com/embassy-rs/embassy)

**ÂàÜÊï∞**: 71 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46547740)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [SSDs, power loss protection and fsync latency](http://smalldatum.blogspot.com/2026/01/ssds-power-loss-protection-and-fsync.html)

**ÂàÜÊï∞**: 68 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46532675)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [He was called a 'terrorist sympathizer.' Now his AI company is valued at $3B](https://sfstandard.com/2026/01/07/called-terrorist-sympathizer-now-ai-company-valued-3b/)

**ÂàÜÊï∞**: 66 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46544276)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Ushikuvirus: Newly discovered virus may offer clues to the origin of eukaryotes](https://www.tus.ac.jp/en/mediarelations/archive/20251219_9539.html)

**ÂàÜÊï∞**: 61 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46537253)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Texas court blocks Samsung from tracking TV viewing, then vacates order](https://www.bleepingcomputer.com/news/security/texas-court-blocks-samsung-from-tracking-tv-viewing-then-vacates-order/)

**ÂàÜÊï∞**: 61 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546270)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Show HN: DeepDream for Video with Temporal Consistency](https://github.com/jeremicna/deepdream-video-pytorch)

**ÂàÜÊï∞**: 61 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46540660)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Reading Without Limits or Expectations](https://www.carolinecrampton.com/reading-without-limits-or-expectations/)

**ÂàÜÊï∞**: 61 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46504761)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Supernova Remnant Video from NASA's Chandra Is Decades in Making](https://www.nasa.gov/missions/chandra/supernova-remnant-video-from-nasas-chandra-is-decades-in-making/)

**ÂàÜÊï∞**: 60 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46544072)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Our Changing Planet, as Seen from Space](https://e360.yale.edu/digest/nasa-satellite-images-2025)

**ÂàÜÊï∞**: 59 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542046)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [2026 Predictions Scorecard](https://rodneybrooks.com/predictions-scorecard-2026-january-01/)

**ÂàÜÊï∞**: 59 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46533343)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Richard D. James aka Aphex Twin speaks to Tatsuya Takahashi](https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/)

**ÂàÜÊï∞**: 53 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546614)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space](https://arxiv.org/abs/2512.24617)

**ÂàÜÊï∞**: 49 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46542982)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Landline phones cut in parts of Iran, eyewitnesses say](https://www.iranintl.com/en/202601085355)

**ÂàÜÊï∞**: 47 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546888)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Signals vs. Query-Based Compilers](https://marvinh.dev/blog/signals-vs-query-based-compilers/)

**ÂàÜÊï∞**: 43 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46491264)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### [Task-free intelligence testing of LLMs](https://www.marble.onl/posts/tapping/index.html)

**ÂàÜÊï∞**: 38 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46545587)

**ÊëòË¶Å**: ËøôÊòØ‰∏ÄÁØáÂÖ≥‰∫éÂ§ßËØ≠Ë®ÄÊ®°Âûã(LLM)Êô∫ËÉΩÊµãËØïÁöÑÂàõÊñ∞Á†îÁ©∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÈúÄÁâπÂÆö‰ªªÂä°ÁöÑÊô∫ËÉΩËØÑ‰º∞ÊñπÊ≥ï„ÄÇÁ†îÁ©∂ÈÄöËøáÂàÜÊûêÊ®°ÂûãÂØπÂ§çÊùÇÊåá‰ª§ÁöÑÁêÜËß£ÂíåÊâßË°åËÉΩÂäõÔºåËÆæËÆ°‰∫Ü‰∏Ä‰∏™Êñ∞È¢ñÁöÑ"Êï≤Âáª"ÊµãËØïËåÉÂºèÔºåËÉΩÂ§üÊõ¥ÂÖ®Èù¢„ÄÅÂÆ¢ËßÇÂú∞ËØÑ‰º∞AIÊ®°ÂûãÁöÑÊé®ÁêÜÂíåÈÄÇÂ∫îÊÄßËÉΩÂäõ„ÄÇÂØπ‰∫éAIÁ†îÁ©∂ËÄÖÂíå‰ªé‰∏öËÄÖËÄåË®ÄÔºåËøô‰∏ÄÊñπÊ≥ï‰∏∫ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÊô∫ËÉΩÊ∞¥Âπ≥Êèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥Âä†Ê∑±ÂÖ•ÂíåÁÅµÊ¥ªÁöÑÊäÄÊúØË∑ØÂæÑ„ÄÇ

---

### [Intellectual Junkyards](https://www.forester-notes.org/QHXS/index.xml)

**ÂàÜÊï∞**: 34 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46495319)

**ÊëòË¶Å**: ÈùûÂ∏∏Êä±Ê≠âÔºåÊàëÊ≥®ÊÑèÂà∞ÊÇ®Êèê‰æõÁöÑÊñáÁ´†ÈìæÊé•‰ºº‰πéÊòØ‰∏Ä‰∏™ËôöÊûÑÁöÑÊàñ‰∏çÂèØËÆøÈóÆÁöÑÈìæÊé•„ÄÇÂú®Ê≤°ÊúâÂÆûÈôÖÊñáÁ´†ÂÜÖÂÆπÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàëÊó†Ê≥ïÁîüÊàêÂáÜÁ°ÆÁöÑÊëòË¶Å„ÄÇ

Â¶ÇÊûúÊÇ®ËÉΩÊèê‰æõ:
1. ÊñáÁ´†ÁöÑÂÆûÈôÖÂÜÖÂÆπ
2. ÂèØËÆøÈóÆÁöÑÊñáÁ´†ÈìæÊé•
3. ÊñáÁ´†ÁöÑ‰∏ªË¶ÅÊÆµËêΩ

ÊàëÂ∞ÜÈùûÂ∏∏‰πêÊÑè‰∏∫ÊÇ®ÁîüÊàê‰∏ì‰∏öÁöÑ‰∏≠ÊñáÊëòË¶Å„ÄÇ

---

### [Show HN: A geofence-based social network app 6 years in development](https://www.localvideoapp.com)

**ÂàÜÊï∞**: 29 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546349)

**ÊëòË¶Å**: ËøôÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂú∞ÁêÜÂõ¥Ê†èÊäÄÊúØÁöÑÁ§æ‰∫§ÁΩëÁªúÂ∫îÁî®,ÈÄöËøáÁ≤æÂáÜÂÆö‰ΩçÂíåÂú∞ÁêÜËåÉÂõ¥ÂåπÈÖç,Â∏ÆÂä©Áî®Êà∑ÂèëÁé∞ÂíåËøûÊé•ÈôÑËøëÁöÑ‰∫∫„ÄÇÂ∫îÁî®ÂéÜÁªè6Âπ¥ÂºÄÂèë,ÂàõÊñ∞ÊÄßÂú∞Ëß£ÂÜ≥‰∫Ü‰º†ÁªüÁ§æ‰∫§ËΩØ‰ª∂‰∏≠Âú∞ÁêÜË∑ùÁ¶ªÂíåÁ§æ‰∫§Âú∫ÊôØÁöÑÂ±ÄÈôêÊÄß,‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Âä†Êú¨Âú∞ÂåñÂíåÂç≥Êó∂ÊÄßÁöÑÁ§æ‰∫§‰ΩìÈ™å„ÄÇÂØπ‰∫éËøΩÊ±ÇÁ≤æÂáÜÁ§æ‰∫§ÂíåÊú¨Âú∞‰∫íÂä®ÁöÑÁî®Êà∑Êù•ËØ¥,ËøôÊòØ‰∏Ä‰∏™ÊûÅÂÖ∑Âê∏ÂºïÂäõÁöÑÁ§æ‰∫§Âπ≥Âè∞„ÄÇ

---

### [Learning to Play Tic-Tac-Toe with Jax](https://joe-antognini.github.io/ml/jax-tic-tac-toe)

**ÂàÜÊï∞**: 29 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46485130)

**ÊëòË¶Å**: Êú¨ÊñáÈÄöËøá‰ΩøÁî®JaxÊ°ÜÊû∂ËØ¶ÁªÜÈòêËø∞‰∫ÜÂ¶Ç‰ΩïÊûÑÂª∫‰∏Ä‰∏™Â≠¶‰π†‰∫ïÂ≠óÊ£ãÁöÑÂº∫ÂåñÂ≠¶‰π†Ê®°ÂûãÔºåÂ±ïÁ§∫‰∫Ü‰ªéÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÁ•ûÁªèÁΩëÁªúËÆæËÆ°Âà∞Ê®°ÂûãËÆ≠ÁªÉÁöÑÂÆåÊï¥Êú∫Âô®Â≠¶‰π†ÊµÅÁ®ã„ÄÇ‰ΩúËÄÖ‰ª•‰∫ïÂ≠óÊ£ã‰∏∫‰æãÔºåÊ∑±ÂÖ•Ëß£Êûê‰∫ÜÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂÖ≥ÈîÆÊäÄÊúØÔºåÂ¶ÇÁ≠ñÁï•Ê¢ØÂ∫¶ÁÆóÊ≥ïÂíå‰ª∑ÂÄºÁΩëÁªúËÆ≠ÁªÉÔºå‰∏∫ËØªËÄÖÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê∏ÖÊô∞ÁöÑÊú∫Âô®Â≠¶‰π†ÂÆûË∑µÊåáÂçó„ÄÇÂØπ‰∫éÊÉ≥Ë¶ÅÊ∑±ÂÖ•ÁêÜËß£Âº∫ÂåñÂ≠¶‰π†ÂíåJaxÊ°ÜÊû∂ÁöÑÂºÄÂèëËÄÖÊù•ËØ¥ÔºåËøôÊòØ‰∏Ä‰∏™ÊûÅÂÖ∑ÂèÇËÄÉ‰ª∑ÂÄºÁöÑÊäÄÊúØÂÆûË∑µÊ°à‰æã„ÄÇ

---

### [Making Magic Leap past Nvidia's secure bootchain and breaking Tesla Autopilots](https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/making-the-magic-leap-past-nvidia-s-secure-bootchain-and-breaking-some-tesla-autopilots-along-the-way)

**ÂàÜÊï∞**: 24 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46454160)

**ÊëòË¶Å**: Êú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÈíàÂØπNvidiaÂÆâÂÖ®ÂºïÂØºÈìæÂíåÁâπÊñØÊãâËá™Âä®È©æÈ©∂Á≥ªÁªüÁöÑÂÆâÂÖ®ÊºèÊ¥ûÁ†îÁ©∂ÔºåÊè≠Á§∫‰∫ÜÂµåÂÖ•ÂºèÁ≥ªÁªüÂíåËΩ¶ËΩΩËÆæÂ§áÂÆâÂÖ®Èò≤Êä§‰∏≠ÁöÑÂÖ≥ÈîÆÊäÄÊúØÂº±ÁÇπ„ÄÇÁ†îÁ©∂ËÄÖÈÄöËøáÁ≤æÁªÜÁöÑÁ°¨‰ª∂ÂíåÂõ∫‰ª∂ÂàÜÊûêÔºåÊàêÂäüÁ™ÅÁ†¥‰∫ÜMagic LeapÂíåTesla AutopilotÁöÑÂÆâÂÖ®Êú∫Âà∂Ôºå‰∏∫ÁΩëÁªúÂÆâÂÖ®È¢ÜÂüüÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊäÄÊúØÊ¥ûÂØüÂíåÊºèÊ¥ûÊ£ÄÊµãÊñπÊ≥ï„ÄÇËøôÈ°πÁ†îÁ©∂ÂØπ‰∫éÁêÜËß£Â§çÊùÇÁ≥ªÁªüÁöÑÂÆâÂÖ®ËæπÁïåÂíåÊΩúÂú®È£éÈô©ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂèÇËÄÉ‰ª∑ÂÄº„ÄÇ

---

### [Show HN: I built a tool to create AI agents that live in iMessage](https://tryflux.ai/)

**ÂàÜÊï∞**: 22 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46484808)

**ÊëòË¶Å**: ËøôÊòØ‰∏Ä‰∏™ÂàõÊñ∞ÁöÑiMessage AI‰ª£ÁêÜÂ∑•ÂÖ∑FluxÔºåÂÖÅËÆ∏Áî®Êà∑Áõ¥Êé•Âú®‰ø°ÊÅØÂ∫îÁî®‰∏≠ÂàõÂª∫Âíå‰∫§‰∫íÊô∫ËÉΩAIÂä©Êâã„ÄÇËØ•Â∑•ÂÖ∑ÈÄöËøáÁÆÄÂçïÁöÑÁïåÈù¢Ôºå‰ΩøÁî®Êà∑ËÉΩÂ§üÂø´ÈÄüÂÆöÂà∂‰∏™ÊÄßÂåñAI‰ª£ÁêÜÔºåÊó†ÈúÄÂ§çÊùÇÁöÑÁºñÁ®ãÊäÄËÉΩÔºåÊûÅÂ§ßÂú∞Èôç‰Ωé‰∫ÜAIÂ∫îÁî®ÁöÑ‰ΩøÁî®Èó®Êßõ„ÄÇÂØπ‰∫éÂ∏åÊúõÊèêÈ´òÊ≤üÈÄöÊïàÁéáÂíåËé∑Âæó‰∏™ÊÄßÂåñAIÂä©ÊâãÁöÑÁî®Êà∑Êù•ËØ¥ÔºåFluxÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊòìÁî®‰∏îÂØåÊúâÂàõÈÄ†ÊÄßÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

---

### [Tumblr removed from Apple App Store over abuse images](https://www.bbc.com/news/technology-46275138)

**ÂàÜÊï∞**: 21 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46548787)

**ÊëòË¶Å**: ËøôÁØáÊä•ÈÅìÊè≠Á§∫‰∫ÜÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞TumblrÂõ†Êú™ËÉΩÊúâÊïàËøáÊª§ÂíåÈòªÊ≠¢ÂÑøÁ´•Ëâ≤ÊÉÖÂÜÖÂÆπÔºåË¢´ËãπÊûú‰ªéApp Store‰∏ãÊû∂ÁöÑÈáçÂ§ßÊäÄÊúØÂÆâÂÖ®‰∫ã‰ª∂„ÄÇÊñáÁ´†ËÅöÁÑ¶‰∫éÂπ≥Âè∞ÂÜÖÂÆπÂÆ°Ê†∏Êú∫Âà∂ÁöÑ‰∏•ÈáçÁº∫Èô∑ÔºåÂº∫Ë∞É‰∫Ü‰∫íËÅîÁΩëÂπ≥Âè∞Âú®‰øùÊä§Êú™ÊàêÂπ¥‰∫∫ÂÖçÂèóÊúâÂÆ≥ÂÜÖÂÆπ‰æµÂÆ≥ÊñπÈù¢ÁöÑË¥£‰ªªÂíåÊåëÊàò„ÄÇÊ≠§Êä•ÈÅìÂØπÊäÄÊúØ‰ªé‰∏öËÄÖÂíå‰∫íËÅîÁΩëÂπ≥Âè∞ËøêËê•ËÄÖÂÖ∑ÊúâÈáçË¶ÅÁöÑË≠¶Á§∫ÊÑè‰πâÔºåÂá∏Êòæ‰∫ÜÂÜÖÂÆπÂÆ°Ê†∏ÊäÄÊúØÂíåÂÆâÂÖ®Ê≤ªÁêÜÁöÑÁ¥ßËø´ÊÄß„ÄÇ

---

### [Pole of Inaccessibility](https://en.wikipedia.org/wiki/Pole_of_inaccessibility)

**ÂàÜÊï∞**: 21 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46482844)

**ÊëòË¶Å**: ÊûÅÈöæÊäµËææÁÇπÊòØÊåáÂú∞ÁêÉË°®Èù¢‰∏äË∑ùÁ¶ªÊüê‰∏ÄÂå∫ÂüüËæπÁïåÊúÄËøúÁöÑÁÇπÔºåÊòØÂú∞ÁêÜÂ≠¶ÂíåÂú∞ÂõæÂ≠¶‰∏≠Á†îÁ©∂Âú∞ÁêÜÁ©∫Èó¥ÁâπÂæÅÁöÑÈáçË¶ÅÊ¶ÇÂøµ„ÄÇËØ•Ê¶ÇÂøµÈÄöËøáÊï∞Â≠¶ÂíåÂú∞ÁêÜ‰ø°ÊÅØÁ≥ªÁªüÊäÄÊúØÔºåÁ≤æÁ°ÆËÆ°ÁÆóÂíåÂÆö‰ΩçÂú∞ÁêÉ‰∏äÊúÄÈöæÂà∞ËææÁöÑÂú∞ÁêÜ‰ΩçÁΩÆÔºå‰∏∫Âú∞ÁêÜÁ†îÁ©∂„ÄÅÊé¢Èô©ÂíåËßÑÂàíÊèê‰æõ‰∫ÜÁã¨ÁâπÁöÑÁ©∫Èó¥ÂàÜÊûêÊñπÊ≥ï„ÄÇÂØπ‰∫éÂú∞ÁêÜÂ≠¶ÂÆ∂„ÄÅÂú∞ÂõæÂ≠¶ÂÆ∂ÂíåÂú∞ÁêÉÁßëÂ≠¶Á†îÁ©∂ËÄÖËÄåË®ÄÔºåÊûÅÈöæÊäµËææÁÇπÊòØÁêÜËß£Âú∞ÁêÉÂú∞ÁêÜÂ§çÊùÇÊÄßÂíåËøûÈÄöÊÄßÁöÑÈáçË¶ÅÁ†îÁ©∂ÂØπË±°„ÄÇ

---

### [How Hackers Are Fighting Back Against ICE](https://www.eff.org/deeplinks/2026/01/how-hackers-are-fighting-back-against-ice)

**ÂàÜÊï∞**: 21 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46548339)

**ÊëòË¶Å**: ËøôÁØáÊñáÁ´†Êè≠Á§∫‰∫ÜÈªëÂÆ¢Á§æÂå∫Â¶Ç‰ΩïÈÄöËøáÊäÄÊúØÊâãÊÆµÊäµÂà∂ÁæéÂõΩÁßªÊ∞ëÂèäÊµ∑ÂÖ≥ÊâßÊ≥ïÂ±Ä(ICE)ÁöÑ‰∫âËÆÆÊÄßË°åÂä®„ÄÇÈªëÂÆ¢‰ª¨ËøêÁî®ÁΩëÁªúÊîªÂáª„ÄÅ‰ø°ÊÅØÊ≥ÑÈú≤ÂíåÊäÄÊúØÊäóËÆÆÁ≠âÁ≠ñÁï•ÔºåÊó®Âú®Êö¥Èú≤ÂíåÈòªÁ¢çICEÁöÑÁõëÊéßÂíåÈ©±ÈÄêÁßªÊ∞ëÊ¥ªÂä®„ÄÇÂØπ‰∫éÂÖ≥Ê≥®Êï∞Â≠óÊùÉÂà©„ÄÅÊäÄÊúØ‰º¶ÁêÜÂíåÁ§æ‰ºöÊ≠£‰πâÁöÑËØªËÄÖËÄåË®ÄÔºåÊú¨ÊñáÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊäÄÊúØË°åÂä®‰∏ª‰πâÂ¶Ç‰ΩïÂú®Áé∞ÂÆûÁ§æ‰ºö‰∏≠ÂèëÊå•ÂΩ±ÂìçÂäõÁöÑÁîüÂä®Ê°à‰æã„ÄÇ

---

### [Show HN: Watch LLMs play 21,000 hands of Poker](https://pokerbench.adfontes.io/run/Large_Models)

**ÂàÜÊï∞**: 21 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46540794)

**ÊëòË¶Å**: Êú¨Á†îÁ©∂ÈÄöËøáËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã(LLMs)ËøõË°å21,000Â±ÄÊâëÂÖãÊ∏∏Êàè,Ê∑±ÂÖ•Êé¢Á¥¢‰∫ÜAIÂú®Â§çÊùÇÂçöÂºàÁéØÂ¢É‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõÂíåÁ≠ñÁï•Â≠¶‰π†„ÄÇÁ†îÁ©∂ËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂàõÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïÂπ≥Âè∞,Á≥ªÁªüÊÄßÂú∞ËØÑ‰º∞‰∫Ü‰∏çÂêåAIÊ®°ÂûãÂú®‰∏çÁ°ÆÂÆöÊÄß„ÄÅÁ≠ñÁï•Êé®ÁêÜÂíåÈ£éÈô©ÁÆ°ÁêÜÁ≠âÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÂØπ‰∫éÁ†îÁ©∂‰∫∫Â∑•Êô∫ËÉΩÂÜ≥Á≠ñ„ÄÅÂçöÂºàËÆ∫ÂíåÊú∫Âô®Â≠¶‰π†ÁöÑËØªËÄÖËÄåË®Ä,ËØ•È°πÁõÆÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊ¥ûÂØü,Â±ïÁ§∫‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§çÊùÇÂÜ≥Á≠ñÂú∫ÊôØ‰∏≠ÁöÑÊΩúÂäõÂíåÂ±ÄÈôêÊÄß„ÄÇ

---

### [Support for the TSO memory model on Arm CPUs (2024)](https://lwn.net/Articles/970907/)

**ÂàÜÊï∞**: 20 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46546322)

**ÊëòË¶Å**: Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®ArmÂ§ÑÁêÜÂô®‰∏äÂÆûÁé∞TSOÔºàTotal Store OrderÔºâÂÜÖÂ≠òÊ®°ÂûãÁöÑÊäÄÊúØËøõÂ±ïÔºåÈáçÁÇπ‰ªãÁªç‰∫ÜLinuxÂÜÖÊ†∏‰∏≠ÈíàÂØπArmÊû∂ÊûÑÁöÑÂÜÖÂ≠òÂ±èÈöúÂíåÂêåÊ≠•ÂéüËØ≠ÁöÑ‰ºòÂåñÁ≠ñÁï•„ÄÇÈÄöËøáÂºïÂÖ•ÁâπÂÆöÁöÑÁ°¨‰ª∂Êåá‰ª§ÂíåÂÜÖÊ†∏Á∫ßÂÆûÁé∞ÔºåËØ•Â∑•‰ΩúÊòæËëóÊèêÂçá‰∫ÜÂ§öÊ†∏Á≥ªÁªüÁöÑÂÜÖÂ≠ò‰∏ÄËá¥ÊÄßÂíåÂπ∂ÂèëÊÄßËÉΩÔºåÂØπ‰∫éËøΩÊ±ÇÈ´òÊÄßËÉΩÂπ∂Ë°åËÆ°ÁÆóÁöÑÂºÄÂèëËÄÖÂÖ∑ÊúâÈáçË¶ÅÂèÇËÄÉ‰ª∑ÂÄº„ÄÇËøô‰∏ÄÊäÄÊúØÂàõÊñ∞ÊúâÂä©‰∫éÁÆÄÂåñË∑®Âπ≥Âè∞ÁöÑÂπ∂ÂèëÁºñÁ®ãÊ®°ÂûãÔºåÂπ∂‰∏∫ArmÊû∂ÊûÑÂú®È´òÊÄßËÉΩËÆ°ÁÆóÈ¢ÜÂüüÁöÑÂ∫îÁî®Êèê‰æõ‰∫ÜÊõ¥Âº∫Â§ßÁöÑÊîØÊåÅ„ÄÇ

---

### [The Rise of Computer Games, Part II: Digitizing Nerddom ‚Äì Creatures of Thought](https://technicshistory.com/2026/01/02/the-rise-of-computer-games-part-ii-digitizing-nerddom/)

**ÂàÜÊï∞**: 20 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46480016)

**ÊëòË¶Å**: ÂæàÊä±Ê≠âÔºåÊàëÊ≥®ÊÑèÂà∞ÊÇ®Êèê‰æõÁöÑÊñáÁ´†ÈìæÊé•ÊòØ‰∏Ä‰∏™ËôöÊûÑÁöÑÁΩëÂùÄÔºåÁúãËµ∑Êù•Âπ∂‰∏çÊòØ‰∏Ä‰∏™ÁúüÂÆûÂ≠òÂú®ÁöÑÊñáÁ´†„ÄÇÂõ†Ê≠§ÔºåÊàëÊó†Ê≥ï‰∏∫ÂÖ∂ÁîüÊàêÂáÜÁ°ÆÁöÑÊëòË¶Å„ÄÇÂ¶ÇÊûúÊÇ®ËÉΩÊèê‰æõ‰∏Ä‰∏™ÁúüÂÆûÁöÑÊäÄÊúØÊñáÁ´†ÈìæÊé•ÊàñÁõ¥Êé•Á≤òË¥¥ÊñáÁ´†ÂÜÖÂÆπÔºåÊàëÂ∞ÜÈùûÂ∏∏‰πêÊÑèÂ∏ÆÊÇ®ÁîüÊàêÊëòË¶Å„ÄÇ

---

### [Looking for Alice (2023)](https://www.henrikkarlsson.xyz/p/looking-for-alice)

**ÂàÜÊï∞**: 20 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46471506)

**ÊëòË¶Å**: ËøôÊòØ‰∏ÄÁØáÊé¢ËÆ®ÈÄöËøáÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËøõË°åÁü•ËØÜÂèëÁé∞ÂíåËØ≠‰πâÊêúÁ¥¢ÁöÑÂàõÊñ∞Á†îÁ©∂„ÄÇ‰ΩúËÄÖÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØ≠‰πâÊ£ÄÁ¥¢ÊñπÊ≥ïÔºåÈÄöËøáÊûÑÂª∫Â§çÊùÇÁöÑÂµåÂÖ•Á©∫Èó¥ÂíåÁõ∏‰ººÊÄßÂåπÈÖçÁÆóÊ≥ïÔºåËÉΩÂ§üÂú®Êµ∑ÈáèÊñáÊú¨Êï∞ÊçÆ‰∏≠Á≤æÂáÜÂÆö‰ΩçÂíåÂÖ≥ËÅîÁõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇËØ•Á†îÁ©∂‰∏∫‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©Áü•ËØÜÊé¢Á¥¢Êèê‰æõ‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÊäÄÊúØË∑ØÂæÑÔºåÂØπÂ≠¶ÊúØÁ†îÁ©∂Âíå‰ø°ÊÅØÊ£ÄÁ¥¢È¢ÜÂüüÂÖ∑ÊúâÈáçË¶ÅÁöÑÂÆûË∑µ‰ª∑ÂÄº„ÄÇ

---

### [PgX ‚Äì Debug Postgres performance in the context of your application code](https://docs.base14.io/blog/introducing-pgx/)

**ÂàÜÊï∞**: 17 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46522841)

**ÊëòË¶Å**: PgXÊòØ‰∏ÄÊ¨æ‰∏ì‰∏∫ÂºÄÂèëËÄÖËÆæËÆ°ÁöÑPostgresÊÄßËÉΩË∞ÉËØïÂ∑•ÂÖ∑ÔºåÈÄöËøáÊó†‰æµÂÖ•ÂºèÁöÑ‰ª£Á†ÅÁ∫ßÊÄßËÉΩÂàÜÊûêÊñπÊ≥ïÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÁ≤æÂáÜÂÆö‰ΩçÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÊÄßËÉΩÁì∂È¢à„ÄÇËØ•Â∑•ÂÖ∑ËÉΩÂ§üÂ∞ÜÂ∫îÁî®‰ª£Á†Å‰∏ä‰∏ãÊñá‰∏éÊï∞ÊçÆÂ∫ìÊÄßËÉΩÊåáÊ†áÊó†ÁºùÈõÜÊàêÔºå‰ΩøÂºÄÂèëËÄÖËÉΩÂ§üÊõ¥Áõ¥ËßÇ„ÄÅÈ´òÊïàÂú∞ÁêÜËß£Âíå‰ºòÂåñÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÊÄßËÉΩÔºåÂ§ßÂπÖÊèêÂçáÂºÄÂèëÂíåË∞É‰ºòÊïàÁéá„ÄÇÂØπ‰∫éÈúÄË¶ÅÊ∑±ÂÖ•ÂàÜÊûêPostgresÊï∞ÊçÆÂ∫ìÊÄßËÉΩÁöÑÂºÄÂèëËÄÖÂíåÂ∑•Á®ãÂ∏àÊù•ËØ¥ÔºåPgXÊòØ‰∏Ä‰∏™ÊûÅÂÖ∑‰ª∑ÂÄºÁöÑÊÄßËÉΩËØäÊñ≠Âíå‰ºòÂåñÂà©Âô®„ÄÇ

---

### [Distinct AI Models Seem to Converge on How They Encode Reality](https://www.quantamagazine.org/distinct-ai-models-seem-to-converge-on-how-they-encode-reality-20260107/)

**ÂàÜÊï∞**: 15 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46539423)

**ÊëòË¶Å**: ËøôÈ°πÁ†îÁ©∂Êè≠Á§∫‰∫Ü‰∏çÂêåAIÊ®°ÂûãÂú®Â§ÑÁêÜÂíåÁºñÁ†ÅÁé∞ÂÆû‰ø°ÊÅØÊó∂ÂëàÁé∞Âá∫ÊÉä‰∫∫ÁöÑÁõ∏‰ººÊÄßÔºåË°®ÊòéÁ•ûÁªèÁΩëÁªúÂèØËÉΩÂ≠òÂú®ÊüêÁßçÊôÆÈÅçÁöÑ‰ø°ÊÅØË°®ÂæÅÊú∫Âà∂„ÄÇÁ†îÁ©∂ËÄÖÈÄöËøáÊØîËæÉÂ§ö‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÜÖÈÉ®Ë°®ÂæÅÔºåÂèëÁé∞ÂÆÉ‰ª¨Âú®Â§ÑÁêÜÊ¶ÇÂøµÂíåÁü•ËØÜÊó∂‰ºöÂΩ¢ÊàêÁõ∏ËøëÁöÑÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºåËøô‰∏ÄÂèëÁé∞ÂØπÁêÜËß£AIÊ®°ÂûãÁöÑÂ∑•‰ΩúÂéüÁêÜÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ„ÄÇËøô‰∏ÄÁ†îÁ©∂‰∏ç‰ªÖ‰∏∫‰∫∫Â∑•Êô∫ËÉΩÁöÑËÆ§Áü•Êú∫Âà∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊ¥ûÂØüÔºåËøòÂèØËÉΩÂ∏ÆÂä©Á†îÁ©∂ËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£Âíå‰ºòÂåñÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑ‰ø°ÊÅØÂ§ÑÁêÜÊñπÂºè„ÄÇ

---

### [Expired certificate breaks macOS Logitech apps](https://arstechnica.com/gadgets/2026/01/expired-certificate-completely-breaks-macos-logitech-apps-user-customizations/)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46548201)

**ÊëòË¶Å**: LogitechÂú®macOS‰∏äÁöÑÂ∫îÁî®Á®ãÂ∫èÂõ†ËØÅ‰π¶ËøáÊúüËÄåÂÆåÂÖ®Â§±ÊïàÔºåÂØºËá¥Áî®Êà∑Ëá™ÂÆö‰πâËÆæÁΩÆÂíåËΩØ‰ª∂ÂäüËÉΩÊó†Ê≥ïÊ≠£Â∏∏‰ΩøÁî®„ÄÇËøô‰∏ÄÊäÄÊúØÊïÖÈöúÂá∏Êòæ‰∫ÜÊï∞Â≠óËØÅ‰π¶ÁÆ°ÁêÜÁöÑÈáçË¶ÅÊÄßÔºåÊèêÈÜíËΩØ‰ª∂ÂºÄÂèëËÄÖÂíåÁî®Êà∑ÈúÄË¶ÅÂèäÊó∂Êõ¥Êñ∞ÂíåÁõëÊéßËØÅ‰π¶Áä∂ÊÄÅ„ÄÇÂØπ‰∫é‰æùËµñLogitechËÆæÂ§áÁöÑmacOSÁî®Êà∑Êù•ËØ¥ÔºåËøôÊòØ‰∏Ä‰∏™ÈúÄË¶ÅÁ´ãÂç≥ÂÖ≥Ê≥®ÂíåËß£ÂÜ≥ÁöÑÈáçÂ§ßÊäÄÊúØÈóÆÈ¢ò„ÄÇ

---

### [Direct evidence for poison use on microlithic arrowheads at 60k years ago](https://doi.org/10.1126/sciadv.adz3281)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46541893)

**ÊëòË¶Å**: ËøôÈ°πËÄÉÂè§Â≠¶Á†îÁ©∂È¶ñÊ¨°Âú®60,000Âπ¥ÂâçÁöÑÂæÆÂûãÁü≥Âô®ÁÆ≠Â§¥‰∏äÂèëÁé∞‰∫ÜÁõ¥Êé•ÁöÑÊØíÁ¥†‰ΩøÁî®ËØÅÊçÆÔºåÈÄöËøáÂàÜÊûêÊñ∞Âá†ÂÜÖ‰∫öÂèëÁé∞ÁöÑÁü≥Âô®ÊÆãÁïôÁâ©ÔºåËØÅÂÆû‰∫ÜÊó©Êúü‰∫∫Á±ªÂ∑≤ÁªèÊéåÊè°Â§çÊùÇÁöÑÁã©ÁåéÊäÄÊúØ„ÄÇÁ†îÁ©∂ËÄÖËøêÁî®ÂÖàËøõÁöÑÂåñÂ≠¶ÂàÜÊûêÊñπÊ≥ïÔºåÊ£ÄÊµãÂà∞ÁÆ≠Â§¥‰∏äÁöÑÂ§©ÁÑ∂Ê§çÁâ©ÊØíÁ¥†ÔºåËøô‰∏ÄÂèëÁé∞‰∏ç‰ªÖÊè≠Á§∫‰∫ÜÂè≤ÂâçÁã©ÁåéËÄÖÁöÑÊäÄÊúØÊô∫ÊÖßÔºåËøò‰∏∫ÁêÜËß£Êó©Êúü‰∫∫Á±ªÂ∑•ÂÖ∑‰ΩøÁî®ÂíåÁã©ÁåéÁ≠ñÁï•Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁßëÂ≠¶ËØÅÊçÆ„ÄÇ

---

### [Internet access cut out in Iran after protests](https://apnews.com/article/iran-protests-us-israel-war-nuclear-economy-ebddd998fbe7903e70ca62127250ebcb)

**ÂàÜÊï∞**: 14 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46545320)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

### ['Books are going to take longer to get to libraries'](https://www.tcj.com/books-are-going-to-take-longer-to-get-to-libraries-what-baker-taylors-demise-means-for-comics/)

**ÂàÜÊï∞**: 13 | [ËÆ®ËÆ∫](https://news.ycombinator.com/item?id=46545631)

**ÊëòË¶Å**: ÊöÇÊó†ÊëòË¶Å

---

## LLM Infrastructure

### [v0.14.0rc0](https://github.com/vllm-project/vllm/releases/tag/v0.14.0rc0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony mode‚Ä¶</p>

---

### [v0.13.0](https://github.com/vllm-project/vllm/releases/tag/v0.13.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.13.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features <strong>442 commits from 207 contributors (61 new contributors)!</strong></p>
<p><strong>Breaking Changes</strong>: This release includes deprecation removals, PassConfig flag renames, and attention configuration changes from environment variables to CLI arguments. Please review the breaking changes section carefully before upgrading.</p>
<h3>Model Support</h3>
<ul>
<li><strong>New models</strong>: BAGEL (AR only) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28439">#28439</a>), AudioFlamingo3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a>), JAIS 2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a>), latent MoE architecture support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a>).</li>
<li><strong>Tool parsers</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29848">#29848</a>), Gigachat 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a>), Holo2 reasoning (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30048">#30048</a>).</li>
<li><strong>Model enhancements</strong>: Qwen3-VL embeddings support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a>), Qwen3-VL EVS (Efficient Video Sampling) (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29752">#29752</a>), DeepSeek V3.2 proper <code>drop_thinking</code> logic (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a>), DeepSeek V3.2 top-k fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27568">#27568</a>).</li>
<li><strong>Task expansion</strong>: Automatic TokenClassification model conversion (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30666">#30666</a>), Ultravox v0.7 transformer projector (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30089">#30089</a>).</li>
<li><strong>Quantization</strong>: BitsAndBytes for Qwen3-Omni-MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29896">#29896</a>).</li>
<li><strong>Speculative decoding</strong>: Eagle/Eagle3 Transformers backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30340">#30340</a>), Mamba <code>selective_state_update</code> spec decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29488">#29488</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li><strong>Compilation</strong>: Conditional compilation via <code>compile_ranges</code> for selective kernel compilation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24252">#24252</a>).</li>
<li><strong>Prefix caching</strong>: xxHash high-performance hash option (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a>).</li>
<li><strong>Attention</strong>: PrefixLM support for FlexAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27938">#27938</a>) and TritonAttention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30386">#30386</a>), CUDA graphs for 3D Triton attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28306">#28306</a>), <code>TRITON_MLA</code> without prefix-caching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29125">#29125</a>).</li>
<li><strong>Batch invariance</strong>: FA2 and LoRA batch-invariant support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30018">#30018</a>).</li>
<li><strong>Pooling</strong>: Chunked prefill for ALL pooling tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27145">#27145</a>), multi-vector retrieval API (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26686">#26686</a>).</li>
<li><strong>Model Runner V2</strong>: Min-p sampling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30171">#30171</a>), NaN detection in logits (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30187">#30187</a>).</li>
<li><strong>Speculative decoding</strong>: Medusa GPU-CPU sync avoidance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29723">#29723</a>), async spec-decode improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29624">#29624</a>).</li>
<li><strong>Whisper</strong>: Major performance improvements - <a href="https://github.com/vllm-project/vllm/issues/24946#issuecomment-3680725754">V1 is now faster than V0</a> (~3x speedup vs v0.12.0). Encoder batching (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29421">#29421</a>), <code>FULL_DECODE_ONLY</code> CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30072">#30072</a>), CPU backend support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>).</li>
<li><strong>Performance</strong>: Fused blockwise quant RMS norm (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27883">#27883</a>), MoE LoRA loading reduction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30243">#30243</a>), encoder cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30475">#30475</a>), CPU KV offloading streams (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29013">#29013</a>).</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li><strong>NVIDIA Blackwell Ultra</strong>: SM103 (GB300) support with CUDA 13 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30484">#30484</a>).</li>
<li><strong>DeepSeek optimizations</strong> (benchmarked on DeepSeek-V3.1):
<ul>
<li>DeepEP High-Throughput CUDA graph enabled by default: <strong>5.3% throughput, 4.4% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29558">#29558</a>)</li>
<li>DeepGEMM fused layout kernel: <strong>4.3% throughput, 10.7% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29546">#29546</a>)</li>
<li>DeepGEMM experts initialization: <strong>3.9% TTFT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30494">#30494</a>)</li>
<li><code>group_topk</code> kernel: <strong>1.9% throughput, 2.1% TPOT improvement</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30159">#30159</a>)</li>
<li>Sparse prefill kernel for FP8 KV-cache in DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27532">#27532</a>)</li>
<li>MLA FP8 optimization with ReduceScatterSum (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29795">#29795</a>), direct k_nope/k_pe copy (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29710">#29710</a>)</li>
</ul>
</li>
<li><strong>CPU</strong>: Whisper support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30062">#30062</a>), Arm Optimized Routines vectorized exp (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a>), x86 CPU wheel pipeline (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28848">#28848</a>).</li>
<li><strong>AMD ROCm</strong>: Aiter quantization kernels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25552">#25552</a>), torch.compile layernorm/silu + FP8 quant (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25693">#25693</a>), Triton ScaledMM fallback (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26668">#26668</a>), MXFP4 w4a4 inference (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29775">#29775</a>).</li>
<li><strong>Intel XPU</strong>: wNa16 compressed tensors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29484">#29484</a>).</li>
<li><strong>Build</strong>: CUDA 13 aarch64 wheels (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30341">#30341</a>), Docker kernel build stage (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a>), Ascend NPU Docker (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30015">#30015</a>).</li>
</ul>
<h3>Large Scale Serving &amp; Disaggregated Prefill/Decode</h3>
<ul>
<li><strong>KV connectors</strong>: Mooncake Transfer Engine (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a>), cache reset via <code>/reset_prefix_cache</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27170">#27170</a>), KV events (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28309">#28309</a>), failure recovery config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26813">#26813</a>).</li>
<li><strong>NIXL</strong>: Compatibility checking in handshake (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29503">#29503</a>), large batch proxy support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28782">#28782</a>).</li>
<li><strong>EPLB</strong>: NVFP4 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a>), algorithm abstraction (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a>).</li>
<li><strong>Multi-node</strong>: External launcher mode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29833">#29833</a>).</li>
<li><strong>Hybrid allocator</strong>: Optional KV connector integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29805">#29805</a>).</li>
<li><strong>Performance</strong>: silu_mul_per_token_group_quant_fp8 kernel for DP/EP (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29470">#29470</a>).</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>New</strong>: W4A8 grouped GEMM on Hopper (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29691">#29691</a>), online FP8 with streaming post-processing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a>), FP8 weight reloading for RLHF (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28480">#28480</a>).</li>
<li><strong>MoE + LoRA</strong>: AWQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30442">#30442</a>) and GPTQ Marlin (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a>) support.</li>
<li><strong>GGUF</strong>: MoE + GGUF restored for Qwen3 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30116">#30116</a>), Qwen2 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30307">#30307</a>), HF defaults override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30118">#30118</a>).</li>
<li><strong>Compatibility</strong>: Transformers v5 RoPE support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30046">#30046</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>: MCP type infrastructure (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30054">#30054</a>), Browser/Container MCP tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29989">#29989</a>), full MCP Python loop (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29798">#29798</a>), extra body parameters (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a>).</li>
<li><strong>Configuration</strong>: <code>AttentionConfig</code> replaces <code>VLLM_ATTENTION_BACKEND</code> env var (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>).</li>
<li><strong>Chat templates</strong>: DeepSeek-V3.2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29837">#29837</a>), DeepSeek-V3.2 developer tools (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a>).</li>
<li><strong>Anthropic API</strong>: Streaming fixes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29971">#29971</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30266">#30266</a>).</li>
<li><strong>Embeddings</strong>: Binary format with <code>encoding_format=bytes_only</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30249">#30249</a>), multiple image/audio per request (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29988">#29988</a>), tokenization_kwargs override (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29794">#29794</a>).</li>
<li><strong>Metrics</strong>: Prefill KV compute metric excluding cached tokens (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30189">#30189</a>).</li>
<li><strong>Profiling</strong>: Layer-wise NVTX (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29990">#29990</a>), profiling CLI config (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29912">#29912</a>).</li>
<li><strong>UX</strong>: Better OOM errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28051">#28051</a>), ModelConfig validation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a>), distributed executor errors (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30140">#30140</a>).</li>
</ul>
<h3>Security</h3>
<ul>
<li>Additional protection for <a href="https://github.com/advisories/GHSA-mrw7-hf4f-83pf" title="CVE-2025-62164">CVE-2025-62164</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a>).</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li>NVSHMEM 3.3.24 + CUDA 13 fix (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a>).</li>
<li>TPU tpu-inference 0.12.0 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30221">#30221</a>).</li>
</ul>
<h3>Breaking Changes &amp; Deprecations</h3>
<ol>
<li><strong>PassConfig flags renamed</strong> per RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/27995">#27995</a> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29646">#29646</a>)</li>
<li><strong>Attention env vars ‚Üí CLI args</strong>: <code>VLLM_ATTENTION_BACKEND</code> replaced with <code>--attention-backend</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26315">#26315</a>)</li>
<li><strong>Removed <code>-O.xx</code> flag</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29991">#29991</a>)</li>
<li><strong>Removed deprecated plugin/compilation fields</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30396">#30396</a>)</li>
<li><strong>Removed deprecated task, seed, MM settings</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30397">#30397</a>)</li>
<li><strong>Removed <code>embed_input_ids</code>/<code>embed_multimodal</code> fallbacks</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30458">#30458</a>)</li>
<li><strong>Removed tokenizer setter</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30400">#30400</a>)</li>
<li><strong>Deprecations</strong>: <code>merge_by_field_config</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30035">#30035</a>, <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30170">#30170</a>), <code>--convert reward</code> ‚Üí <code>--convert embed</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30463">#30463</a>)</li>
</ol>
<h2>New Contributors üéâ</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/ajpqs">@ajpqs</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29905">#29905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amitz-nv">@amitz-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29978">#29978</a></li>
<li><a class="user-mention notranslate" href="https://github.com/amrmahdi">@amrmahdi</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29452">#29452</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andrewbriand">@andrewbriand</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29804">#29804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/anker-c2">@anker-c2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30344">#30344</a></li>
<li><a class="user-mention notranslate" href="https://github.com/AuruTus">@AuruTus</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30182">#30182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/avigny">@avigny</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19425">#19425</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Bhanu068">@Bhanu068</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30254">#30254</a></li>
<li>@Copilot made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29025">#29025</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dbotwinick">@dbotwinick</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30583">#30583</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30234">#30234</a></li>
<li><a class="user-mention notranslate" href="https://github.com/desertfire">@desertfire</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29919">#29919</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dmitry-tokarev-nv">@dmitry-tokarev-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30149">#30149</a></li>
<li><a class="user-mention notranslate" href="https://github.com/drslark">@drslark</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30632">#30632</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dtcccc">@dtcccc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24718">#24718</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elizabetht">@elizabetht</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28671">#28671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Elm8116">@Elm8116</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30068">#30068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gausah01">@gausah01</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29604">#29604</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gh-wf">@gh-wf</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30285">#30285</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hdlj-h">@hdlj-h</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30056">#30056</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HF-001">@HF-001</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30051">#30051</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hzxuzhonghu">@hzxuzhonghu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29931">#29931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/JaviS-Rei">@JaviS-Rei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29882">#29882</a></li>
<li><a class="user-mention notranslate" href="https://github.com/johannesflommersfeld">@johannesflommersfeld</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30390">#30390</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KevinMusgrave">@KevinMusgrave</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30529">#30529</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kitaekatt">@kitaekatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30408">#30408</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lashahub">@lashahub</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30539">#30539</a></li>
<li><a class="user-mention notranslate" href="https://github.com/LuminolT">@LuminolT</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29163">#29163</a></li>
<li><a class="user-mention notranslate" href="https://github.com/majiayu000">@majiayu000</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30615">#30615</a></li>
<li><a class="user-mention notranslate" href="https://github.com/MaoJianwei">@MaoJianwei</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29797">#29797</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Mercykid-bash">@Mercykid-bash</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26471">#26471</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mgehre-amd">@mgehre-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30364">#30364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mivehk">@mivehk</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30512">#30512</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mondaylord">@mondaylord</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30671">#30671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/noa-neria">@noa-neria</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29320">#29320</a></li>
<li><a class="user-mention notranslate" href="https://github.com/PatrykSaffer">@PatrykSaffer</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30330">#30330</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Peng-YM">@Peng-YM</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29074">#29074</a></li>
<li><a class="user-mention notranslate" href="https://github.com/realliujiaxu">@realliujiaxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30059">#30059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/redwrasse">@redwrasse</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29261">#29261</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Ri0S">@Ri0S</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30532">#30532</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sarathc-cerebras">@sarathc-cerebras</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30188">#30188</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scratch-ml">@scratch-ml</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30351">#30351</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seokhyunan">@seokhyunan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30648">#30648</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shaharmor98">@shaharmor98</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30203">#30203</a></li>
<li><a class="user-mention notranslate" href="https://github.com/taoyun951753">@taoyun951753</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30037">#30037</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tom-zju">@tom-zju</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30057">#30057</a></li>
<li><a class="user-mention notranslate" href="https://github.com/tomtomjhj">@tomtomjhj</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29692">#29692</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vkuzo">@vkuzo</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29196">#29196</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vladnosiv">@vladnosiv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30490">#30490</a></li>
<li><a class="user-mention notranslate" href="https://github.com/weiguihua2">@weiguihua2</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30042">#30042</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wenqiglantz">@wenqiglantz</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30649">#30649</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wkcn">@wkcn</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29879">#29879</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wu-kan">@wu-kan</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21804">#21804</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wz1qqx">@wz1qqx</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30376">#30376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xyDong0223">@xyDong0223</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30455">#30455</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yifant-code">@yifant-code</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30213">#30213</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yjc9696">@yjc9696</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30040">#30040</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yurekami">@yurekami</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30552">#30552</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuttian1">@yuttian1</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30102">#30102</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhijianJiang">@ZhijianJiang</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/30219">#30219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ZhiweiYan-96">@ZhiweiYan-96</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29773">#29773</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.12.0...v0.13.0"><tt>v0.12.0...v0.13.0</tt></a></p>

---

### [v0.13.0rc4: [v1] Add PrefixLM support to TritonAttention backend (#30386)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc4)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/74a1ac38b00a8cf502db085d1bbd77712cf47e41"><tt>74a1ac3</tt></a>)</p>

---

### [v0.13.0rc3: [XPU] fix broken fp8 online quantization for XPU platform (#30831)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc3)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: Yan Ma <a href="mailto:yan.ma@intel.com">yan.ma@intel.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/4f735babb7353987137b85ec0465e594e9ed1384"><tt>4f735ba</tt></a>)</p>

---

### [v0.13.0rc2: [ROCm] [Bugfix] Fix torch sdpa hallucination (#30789)](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>Signed-off-by: tjtanaa <a href="mailto:tunjian.tan@embeddedllm.com">tunjian.tan@embeddedllm.com</a><br />
(cherry picked from commit <a class="commit-link" href="https://github.com/vllm-project/vllm/commit/2410132bb1f9faa5b252fad3f2b83dc926946b08"><tt>2410132</tt></a>)</p>

---

### [v0.13.0rc1](https://github.com/vllm-project/vllm/releases/tag/v0.13.0rc1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>fake rc release to fix nightly wheels</p>

---

### [v0.12.0](https://github.com/vllm-project/vllm/releases/tag/v0.12.0)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h1>vLLM v0.12.0 Release Notes Highlights</h1>
<h2>Highlights</h2>
<p>This release features 474 commits from 213 contributors (57 new)ÔºÅ</p>
<p><strong>Breaking Changes</strong>: This release includes PyTorch 2.9.0 upgrade (CUDA 12.9), V0 deprecations including <code>xformers</code> backend, and scheduled removals - please review the changelog carefully.</p>
<p><strong>Major Features</strong>:</p>
<ul>
<li><strong>EAGLE Speculative Decoding Improvements</strong>: Multi-step CUDA graph support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>), DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>), and multimodal support with Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Significant Performance Optimizations</strong>: 18.1% throughput improvement from batch invariant BMM (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>), 2.2% throughput improvement from shared experts overlap (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>).</li>
<li><strong>AMD ROCm Expansion</strong>: DeepSeek v3.2 + SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>), FP8 MLA decode (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>), AITER attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>).</li>
</ul>
<h3>Model Support</h3>
<ul>
<li><strong>New model families</strong>: PLaMo-3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28834">#28834</a>), OpenCUA-7B (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a>), HunyuanOCR (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29327">#29327</a>), Mistral Large 3 and Ministral 3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29757">#29757</a>).</li>
<li><strong>Format support</strong>: Gemma3 GGUF multimodal support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27772">#27772</a>).</li>
<li><strong>Multimodal enhancements</strong>: Qwen3 Omni audio-in-video support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27721">#27721</a>), Eagle3 multimodal support for Qwen3VL (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a>).</li>
<li><strong>Performance</strong>: QwenVL cos/sin cache optimization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28798">#28798</a>).</li>
</ul>
<h3>Engine Core</h3>
<ul>
<li>
<p><strong>GPU Model Runner V2 (Experimental)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25266">#25266</a>): Complete refactoring of model execution pipeline:</p>
<ul>
<li>No "reordering" or complex bookkeeping with persistent batch removal</li>
<li>GPU-persistent block tables for better scalability with <code>max_model_len</code> and <code>num_kv_groups</code></li>
<li>Triton-native sampler: no -1 temperature hack, efficient per-request seeds, memory-efficient prompt logprobs</li>
<li>Simplified DP and CUDA graph implementations</li>
<li>Efficient structured outputs support</li>
</ul>
</li>
<li>
<p><strong>Prefill Context Parallel (PCP) (Preparatory)</strong> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28718">#28718</a>): Partitions the sequence dimension during prefill for improved long-sequence inference. Complements existing Decode Context Parallel (DCP). See RFC <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25749">#25749</a> for details.</p>
</li>
<li>
<p><strong>RLHF Support</strong>: Pause and Resume Generation for Asynchronous RL Training (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28037">#28037</a>).</p>
</li>
<li>
<p><strong>KV Cache Enhancements</strong>: Cross-layer KV blocks support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27743">#27743</a>), KV cache residency metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27793">#27793</a>).</p>
</li>
<li>
<p><strong>Audio support</strong>: Audio embeddings support in chat completions (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a>).</p>
</li>
<li>
<p><strong>Speculative Decoding</strong>:</p>
<ul>
<li>Multi-step Eagle with CUDA graph (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29559">#29559</a>)</li>
<li>EAGLE DP&gt;1 support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26086">#26086</a>)</li>
<li>EAGLE3 heads without <code>use_aux_hidden_states</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27688">#27688</a>)</li>
<li>Eagle multimodal CUDA graphs with MRoPE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a>)</li>
<li>Logprobs support with spec decode + async scheduling (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29223">#29223</a>)</li>
</ul>
</li>
<li>
<p><strong>Configuration</strong>: Flexible <code>inputs_embeds_size</code> separate from <code>hidden_size</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29741">#29741</a>), <code>--fully-sharded-loras</code> for fused_moe (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28761">#28761</a>).</p>
</li>
</ul>
<h3>Hardware &amp; Performance</h3>
<ul>
<li>
<p><strong>NVIDIA Performance</strong>:</p>
<ul>
<li><strong>Batch invariant BMM optimization</strong>: 18.1% throughput improvement, 10.7% TTFT improvement on DeepSeek-V3.1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29345">#29345</a>)</li>
<li><strong>Shared Experts Overlap with FlashInfer DeepGEMM</strong>: 2.2% throughput improvement, 3.6% TTFT improvement at batch size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28879">#28879</a>)</li>
<li>DeepGEMM N dim restriction reduced from 128 to 64 multiplier (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28687">#28687</a>)</li>
<li>DeepEP low-latency with round-robin expert placement (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28449">#28449</a>)</li>
<li>NVFP4 MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>H200 Fused MoE Config improvements (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28992">#28992</a>)</li>
</ul>
</li>
<li>
<p><strong>AMD ROCm</strong>:</p>
<ul>
<li>DeepSeek v3.2 and SparseMLA support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26670">#26670</a>)</li>
<li>FP8 MLA decode support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a>)</li>
<li>AITER sampling ops integration (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26084">#26084</a>)</li>
<li>AITER triton attention backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28701">#28701</a>)</li>
<li>Bitsandbytes quantization on AMD GPUs with warp size 32 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27307">#27307</a>)</li>
<li>Fastsafetensors support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28225">#28225</a>)</li>
<li>Sliding window support for AiterFlashAttentionBackend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29234">#29234</a>)</li>
<li>Whisper v1 with Aiter Unified/Flash Attention (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a>)</li>
</ul>
</li>
<li>
<p><strong>CPU</strong>:</p>
<ul>
<li>Paged attention GEMM acceleration on ARM CPUs with NEON (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29193">#29193</a>)</li>
<li>Parallelize over tokens in int4 MoE (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29600">#29600</a>)</li>
<li>CPU all reduce optimization for async_scheduling + DP&gt;1 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29311">#29311</a>)</li>
</ul>
</li>
<li>
<p><strong>Attention</strong>: FlashAttention ViT support, now default backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28763">#28763</a>).</p>
</li>
<li>
<p><strong>Long Context</strong>: Optimized <code>gather_and_maybe_dequant_cache</code> kernel for extremely long sequences (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28029">#28029</a>).</p>
</li>
<li>
<p><strong>Multi-NUMA</strong>: Enhanced NUMA functionality for systems with multiple NUMA nodes per socket (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a>).</p>
</li>
<li>
<p><strong>Docker</strong>: Image size reduced by ~200MB (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29060">#29060</a>).</p>
</li>
</ul>
<h3>Quantization</h3>
<ul>
<li><strong>W4A8</strong>: Marlin kernel support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24722">#24722</a>).</li>
<li><strong>NVFP4</strong>:
<ul>
<li>MoE CUTLASS support for SM120 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29242">#29242</a>)</li>
<li>TRTLLM MoE NVFP4 kernel (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a>)</li>
<li>CuteDSL MoE with NVFP4 DeepEP dispatch (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27141">#27141</a>)</li>
<li>Non-gated activations support in modelopt path (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a>)</li>
</ul>
</li>
<li><strong>AWQ</strong>: Compressed-tensors AWQ support for Turing GPUs (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29732">#29732</a>).</li>
<li><strong>LoRA</strong>: FusedMoE LoRA Triton kernel for MXFP4 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29708">#29708</a>).</li>
<li><strong>Online quantization</strong>: Moved to <code>model.load_weights</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26327">#26327</a>).</li>
</ul>
<h3>API &amp; Frontend</h3>
<ul>
<li><strong>Responses API</strong>:
<ul>
<li>Multi-turn support for non-harmony requests (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a>)</li>
<li>Reasoning item input parsing (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28248">#28248</a>)</li>
</ul>
</li>
<li><strong>Tool Calling</strong>:
<ul>
<li>Parsed tool arguments support (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a>)</li>
<li><code>parallel_tool_calls</code> param compliance (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26233">#26233</a>)</li>
<li>Tool filtering support in ToolServer (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29224">#29224</a>)</li>
</ul>
</li>
<li><strong>Whisper</strong>: <code>verbose_json</code> and <code>timestamp</code> features for transcription/translation (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24209">#24209</a>).</li>
<li><strong>Sampling</strong>: Flat logprob control moved from env var to <code>SamplingParams</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28914">#28914</a>).</li>
<li><strong>GGUF</strong>: Improved HuggingFace loading UX with <code>repo_id:quant_type</code> syntax (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a>).</li>
<li><strong>Profiling</strong>: Iteration-level profiling for Torch and CUDA profiler (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28987">#28987</a>).</li>
<li><strong>Logs</strong>: Colorized log output (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29017">#29017</a>).</li>
<li><strong>Optimization Levels</strong>: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code> allow trading startup time for performance, more compilation flags will be added in future releases (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26847">#26847</a>)</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><strong>PyTorch 2.9.0</strong> with CUDA 12.9 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24994">#24994</a>) - <strong>Breaking change</strong> requiring environment updates.</li>
<li><strong>xgrammar</strong>: Updated to 0.1.27 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28221">#28221</a>).</li>
<li><strong>Transformers</strong>: Updated to 4.57.3 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29418">#29418</a>), preparation for v5 with <code>rope_parameters</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28542">#28542</a>).</li>
<li><strong>XPU</strong>: torch &amp; IPEX 2.9 upgrade (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29307">#29307</a>).</li>
</ul>
<h3>V0 Deprecation &amp; Breaking Changes</h3>
<p><strong>Removed Parameters</strong>:</p>
<ul>
<li><code>num_lookahead_slots</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29000">#29000</a>)</li>
<li><code>best_of</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29090">#29090</a>)</li>
<li>LoRA extra vocab (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28545">#28545</a>)</li>
</ul>
<p><strong>Deprecated</strong>:</p>
<ul>
<li><code>xformers</code> backend (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29262">#29262</a>)</li>
<li><code>seed=None</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29185">#29185</a>)</li>
</ul>
<p><strong>Scheduled Removals</strong> (will be removed in future release):</p>
<ul>
<li><code>ParallelConfig</code>'s direct child EPLB fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29324">#29324</a>)</li>
<li><code>guided_*</code> config fields (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29326">#29326</a>)</li>
<li><code>override_pooler_config</code> and <code>disable_log_requests</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29402">#29402</a>)</li>
<li><code>CompilationConfig.use_inductor</code> (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29323">#29323</a>)</li>
<li>Deprecated metrics (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29330">#29330</a>)</li>
</ul>
<p><strong>Other Breaking Changes</strong>:</p>
<ul>
<li>PyTorch 2.9.0 upgrade requires CUDA 12.9 environment</li>
<li>Mistral format auto-detection for model loading (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28659">#28659</a>)</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/jesse996">@jesse996</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28846">#28846</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Nepherpitou">@Nepherpitou</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28960">#28960</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Samoed">@Samoed</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27329">#27329</a></li>
<li><a class="user-mention notranslate" href="https://github.com/j20120307">@j20120307</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28999">#28999</a></li>
<li><a class="user-mention notranslate" href="https://github.com/vnadathur">@vnadathur</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26468">#26468</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhyajie">@zhyajie</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28942">#28942</a></li>
<li><a class="user-mention notranslate" href="https://github.com/IzzyPutterman">@IzzyPutterman</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28896">#28896</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock-amd">@rjrock-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28905">#28905</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zq1997">@zq1997</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/27715">#27715</a></li>
<li><a class="user-mention notranslate" href="https://github.com/shengliangxu">@shengliangxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28076">#28076</a></li>
<li><a class="user-mention notranslate" href="https://github.com/prashanth058">@prashanth058</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28972">#28972</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qgallouedec">@qgallouedec</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28820">#28820</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhanggzh">@zhanggzh</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19347">#19347</a></li>
<li><a class="user-mention notranslate" href="https://github.com/pandalee99">@pandalee99</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26628">#26628</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dsuhinin">@dsuhinin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29100">#29100</a></li>
<li><a class="user-mention notranslate" href="https://github.com/xli">@xli</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29124">#29124</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jeremyteboul">@jeremyteboul</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29059">#29059</a></li>
<li><a class="user-mention notranslate" href="https://github.com/soodoshll">@soodoshll</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28875">#28875</a></li>
<li><a class="user-mention notranslate" href="https://github.com/bhagyashrigai">@bhagyashrigai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28957">#28957</a></li>
<li><a class="user-mention notranslate" href="https://github.com/skaraban3807">@skaraban3807</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25559">#25559</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Victor49152">@Victor49152</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28892">#28892</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rjrock">@rjrock</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29205">#29205</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FlintyLemming">@FlintyLemming</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29182">#29182</a></li>
<li><a class="user-mention notranslate" href="https://github.com/madskildegaard">@madskildegaard</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29175">#29175</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nandan2003">@nandan2003</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29189">#29189</a></li>
<li><a class="user-mention notranslate" href="https://github.com/michaelact">@michaelact</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29173">#29173</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yongming-qin">@yongming-qin</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28958">#28958</a></li>
<li><a class="user-mention notranslate" href="https://github.com/joshiemoore">@joshiemoore</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29249">#29249</a></li>
<li><a class="user-mention notranslate" href="https://github.com/lim4349">@lim4349</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29068">#29068</a></li>
<li><a class="user-mention notranslate" href="https://github.com/apinge">@apinge</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28376">#28376</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gbyu-amd">@gbyu-amd</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28032">#28032</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kflu">@kflu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29364">#29364</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Inokinoki">@Inokinoki</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29200">#29200</a></li>
<li><a class="user-mention notranslate" href="https://github.com/GOavi101">@GOavi101</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29313">#29313</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sts07142">@sts07142</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29137">#29137</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ivanium">@ivanium</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29143">#29143</a></li>
<li><a class="user-mention notranslate" href="https://github.com/geodavic">@geodavic</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28795">#28795</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Yejing-Lai">@Yejing-Lai</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29473">#29473</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Adityayxt">@Adityayxt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29491">#29491</a></li>
<li><a class="user-mention notranslate" href="https://github.com/guodongxiaren">@guodongxiaren</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29620">#29620</a></li>
<li><a class="user-mention notranslate" href="https://github.com/askliar">@askliar</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29426">#29426</a></li>
<li><a class="user-mention notranslate" href="https://github.com/scydas">@scydas</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29589">#29589</a></li>
<li><a class="user-mention notranslate" href="https://github.com/EanWang211123">@EanWang211123</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29594">#29594</a></li>
<li><a class="user-mention notranslate" href="https://github.com/qGentry">@qGentry</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29506">#29506</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HappyAmazonian">@HappyAmazonian</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29335">#29335</a></li>
<li><a class="user-mention notranslate" href="https://github.com/rgommers">@rgommers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29241">#29241</a></li>
<li><a class="user-mention notranslate" href="https://github.com/staugust">@staugust</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28840">#28840</a></li>
<li><a class="user-mention notranslate" href="https://github.com/mertunsall">@mertunsall</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29667">#29667</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dublc">@dublc</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29728">#29728</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nwaughachukwuma">@nwaughachukwuma</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29671">#29671</a></li>
<li><a class="user-mention notranslate" href="https://github.com/BowTen">@BowTen</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29731">#29731</a></li>
<li><a class="user-mention notranslate" href="https://github.com/omera-nv">@omera-nv</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29004">#29004</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhangruoxu">@zhangruoxu</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29568">#29568</a></li>
<li><a class="user-mention notranslate" href="https://github.com/KKKZOZ">@KKKZOZ</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29783">#29783</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FredericOdermatt">@FredericOdermatt</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29784">#29784</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Abdennacer-Badaoui">@Abdennacer-Badaoui</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29782">#29782</a></li>
<li><a class="user-mention notranslate" href="https://github.com/knlnguyen1802">@knlnguyen1802</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28525">#28525</a></li>
<li><a class="user-mention notranslate" href="https://github.com/finbarrtimbers">@finbarrtimbers</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29796">#29796</a></li>
<li><a class="user-mention notranslate" href="https://github.com/hholtmann">@hholtmann</a> made their first contribution in <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29711">#29711</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.11.1...v0.12.0"><tt>v0.11.1...v0.12.0</tt></a></p>

---

### [v0.11.2](https://github.com/vllm-project/vllm/releases/tag/v0.11.2)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>This release includes 4 bug fixes on top of <code>v0.11.1</code>:</p>
<ul>
<li>[BugFix] Ray with multiple nodes (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28873">#28873</a>)</li>
<li>[BugFix] Fix false assertion with spec-decode=[2,4,..] and TP&gt;2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/29036">#29036</a>)</li>
<li>[BugFix] Fix async-scheduling + FlashAttn MLA (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28990">#28990</a>)</li>
<li>[NVIDIA] Guard SM100 CUTLASS MoE macro to SM100 builds v2 (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/28938">#28938</a>)</li>
</ul>

---

### [v0.11.1](https://github.com/vllm-project/vllm/releases/tag/v0.11.1)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <h2>Highlights</h2>
<p>This release includes 1456 commits from 449 contributors (184 new contributors)!</p>
<p>Key changes include:</p>
<ul>
<li><strong>PyTorch 2.9.0 + CUDA 12.9.1</strong>: Updated the default CUDA build to <code>torch==2.9.0+cu129</code>, enabling Inductor partitioning and landing multiple fixes in graph-partition rules and compile-cache integration.</li>
<li><strong>Batch-invariant <code>torch.compile</code></strong>: Generalized batch-invariant support across attention and MoE backends, with explicit support for DeepGEMM and FlashInfer on Hopper and Blackwell GPUs.</li>
<li><strong>Robust async scheduling</strong>: Fixed several correctness and stability issues in async scheduling, especially when combined with chunked prefill, structured outputs, priority scheduling, MTP, and DeepEP / DCP. We expect <code>--async-scheduling</code> to be enabled by default in the next release.</li>
<li><strong>Stronger scheduler + KV ecosystem</strong>: Improved test coverage in CI and made scheduler behavior more robust with KV connectors, prefix caching, and multi-node deployments.</li>
<li><strong>Anthropic API Support</strong>: Added support for the <code>/v1/messages</code> endpoint, allowing users to interact with <code>vllm serve</code> using Anthropic-compatible clients.</li>
</ul>
<p>Detailed release notes will be updated in the next few days.</p>
<h2>What's Changed</h2>
<ul>
<li>[Bugfix] Improve GLM4 MoE Reasoning Parser's is_reasoning_end Condition (<a class="user-mention notranslate" href="https://github.com/frankwang28">@frankwang28</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25355">#25355</a>)</li>
<li>[Docs] Add Toronto Meetup (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25773">#25773</a>)</li>
<li>[CI] Add E2E Blackwell Quantized MoE Test (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25723">#25723</a>)</li>
<li>[V1] address post issues related to <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20059">#20059</a> (part 1); cascade attention reenable by default (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23046">#23046</a>)</li>
<li>[CI] Fix FlashInfer AOT in release docker image (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25730">#25730</a>)</li>
<li>[spec decode] Consolidate speculative decode method name for MTP (<a class="user-mention notranslate" href="https://github.com/zixi-qi">@zixi-qi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25232">#25232</a>)</li>
<li>Reduce the Cuda Graph memory footprint when running with DBO (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25779">#25779</a>)</li>
<li>Kernel-override Determinism [1/n] (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25603">#25603</a>)</li>
<li>[Bugfix] Optimize CpuGpuBuffer initialization (<a class="user-mention notranslate" href="https://github.com/namanlalitnyu">@namanlalitnyu</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25447">#25447</a>)</li>
<li>[Spec decode] automatically disable mm for text-only draft models (<a class="user-mention notranslate" href="https://github.com/jmkuebler">@jmkuebler</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25667">#25667</a>)</li>
<li>[Core] Don't count preempted tokens in prefix cache hit rate (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25787">#25787</a>)</li>
<li>Add option to restrict media domains (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25783">#25783</a>)</li>
<li>Add flashinfer-build.sh and register precompiled cu128 wheel in Dockerfile (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25782">#25782</a>)</li>
<li>[Multimodal][Speculative Decoding]Eagle Eagle3 mm support, enablement on qwen2.5vl (<a class="user-mention notranslate" href="https://github.com/david6666666">@david6666666</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22872">#22872</a>)</li>
<li>[Bugfix] Allow Only SDPA Backend for ViT on B200 for Qwen3-VL (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25788">#25788</a>)</li>
<li>[CI/Build] Consolidate model loader tests and requirements (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25765">#25765</a>)</li>
<li>[CI/Build] Add timing to Model Executor Test (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25799">#25799</a>)</li>
<li>[CI/Build] Reorganize root-level V1 tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25767">#25767</a>)</li>
<li>[Misc] Fix codeowners override for v1 sample and attention (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25037">#25037</a>)</li>
<li>[Misc] Update openai client example file for multimodal (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25795">#25795</a>)</li>
<li>[Bugfix] Add missing <code>image_size</code> for phi4_multimodal (<a class="user-mention notranslate" href="https://github.com/Renovamen">@Renovamen</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25796">#25796</a>)</li>
<li>[Bugfix] Merge MM embeddings by index instead of token IDs (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/16229">#16229</a>)</li>
<li>Validate API tokens in constant time (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25781">#25781</a>)</li>
<li>Add filtering for chat template kwargs (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25794">#25794</a>)</li>
<li>Fix GPTQ model loading in Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25770">#25770</a>)</li>
<li>[Bugfix] Fix triton import precommit failure (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25803">#25803</a>)</li>
<li>[Bugfix][WideEP] Apply TP Attn + EP MoE fix to other models (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24982">#24982</a>)</li>
<li>[docs] Resolve transcriptions API TODO (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25446">#25446</a>)</li>
<li>[env] default nixl side port conflicts  with kv-event zmq port (<a class="user-mention notranslate" href="https://github.com/panpan0000">@panpan0000</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25056">#25056</a>)</li>
<li>[Core] Refactor self.model() to call a helper for subclassing. (<a class="user-mention notranslate" href="https://github.com/patrick-toulme">@patrick-toulme</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25084">#25084</a>)</li>
<li>[torch.compile]: Add VLLM_DEBUG_DUMP_PATH environment variable (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25651">#25651</a>)</li>
<li>[Bug]: Set LD_LIBRARY_PATH to include the 'standard' CUDA location (<a class="user-mention notranslate" href="https://github.com/smarterclayton">@smarterclayton</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25766">#25766</a>)</li>
<li>[Core] GC Debug callback (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24829">#24829</a>)</li>
<li>[Bugfix][NIXL] Fix Async Scheduler timeout issue (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25808">#25808</a>)</li>
<li>[MM] Optimize memory profiling for scattered multimodal embeddings (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25810">#25810</a>)</li>
<li>[Bugfix] Fix Qwen3-VL regression from <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24982">#24982</a> (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25814">#25814</a>)</li>
<li>[VLM] Update Qwen3-VL max_num_video_tokens calculation for configurable video profiling (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25557">#25557</a>)</li>
<li>Fix random dataset mismatched token length with config. (<a class="user-mention notranslate" href="https://github.com/weireweire">@weireweire</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24937">#24937</a>)</li>
<li>Update GLM-4.5 Doc transformers version (<a class="user-mention notranslate" href="https://github.com/zRzRzRzRzRzRzR">@zRzRzRzRzRzRzR</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25830">#25830</a>)</li>
<li>[Bugfix] fix Qwen3VLMoe load when pp &gt; 1 (<a class="user-mention notranslate" href="https://github.com/JJJYmmm">@JJJYmmm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25838">#25838</a>)</li>
<li>Remove redundant cudagraph dispatcher warning (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25841">#25841</a>)</li>
<li>[Misc] fix tests failure by using current_platform (<a class="user-mention notranslate" href="https://github.com/kingsmad">@kingsmad</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25825">#25825</a>)</li>
<li>[P/D] NIXL Updates (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25844">#25844</a>)</li>
<li>Add Phi4FlashForCausalLM to _PREVIOUSLY_SUPPORTED_MODELS (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25832">#25832</a>)</li>
<li>[XPU]Fix xpu spec decoding UTs, avoid using cuda graph (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25847">#25847</a>)</li>
<li>[Bugfix] Fallback ViT attn backend to SDPA for blackwell (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25851">#25851</a>)</li>
<li>[V0 Deprecation][Models] Remove all V0 condition for mm embeddings merge (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25331">#25331</a>)</li>
<li>[Misc] Remove more <code>get_input_embeddings_v0</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25857">#25857</a>)</li>
<li>update to latest deepgemm for dsv3.2 (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25871">#25871</a>)</li>
<li>[Bugfix] Fix requirements paths in install instructions (<a class="user-mention notranslate" href="https://github.com/yingjun-mou">@yingjun-mou</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25827">#25827</a>)</li>
<li>[Model][Bugfix] Fix issues in MiDashengLM implementation for quantized models (<a class="user-mention notranslate" href="https://github.com/zhoukezi">@zhoukezi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25854">#25854</a>)</li>
<li>[torch.compile] serialize cudagraph_mode as its enum name instead of value (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25868">#25868</a>)</li>
<li>[Cuda2CPU][P/D] Add cuda2cpu support in NixlConnector (<a class="user-mention notranslate" href="https://github.com/chenxi-yang">@chenxi-yang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24690">#24690</a>)</li>
<li>[Bugfix][Speculative Decoding] Fix Eagle3 quantization config issue (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25883">#25883</a>)</li>
<li>[CI/Build] Include Transformers backend test in nightly transformers test (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25885">#25885</a>)</li>
<li>[Model] Remove MotifForCausalLM (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25866">#25866</a>)</li>
<li>[Bugfix] Use correct key "ignore" for config.json non-quantized layers (<a class="user-mention notranslate" href="https://github.com/leejnau">@leejnau</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25706">#25706</a>)</li>
<li>[BugFix][torch.compile] KV scale calculation issues with FP8 quantization (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/21640">#21640</a>) (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25513">#25513</a>)</li>
<li>[Doc] Add documentation for vLLM continuous benchmarking and profiling (<a class="user-mention notranslate" href="https://github.com/namanlalitnyu">@namanlalitnyu</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25819">#25819</a>)</li>
<li>[Bugfix][ROCm] Fixing trying to import non-existent symbols from libnccl.so (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25605">#25605</a>)</li>
<li>[Kernel] Chunk-aligned mamba2 (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24683">#24683</a>)</li>
<li>[Doc] Polish example for torchrun dp (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25899">#25899</a>)</li>
<li>[NIXL] Increase default KV block eviction timeout on P (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25897">#25897</a>)</li>
<li>[V0 Deprecation] Remove <code>vllm.worker</code> and update according imports (<a class="user-mention notranslate" href="https://github.com/aarnphm">@aarnphm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25901">#25901</a>)</li>
<li>Test Prompt Embeds/LoRA compatibility and Enable LoRA Support for OPT Models  (<a class="user-mention notranslate" href="https://github.com/qthequartermasterman">@qthequartermasterman</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25717">#25717</a>)</li>
<li>[Bug] Fix Weight Loading for Block FP8 Cutlass SM90 (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25909">#25909</a>)</li>
<li>[Benchmark] Support benchmark throughput for external launcher DP (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25913">#25913</a>)</li>
<li>Move<code>VllmConfig</code> from <code>config/__init__.py</code> to <code>config/vllm.py</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25271">#25271</a>)</li>
<li>[BugFix] Fix DP/EP hang  (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25906">#25906</a>)</li>
<li>[BugFix] Pass config_format via try_get_generation_config (<a class="user-mention notranslate" href="https://github.com/acisseJZhong">@acisseJZhong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25912">#25912</a>)</li>
<li>[Model][Bugfix] Fix MiDashengLM audio encoder mask by removing incorrect <code>logical_not</code> (<a class="user-mention notranslate" href="https://github.com/zhoukezi">@zhoukezi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25925">#25925</a>)</li>
<li>[Bugfix]: Clean up chunked prefill logging when using whisper (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25075">#25075</a>)</li>
<li>[New Model] DeepSeek-V3.2 (Rebased to Main) (<a class="user-mention notranslate" href="https://github.com/zyongye">@zyongye</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25896">#25896</a>)</li>
<li>[Doc] Add Cambricon MLU support (<a class="user-mention notranslate" href="https://github.com/a120092009">@a120092009</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25942">#25942</a>)</li>
<li>Updated TRL integration docs (<a class="user-mention notranslate" href="https://github.com/sergiopaniego">@sergiopaniego</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25684">#25684</a>)</li>
<li>[Bugfix][Model]fix ernie45 moe gate&amp;bias dtype to float32 (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25936">#25936</a>)</li>
<li>[Model] Move <code>vision_feature_select_strategy</code> into <code>resolve_visual_encoder_outputs</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25938">#25938</a>)</li>
<li>[perf] Use CPU tensor to reduce GPU-&gt;CPU sync (<a class="user-mention notranslate" href="https://github.com/lhtin">@lhtin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25884">#25884</a>)</li>
<li>[NIXL] Add support for MLA caches with different latent dim (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25902">#25902</a>)</li>
<li>[CI] Move applicable tests to CPU (<a class="user-mention notranslate" href="https://github.com/rzabarazesh">@rzabarazesh</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24080">#24080</a>)</li>
<li>[Fix] Improve CPU backend compatibility for RISC-V (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25816">#25816</a>)</li>
<li>[Kernel][Moe Configs] Add more tuned triton configs for ExpertsInt8 and FP8 (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25858">#25858</a>)</li>
<li>Add Hugging Face Inference Endpoints guide to Deployment docs (<a class="user-mention notranslate" href="https://github.com/sergiopaniego">@sergiopaniego</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25886">#25886</a>)</li>
<li>[Bugfix][Model] Fix inference for Hunyuan dense models (<a class="user-mention notranslate" href="https://github.com/Anionex">@Anionex</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25354">#25354</a>)</li>
<li>[Bugfix] Fix accuracy issue of TRTLLM FP8 MOE and improve logging (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25895">#25895</a>)</li>
<li>[Bugfix] Token type and position embeddings fail to be applied to <code>inputs_embeds</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25922">#25922</a>)</li>
<li>[bugfix][deepseek] fix flashmla kernel selection (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25956">#25956</a>)</li>
<li>[Bug] Fix AttributeError: 'QKVParallelLinear' object has no attribute 'orig_dtype' (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25958">#25958</a>)</li>
<li>[Doc] Improve MM Pooling model documentation (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25966">#25966</a>)</li>
<li>[Docs] Add moe kernel features doc  (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25297">#25297</a>)</li>
<li>OffloadingConnector: Fix GPU block tracking bug (<a class="user-mention notranslate" href="https://github.com/orozery">@orozery</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25856">#25856</a>)</li>
<li>[Llama4] [multimodal] Fix misplaced dtype cast of <code>cos_sin_cache</code> in <code>Llama4VisionRotaryEmbedding</code> (<a class="user-mention notranslate" href="https://github.com/cjackal">@cjackal</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25889">#25889</a>)</li>
<li>[Bench] Add DeepSeekV32 to MoE benchmark (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25962">#25962</a>)</li>
<li>[V1] [P/D] Add Support for KV Load Failure Recovery (<a class="user-mention notranslate" href="https://github.com/sdavidbd">@sdavidbd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19330">#19330</a>)</li>
<li>Add explicit pooling classes for the Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25322">#25322</a>)</li>
<li>[Docs] Remove API Reference from search index (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25949">#25949</a>)</li>
<li>[gpt-oss] use vLLM instead of openai types for streaming (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25186">#25186</a>)</li>
<li>[Misc] Make EP kernels install script support uv (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25785">#25785</a>)</li>
<li>[Model] MTP fallback to eager for DeepSeek v32 (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25982">#25982</a>)</li>
<li>Update launch_bounds_utils.h for correct compile on Multiple Cuda Arch - PTXAS out of range Warning (<a class="user-mention notranslate" href="https://github.com/DrStone1971">@DrStone1971</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25843">#25843</a>)</li>
<li>[Log] Optimize Log for FP8MOE (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25709">#25709</a>)</li>
<li>Fix INT8 quantization error on Blackwell GPUs (SM100+) (<a class="user-mention notranslate" href="https://github.com/certainly-param">@certainly-param</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25935">#25935</a>)</li>
<li>[MM] Add text-only mode for Qwen3-VL (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26000">#26000</a>)</li>
<li>[Bugfix] Fix <code>__syncwarp</code> on ROCM (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25996">#25996</a>)</li>
<li>[BugFix] Fix default kv-cache-dtype default for DeepseekV3.2 (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25988">#25988</a>)</li>
<li>Update to Transformers <code>v4.56.2</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24638">#24638</a>)</li>
<li>[Misc]allow disable pynccl (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25421">#25421</a>)</li>
<li>[Doc] updating torch.compile doc link <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25989">#25989</a>)</li>
<li>[BugFix][MM] Fix Nonetype error when video is cache in qwen2.5-omni-thinker (<a class="user-mention notranslate" href="https://github.com/wwl2755">@wwl2755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26004">#26004</a>)</li>
<li>[Misc] Factor out common <code>_apply_feature_select_strategy</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26003">#26003</a>)</li>
<li>[CI] Only capture a single CUDA graph size in CI by default (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25951">#25951</a>)</li>
<li>[MISC] Fix misleading batch_size_capture_list when cuda_graph_sizes &lt; 4 (<a class="user-mention notranslate" href="https://github.com/billishyahao">@billishyahao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25829">#25829</a>)</li>
<li>[Benchmark] Finish documented v0.11.0 deprecation of --endpoint-type (<a class="user-mention notranslate" href="https://github.com/natoscott">@natoscott</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26007">#26007</a>)</li>
<li>[Bugfix] Apply same sampling parameters for both <code>n=1</code> and <code>n&gt;1</code> (<a class="user-mention notranslate" href="https://github.com/kmaehashi">@kmaehashi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26005">#26005</a>)</li>
<li>[NVIDIA] Blackwell Family (<a class="user-mention notranslate" href="https://github.com/johnnynunez">@johnnynunez</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24673">#24673</a>)</li>
<li>Fix test_mamba_ssm_ssd.py due to missing _query_start_loc_to_chunk_indices_offsets (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25995">#25995</a>)</li>
<li>[CI] Tweaks to GPT-OSS Eval (Blackwell) for stability (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26030">#26030</a>)</li>
<li>[BugFix][DP/EP] Fix CUTLASS MLA hang under load (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26026">#26026</a>)</li>
<li>[ROCm][Build] Add support for AMD Ryzen AI MAX / AI 300 Series (<a class="user-mention notranslate" href="https://github.com/hyoon1">@hyoon1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25908">#25908</a>)</li>
<li>[Bug] Fix Negative Cuda Memory Usage (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25683">#25683</a>)</li>
<li>[BugFix] ChunkedLocalAttention is currently not CG compatible (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26034">#26034</a>)</li>
<li>Support RL online quantization with torchao (<a class="user-mention notranslate" href="https://github.com/jerryzh168">@jerryzh168</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23014">#23014</a>)</li>
<li>[ROCm][Bugfix] Add missing parameter to ROCm backend (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26029">#26029</a>)</li>
<li>[Misc] Make handling of SamplingParams clearer in n&gt;1 case (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26032">#26032</a>)</li>
<li>Run:ai model streamer add GCS package support (<a class="user-mention notranslate" href="https://github.com/pwschuurman">@pwschuurman</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24909">#24909</a>)</li>
<li>Update base image to 22.04 (jammy) (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26065">#26065</a>)</li>
<li>Change size of single CUDA graph for CI to 4 (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26089">#26089</a>)</li>
<li>[FA/Chore] Bump vllm-flash-attention (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25537">#25537</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (A-C) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26073">#26073</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (D-F) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26076">#26076</a>)</li>
<li>[Platform][CI] Added OOT platform interface e2e test that running on Ascend NPU (<a class="user-mention notranslate" href="https://github.com/leo-pony">@leo-pony</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25470">#25470</a>)</li>
<li>[Qwen][ROCm] Flash Attention Rotary Embeddings (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24642">#24642</a>)</li>
<li>[CI] Add Blackwell DeepSeek FP8 FlashInfer MoE tests (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26040">#26040</a>)</li>
<li>[CI/Build] Replace <code>vllm.entrypoints.openai.api_server</code> entrypoint with <code>vllm serve</code> command (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25967">#25967</a>)</li>
<li>[BugFix] Fix FI accuracy issue when used for MLA prefill (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26063">#26063</a>)</li>
<li>[Small] Prevent bypassing media domain restriction via HTTP redirects (<a class="user-mention notranslate" href="https://github.com/huachenheli">@huachenheli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26035">#26035</a>)</li>
<li>[Deepseek v3.2] Support indexer prefill chunking (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25999">#25999</a>)</li>
<li>EAGLE 3: Fix preamble so that measured speedup over Eagle 1 becomes 32% instead of 5% on MTBench (<a class="user-mention notranslate" href="https://github.com/ekagra-ranjan">@ekagra-ranjan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25916">#25916</a>)</li>
<li>[Mamba][KVCacheManager] Simplify kv cache manage logic for mamba + MTP (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25119">#25119</a>)</li>
<li>[Perf] Fix and reapply move apply w8a8 block fp8 linear to class (<a class="user-mention notranslate" href="https://github.com/ElizaWszola">@ElizaWszola</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25696">#25696</a>)</li>
<li>Fix MTP with deepep_low_latency (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25904">#25904</a>)</li>
<li>[Bugfix] Disable cascade attention with FlashInfer (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26130">#26130</a>)</li>
<li>[Log] Optimize DeepGEMM Missing Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26106">#26106</a>)</li>
<li>[Bug][Benchmark] Fix duplicate req in oversampling (<a class="user-mention notranslate" href="https://github.com/ekagra-ranjan">@ekagra-ranjan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26140">#26140</a>)</li>
<li>[Attention] Move Backend enum into registry (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25893">#25893</a>)</li>
<li>[CI/Build] Conditionally register cutlass_fp4_group_mm to fix building on Hopper (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26138">#26138</a>)</li>
<li>[DeepSeek] Improve performance of DS MLA cache kernel (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26132">#26132</a>)</li>
<li>[Bug]: Limit num_reqs in dummy_run when max_num_seqs is small (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26144">#26144</a>)</li>
<li>[gpt-oss] disable tool server initialization if no tool in request (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25790">#25790</a>)</li>
<li>[Build/CI] Revert back to Ubuntu 20.04, install python 3.12 with uv (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26103">#26103</a>)</li>
<li>[ROCm] [VL] [Bugfix] Fix vit flash attn dispatcher logic for ROCm (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26104">#26104</a>)</li>
<li>[Bugfix] Fix import <code>gemm_afp4wfp4</code> failure on AMD (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26068">#26068</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (G) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26117">#26117</a>)</li>
<li><code>FusedMoE</code> support for the Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22650">#22650</a>)</li>
<li>[BUG] Reorder model config creation (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26124">#26124</a>)</li>
<li>[Misc] Remove typing.List (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26150">#26150</a>)</li>
<li>[Input] Remove unused <code>prompt</code> field (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26097">#26097</a>)</li>
<li>[Perf] Optimize <code>reshape_and_cache</code> CUDA Kernel (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25955">#25955</a>)</li>
<li>add(v1): RequestStatesStats to RequestOutput (<a class="user-mention notranslate" href="https://github.com/huijjj">@huijjj</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24947">#24947</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (InternVL family) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26153">#26153</a>)</li>
<li>[test utils] correct wrong typing (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26159">#26159</a>)</li>
<li>[CI] Fix distributed hybrid tests in CI (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26155">#26155</a>)</li>
<li>[NIXL][Misc] Expose metrics from NIXL for logging to CLI (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25388">#25388</a>)</li>
<li>[openai] Fix missing tool usage check (system message) (<a class="user-mention notranslate" href="https://github.com/levunet">@levunet</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24768">#24768</a>)</li>
<li>[Multi Modal] Configurable MM Profiling (<a class="user-mention notranslate" href="https://github.com/wwl2755">@wwl2755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25631">#25631</a>)</li>
<li>[Doc] Fixed shape description for fused_batched_moe.py (<a class="user-mention notranslate" href="https://github.com/Egor-Krivov">@Egor-Krivov</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25668">#25668</a>)</li>
<li>Quick fix for IMA with the Prefix Prefill kernel during graph capture (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25983">#25983</a>)</li>
<li>[Renderer] Move Processor out of AsyncLLM  (<a class="user-mention notranslate" href="https://github.com/KKSK-DON">@KKSK-DON</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24138">#24138</a>)</li>
<li>Re-enable prefill of max model length (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24446">#24446</a>)</li>
<li>[backends][short_conv] CUDA graph piecewise edits (<a class="user-mention notranslate" href="https://github.com/paulpak58">@paulpak58</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24215">#24215</a>)</li>
<li>[Model] Supplement to PR 24862: Pass param prefix to LLMHead (<a class="user-mention notranslate" href="https://github.com/whx-sjtu">@whx-sjtu</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25805">#25805</a>)</li>
<li>[CI/Build] do not enforce precompilation on tpu ci tests (<a class="user-mention notranslate" href="https://github.com/sixiang-google">@sixiang-google</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25992">#25992</a>)</li>
<li>[Model] Fixed stream generator for gpt-oss + spec-decoding (<a class="user-mention notranslate" href="https://github.com/astralord">@astralord</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26027">#26027</a>)</li>
<li>[Renderer] Move Processor out of LLMEngine (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26165">#26165</a>)</li>
<li>Fix undefined symbol: cutlass_moe_mm_sm100 (<a class="user-mention notranslate" href="https://github.com/jasl">@jasl</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26098">#26098</a>)</li>
<li>[BugFix][QWEN-VL]fix wrong apply_rotary_emb_torch selection introduced by <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24642">#24642</a> (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26123">#26123</a>)</li>
<li>Stop mergify from keeping stale PRs alive (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26169">#26169</a>)</li>
<li>Avoid division by zero in cache DS MLA kernel (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26174">#26174</a>)</li>
<li>Fix V1 engine serialization error with Ray distributed executor (<a class="user-mention notranslate" href="https://github.com/nrghosh">@nrghosh</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26148">#26148</a>)</li>
<li>[Quantization/NVFP4] Speed up TRTLLM NVFP4 MOE weight loading and fix K/V scale loading for MLA Attn (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25968">#25968</a>)</li>
<li>[Perf] Remove hardcoded num_warps=1 (<a class="user-mention notranslate" href="https://github.com/chelsea0x3b">@chelsea0x3b</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26183">#26183</a>)</li>
<li>[Refactor] Optimize FP8 MOE Backend Choice and Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26044">#26044</a>)</li>
<li>[responsesAPI] add better error messaging for long prompts (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25724">#25724</a>)</li>
<li>[Bugfix] Relax tokenizer regex for mixtral to include 'tokenizer.model' (<a class="user-mention notranslate" href="https://github.com/BowenBao">@BowenBao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25964">#25964</a>)</li>
<li>[CI] Push multiarch manifests as nightly builds (<a class="user-mention notranslate" href="https://github.com/csahithi">@csahithi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25764">#25764</a>)</li>
<li>[Misc] Add penalties sampling parameters to serve tool (<a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25974">#25974</a>)</li>
<li>[BugFix] Fix de-functionalization pass for rotary_embedding (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23953">#23953</a>)</li>
<li>[CI] Fix Pre-commit Mypy Error (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26181">#26181</a>)</li>
<li>[GPTOSS][DP/EP][Marlin] Enable GPTOSS DP/EP using Marlin kernels (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25488">#25488</a>)</li>
<li>Fix issue of using only the part of video frame [Nemotron Nano] (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26186">#26186</a>)</li>
<li>[Bugfix] Fix qwen3 vl dummy data generation with overrides (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26193">#26193</a>)</li>
<li>[BugFix] Use async Mistral Tokenizer in Chat Completions (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26134">#26134</a>)</li>
<li>Add batch invariant kernel override for FlashInfer backend [2/n] (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25769">#25769</a>)</li>
<li>[cpu][perf] Accelerate unquantized-linear for AArch64 through oneDNN/ACL and weight prepack (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25948">#25948</a>)</li>
<li>[V1] [Hybrid] Mamba2 Automatic Prefix Caching (<a class="user-mention notranslate" href="https://github.com/s3woz">@s3woz</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25752">#25752</a>)</li>
<li>Support expert parallel in Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26162">#26162</a>)</li>
<li>[Model] Support nested structures for TensorSchema (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26212">#26212</a>)</li>
<li>[Misc] Require <code>merge_by_field_config</code> argument (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26214">#26214</a>)</li>
<li>[Misc] Remove unused <code>executor.apply_model</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26215">#26215</a>)</li>
<li>[CI Failure] fix_test_auto_prefix_cache_support (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26053">#26053</a>)</li>
<li>Revert "Add batch invariant kernel override for FlashInfer backend [2/n]" (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26220">#26220</a>)</li>
<li>Add Olmo 3 reasoning parser (<a class="user-mention notranslate" href="https://github.com/soldni">@soldni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26054">#26054</a>)</li>
<li>[Core] Enable decode of context length equal to max model length (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26168">#26168</a>)</li>
<li>[Bugfix] Fix <code>_reqs_to_process</code> leak on abort (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26012">#26012</a>)</li>
<li>[Model] CLIP Embedding Support (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26010">#26010</a>)</li>
<li>Fix tensor device and dtype placement in Qwen2VL model (<a class="user-mention notranslate" href="https://github.com/yuafng">@yuafng</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26219">#26219</a>)</li>
<li>[V1] [Hybrid] Remove code to override default CUDA graph configuration (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26226">#26226</a>)</li>
<li>[CPU] Refine batch reorder of CPU attention backend (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26096">#26096</a>)</li>
<li>[Frontend] Cache chat template kwargs resolution (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26227">#26227</a>)</li>
<li>[Renderer] Clean up renderer code (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26216">#26216</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (H-L) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26230">#26230</a>)</li>
<li>[Easy] Add str repr for IterationStats (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26232">#26232</a>)</li>
<li>[Bugfix] Allow <code>--skip-tokenizer-init</code> with <code>echo and return_token_ids</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26238">#26238</a>)</li>
<li>Add documentation for granite 4 tool calling (<a class="user-mention notranslate" href="https://github.com/maxdebayser">@maxdebayser</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26175">#26175</a>)</li>
<li>[Perf][Easy] Early stop in request_block_hasher (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26112">#26112</a>)</li>
<li>[Bugfix]: Assertion error when using FlashInfer backend (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25933">#25933</a>)</li>
<li>[Bugfix] Always apply MM processor even when no MM items are passed (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26240">#26240</a>)</li>
<li>[Bugfix][Hardware][RISC-V] Limit supported dtypes to float32 to avoid scheduler segfault (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26228">#26228</a>)</li>
<li>[Refactor][Kernel] support loading kernel from other place (<a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25823">#25823</a>)</li>
<li>Convert formatting to use <code>ruff</code> instead of <code>yapf</code> + <code>isort</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26247">#26247</a>)</li>
<li>Remove all references to <code>yapf</code> as it's no longer used (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26251">#26251</a>)</li>
<li>Remove all cases of <code>fmt: on/off</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26253">#26253</a>)</li>
<li>fix(tests): Resolve late binding of loop variable in assert message lambda (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26249">#26249</a>)</li>
<li>Fix per file ruff ignores related to typing (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26254">#26254</a>)</li>
<li>Update <code>ruff</code> pre-commit hooks version (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26255">#26255</a>)</li>
<li>[CI] fix mamba kernel test (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26250">#26250</a>)</li>
<li>[NVIDIA] flashinfer TRTLLM attention prefill token limit (<a class="user-mention notranslate" href="https://github.com/jasonlizhengjian">@jasonlizhengjian</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25998">#25998</a>)</li>
<li>Fix per file ruff ignores related to simplification (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26259">#26259</a>)</li>
<li>[CI] Add Blackwell LM Eval Small Models test to nightly (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26052">#26052</a>)</li>
<li>[DOC] Update production-stack.md (<a class="user-mention notranslate" href="https://github.com/elieserr">@elieserr</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26177">#26177</a>)</li>
<li>[CI] Add comment about the single cudagraph capture size that is used (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26252">#26252</a>)</li>
<li>[V1] [Hybrid] Some additional clean-up in Mamba2 prefix caching (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26222">#26222</a>)</li>
<li>[Doc] Edited minor typo (<a class="user-mention notranslate" href="https://github.com/orangeng">@orangeng</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26266">#26266</a>)</li>
<li>[MISC] Add heheda12345 to CODEOWNERS of vllm/config/cache.py (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26270">#26270</a>)</li>
<li>[CI][gpt-oss] Enable python tool tests in CI (<a class="user-mention notranslate" href="https://github.com/wuhang2014">@wuhang2014</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24315">#24315</a>)</li>
<li>Fix per file ruff ignores related to line length (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26262">#26262</a>)</li>
<li>Bump actions/stale from 10.0.0 to 10.1.0 (<a class="user-mention notranslate" href="https://github.com/dependabot">@dependabot</a>[bot] <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26272">#26272</a>)</li>
<li>[Benchmarking] Add disable_shuffle option for dataset loading (<a class="user-mention notranslate" href="https://github.com/ymoslem">@ymoslem</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26258">#26258</a>)</li>
<li>[Misc] Clean up unnecessary E501 ignore (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26274">#26274</a>)</li>
<li>[Docs] Edit HF Inference Endpoints documentation (<a class="user-mention notranslate" href="https://github.com/ariG23498">@ariG23498</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26275">#26275</a>)</li>
<li>[Doc] add KAITO to integrations (<a class="user-mention notranslate" href="https://github.com/abhisheksheth28">@abhisheksheth28</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25521">#25521</a>)</li>
<li>[Frontend] Consolidate tokenizer init code (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26276">#26276</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (Llava family) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26280">#26280</a>)</li>
<li>Support expert parallel load balancing in Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26287">#26287</a>)</li>
<li>[Bugfix] Fix mrope in Transformers Backend (<a class="user-mention notranslate" href="https://github.com/zucchini-nlp">@zucchini-nlp</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26087">#26087</a>)</li>
<li>Fix <code>DotsOCR</code> tensor type (<a class="user-mention notranslate" href="https://github.com/what-in-the-nim">@what-in-the-nim</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26281">#26281</a>)</li>
<li>[Model] EVS support for nano_nemotron_vl (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26269">#26269</a>)</li>
<li>[Attention] Remove unused reorder_batch method (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24463">#24463</a>)</li>
<li>[Tests] conftest: Extending VllmRunner and HfRunner to accept token_ids as input (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26295">#26295</a>)</li>
<li>[CI Bugfix] Make sure TRTLLM attention is available in test_blackwell_moe (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26188">#26188</a>)</li>
<li>Support llama3 eagle3 head with llama4 verifier (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25961">#25961</a>)</li>
<li>[Misc] auto_tune: kill specific vllm process (<a class="user-mention notranslate" href="https://github.com/karan">@karan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26304">#26304</a>)</li>
<li>[Bugfix][Spec Decode] Fix wrong valid_mask for padded speculation when chunked prefill occurs (<a class="user-mention notranslate" href="https://github.com/seven-mile">@seven-mile</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26231">#26231</a>)</li>
<li>Add bias handling to CPUFusedMOE kernel (<a class="user-mention notranslate" href="https://github.com/cfRod">@cfRod</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26289">#26289</a>)</li>
<li>[Bugfix] Fix gemma3 with transformers backend (<a class="user-mention notranslate" href="https://github.com/zucchini-nlp">@zucchini-nlp</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23178">#23178</a>)</li>
<li>[Benchmark] Enable MM Embedding benchmarks (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26310">#26310</a>)</li>
<li>[Docs] Fix broken table in moe_kernel_features doc (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26314">#26314</a>)</li>
<li>[BugFix] Pad input buffers in _dummy_run (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26209">#26209</a>)</li>
<li>[Bugfix] Allow skipping MoE in NVFP4 (fix for MTP) (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25987">#25987</a>)</li>
<li>[ROCm] Split AITER unified attention into its own backend (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25507">#25507</a>)</li>
<li>[Perf] Add decode full-graph support to FlashInfer-MLA backend (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26313">#26313</a>)</li>
<li>[Misc] Define EP kernel arch list in Dockerfile (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25635">#25635</a>)</li>
<li>[Docs][DBO] Add initial doc that describes the DBO implementation (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26024">#26024</a>)</li>
<li>[Core] Simplify the Dp padding/should ubatch coordination logic (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25768">#25768</a>)</li>
<li>[UX] Support nested dicts in hf_overrides (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25727">#25727</a>)</li>
<li>[BUG] Fix file parsing for load_format runai_streamer_sharded (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26324">#26324</a>)</li>
<li>[Model] Define merge_by_field_config MM interface (U-Z) (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26261">#26261</a>)</li>
<li>[Deprecation] Deprecate <code>LLM.set_tokenizer</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26333">#26333</a>)</li>
<li>[responsesAPI][bugfix] serialize harmony messages (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26185">#26185</a>)</li>
<li>[Model] Define merge_by_field_config MM interface (R-T) (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26260">#26260</a>)</li>
<li>[BugFix] Update KV block hash type from BlockHash to ExternalBlockHash in kv_events_subscriber - <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/26264">#26264</a> (<a class="user-mention notranslate" href="https://github.com/atalhens">@atalhens</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26265">#26265</a>)</li>
<li>[V0 Deprecation] Remove <code>VLLM_USE_V1</code> from docs and scripts (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26336">#26336</a>)</li>
<li>Optimize KV cache distribution for asymmetric pipeline parallelism (<a class="user-mention notranslate" href="https://github.com/gholmes829">@gholmes829</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25164">#25164</a>)</li>
<li>Add topk logits torch op for DS3.2. (<a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25945">#25945</a>)</li>
<li>Add TRL example notebook to RLHF docs (<a class="user-mention notranslate" href="https://github.com/sergiopaniego">@sergiopaniego</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26346">#26346</a>)</li>
<li>[Docs] add docs for cuda graph v1 (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24374">#24374</a>)</li>
<li>[Model] Use <code>merge_by_field_config</code> for MM models (Ovis family) (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26308">#26308</a>)</li>
<li>[Feature][OCP MX] Support mxfp6 and mixed mxfp6-mxfp4 (<a class="user-mention notranslate" href="https://github.com/fxmarty-amd">@fxmarty-amd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/21166">#21166</a>)</li>
<li>[Model] Add support for ModernBertForTokenClassification (<a class="user-mention notranslate" href="https://github.com/antrec">@antrec</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26340">#26340</a>)</li>
<li>[Misc] Move <code>LRUCache</code> into its own file (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26342">#26342</a>)</li>
<li>[V0 Deprecation] Remove <code>VLLM_USE_V1</code> from tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26341">#26341</a>)</li>
<li>[Model] Lfm2Moe (<a class="user-mention notranslate" href="https://github.com/paulpak58">@paulpak58</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26344">#26344</a>)</li>
<li>[ci] Rename <code>test_mxfp4_moe.py</code> to <code>test_ocp_mx_moe.py</code> (<a class="user-mention notranslate" href="https://github.com/fxmarty-amd">@fxmarty-amd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26364">#26364</a>)</li>
<li>[CI] Add Qwen3 MoE NVFP4 to Blackwell lm-eval (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26316">#26316</a>)</li>
<li>[deepseek] add EP8 FusedMOE config for H200 and B200 (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26331">#26331</a>)</li>
<li>[Bug] Fix Shape Validation for Fallback while Enabling E8M0 for DeepGEMM (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26322">#26322</a>)</li>
<li>[Bugfix] Add missing sink tensor into flash attn cascade attn implementation (<a class="user-mention notranslate" href="https://github.com/plliao">@plliao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26325">#26325</a>)</li>
<li>[Frontend] CompilationConfig overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): deprecate use_inductor in favor of backend, simplify custom_ops (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26113">#26113</a>)</li>
<li>[V1] Logit processors for rejection sampler (<a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19482">#19482</a>)</li>
<li>[Spec Decode] Enable efficient speculative decoding with FlashInfer-MLA (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25984">#25984</a>)</li>
<li>[TPU] update TPU benchmark threshold (<a class="user-mention notranslate" href="https://github.com/jcyang43">@jcyang43</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25713">#25713</a>)</li>
<li>Add more libraries to rlhf.md (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26374">#26374</a>)</li>
<li>[Bugfix] Fix MTP+FlashInfer crash when trtllm kernels are available but disabled (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26361">#26361</a>)</li>
<li>Revert <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24446">#24446</a> and <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26168">#26168</a> (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26332">#26332</a>)</li>
<li>[Misc] Clean up cruft from previous FlashMLA sparse implementation (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26125">#26125</a>)</li>
<li>[torchao] safetensors integration (<a class="user-mention notranslate" href="https://github.com/liangel-02">@liangel-02</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25969">#25969</a>)</li>
<li>Add SwigluOAI implementation for CPUFusedMOE (<a class="user-mention notranslate" href="https://github.com/isharif168">@isharif168</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26347">#26347</a>)</li>
<li>[Core] Simplify setting new_token_ids in CachedRequestData (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26388">#26388</a>)</li>
<li>fix(v1/kv_cache): resolve async KV transfer bug in cascade attention (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23485">#23485</a>)</li>
<li>Add gather_indexer_k_quant_cache kernel (<a class="user-mention notranslate" href="https://github.com/Barry-Delaney">@Barry-Delaney</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25931">#25931</a>)</li>
<li>[Bugfix] Incorrect MM data format in <code>vllm bench throughput</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26395">#26395</a>)</li>
<li>fix[DP][v1]: Prevent hangs from mismatched worker configurations (<a class="user-mention notranslate" href="https://github.com/ayushsatyam146">@ayushsatyam146</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26218">#26218</a>)</li>
<li>[TPU] Rename tpu_commons to tpu_inference (<a class="user-mention notranslate" href="https://github.com/utkarshsharma1">@utkarshsharma1</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26279">#26279</a>)</li>
<li>[Feature] Enable E8M0 by Default on Hopper for DeepGEMM, 5% E2E throughput improvement (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26197">#26197</a>)</li>
<li>[Misc] add usedforsecurity=False in md5 hash call (<a class="user-mention notranslate" href="https://github.com/dtrifiro">@dtrifiro</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26357">#26357</a>)</li>
<li>[Model] Allow passing custom number of max tiles to Nano 2 VL (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26403">#26403</a>)</li>
<li>[Docs] Have mergify leave a comment with the docs preview link (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26412">#26412</a>)</li>
<li>[CI] Pooling models mteb test disable enforce_eager  (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26408">#26408</a>)</li>
<li>[Benchmarks] Add support for Qwen 3 VL MoE tuning (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26419">#26419</a>)</li>
<li>Tidy <code>vllm/config/__init__.py</code> to only add classes and functions (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26405">#26405</a>)</li>
<li>[NIXL][non-cuda] Add install script for nixl with non-cuda ucx (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25959">#25959</a>)</li>
<li>[Refactor] Refactor FP8 &amp; INT8 Quant Folder inside <code>w8a8</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25293">#25293</a>)</li>
<li>[CI Failure] Fix pre-commit issue for install_nixl_from_source_ubuntu.py (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26424">#26424</a>)</li>
<li>[Bugfix] Fix <code>vllm bench ...</code> on CPU-only head nodes (<a class="user-mention notranslate" href="https://github.com/Aydin-ab">@Aydin-ab</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25283">#25283</a>)</li>
<li>[Bug] Fix DeepGEMM Attention Test (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26423">#26423</a>)</li>
<li>[Benchmarks] Fix imports in FP8 tuning script (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26407">#26407</a>)</li>
<li>[Bug] Fix Test in Batch Invariant (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26128">#26128</a>)</li>
<li>Remove Python 3.9 support ahead of PyTorch 2.9 in v0.11.1 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26416">#26416</a>)</li>
<li>[Feature] Change cache.py with pydantic validation (<a class="user-mention notranslate" href="https://github.com/vrdn-23">@vrdn-23</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26390">#26390</a>)</li>
<li>[Attention] Implement universal BACKEND_MAP (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25900">#25900</a>)</li>
<li>[Bugfix][Flashinfer] fix VLLM_USE_TRTLLM_ATTENTION issue for models with diff hyperparameters (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25924">#25924</a>)</li>
<li>[BugFix] Fix failing test quantization/test_compressed_tensors.py::test_compressed_tensors_fp8_block_enabled (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26436">#26436</a>)</li>
<li>[Kernel] Centralize platform kernel import in <code>current_platform.import_kernels</code> (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26286">#26286</a>)</li>
<li>[Models] Improve iteration over layers (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26425">#26425</a>)</li>
<li>[Bugfix] Respect min_tokens in scheduler stop check (<a class="user-mention notranslate" href="https://github.com/elaineyz">@elaineyz</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26317">#26317</a>)</li>
<li>[Kernels] Modular kernel refactor (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24812">#24812</a>)</li>
<li>[Attention] Register FLASHMLA_SPARSE (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26441">#26441</a>)</li>
<li>Separate MLAAttention class from Attention (<a class="user-mention notranslate" href="https://github.com/therealnaveenkamal">@therealnaveenkamal</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25103">#25103</a>)</li>
<li>[Misc] Redact ray runtime env before logging (<a class="user-mention notranslate" href="https://github.com/ruisearch42">@ruisearch42</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26302">#26302</a>)</li>
<li>[Bugfix] Set the minimum python version for gpt-oss (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26392">#26392</a>)</li>
<li>[Minor] Change warning-&gt;warning_once in preprocess (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26455">#26455</a>)</li>
<li>[Bugfix] Catch and log invalid token ids in detokenizer <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/2">#2</a> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26445">#26445</a>)</li>
<li>[Bugfix] Incorrect another MM data format in vllm bench throughput (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26462">#26462</a>)</li>
<li>[Hardware][AMD] Enable FlexAttention backend on ROCm (<a class="user-mention notranslate" href="https://github.com/mawong-amd">@mawong-amd</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26439">#26439</a>)</li>
<li>[MM][Doc] Add documentation for configurable mm profiling (<a class="user-mention notranslate" href="https://github.com/wwl2755">@wwl2755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26200">#26200</a>)</li>
<li>[Core][KVConnector] Propagate all tokens on resumed preemptions (<a class="user-mention notranslate" href="https://github.com/QierLi">@QierLi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24926">#24926</a>)</li>
<li>[Hybrid]: Decouple Kernel Block Size from KV Page Size (<a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24486">#24486</a>)</li>
<li>[CI/Build] Fix model nightly tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26466">#26466</a>)</li>
<li>[Core] Relax the LoRA  max rank (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26461">#26461</a>)</li>
<li>Update Dockerfile and install runai-model-streamer[gcs] package (<a class="user-mention notranslate" href="https://github.com/pwschuurman">@pwschuurman</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26464">#26464</a>)</li>
<li>Bump Flashinfer to v0.4.0 (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26326">#26326</a>)</li>
<li>[Model] Gemma3: Fix GGUF loading and quantization (<a class="user-mention notranslate" href="https://github.com/lucianommartins">@lucianommartins</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26189">#26189</a>)</li>
<li>Enable <code>RMSNorm</code> substitution for Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26353">#26353</a>)</li>
<li>Add: Support for multiple hidden layers in Eagle3 (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26164">#26164</a>)</li>
<li>[torchao] Add support for ModuleFqnToConfig using regex (<a class="user-mention notranslate" href="https://github.com/jerryzh168">@jerryzh168</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26001">#26001</a>)</li>
<li>[Misc] Misc code simplifications (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26450">#26450</a>)</li>
<li>[doc] add Volcengine as a compute sponsor (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26477">#26477</a>)</li>
<li>[Feature] Use pydantic validation in lora.py and load.py configs (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26413">#26413</a>)</li>
<li>[Misc] Upgrade more code to Python 3.10 (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26463">#26463</a>)</li>
<li>[Bugfix] Fix SHM cache initialization (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26427">#26427</a>)</li>
<li>[Models][Qwen3VL] Optimise <code>_validate_and_reshape_mm_tensor</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26426">#26426</a>)</li>
<li>[Bugfix] Move current_platform import to avoid python import cache. (<a class="user-mention notranslate" href="https://github.com/iwzbi">@iwzbi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/16601">#16601</a>)</li>
<li>[V0 deprecation] Remove <code>QKVCrossParallelLinear</code> implementation  (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26475">#26475</a>)</li>
<li>[Feature] Use pydantic validation in parallel.py config (<a class="user-mention notranslate" href="https://github.com/simondanielsson">@simondanielsson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26417">#26417</a>)</li>
<li>Revert  <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26113">#26113</a> "[Frontend] CompilationConfig overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): deprecate use_inductor in favor of backend, simplify custom_ops" (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26472">#26472</a>)</li>
<li>Upgrade Pydantic to v2.12.0 and remove hack for Python 3.13 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26481">#26481</a>)</li>
<li>[Models][Qwen] Replace <code>pad</code> with <code>cat</code> for better performance (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26486">#26486</a>)</li>
<li>[Attention][DCP] Support DCP with query length &gt; 1 (MTP) with FA3 (<a class="user-mention notranslate" href="https://github.com/minosfuture">@minosfuture</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25049">#25049</a>)</li>
<li>[Model] Apply shared experts overlap optimization to all models with shared experts (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26145">#26145</a>)</li>
<li>[BUGFIX] Add cu_tokens_across_sp to DPMetadata (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26457">#26457</a>)</li>
<li>[Bugfix] Enable padded FP4 quantization (<a class="user-mention notranslate" href="https://github.com/roikoren755">@roikoren755</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25947">#25947</a>)</li>
<li>[Bugfix] Disable moe inplace for torch &gt;= 2.9 (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26497">#26497</a>)</li>
<li>[Flashinfer][gpt-oss] Support FP8-qkv Flashinfer TRTLLM Sinks Attention (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25674">#25674</a>)</li>
<li>[Core] Remove unused <code>prev_sampled_token_ids_invalid_indices</code> input batch field (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26514">#26514</a>)</li>
<li>[UX] Add FlashInfer as default CUDA dependency (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26443">#26443</a>)</li>
<li>[Bugfix] Fix CUDA graph selection bug in FlashInfer at high concurrency (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26499">#26499</a>)</li>
<li>[Bug] Fix modular_kernel: ZeroDivisionError: integer division or modulo by zero (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26528">#26528</a>)</li>
<li>[CI] Fix Pre-commit Issue Cannot determine type of "rank" and "world_size" (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26448">#26448</a>)</li>
<li>Refactor MistralTokenizer (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26358">#26358</a>)</li>
<li>[DP][ray] Support different VLLM_RAY_DP_PACK_STRATEGY (<a class="user-mention notranslate" href="https://github.com/ruisearch42">@ruisearch42</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/23849">#23849</a>)</li>
<li>[Core] Small simplification in <code>GPUModelRunner._update_states()</code> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26508">#26508</a>)</li>
<li>[Chore]: One pythonic tool parser test uses the wrong parser (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26515">#26515</a>)</li>
<li>[Spec-Decode] Support piecewise cudagraphs for Eagle head (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25109">#25109</a>)</li>
<li>fix test_simple_inductor_graph_partition (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26522">#26522</a>)</li>
<li>[deepseek] kernel block size for UniformTypeKVCacheSpecs (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26559">#26559</a>)</li>
<li>[Metrics] Log multi-modal cache stats and fix reset (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26285">#26285</a>)</li>
<li>[GPT-OSS] Add support for arrays  at tool message content (<a class="user-mention notranslate" href="https://github.com/luis5tb">@luis5tb</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25593">#25593</a>)</li>
<li>Remove LoRA bias support (<a class="user-mention notranslate" href="https://github.com/ashwin-phadke">@ashwin-phadke</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25807">#25807</a>)</li>
<li>[CI] fix ruff format (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26579">#26579</a>)</li>
<li>[bugfix][DCP] fix block_size of hash in DCP prefix caching (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26296">#26296</a>)</li>
<li>[NIXL] Ignore abort on already-finished request (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25067">#25067</a>)</li>
<li>[Bugfix] Convert untraceable GroupShape to list for AMD impl (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26535">#26535</a>)</li>
<li>[BugFix] Fix noop elimination edge case (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26394">#26394</a>)</li>
<li>[CI] fix test_run_batch.py::test_completions - AssertionError (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26578">#26578</a>)</li>
<li>[BugFix][torch.compile] Fix fused_scaled_matmul_reduce_scatter signature for PyTorch 2.8 (<a class="user-mention notranslate" href="https://github.com/jasonlizhengjian">@jasonlizhengjian</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26038">#26038</a>)</li>
<li>Added test_top_k_per_row to test-pipeline.yaml. (<a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26569">#26569</a>)</li>
<li>[Bugfix] Make DP padding optional in coordinate_batch_across_dp (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26375">#26375</a>)</li>
<li>Silu v2 (<a class="user-mention notranslate" href="https://github.com/elvircrn">@elvircrn</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25074">#25074</a>)</li>
<li>[Metrics] Add test for multi-modal cache stats logging (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26588">#26588</a>)</li>
<li>[torch.compile] Make inductor partition rules respect splitting_ops <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25691">#25691</a> (<a class="user-mention notranslate" href="https://github.com/baonudesifeizhai">@baonudesifeizhai</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25845">#25845</a>)</li>
<li>[Bugfix] fixed top_logprobs: -1 does not appear to work as intended (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26470">#26470</a>)</li>
<li>[Model][Qwen3VL] Compute <code>cu_seqlens</code> on CPU to remove  (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26496">#26496</a>)</li>
<li>[Model] Add FlexOlmo model implementation (<a class="user-mention notranslate" href="https://github.com/2015aroras">@2015aroras</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24923">#24923</a>)</li>
<li>[Transform] [Quantization] Add QuTLASS support to vLLM (<a class="user-mention notranslate" href="https://github.com/LopezCastroRoberto">@LopezCastroRoberto</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24440">#24440</a>)</li>
<li>Add Qwen3-Omni moe thinker (<a class="user-mention notranslate" href="https://github.com/wangxiongts">@wangxiongts</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25550">#25550</a>)</li>
<li>Update <code>pre-commit</code> hook versions (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26591">#26591</a>)</li>
<li>Update CUDA architecture list in build pipeline for 12.9.1 wheels (<a class="user-mention notranslate" href="https://github.com/wseaton">@wseaton</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26592">#26592</a>)</li>
<li>Fix some typing issues found by <code>mypy==1.18.2</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26596">#26596</a>)</li>
<li>[BUG] Qwen3-next MTP. Fix attn metadata build bug (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26564">#26564</a>)</li>
<li>[BugFix] Fix async scheduling + request preemption (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26385">#26385</a>)</li>
<li>Cache the environment variable check for batch invariance (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26510">#26510</a>)</li>
<li>AOT Compilation for torch.compile (Bundled) (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24274">#24274</a>)</li>
<li>[BugFix] Make penalties and bad_words work with async scheduling (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26467">#26467</a>)</li>
<li>[Frontend] Improve the performance of <code>is_reasoning_end</code> (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25735">#25735</a>)</li>
<li>[CI/Build] Fix ppc64le CPU build and tests (<a class="user-mention notranslate" href="https://github.com/npanpaliya">@npanpaliya</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22443">#22443</a>)</li>
<li>[XPU] Upgrade NIXL to remove CUDA dependency (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26570">#26570</a>)</li>
<li>[MM] Move Qwen3Omni MRoPE impl to model file (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26608">#26608</a>)</li>
<li>[Bugfix][Multi Modal] Fix incorrect Molmo image processing (<a class="user-mention notranslate" href="https://github.com/sangho-vision">@sangho-vision</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26563">#26563</a>)</li>
<li>[Refactor]: Use M-RoPE interface directly while defining model class instead of maintaining model specific M-RoPE implementation in mrope.py (<a class="user-mention notranslate" href="https://github.com/divyanshsinghvi">@divyanshsinghvi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24172">#24172</a>)</li>
<li>fix(nix): Allow local oneDNN path to fix vLLM CPU build failure (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26401">#26401</a>)</li>
<li>Add EAGLE-3 Speculative Decoding Support for Qwen3 MoE (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26485">#26485</a>)</li>
<li>[CPU] fix the issue when the node is '-' cause json decode error. (<a class="user-mention notranslate" href="https://github.com/muzian666">@muzian666</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26562">#26562</a>)</li>
<li>[Refactor]Reduce duplicate code in serving_chat (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26627">#26627</a>)</li>
<li>[compile] Add patched_fused_scaled_matmul_reduce_scatter (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26604">#26604</a>)</li>
<li>[Bugfix][Qwen3VL] fix deepstack in qwen3vl (<a class="user-mention notranslate" href="https://github.com/JJJYmmm">@JJJYmmm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26626">#26626</a>)</li>
<li>[Bugfix] Fix qwen-moe packed_modules_mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26634">#26634</a>)</li>
<li>[Benchmark] Support Infinity API (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26641">#26641</a>)</li>
<li>CP: make correct_attn_out robust to 4‚ÄëD views and fix Triton arg binding (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26509">#26509</a>)</li>
<li>[compile] Fix inductor partition config (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26645">#26645</a>)</li>
<li>[EPLB] Support ernie4.5-moe (<a class="user-mention notranslate" href="https://github.com/HsChen-sys">@HsChen-sys</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22100">#22100</a>)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> to codeowner for pooling models (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26652">#26652</a>)</li>
<li>[PERF] [Qwen3-next] Speed up gated RMSNorm (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26207">#26207</a>)</li>
<li>[MISC] Rename the torch profiler filename as instance_id+rank_id for merging the Profiler results of each Rank (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25867">#25867</a>)</li>
<li>[Bugfix][CI/Build] Fix failing Mteb CI (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26638">#26638</a>)</li>
<li>[Bugfix][DCP] Set default CUDAGraphMode to PIECEWISE for DCP (<a class="user-mention notranslate" href="https://github.com/FENP">@FENP</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26574">#26574</a>)</li>
<li>[TEST][BUG FIX] Fix DP GPU_ID issue (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26442">#26442</a>)</li>
<li>Update <code>Optional[x]</code> -&gt; <code>x | None</code> and <code>Union[x, y]</code> to <code>x | y</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26633">#26633</a>)</li>
<li>[Feature] Add support for naver/splade-v3 (BERT-based sparse embedding model) (<a class="user-mention notranslate" href="https://github.com/gjgjos">@gjgjos</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26339">#26339</a>)</li>
<li>[Models][Qwen3VL] Speedup <code>fast_pos_embed_interpolate</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26647">#26647</a>)</li>
<li>[easy] fix pre commit error on trunk (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26665">#26665</a>)</li>
<li>[CI/Build] Add tool to build vllm-tpu wheel (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/19165">#19165</a>)</li>
<li>[Misc] cache result of disable_inplace (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26666">#26666</a>)</li>
<li>[Bugfix][Core]Fix block table out-of-range issue in priority scheduling (<a class="user-mention notranslate" href="https://github.com/quanliu1991">@quanliu1991</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26661">#26661</a>)</li>
<li>[FIX] Throwing an exception when the model does not support pool tasks (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/25840">#25840</a>) (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25855">#25855</a>)</li>
<li>docs: wrong command in structured_outputs README (<a class="user-mention notranslate" href="https://github.com/yihong0618">@yihong0618</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26677">#26677</a>)</li>
<li>[Model] Fix  Skywork R1V mlp (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26673">#26673</a>)</li>
<li>[Model] Add reasoning_parser and tool_parser for Ernie45 thinking (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25027">#25027</a>)</li>
<li>Ignore large reformatting PRs in <code>git blame</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26690">#26690</a>)</li>
<li>[Model][0/N] Improve all pooling task | clean up (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25817">#25817</a>)</li>
<li>[ResponseAPI] Simplify input/output message serialization (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26620">#26620</a>)</li>
<li>[Bugfix] Fix out of bound index issue for Jina-embedding-v3 RoPE with cuda graph (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26687">#26687</a>)</li>
<li>[unrevert] Add batch invariant kernel override for FlashInfer backend [2/n] (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26373">#26373</a>)</li>
<li>[Hardware][CPU] Disable torch.compile for RISC-V to prevent APIError (<a class="user-mention notranslate" href="https://github.com/ihb2032">@ihb2032</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26693">#26693</a>)</li>
<li>[FEATURE]: Use pydantic validation in <code>multimodal.py</code> config (<a class="user-mention notranslate" href="https://github.com/andycandy">@andycandy</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26629">#26629</a>)</li>
<li>[UX] Speedup DeepGEMM warmup with heuristics (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25619">#25619</a>)</li>
<li>[P/D] [NixlConnector] kv load recovery integration (<a class="user-mention notranslate" href="https://github.com/wseaton">@wseaton</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26171">#26171</a>)</li>
<li>[Misc] Separate prompt logging to debug (<a class="user-mention notranslate" href="https://github.com/aitsvet">@aitsvet</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26713">#26713</a>)</li>
<li>[CI/Build] upgrade compressed-tensors to 0.12.2 to address LGPLv3 (<a class="user-mention notranslate" href="https://github.com/csy1204">@csy1204</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26501">#26501</a>)</li>
<li>[Bugfix][Rocm] fix qr error when different inp shape (@haoyangli-amd <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25892">#25892</a>)</li>
<li>[Bugfix][Speculative Decoding] Extend Eagle quantization config fix to llama_eagle.py (<a class="user-mention notranslate" href="https://github.com/rahul-tuli">@rahul-tuli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26590">#26590</a>)</li>
<li>[Model] Use merge_by_field_config for MM models (M-N) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26710">#26710</a>)</li>
<li>[Log] Optimize Startup Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26601">#26601</a>)</li>
<li>[CI][Release][Arm64]: Build arm64 release for gpu arch 8.9 (<a class="user-mention notranslate" href="https://github.com/cyb70289">@cyb70289</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26698">#26698</a>)</li>
<li>[Quantization] [Performance] Enable Marlin GEMM kernels for the calibration-free RTN-based quantization (<a class="user-mention notranslate" href="https://github.com/sakogan">@sakogan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26051">#26051</a>)</li>
<li>[Frontend][1/N] Improve all pooling task | Support FP16 Embedding Base64 (Still uses fp32 by default). (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26414">#26414</a>)</li>
<li>[CI] Fix mypy for <code>vllm/distributed</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26593">#26593</a>)</li>
<li>[CI Perf]Prune Tests in kernel/mamba (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26538">#26538</a>)</li>
<li>[Bug] Fix Assertion error DeepEP/csrc/kernels/intranode.cu:928: 'false and Unsupported type' (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26532">#26532</a>)</li>
<li>[FrontEnd] UNREVERT CompilationConfig overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): deprecate use_inductor in favor of backend, simplify custom_ops  (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26502">#26502</a>)</li>
<li>Pruning kernel Core Tests (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26727">#26727</a>)</li>
<li>[ResponseAPI] Further polish message serialization and unit tests (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26728">#26728</a>)</li>
<li>Add tests for chunked prefill and prefix cache with causal pooling models (<a class="user-mention notranslate" href="https://github.com/maxdebayser">@maxdebayser</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26526">#26526</a>)</li>
<li>[Misc][DP] support customized aggregated logger for dp (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24354">#24354</a>)</li>
<li>[UX] Replace VLLM_ALL2ALL_BACKEND with --all2all-backend (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26732">#26732</a>)</li>
<li>[compile] Enable sequence parallelism for full cuda graph without specifying compile sizes (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26681">#26681</a>)</li>
<li>[Easy] Fix env type check errors from VLLM_DEBUG_LOG_API_SERVER_RESPONSE (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26742">#26742</a>)</li>
<li>[build][torch.compile] upgrade depyf version (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26702">#26702</a>)</li>
<li>[torch.compile] Unwrap fused_marlin_moe custom op (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26739">#26739</a>)</li>
<li>[Feature][Quantization] auto_round format add support for regex (<a class="user-mention notranslate" href="https://github.com/n1ck-guo">@n1ck-guo</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24024">#24024</a>)</li>
<li>Add support for the /rerank endpoint in vllm bench serve (<a class="user-mention notranslate" href="https://github.com/maxdebayser">@maxdebayser</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26602">#26602</a>)</li>
<li>[Docs] Add a start tag to build.inc.md (<a class="user-mention notranslate" href="https://github.com/windsonsea">@windsonsea</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26747">#26747</a>)</li>
<li>Fix lora tests failure in TPU CI due to the removal of LoRA bias (<a class="user-mention notranslate" href="https://github.com/vanbasten23">@vanbasten23</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26723">#26723</a>)</li>
<li>[CI] [ROCm] Automate CC list for ROCm related issue (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26753">#26753</a>)</li>
<li>Adding the test-amd.yaml for test definitions for the AMD backend. (alternative PR) (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26718">#26718</a>)</li>
<li>scheduler.py: Update the name of the default scheduler. (<a class="user-mention notranslate" href="https://github.com/ryanli">@ryanli</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26758">#26758</a>)</li>
<li>[Model][Bugfix]fix ernie45 load failed due to ernie45 eplb code (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26684">#26684</a>)</li>
<li>[CI/Build] Use 127.0.0.1 instead of localhost in utils (<a class="user-mention notranslate" href="https://github.com/yeqcharlotte">@yeqcharlotte</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26750">#26750</a>)</li>
<li>fix(frontend): always include usage, when configured to do so (<a class="user-mention notranslate" href="https://github.com/max-wittig">@max-wittig</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/20983">#20983</a>)</li>
<li>[Plugin] Make plugin group clear (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26757">#26757</a>)</li>
<li>[Bugfix] Standardize merging multimodal embeddings (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26771">#26771</a>)</li>
<li>[Model] Use merge_by_field_config for MM models (O-P) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26776">#26776</a>)</li>
<li>[NIXL][HeteroTP]Enable KV transfer from HND prefill to NHD decode (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26556">#26556</a>)</li>
<li>[Chore] Use <code>max_transformers_version</code> for Qwen-VL test (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26792">#26792</a>)</li>
<li>Don't allow <code>typos</code> to fix by default (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26785">#26785</a>)</li>
<li>[Doc] ruff format some Python examples (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26767">#26767</a>)</li>
<li>[CI] Fix test_tool_id_kimi_k2 (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26787">#26787</a>)</li>
<li>[Chore] Remove <code>SupportsV0Only</code> interface and update supported models docs (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26783)</li>
<li>[Feature] Change vllm.py with pydantic validation (<a class="user-mention notranslate" href="https://github.com/VladOS95-cyber">@VladOS95-cyber</a> #26726)</li>
<li>[CI/Build] Cleanup LoRA test (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #26752)</li>
<li>[DCP] Support Decode Context Parallel (DCP) for GQA with FlashAttention (<a class="user-mention notranslate" href="https://github.com/FENP">@FENP</a> #24864)</li>
<li>Adjusted the model order of the model registration file (@princepride #26798)</li>
<li>use combo kernel to fuse qk-norm and qk-rope (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26682)</li>
<li>[issues template] Encourage the author implement their own ideas (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #26671)</li>
<li>[KVConnector][Metrics] Aggregate scheduler-side KVConnectorStats (<a class="user-mention notranslate" href="https://github.com/QierLi">@QierLi</a> #26046)</li>
<li>[Feature][Responses API] Stream Function Call - harmony (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #24317)</li>
<li>Revert "[issues template] Encourage the author implement their own ideas" (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #26814)</li>
<li>[Config] Remove Unused Environment Variable <code>VLLM_DISABLE_PAD_FOR_CUDAGRAPH</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26743)</li>
<li>Update coveragerc and add codecov.yml for path fixes (<a class="user-mention notranslate" href="https://github.com/rzabarazesh">@rzabarazesh</a> #26435)</li>
<li>[CI] Raise VLLM_MAX_SIZE_MB to 500 due to failing Build wheel - CUDA 12.9 (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26722)</li>
<li>[Kernel][MoE] Add MoE tunings for GLM 4.6-FP8 and GLM 4.5 Air on NVidia B200 (@zklapow #26818)</li>
<li>[CI Failure] Fix tests with missing TinyLlama-1.1B-Chat-v1.0-FP8-e2e (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26816)</li>
<li>llama4_vision_rope: add HIP override to accept (q, k) and avoid (positions, q, k) mismatch (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #26790)</li>
<li>[Attention][Spec Decode] FlashMLA spec decode support (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #26541)</li>
<li>[Core] Reuse empty block lists whenever possible in KVCacheBlocks to mitigate GC costs (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #24964)</li>
<li>Notice for deprecation of AutoAWQ (@HDCharles #26820)</li>
<li>[Perf] Cache vllm.env.<strong>getattr</strong> result to avoid recomputation (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26146)</li>
<li>Added MoE configs for llama 4, H200 device with tp=4/8 tuning (@Dhruvilbhatt #26837)</li>
<li>fix: response_format for completion (@Nan2018 #23212)</li>
<li>[Minor] Group async_scheduling related fields in model runner init (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26736)</li>
<li>remove attn output view kernel (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26680)</li>
<li>[Core] Streamline some structured output related code (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26737)</li>
<li>[CI Failure] Fix torchao dep failure for Quantization Test (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26824)</li>
<li>[frontend][gptoss] Add per turn stats into Harmony Context (@lacora #25061)</li>
<li>[WideEP][P/D] Add usage stats for DP+EP and KV Connector (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #26836)</li>
<li>[torch.compile] Fix tests for torch==2.9 inductor partition (@ProExpertProg #26116)</li>
<li>[Core][Easy] Use envs.<strong>getattr</strong> for all Unify to environment variable access (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26810)</li>
<li>[Bugfix]fix Qwen3 xml tool parser (@Zhikaiiii #26345)</li>
<li>[BUGFIX][NIXL] quick fix for 'assert self.connector_worker is not None' in get_kv_connector_stats (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #26851)</li>
<li>Disable FlashInfer sampler by default (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26859)</li>
<li>[Frontend][torch.compile] CompilationConfig Overhaul (<a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/issues/20283">#20283</a>): name change  compilation level to compilation mode, deprecation compilation level (<a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> #26355)</li>
<li>[Bugfix] Fixes prefix-repetition benchmark script (@kouroshHakha #26828)</li>
<li>[Model] Add DeepSeek-V3.1 reasoning parser (split from PR #24972) (@taohui #25589)</li>
<li>[Docs] Move build.inc into arm.inc (<a class="user-mention notranslate" href="https://github.com/windsonsea">@windsonsea</a> #26862)</li>
<li>[CI/Build][Bugfix] fix qutlass cmake error when set QUTLASS_SRC_DIR (@izhuhaoran #26773)</li>
<li>[Feature] default --extra-body param to disable thinking in vllm bench serve (@lengrongfu #26784)</li>
<li>[BugFix] Patch inductor partitioning logic (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #26735)</li>
<li>[Bugfix] Fix qwen3-omni audio truncation issue (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #26815)</li>
<li>[Graph Partition] pass tests for decorator (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26831)</li>
<li>[Bugfix][Multi Modal] Fix incorrect Molmo token processing (<a class="user-mention notranslate" href="https://github.com/sangho-vision">@sangho-vision</a> #26873)</li>
<li>[DSA][MLA] Tiny refactor on DeepSeek to make it reusable for different backends (@MengqingCao #26656)</li>
<li>[Misc] Use helper function to generate dummy messages in OpenAI MM tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26875)</li>
<li>[bugfix] Lazy import cv2 (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #26869)</li>
<li>[Deepseek-V3.2][Kernel] Integrate cuda indexer k cache gather (<a class="user-mention notranslate" href="https://github.com/zyongye">@zyongye</a> #26456)</li>
<li>[CI/Build] Add Qwen2.5-VL-7B-Instruct ChartQA Accuracy Tests in CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #21810)</li>
<li>[CI] Fix mypy for <code>vllm/executor</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26845)</li>
<li>[Doc] ruff format remaining Python examples (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26795)</li>
<li>[doc] add Context Parallel Deployment doc (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> #26877)</li>
<li>[Misc] Update TritonLanguagePlaceholder to have attributes that are used by Flash Linear Attention ops. (@madongfly #26853)</li>
<li>[Fix] Remove divisibility requirement between num_kv_heads and tp_size in bailing_moe (@ant-yy #26876)</li>
<li>[Easy] Get rid of unnecessary paraenthesis in kv_cache_manager (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26842)</li>
<li>[Platform] allow platform to init dp group (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #22243)</li>
<li>[Lora]Load tuned multi-lora kernel configs from json files (@li2haipeng #26319)</li>
<li>[Model][2/N] Improve all pooling task | Support multi-vector retrieval (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #25370)</li>
<li>[Misc] Remove <code>isort</code> and <code>yapf</code> ignores (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26888)</li>
<li>[Misc] rename torch_dtype to dtype (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #26695)</li>
<li>chore: remove unused marker (<a class="user-mention notranslate" href="https://github.com/max-wittig">@max-wittig</a> #26890)</li>
<li>[BugFix] Patch inductor memory plan logic (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26878)</li>
<li>[Chore] Separate out <code>vllm.utils.func</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26904)</li>
<li>[Chore] Separate out <code>vllm.utils.async_utils</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26913)</li>
<li>Lower severity of log when model info cache misses due to exception (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #26917)</li>
<li>Olmo 3 tool parser and tests (@pdasigi #26143)</li>
<li>[Feature]: Use pydantic validation in observability.py config (@cern1710 #26637)</li>
<li>[ModelOpt] Remove NVFP4 MoE K%16==0 constraint (@XiaobingSuper #26891)</li>
<li>[Chore] Clean up CODEOWNERS (@WoosukKwon #26923)</li>
<li>[NVIDIA] Add support for cudnn fp4 gemm via flashinfer (@kaixih #26107)</li>
<li>Vectorize RMS norm variance using vectorize_read_with_alignment (@bbeckca #26234)</li>
<li>support flashinfer_fp4 moe for 5090 gpu (@XiaobingSuper #26669)</li>
<li>[Bug] Temporally Disable <code>VLLM_ALLREDUCE_USE_SYMM_MEM</code> by Default (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26925)</li>
<li>Move query quantization to attention layer for Flashinfer &amp; Triton. (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> #26534)</li>
<li>Adjusting AMD test composition 2025-10-14 (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #26852)</li>
<li>[Qwen3-Next] Add tuned MoE config for Qwen3-Next FP8 on H100 tp2 (@felixzhu555 #26887)</li>
<li>[Bugfix] reasoning_parser parameter handling in run_batch.py (@inc-jeong #26225)</li>
<li>[ROCm][FEAT] Fuse DeepSeek shared experts into AITER fused_moe ops (@kliuae #24097)</li>
<li>[CI] Enable Blackwell Llama4 MoE tests (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #26731)</li>
<li>[BUG] Allow runai_streamer_sharded in config check (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> #26958)</li>
<li>[bugfix] Fix SP + PP without specifying compile size (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #26955)</li>
<li>[BugFix] Work around graph partition x torch.compile cache issue (@zou3519 #26956)</li>
<li>[DOC][XPU]update feature parity with Intel GPU (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #26954)</li>
<li>[Chore] Rename <code>utils</code> submodules (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26920)</li>
<li>[PERF] Qwen3-next MTP speedup (change bool mask indexing to index_select / index_copy to reduce d2h) (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #26437)</li>
<li>Deepseek-v3 Batch Invariant on 8xH100 (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #26609)</li>
<li>[CI/Build] Update expected beam search output for Phi3V (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26978)</li>
<li>[Hardware][CPU][PowerPC]Disable torch.compile() in toptopk sampling (@Akashcodes732 #26987)</li>
<li>[CI/Build] Fix AMD import failures in CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #26841)</li>
<li>[Benchmark] Use truncation by default for pooling benchmarks (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26992)</li>
<li>[Chore] Separate out <code>vllm.utils.collections</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26990)</li>
<li>[Model][Bugfix] fix ernie45 vl run failed from shared experts optimization (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> #26885)</li>
<li>Cleanup code after Python 3.10 upgrade (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #26520)</li>
<li>[MISC] fix import violations for re and triton modules (@llsj14 #26654)</li>
<li>[Bugfix] Correct LayerNorm epsilon parameter in modernbert.py (@bogdanminko #27008)</li>
<li>[Benchmark] Show E2EL by default for pooling models (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27014)</li>
<li>[Attention] Tune CUTLASS MLA num_splits (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #26846)</li>
<li>[NIXL] Improve request_finished() debug logs (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #25665)</li>
<li>[docs] standardize Hugging Face env var to <code>HF_TOKEN</code> (deprecates <code>HUGGING_FACE_HUB_TOKEN</code>) (@yankay #27020)</li>
<li>[CI] Replace large models with tiny alternatives in tests (@tahsintunan #24057)</li>
<li>[Feature] Add process_weights_after_loading to AttentionImpl (@lengrongfu #26870)</li>
<li>[Model] Fix Qwen3VL mm mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27027)</li>
<li>Fix Qwen2.5 VL image grid docstring (@skyloevil #27033)</li>
<li>Support <code>set</code> in the CLI generation (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27031)</li>
<li>[gpt-oss][1/N] EZ: refactor serving_responses for modularity (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #26948)</li>
<li>Support block size of 256 used by Intel HPU (@mandy-li #26883)</li>
<li>[Compressed Tensors] Always clone output for compile robustness (@kylesayrs #26849)</li>
<li>Adding Warmup to Benchmark Serving (@kimbochen #26943)</li>
<li>[Bug] Fix batch invariant test <code>has</code> to <code>is</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27032)</li>
<li>[GPTOSS][DP/EP][Marlin] Enable GPTOSS Batched DP/EP using Marlin kernels (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #25997)</li>
<li>[Feature] Migrate DeepGEMM API from <code>get_m_alignment_for_contiguous_layout</code> to <code>get_mk_alignment_for_contiguous_layout</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26935)</li>
<li>[CI] Prune Quantization Tests and skip compilation (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #27038)</li>
<li>[Bug] Add Assertion for <code>random-input-len</code> / <code>random-output-len</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26834)</li>
<li>[small][batch invariance] Rename the env and internal flags to simplify usage (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #26855)</li>
<li>Refactor Transformers backend to use mixins (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #26906)</li>
<li>[NVIDIA] [Perf] Update to leverage flashinfer trtllm FP4 MOE throughput kernel (@jiahanc #26714)</li>
<li>[torch.compile] Passing only necessary compilation config to inductor pass config (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #27041)</li>
<li>[Chore] Separate out <code>vllm.utils.import_utils</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27022)</li>
<li>[torch.compile] fix simple inductor graph partition test (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27050)</li>
<li>Remove unused imports (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #26972)</li>
<li>vllm bench serve shows num of failed requests (@tomasruizt #26478)</li>
<li>[Docs] Reduce custom syntax used in docs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27009)</li>
<li>[Perf] Exploit out-of-band buffers in shm_broadcast (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26961)</li>
<li>disable graph partition in custom op (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #26952)</li>
<li>[Bugfix][Qwen] fixes the weights dtype in qwen3_next: it is actually a bfloat16 (@sighingnow #27030)</li>
<li>[Core] Change <code>execute_model_with_error_logging()</code> to be a ctx manager (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27060)</li>
<li>[Bugfix] Fix ReplicatedLinearWithLoRA  (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27065)</li>
<li>[Kernel] Lazy import FlashInfer (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #26977)</li>
<li>[CI/Build] Update Llama4 eval yaml (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27070)</li>
<li>[Model] Always use Transformers backend for PaliGemma and Gemma3-MM (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #26715)</li>
<li>[Model] Add support for LightOnOCR (@staghado #26916)</li>
<li>[CI/Build] Update compressed tensor test path to fix CPU CI (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27068)</li>
<li>[Kernel][Performance] Fuse float cast and renormalize to topk softmax kernel  (@izhuhaoran #26717)</li>
<li>[CI] fix docs build failed (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27082)</li>
<li>Update troubleshooting.md and remind VLLM_TRACE_FUNCTION usage (@Prowindy #27069)</li>
<li>[VLM][Refactor] Remove useless func <code>get_input_positions</code> in <code>MRotaryEmbedding</code> (@MengqingCao #27088)</li>
<li>[Docs] Replace all explicit anchors with real links (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27087)</li>
<li>[Docs] Replace <code>rst</code> style double-backtick with <code>md</code> single-backtick (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27091)</li>
<li>[Model]Improve Qwen3VLMoeForConditionalGeneration packed_modules_mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27096)</li>
<li>[Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI350 and MI355 (@rkarhila-amd #25586)</li>
<li>Fix incorrect docstring for stop_profile() method (@hyongtao-code #27101)</li>
<li>[torch.compile] Enable attention and allreduce fusion without custom ops enabled (@ProExpertProg #24604)</li>
<li>[CI] Nixl integration tests (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27010)</li>
<li>[Data-parallel] Allow DP&gt;1 for world_size &gt; num_gpus on node (8) (@patrickvonplaten #26367)</li>
<li>[bugfix] Qwen3-VL fix video incorrect timestamp calculations while do_sample_frames=True (@wulipc #27104)</li>
<li>[CI] Remove forbidden slash (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27112)</li>
<li>[ROCM] MoE fp4 CK kernel (@maleksan85 #26545)</li>
<li>[ROCm][Bugfix][Model] Fix illegal memory access when running qwen3_moe models with  rms_norm (Qwen3-235B-A22B,  Qwen3-30B-A3B, etc.) (@rasmith #26192)</li>
<li>[Bugfix] [AITER] [ROCm] Fix Quark MoE Quant Config and AITER Fused MoE quant type logic (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #27029)</li>
<li>[Chore] Remove unused <code>PolyNorm</code> layer (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27110)</li>
<li>[Bugfix] Use PIECEWISE cudagraphs on Blackwell if max_model_len &gt; 131072 (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #27114)</li>
<li>[Minor] Remove unnecessary error message (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #27115)</li>
<li>[V1][Spec Decode] Fix greedy temperature detection after sampler refactor (@Pradyun92 #27077)</li>
<li>[Test] Make <code>test_failure</code> more stable for batch invariance (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27054)</li>
<li>[BugFix][Core] Fix error when enable async-scheduling in multi-node env (<a class="user-mention notranslate" href="https://github.com/lhtin">@lhtin</a> #25887)</li>
<li>[Perf]  Add H100 fused MoE config (@skyloevil #25398)</li>
<li>[CI/Build] tests(v1): feed Triton attention the (num_blocks, 2, ‚Ä¶) KV cache layout in backend-correctness tests (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #26663)</li>
<li>[GPT-OSS] Structure_Tag support for gpt-oss tool-call in cot (@Hanchenli #25515)</li>
<li>[Misc] Rev DeepEP (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27122)</li>
<li>[DOC][FEATURES][CPU]update cpu feature for v1 (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #27135)</li>
<li>[Test] Add test for /health endpoint on engine failure (@dongbo910220 #26074)</li>
<li>[Chore] Separate out <code>vllm.utils.mem_utils</code> (@iAmir97 #27143)</li>
<li>[Feature] Batch Invariant: Support DeepGEMM and Blackwell (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27127)</li>
<li>[fix][cpu] fix prefill attention in CPU attention backend (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27035)</li>
<li>[Misc] Refactor <code>get_kv_cache_spec</code> into <code>AttentionLayerBase</code> (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26587)</li>
<li>[Models][QwenVL] Remove unnecessary <code>.contiguous()</code> calls (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #27106)</li>
<li>[Chore] Clean up pytorch helper functions in <code>vllm.utils</code> (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #26908)</li>
<li>Fix incorrect string formatting in barrier timeout exceptions (@hyongtao-code #27149)</li>
<li>[Minor] Add some clarifying comments to recent changes (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27130)</li>
<li>[BugFix] Fix failing gemma-3-1b-it test: <code>test_lm_eval_accuracy_v1_engine[google/gemma-3-1b-it]</code> (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27111)</li>
<li>[Chore] Separate out profiling utilities from vllm.utils (@dongbo910220 #27150)</li>
<li>[BugFix] fix graph partition signature (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27139)</li>
<li>[BugFix] Disable fp8 kv-cache by default for DeepSeek V3.2 (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27121)</li>
<li>[V1][Metrics][Plugin] Add plugin support for custom <code>StatLoggerBase</code> implementations (@ptovam #22456)</li>
<li>[Minor] Remove unused env variable (@WoosukKwon #27161)</li>
<li>[BugFix] Fix lazy imports involving outlines_core (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27158)</li>
<li>[Chore] Separate out hashing utilities from vllm.utils (@dongbo910220 #27151)</li>
<li>[Benchmark] Convenience script for multiple parameter combinations (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27085)</li>
<li>output type conversion fix (@jianyuh #27159)</li>
<li>[Chore] Separate out <code>vllm.utils.network_utils</code> (@iAmir97 #27164)</li>
<li>[Misc] Move utils to avoid conflicts with stdlib, and move tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27169)</li>
<li>[Bugfix] Fix error with penalties when speculative decoding and structural output are enabled (<a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> #26586)</li>
<li>Fix typo in ValueError message: use <code>kv_role</code> instead of <code>kv_disagg_role</code> (@hyongtao-code #27166)</li>
<li>[Model][VLM] Support Bee-8B Model (@uyzhang #27012)</li>
<li>[LoRA] LoRA cuda graph specialization (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #25914)</li>
<li>[Kernel] Accelerate solve_tril with TMA (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #26746)</li>
<li>AArch64 CPU Docker pipeline #26931)</li>
<li>Nemotron Nano V2 VL + EVS Video Support (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> #27107)</li>
<li>[Kernel][Model] Tune fused_moe Triton configs for Qwen3-30B A3/A3B on H100 (FP8/BF16) (@shivampr #26268)</li>
<li>[Bugfix][CI] Fix <code>Distributed Tests (4 GPUs)</code> async_sched+ray test (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27195)</li>
<li>[Feature][Quantization] auto_round support for mixed bits quantization (<a class="user-mention notranslate" href="https://github.com/n1ck-guo">@n1ck-guo</a> #23812)</li>
<li>[ROCm] enable some tests in entrypoints test groups on AMD (@Concurrensee #26725)</li>
<li>[ez] add uv lock to gitignore (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #27212)</li>
<li>[Quantization] Automatically infer AWQ <code>modules_to_not_convert</code> field (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #26909)</li>
<li>[V0 Deprecation] Remove V0 metrics code (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27215)</li>
<li>[cpu] Dispatch un-quantized linear to oneDNN/ACL by default for AArch64 (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27183)</li>
<li>create is_in_the_same_node on cpu (@helunwencser #26832)</li>
<li>[Frontend] Enforce tokenize=False when applying chat template (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> #27205)</li>
<li>[Feature][Kernel]FusedMoE LoRA (@wcwuwc #21229)</li>
<li>[BugFix] GPT-OSS Attention DP + MoE TP weight loading issue (@nvpohanh #24032)</li>
<li>[ModelOpt] Load w13/w2_input_scale for all experts, nvfp4 (@wenscarl #26135)</li>
<li>[Bugfix] Fix gpt-oss w4a8 DP/EP on B200 (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #26729)</li>
<li>[Bugfix] Fix broken MTP weight loading for FP8 KV Scales (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #27227)</li>
<li>[Fix][Spec Decode] Fix llama4 draft loading with different quantization (@linzebing #27136)</li>
<li>[Nixl] Minor refactor to handshake related metadata (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26410)</li>
<li>[MM][Core] Decouple ViT backend from LM backend (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #27061)</li>
<li>[Deepseek v3.2] Optimize top_k_per_row (<a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> #26763)</li>
<li>[Chore] Separate out NCCL utilities from vllm.utils (@dongbo910220 #27197)</li>
<li>[CI] Install pre-release version of <code>apache-tvm-ffi</code> for <code>flashinfer</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27262)</li>
<li>[ROCM] Enable CompressedTensorsWNA16 (@JartX #27187)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> to .github/codeowners (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> #27213)</li>
<li>[ROCm] Update Triton, Torch, and AITER branches for ROCm base Dockerfile (@micah-wil #27206)</li>
<li>[Feature] Batch Invariant for R1 TP 8 on Blackwell (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27229)</li>
<li>[Bugfix][P/D] Reduce num_threads used by nixl ucx backend (@dagrayvid #27196)</li>
<li>[V0 Deprecation] Remove V0 executors (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27142)</li>
<li>[Bugfix] fixes the decoding metadata of dense mla's fp8 kvcache. (@sighingnow #27144)</li>
<li>Update PyTorch to 2.9.0+cu129 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #24994)</li>
<li>[Performance] Dual stream execution of "shared_experts" and "selected_experts" inside FusedMoE (@alexm-redhat #26440)</li>
<li>Updated xgrammar backend to not deny supported string formats (@ExtReMLapin #27253)</li>
<li>[Bugfix] skip cuda graph for drafter when running with eager (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #26821)</li>
<li>[P/D] KVConnector for decode benchmarking (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #25986)</li>
<li>[Deepseek v3.2] Remove extra logics in indexer (@IwakuraRein #26465)</li>
<li>[DOC] [ROCm] Add ROCm quickstart guide (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #26505)</li>
<li>[CI] Nixl integration tests DP-EP (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27199)</li>
<li>[Benchmark] Add plot utility for parameter sweep (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27168)</li>
<li>[torch.compile] Enable silu_mul_fp8_quant fusion without custom ops enabled (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27146)</li>
<li>[1/N][Platform] Cleanup useless function (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #26982)</li>
<li>Update release pipeline for PyTorch 2.9.0 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #27303)</li>
<li>Remove last <code>level</code> references not removed #26355 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27260)</li>
<li>fixed reasoning streaming with tool_choice="required" (@ExtReMLapin #24108)</li>
<li>[Frontend][3/N] Improve all pooling task | Support binary embedding response (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #27066)</li>
<li>[Bugfix][CPU] Disable dual stream execution for experts on CPU (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27320)</li>
<li>[Bug] Raise error for <code>LLM(data_parallel_size=k)</code> single-process DP Usage (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27282)</li>
<li>Bugfix - pass 'max_num_tokens_padded' into 'moe_lora_align_block_size' (@gnovack #27311)</li>
<li>[Core] Handle MoE LoRA edge cases (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27335)</li>
<li>[docs] Update v1 metrics design doc (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #27332)</li>
<li>Mirroring changes in test-pipeline.yaml into test-amd.yaml (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #27242)</li>
<li>[Chore] Separate out optional dependency checks from vllm.utils (@dongbo910220 #27207)</li>
<li>[Model] Upstream Deepseek-OCR model (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27247)</li>
<li>[NIXL] Terminate handshake listener thread in shutdown (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #26404)</li>
<li>[Bug] Fix DeepSeek-V2.5-1210-FP8 issue (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27267)</li>
<li>[bugfix] remove unused parameters to reduce unnecessary vram usage (@ReinForce-II #26789)</li>
<li>[Bugfix] Add missing 'is_internal_router' attribute to FusedMoEWithLoRA (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27351)</li>
<li>[NIXL] use Host buffer to support TP_ratio &gt; 1 for XPU (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #27140)</li>
<li>[Bugfix] Make <code>get_mrope_input_positions</code> instance methods (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27342)</li>
<li>[Bugfix] Fix HF format InternVL large variants video processing (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27330)</li>
<li>[Frontend] Require flag for loading text and image embeds (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> #27204)</li>
<li>[P/D] Dynamic <code>kv_output_aggregator</code> collect size (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26734)</li>
<li>Support Anthropic API /v1/messages Endpoint (@LiuLi1998 #22627)</li>
<li>[Bugfix] Disable FlexAttention direct block mask building for encoder-only models (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27344)</li>
<li>[Model] Revert PR #26715: Restore custom PaliGemma and Gemma3-MM impl‚Ä¶ (<a class="user-mention notranslate" href="https://github.com/lucianommartins">@lucianommartins</a> #27309)</li>
<li>[Doc] Fix numbering sequence in prefix caching (@gigit0000 #27357)</li>
<li>[Prefix Cache] Use LoRA name for consistent KV-cache block hashing (@sagiahrac #27211)</li>
<li>[Feature] publisher default set zmq in kv_event config (@lengrongfu #26915)</li>
<li>[BugFix] bugfix for Flash Attention MLA with full cuda graph IMA following pr-25490 (@Daisy-Ma-coder #27128)</li>
<li>[Chore] Separate out system utilities from vllm.utils (@dongbo910220 #27201)</li>
<li>[MLA] Bump FlashMLA (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27354)</li>
<li>[Bugfix] Fix deepseek-ocr multi-image inference and add <code>merge_by_field_config=True</code> with tensor schema support (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27361)</li>
<li>[Bugfix] Fix SLA tuner initialization (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27355)</li>
<li>[Bugfix] Fix incorrect kv cache metrics in grafana.json (@fangpings #27133)</li>
<li>[Bugfix][Core] running queue index leakage exception (@CLFutureX #26754)</li>
<li>[CORE] Support Prefix Caching with Prompt Embeds (<a class="user-mention notranslate" href="https://github.com/qthequartermasterman">@qthequartermasterman</a> #27219)</li>
<li>[V1][spec decode] return logprobs for spec decoding (@TheEpicDolphin #26060)</li>
<li>[Model] Add num_cached_tokens for PoolingRequestOutput (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #27378)</li>
<li>[Chore] Remove duplicate <code>has_</code> functions in vllm.utils (@jonathanc-n #27372)</li>
<li>[CI/Build] Fix Prithvi plugin test (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27393)</li>
<li>[Bugfix] Fix args settings for guided decoding args (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #27375)</li>
<li>[CI/Build] Fix AMD CI: test_cpu_gpu.py (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27388)</li>
<li>add SLA information into comparison graph for vLLM Benchmark Suite (@louie-tsai #25525)</li>
<li>[CI] Reorganize entrypoints tests (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27403)</li>
<li>[Metrics] [KVConnector] Add connector prefix cache hit rate stats (@ptovam #26245)</li>
<li>[Model] Add MoE support for NemotronH (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> #25863)</li>
<li>Run mypy on the lowest supported Python version instead of system Python (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27048)</li>
<li>[Bugfix] Honor --mm_encoder_attn_backend when used (@bradleyhd #27124)</li>
<li>[Feature] Pydantic validation for speculative.py (@Navya1707 #27156)</li>
<li>[Misc] Remove use of CUDA_VISIBLE_DEVICES for device selection (fix DP slow startup time &amp;c) (@ilmarkov #26709)</li>
<li>[CI/Build] Remove unnecessary flags from test registry (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27353)</li>
<li>[Frontend][4/N] Improve all pooling task | Add plugin pooling task (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #26973)</li>
<li>Mirroring the test definitions (2025-10-22) (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #27362)</li>
<li>[Bugfix] Fix dp_chunking enablement logic in FusedMoE layer (@alexm-redhat #27220)</li>
<li>[Bugfix][ROCm][DeepSeek] Fix for forward_hip in rope for DeepSeek (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #27373)</li>
<li>[Bugfix] Fix AWQ marlin layer skipping (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27416)</li>
<li>[Misc] Add triton_kernels dependency (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27370)</li>
<li>[Chore] Separate out <code>vllm.utils.platform_utils.py</code> (@jonathanc-n #27374)</li>
<li>[Attention] Fix FlashMLA metadata builder arguments for q_len &gt; 1 (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27368)</li>
<li>[Bugfix][DP] Fix creating too many DP Placement Groups (@kebe7jun #26880)</li>
<li>[Model] Siglip Embedding Support (@piood #27324)</li>
<li>[Hardware][POWERPC] Disable oneDNN path in vllm/model_executor/layers/utils.py for Powerpc (@Akashcodes732 #27422)</li>
<li>Granite 4.0 quark quantization support (@xiao-llm #26944)</li>
<li>Fix pooling adapters for Transformers backend  (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27338)</li>
<li>[Kernel] Add GPTQv2 format support for low-bit or asymmetric quantization, by adapting gptq_gemm (@xxxxyu #26092)</li>
<li>[Misc] Add TPU usage report when using tpu_inference. (@hfan #27423)</li>
<li>[Bugfix][CI] Move resolving cudagraph_mode before initializing attn_metadata_builder (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> #27427)</li>
<li>Fix EventPublisherFactory logic for disabled KV cache events (@usberkeley #27419)</li>
<li>[Chore] remove structural tags logging lines (<a class="user-mention notranslate" href="https://github.com/aarnphm">@aarnphm</a> #27451)</li>
<li>[Bugfix] Fix Pydantic union resolution for ResponseFunctionToolCall in Responses API (@strinczer #26706)</li>
<li>[Misc] Avoid "PyTorch non-writable tensors" warning in RayPPCommunicator (<a class="user-mention notranslate" href="https://github.com/ruisearch42">@ruisearch42</a> #27443)</li>
<li>[Docs] remove v1 column for embedding models (@piood #27446)</li>
<li>[MM][Bugfix] Replace <code>PatchEmbed</code>'s conv3d to linear layer (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27418)</li>
<li>[BugFix] Fix torchrun DP with LLM class (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27395)</li>
<li>[Refactor] move tool parsing logic from protocol.py to the tool parser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27383)</li>
<li>[Benchmark] Enable benchmark to run with <code>encoding_format="bytes"</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27467)</li>
<li>Fix AArch64 CPU Docker pipeline #27331)</li>
<li>[MISC] <code>cudagraph_capture_sizes</code>  related improvements (<a class="user-mention notranslate" href="https://github.com/fhl2000">@fhl2000</a> #26016)</li>
<li>Fix test named tool use (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27458)</li>
<li>[Doc] Fix minor issues in docs/design/metrics.md (@draftbk #27436)</li>
<li>[cpu][fix] Fix onednn_mm crash on consecutive matmuls with same M,K,N and different dtype (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27472)</li>
<li>[compile] Turn standalone_compile back on (@zou3519 #27460)</li>
<li>[NIXL][BUGFIX] delay done_recving queue cleanup to bottom of get_finished (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #27297)</li>
<li>[Bugfix] Fix MultiConnector stats reconstruction across process boundaries (@kouroshHakha #27366)</li>
<li>[Attention] Add MLA prefill backend: trtllm_ragged_attention_deepseek (<a class="user-mention notranslate" href="https://github.com/minosfuture">@minosfuture</a> #26397)</li>
<li>[Bugfix] Fix interns1-vit qk norm code path (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27480)</li>
<li>[CI/Build] Fix test_torch_utils in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27317)</li>
<li>[Document] Add ms-swift library to rlhf.md (@hjh0119 #27469)</li>
<li>[Perf][Async Scheduling] Remove CPU-&gt;GPU sync in dummy_run (<a class="user-mention notranslate" href="https://github.com/lhtin">@lhtin</a> #27455)</li>
<li>[Distributed] Basic set of configuration for large EP deployment on GB200 (@wpc #27328)</li>
<li>[Log] Optimize Startup Log (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #26740)</li>
<li>[Misc][DP] Guard mxfp4 implementation selection (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27484)</li>
<li>[KVConnector] Migrate the LMCache integration code to be vLLM native (@ApostaC #25542)</li>
<li>[CI] Add tests for cudagraph (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27391)</li>
<li>Revert "[Misc] Remove use of CUDA_VISIBLE_DEVICES for device selectio‚Ä¶ (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #27502)</li>
<li>[Core][Hybrid allocator + kv connector 1/n] Enable hybrid allocator + KV cache connector (@KuntaiDu #25712)</li>
<li>[Misc] Simplify max tokens in multimodal registry (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27500)</li>
<li>[Attention] Add missing kv cache scale setup (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27490)</li>
<li>[CI/Build] Refactor processing tests (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27470)</li>
<li>[CI/Build] Use CPU for mm processing test on CI (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27522)</li>
<li>[BUGFIX][ROCM] ViT FlashAttention on ROCm (no GFX9) and contiguous on qwen3vl ROCm TORCH_SDPA (@JartX #27190)</li>
<li>[Bugfix] Fix processor initialization for model from modelscope instead of HF (@lengrongfu #27461)</li>
<li>[Bugfix] fix empty prompts for async-engine mode in benchmark throughput (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #27494)</li>
<li>[Doc] Remove Molmo warning (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27527)</li>
<li>[Doc] Fix links to GH projects (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27530)</li>
<li>[Chore]:Extract math and argparse utilities to separate modules (@yeshsurya #27188)</li>
<li>Revert "[CI/Build] Use CPU for mm processing test on CI (#27522)" (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27531)</li>
<li>[CI/Build] Update causal-conv1d installation (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27529)</li>
<li>[Model][MiniMax-M2] Support MiniMax-M2 Model (@rogeryoungh #27535)</li>
<li>fix m2 test (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> #27536)</li>
<li>Fix MiniMax-M2 copyright (@rogeryoungh #27537)</li>
<li>[Model][Bugfix] fix ernie45 moe 300B SharedFusedMoE output tuple (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> #27316)</li>
<li>[Model] Use merge_by_field_config for MM models (Qwen series) (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27546)</li>
<li>[Docs] reemove the incorrect <code>enable_reasoning</code> parameter  (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> #27550)</li>
<li>[Performance][LoRA] add context varying params to 'do_not_specialize' in fused moe lora (@gnovack #27445)</li>
<li>[Model] Deprecate <code>merge_by_field_config=False</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27551)</li>
<li>[Doc] Slight improvement to M2 and beyond (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27554)</li>
<li>[Kernel] Adding split_K implementation for fused_moe_lora (@dcmaddix #27291)</li>
<li>[Misc] Clean up utils (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27552)</li>
<li>[Bugfix] Limit the default value of <code>max_model_len</code> when it is not specified by users (@shen-shanshan #27556)</li>
<li>[Bugfix] Fixed when return_token_ids=False, the first event still contains prompt_token_ids. (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27561)</li>
<li>[cpu][perf] Fix low CPU utilization with VLLM_CPU_OMP_THREADS_BIND on AArch64 (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #27415)</li>
<li>[Kernel] Enable moe LoRA kernel support FP16 (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27468)</li>
<li>[Hybrid] Added supports_mamba_prefix_caching Protocol (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> #27339)</li>
<li>[Model] Siglip2 Model Support (@piood #27566)</li>
<li>[Bugfix][LoRA][FusedMoE] Select MxFP4 Backend based on LoRA Enablement (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27487)</li>
<li>fixing mm placeholder replacement issue with gemma3 (@tingtingtangmeta #27538)</li>
<li>[Chore]: Stream tokens vs characters in tool call parser tests (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> #26513)</li>
<li>[Misc] Clean up more utils (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27567)</li>
<li>[ROCm] Update AITER branch for ROCm base docker (@micah-wil #27586)</li>
<li>Code quality improvements: version update, type annotation enhancement, and enum usage simplification (@usberkeley #27581)</li>
<li>[gpt-oss][2/N] Support input_messages in responsesRequest (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #26962)</li>
<li>[Bugfix][CI] Fix config resolving logic with remote models (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #27610)</li>
<li>[Stability fix] turn off HMA allocator when connector is set (@KuntaiDu #27592)</li>
<li>[Bugfix] fixed inconsistent finish_reason handling between V0 and V1 engines (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27555)</li>
<li>[ROCm] [Doc] Update ROCm installation docs  (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #27327)</li>
<li>[Hardware][AMD][Model] Triton MoE tuning configs for GLM-4.6 for MI300X (@minatoaquaMK2 #27323)</li>
<li>[Bugfix][CPU] Fallback oneDNN linear to torch linear to fix half gemm support on legecy platforms (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27526)</li>
<li>[Core][Bookkeeping Optimization] Update against numpy view of is_token_ids tensor (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27618)</li>
<li>[CI/Build] Fix amd model executor test (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27612)</li>
<li>Fix a robust parsing issue in KimiK2ToolParser that causes IndexError (@wangln19 #27565)</li>
<li>[V0 Deprecation] Remove vestigial V0 logits_processors.py file (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27601)</li>
<li>[Bugfix] In LongRoPE, decide short vs long based on max_model_len (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27431)</li>
<li>[Misc] Separate out <code>utils.counter</code> and move <code>utils.Device</code> to engine (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27588)</li>
<li>[Bug] Fix shape issue for eplb expert weights (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27589)</li>
<li>[compile] Add enable_prompt_embeds to compile hash. (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> #27285)</li>
<li>[Hybrid] Add mamba_block_size to Engine Args (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> #27289)</li>
<li>[compile] Disable dynamo guards check for AOT compilation. (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> #27288)</li>
<li>fix: allow HuggingFace standard chat template params via **kwargs (@wangln19 #27622)</li>
<li>[Core] Enable async scheduling for external_launcher mode (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27394)</li>
<li>[Bugfix][Frontend] validate arg priority in frontend LLM class before add request (@junpuf #27596)</li>
<li>[BugFix] Also consider RAY_EXPERIMENTAL_NOSET_* when storing compilation cache (@HollowMan6 #27294)</li>
<li>[nit]: lmcache integration import (@sammshen #27600)</li>
<li>[FLA] Introduce Kimi Delta Attention(KDA) to VLLM (<a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> #27654)</li>
<li>[Bugfix] Fix allocation &amp; free logic of SingleWriterShmRingBuffer (@imkero #27117)</li>
<li>[Bugfix][CI] Fix v1 attention backend tests and add CI coverage (@mmangkad #26597)</li>
<li>[Misc] Make <code>LayerBlockType</code> a <code>Literal</code> instead of <code>Enum</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27658)</li>
<li>[compile] Add fallback path to AOT compile when serialization fails. (<a class="user-mention notranslate" href="https://github.com/zhxchen17">@zhxchen17</a> #27350)</li>
<li>Add load pattern configuration guide to benchmarks (@mpashkovskii #26886)</li>
<li>[Misc] Make reorder batch also separate extends (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27367)</li>
<li>[Test] Batch Invariant: Unit test using parameterized backend (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27478)</li>
<li>[Core] Scheduler: Publish connector events after output (<a class="user-mention notranslate" href="https://github.com/orozery">@orozery</a> #25875)</li>
<li>[AsyncScheduling] Make async overlap work with logprobs (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27615)</li>
<li>[Misc][qwen2_5_vl][torch.compile] Enable <code>supports_torch_compile</code> on generic nn.Module and demonstrate speedup on Qwen Vision model (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> #23207)</li>
<li>[Bug] Fix deepep low latency use nvlink by default (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27677)</li>
<li>[Core] Early return in SlidingWindowManager.remove_skipped_blocks (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27673)</li>
<li>Install pre-built xformers-0.0.32.post2 built with pt-2.9.0 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #27598)</li>
<li>Revert "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> #27714)</li>
<li>[Build] Revert triton_kernels requirements (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27659)</li>
<li>[NIXL][XPU] update name of nixl wheel (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> #27631)</li>
<li>[Model] Fix Qwen3VL and Qwen3Omni after torch.compile changes (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #27705)</li>
<li>[KV cache] Fix lmcache connector (@Shaoting-Feng #27681)</li>
<li>[CI/Build][Bugfix]Fix Quantized Models Test on AMD (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27712)</li>
<li>[Bugfix] Fix non-contiguous tensor error in <code>rocm_unquantized_gemm_impl</code> (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27605)</li>
<li>[Speculators] Move tests + fix integration (@dsikka #27308)</li>
<li>[CI/Build] Move pre-commit only scripts to <code>tools/pre_commit</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27657)</li>
<li>[perf] Enable concurrent execution of "shared_experts" and "selected_experts" in qwen3-next (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27578)</li>
<li>[Bugfix] Fix modular kernel tests (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> #27707)</li>
<li>[Frontend] [gpt-oss] Tool json call parsing error retry (@alecsolder #27675)</li>
<li>[Frontend] [gpt-oss] Mcp type bug (@alecsolder #27689)</li>
<li>[Fix] import get_kv_cache_torch_dtype error in vllm_v1_adapter.py (@KevinCheung2259 #27670)</li>
<li>[Misc] Raise error for missing video metadata in <code>MultiModalDataParser</code> (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27664)</li>
<li>Feature/video support in random mm dataset (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> #25963)</li>
<li>[chore] Remove models weight on S3 logic (@khluu #27725)</li>
<li>[VLM] Add Qwen3-VL generation test (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #25185)</li>
<li>[CI/Build] Skip cpu offloading test on AMD (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27690)</li>
<li>[Frontend] Add <code>vllm bench sweep</code> to CLI (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27639)</li>
<li>Fix MiniMax-M2 rmsnorm precision and remove useless code (@rogeryoungh #27627)</li>
<li>[ROCm][Platform] Add MI308X device id in _ROCM_DEVICE_ID_NAME_MAP (@sammysun0711 #27623)</li>
<li>[CI] Fix flaky <code>test_two_responses_with_same_prev_id</code> test  (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27745)</li>
<li>[Chore] Optimize P2PNCCLEngine <code>http_address</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27488)</li>
<li>[Core] Exposing engine sleep &amp; wake_up state as prometheus metrics (@dumb0002 #24176)</li>
<li>[FIXBUG] Qwen3VL hallucinations without Contiguous on Torch.SDPA (@JartX #27744)</li>
<li><code>use_aot_compile</code> should respect <code>VLLM_DISABLE_COMPILE_CACHE</code> (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27698)</li>
<li>[CI/Build] Test torchrun with 8 cards (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27548)</li>
<li>[Bug] Raise error explicitly if using incompatible backend (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27424)</li>
<li>[KVConnector] Add metrics to Prometheus-Grafana dashboard (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #26811)</li>
<li>[Bug] Fix DeepEP low latency <code>assert self.batched_router_logits.size(-1) == full_router_logits.size(-1)</code> Bug (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27682)</li>
<li>[BugFix] Fix handling of resumed reqs in <code>SharedStorageConnector</code> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27719)</li>
<li>[Bug] Fix DBO IMA issue for DeepEPHT (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27666)</li>
<li>[Temp fix] Disable torch.compile for Qwen2.5 VL's VisionBlock temporarily.  (<a class="user-mention notranslate" href="https://github.com/huachenheli">@huachenheli</a> #27760)</li>
<li>[XPU][bugfix] fix rope for llama4 and deepseek (@yma11 #25145)</li>
<li>[Bugfix] mamba-block-size is set for vision language model (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #27773)</li>
<li>[XPU] Update latest IPEX 2.8 release (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #27735)</li>
<li>[BugFix] Handle unscheduled requests properly when async scheduling (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27756)</li>
<li>[Feat] Adds runai distributed streamer (@bbartels #27230)</li>
<li>kernels/moe test pruning (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> #27053)</li>
<li>[BugFix] Reordering extend logic fix (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27739)</li>
<li>[Benchmark] Cleanup deprecated nightly benchmark and adjust the docstring for performance benchmark (@KuntaiDu #25786)</li>
<li>Add more dims for batch invariant shims (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #27489)</li>
<li>use stringData in secret yaml to store huggingface token (@yitingdc #25685)</li>
<li>[CI/Build]Add eval config for Qwen3-235B-A22B-Instruct-2507-FP8 (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27113)</li>
<li>[BugFix][VL] Fix FA selection on Qwen2.5-VL (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27790)</li>
<li>[V0 deprecation] Remove VLLM_USE_V1 usage in config module (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27784)</li>
<li>[CI Failure] fix test_default_mm_loras (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27795)</li>
<li>[CI] Fix mypy for <code>vllm/v1/core</code> and <code>vllm/v1/engine</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27108)</li>
<li>[Bugfix] Improve GPU validation logging in Ray fallback scenarios (@sairampillai #25775)</li>
<li>[Frontend][Doc][5/N] Improve all pooling task | Polish encode (pooling) api &amp; Document. (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #25524)</li>
<li>[CI Failure] Fix test_kv_cache_model_load_and_run (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27717)</li>
<li>[Model] Introduce Kimi Linear to vLLM (<a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> #27809)</li>
<li>[KV offload] Enable CPU KV offload on CUDA alike Platforms (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27770)</li>
<li>[Model][Ouro] Support Ouro Model (@FlamingoPg #27794)</li>
<li>[Bugfix][CPU] Fix MRoPE dispatch on the CPU backend (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27800)</li>
<li>[BugFix] Stopgap - Flashinfer Autotuner + GPT-OSS + DP/TP (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27762)</li>
<li>[Misc] Replace CUDA_VISIBLE_DEVICES in DP with torch.cuda.set_device for device selection on cuda-like devices (@ilmarkov #27564)</li>
<li>[Docs] add Shanghai Meetup - 2025/10 (@kebe7jun #27545)</li>
<li>Reapply "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #27768)</li>
<li>[MTP] Refactor mtp predictor to avoid d2h operation (@MengqingCao #27643)</li>
<li>[Model] Use the same fused_moe configs for all H200 devices (@bufferoverflow #23642)</li>
<li>[Bugfix] Fix 2 precommit issues - (mamba_block_size, kv_cache_config) (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #27811)</li>
<li>[Core][Bookkeeping] Update cu_num_accepted_tokens for all req_index (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27629)</li>
<li>[EP/DP][API Server] Enable DP-aware routing in OpenAI API requests (@Prowindy #24945)</li>
<li>[Fix] Skip <code>record_sleep_state</code> logic in <code>PrometheusStatsLogger</code> if not in dev mode (@SumanthRH #27789)</li>
<li>[Refactor] Remove <code>VLLM_DEEPEP_LOW_LATENCY_ALLOW_NVLINK</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27750)</li>
<li>[Core][Perf] Only invoke save_new_computed_blocks when computed blocks are not empty (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27799)</li>
<li>[Feature] Batch invariant torch.compile (@PaulZhang12 #27660)</li>
<li>[BugFix] Fix broken import in initialize_ray_cluster() (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27838)</li>
<li>[Misc] Make all tool scripts executable (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27831)</li>
<li>[CI/Build][Intel] Enable performance benchmarks for Intel Gaudi 3 (@jakub-sochacki #26919)</li>
<li>[CI Test] Add Scheduled Integration Test (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27765)</li>
<li>[benchmark] Make request IDs unique across clients by default (@eicherseiji #27723)</li>
<li>[Hardware][Powerpc] Fix VLLM_CPU_OMP_THREADS_BIND="auto"  low CPU utilization for Power (@Akashcodes732 #27734)</li>
<li>[Kimi-Linear] Correct prefixes and add compatibility to AWQ quants (@toncao #27834)</li>
<li>[Bugfix] Avoid too small block m/n for FlexAttention kernel option (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27853)</li>
<li>[BugFix] Don‚Äôt compute reorder threshold when there are no attention groups (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27861)</li>
<li>[Perf] Decouple torch op from GDA to leverage torch.compile (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #27871)</li>
<li>[CI/Build] Add gpt-oss LoRA test (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27870)</li>
<li>[Bugfix] Allow 64-bit integer values for LoRA IDs to avoid overflow/truncation (@shadeMe #27876)</li>
<li>[Bugfix] Fix broken MRoPE for GLM-4.1V/GLM-4.5V (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27860)</li>
<li>[Bugfix] Missing NIXL metadata for handshake initialization if instance spans multi-node (@GuanLuo #26338)</li>
<li>Docs update tpu install instructions (@RobMulla #27824)</li>
<li>[bugfix] Missing cached item in beam search (@fake0fan #27874)</li>
<li>fix incorrect type annotation in KimiMLP (@skyloevil #27885)</li>
<li>Flashinfer_CUTLASS_MOE fuses quantization for TP (@wenscarl #27223)</li>
<li>[Cleanup] Remove no-longer-used <code>SpeculativeConfig.enable_chunked_prefill</code> (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27826)</li>
<li>[Feature] Pydantic validation for scheduler.py and structured_outputs.py (<a class="user-mention notranslate" href="https://github.com/vrdn-23">@vrdn-23</a> #26519)</li>
<li>Add FLASHINFER_MLA to test_mla_backends and add B200 CI run (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27663)</li>
<li>Batch invariance doc (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #27839)</li>
<li>[Hybrid] A simpler algorithm to find kernel_block_size (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #26476)</li>
<li>[Core] Async scheduling + structured outputs compatibility (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #26866)</li>
<li>[Kernel] Enable FusedMoEModularKernel  support  bias (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27754)</li>
<li>[Bugfix] Fix KDA output (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27905)</li>
<li>[Multimodal][XPU]Enable vision attn backend for xpu platform (@yma11 #27525)</li>
<li>Adding SplitK in fused_moe_lora kernel (@yugong333 #27818)</li>
<li>[CI/Build] Bump transformers version (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27528)</li>
<li>[Bugfix] [Model] Missing MRoPE function definition from <code>KeyeForConditionalGeneration</code> (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #27895)</li>
<li>[Add] cmdline argument parsing for KV cache offloading modules (@ApostaC #27621)</li>
<li>feat(benchmarks): support HF model names in multi-turn benchmark (@ai-jz #27850)</li>
<li>[Docs] Mock all imports for docs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27873)</li>
<li>[V0 deprecation] Remove VLLM_USE_V1 usage in platform and v1 module (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27798)</li>
<li>[Bugfix] DeepSeek V3.2 MTP metadata &amp; CUDA graph issues (@xiaohajiayou #26779)</li>
<li>[Bugfix] Python 3.10 compatibility for <code>Self</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27918)</li>
<li>[Core][TPU] Support TPU Data Parallalism (@wenxindongwork #27365)</li>
<li>[BugFix] Fix mixed penalties batch with async scheduling (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27910)</li>
<li>Adds anthropic /v1/messages endpoint to openai api_server (@bbartels #27882)</li>
<li>[KV offload] Offloading connector async scheduling support (@KevinCheung2259 #27648)</li>
<li>[CI/Build] Fix flaky test_transcription_validation.py::test_basic_audio_gemma (<a class="user-mention notranslate" href="https://github.com/bbrowning">@bbrowning</a> #27924)</li>
<li>[Bugfix] Fix Qwen Omni audio inference (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27920)</li>
<li>Performance fix MistralTokenizer: cache special ids and tokens (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #27925)</li>
<li>[V1] [Hybrid] Mamba1 Automatic Prefix Caching (<a class="user-mention notranslate" href="https://github.com/Josephasafg">@Josephasafg</a> #26377)</li>
<li>[Misc] Provide Siglip2 chat template (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #27939)</li>
<li>[Bugfix][llm]: Abort orphaned requests when llm.chat() batch fails (@Flink-ddd #27420)</li>
<li>[BugFix][LoRA] use adapter_id instead of id field of lora_request (@biswapanda #27728)</li>
<li>[Frontend] Align finish_reason when tool is called with OpenAI (@n0gu-furiosa #25054)</li>
<li>[Hybrid] Pass kernel block size to builders (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> #27753)</li>
<li>[Bugfix] Padded Eagle Specdec with Chunked Prefill (@Flechman #26263)</li>
<li>[XPU]Refine Dockerfile.xpu, avoid oneccl dependency issue (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #27964)</li>
<li>Add ORCA endpoint load metrics support (@efimki #24905)</li>
<li>[CI/Build] Remove the flaky gpt-oss lora test (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27966)</li>
<li>[Model] Add PaddleOCR-VL Model Support  (@zhang-prog #27758)</li>
<li>Early exit for MoE LoRA kernels (@gnovack #27131)</li>
<li>[Bugfix] Skip gs:// model paths for speculator detection (<a class="user-mention notranslate" href="https://github.com/pwschuurman">@pwschuurman</a> #27846)</li>
<li>[BUG] Make 'binary' default option for saving torch compile artifacts when using standalone_compile (<a class="user-mention notranslate" href="https://github.com/ahao-anyscale">@ahao-anyscale</a> #27616)</li>
<li>[CI/Testing] Add basic single node dual batch overlap test (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #27235)</li>
<li>[Spec Decode] Integrate Suffix Decoding from Arctic Inference (@aurickq #25784)</li>
<li>[Feature][Benchmarks] Support <code>inf</code> burstiness (@sducouedic #26941)</li>
<li>[Bugfix][Qwen][Multimodal] Move Qwen2_5_vl sdpa to custom op and reenable compile (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> #27764)</li>
<li>[Bugfix] change FlashMLA reorder_batch_threshold (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27777)</li>
<li>[Docs] add runai_streamer_sharded to LoadConfig (@andyxning #27937)</li>
<li>Add TP parameter to attention tests (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27683)</li>
<li>[Bugfix][plugin] fla crash on plugin (<a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> #27322)</li>
<li>[Bugfix] Fix MoE Routing Simulation (<a class="user-mention notranslate" href="https://github.com/tlrmchlsmth">@tlrmchlsmth</a> #28002)</li>
<li>Remove the tpu docker image nightly build. (@QiliangCui #27997)</li>
<li>[Bugfix][ROCm] Fix ViT rotary embeddings for torch.compile compatibility on ROCm (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #27748)</li>
<li>[LoRA] Lora shrink swizzle (@li2haipeng #27694)</li>
<li>[Refactor] Lazy import tool_parser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27974)</li>
<li>[NIXL][XPU] Pin NIXL version to 0.7.0 (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> #27849)</li>
<li>[Metrics] Enable sleep state metric outside of dev mode (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #27867)</li>
<li>[Bug] Batch invariant: Fix flash attn MLA <code>RuntimeError: scheduler_metadata must have shape (metadata_size)</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27884)</li>
<li>[CPU]Improve dynamic 4bit moe performance (@xiangze-arm #27240)</li>
<li>[CI/Build] Update LM Eval Version in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27944)</li>
<li>[KV Connector] Make KVCacheConfig an explicit constructor argument (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #27887)</li>
<li>[Model] fix ernie45 reasoning_parser (<a class="user-mention notranslate" href="https://github.com/CSWYF3634076">@CSWYF3634076</a> #27973)</li>
<li>[CI/Build] Fix OpenAI API correctness on AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28022)</li>
<li>[BugFix][Performance] Restore flashinfer autotuning for all scenarios (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27904)</li>
<li>Load tuned fused_moe_lora shrink and expand kernel configs separately (@yugong333 #27435)</li>
<li>Support using Int4PreshuffledTensor after loading (<a class="user-mention notranslate" href="https://github.com/jerryzh168">@jerryzh168</a> #26066)</li>
<li>[Core] Enable StatLogger in LLMEngine (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28020)</li>
<li>[Model][Bugfix] fix pipeline parallelism support for NemotronH (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> #27968)</li>
<li>[Model] add optimal triton fused moe configs for NemotronH MoE (<a class="user-mention notranslate" href="https://github.com/tomeras91">@tomeras91</a> #27967)</li>
<li>[Kernels] Isolate modular kernel code from FusedMoEMethodBase subclasses. (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> #27123)</li>
<li>[BugFix] Fix incorrect preallocated sampled_token_ids tensor size (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28025)</li>
<li>[Perf] SM100 - add swap AB optimization to CUTLASS FP8 GEMM (@LyrisZhong #27284)</li>
<li>[PERF] Decouple projections from GDN custom op (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #27512)</li>
<li>[model] Add support for openPangu_Ultra_MoE (@yt0428 #27521)</li>
<li>[PerfFix] Avoid separate thread for MP executor shm spin (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28012)</li>
<li>[AsyncScheduling] Don't schedule past request max_tokens (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #27922)</li>
<li>Remove deprecated <code>--rope-scaling</code> and <code>--rope-theta</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28006)</li>
<li>[ROCm][Perf] New design on ROCm AITER MHA backend Implementation (@ganyi1996ppo #25763)</li>
<li>Added disable rule to track files under benchmarks/lib (@nadavkluger #28048)</li>
<li>[Multimodal] Make MediaConnector extensible. (<a class="user-mention notranslate" href="https://github.com/huachenheli">@huachenheli</a> #27759)</li>
<li>[ROCm] gemm_a16w16 upstreaming (@maleksan85 #26969)</li>
<li>Revert "[PERF] Decouple projections from GDN custom op" (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28080)</li>
<li>[Qwen3-Next] MOE configs for A100-SXM4-80GB TP4 TP8 (@toulzx #27740)</li>
<li>[XPU] Add gpt-oss model support for Intel GPU (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #27786)</li>
<li>[CI/Build] Enable some fixed tests in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28078)</li>
<li>[V0 deprecation] Remove VLLM_USE_V1 usage in most modules (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27955)</li>
<li>[Bugfix] Fix encoder-only model support for transformers backend (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28021)</li>
<li>[BugFix] Fix DCP Assert (AssertionError: DCP not support reorder_batch_threshold &gt; 1 now.) (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28100)</li>
<li>[Model, Core] Support Granite Speech &amp; LoRA for STT (@alex-jw-brooks #24455)</li>
<li>[Refactor] Lazy-loaded reasoning_parser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28092)</li>
<li>[Refactor] to simplify and extract the shared logic between chat completion and responses (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #27961)</li>
<li>[bugfix] fix wrong <code>dcp_local_seq_lens</code> calc (@pisceskkk #27518)</li>
<li>[Hybrid allocator + kv connector] revert connector test changes related to hybrid allocator (@KuntaiDu #28011)</li>
<li>[Misc] fix import error for DeepSeekR1ReasoningParser (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28114)</li>
<li>Fix excessive logging noise by reducing the log level of the MinimaxM2ToolParser import success message (@minatoaquaMK2 #27635)</li>
<li>Bugfix: Cutlass FP8 FusedMoE bad scaling factors (@amirkl94 #27255)</li>
<li>[Graph Partition][Cache] Use inductor partition ops config (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #27702)</li>
<li>[XPU] Enable custom routing functions in IPEX for Llama4 (@frost-intel #28004)</li>
<li>add kimi reasoning parser (@MoyanZitto #28128)</li>
<li>[DCP] check return_lse for all layers in dcp (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #27929)</li>
<li>[BugFix] Support EP/DP + EPLB with MTP (@ilmarkov #25311)</li>
<li>Enabling cooperative multi-gpu tests on multi-gpu nodes (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #27986)</li>
<li>[ROCm][MLA] Support block-size &gt; 1 for AITER MLA backend  (@ganyi1996ppo #27224)</li>
<li>[Bugfix] Validate custom logits processor xargs for online serving (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #27560)</li>
<li>[misc] add vLLM Beijing Meetup (@jjzhang #28127)</li>
<li>[Kernel] Fuse computation of g and beta for Gated Delta Net (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28095)</li>
<li>[Core] add support for reasoning parser plugins (@walterbm #28075)</li>
<li>[Bugfix] vLLM should check Inductor config for compile cache enablement status (@gmagogsfm #27637)</li>
<li>[FlashInfer] Avoid FlashInfer block_size 16 + head_size 256 on blackwell (<a class="user-mention notranslate" href="https://github.com/heheda12345">@heheda12345</a> #27994)</li>
<li>[CI]: Add LMCache Unit Tests (@sammshen #27852)</li>
<li>[Feature] Extend batch invariant torch.compile to B200 (@PaulZhang12 #27856)</li>
<li>[Bugfix] Fix Qwen3-Reranker-8B load (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #28117)</li>
<li>[Docs] Clean up README_TUNING.md (<a class="user-mention notranslate" href="https://github.com/windsonsea">@windsonsea</a> #28088)</li>
<li>[Hardware][IBM Z] Optimize s390x Dockerfile (@R3hankhan123 #28023)</li>
<li>[Chore] Remove Nemotron-Nano-VL config copy (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28126)</li>
<li>[Docs] Add guide to debugging vLLM-torch.compile integration (@zou3519 #28094)</li>
<li>[Feature]: Add corrupted request metric to V1 metrics system. (<a class="user-mention notranslate" href="https://github.com/atalhens">@atalhens</a> #27306)</li>
<li>[CI/Build] Update checking logic in cutlass_group_gemm_supported  (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27948)</li>
<li>[CI/Build] Fix <code>test_defaults_with_usage_context</code> in AMD CI (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #27926)</li>
<li>[Core][Hybrid allocator + connector 2/n] Unify <code>remove_skipped_blocks</code> by <code>get_last_useful_token</code> (@KuntaiDu #25431)</li>
<li>[Debugging] Add annotation for easier trace analysis (@dayeol #22496)</li>
<li>[PERF] Decouple projections from GDN custom op. Attempt 2 (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28083)</li>
<li>[Bug] Fix cpu disable shared_experts <code>VLLM_DISABLE_SHARED_EXPERTS_STREAM</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28157)</li>
<li>[Bug] Fix env string <code>"0"</code> same to <code>True</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28159)</li>
<li>[Feature] Enable TP + EP <code>shared_experts</code> overlap with router, 3.7% E2E performance improvement (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28164)</li>
<li>[CI Failure] <code>nm-testing/Qwen2-0.5B-Instruct-FP8-SkipQKV</code> was removed from HF. Skip it in tests (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28170)</li>
<li>[Misc] Remove the duplicate code (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28111)</li>
<li>[Chore] Clean up deepseek v2/v3 config copy (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28055)</li>
<li>[Core][MM] Use non-blocking CPU-GPU copy of multimodal data (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28141)</li>
<li>Make the cv2 dependency optional (@cmpute #27780)</li>
<li>[CI] Add compile/test_multimodal_compile.py to CI (@gmagogsfm #28151)</li>
<li>[flashinfer] fix FI all2all with FI cutlass moe (@mxz297 #28166)</li>
<li>Patch Mistral Tokenizer (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #28146)</li>
<li>Fix hard-coded parameter name in gemma3n.py (@seungduk-yanolja #27946)</li>
<li>[CPU] Enable torch profiling (@aditew01 #28130)</li>
<li>[V0 deprecation]clean up is_v1_supported_oracle (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28116)</li>
<li>[Bugfix][Kernel] fix merge attn states when both prefix and suffix are empty (@courage17340 #28181)</li>
<li>[Frontend] OpenAI Responses API supports Tool/Function calling - non-harmony  (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #26874)</li>
<li>[CPU]Improve cpu fused moe perf (@xiangze-arm #27244)</li>
<li>Disable nm-testing models with issues in CI (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28206)</li>
<li>[Docs] Switch to directory style URLs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28058)</li>
<li>[Kernel][Model] Tune fused_moe Triton configs for MiniMax-M2 on H100 (@minatoaquaMK2 #28200)</li>
<li>[Doc] Add Arm CPUs are on the list of supported targets in vLLM (@milpuz01 #26018)</li>
<li>[HARDWARE][CPU] Add Option for Disabling Binding to Specific CPU Cores (@StanHatko #27953)</li>
<li>[Frontend] Fix logging format when enable response logging (@esmeetu #28049)</li>
<li>CODEOWNERS: Add myself as reviewer on security docs (<a class="user-mention notranslate" href="https://github.com/russellb">@russellb</a> #28216)</li>
<li>[Structured outputs] Upgrade llguidance to 1.3.0 (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #28039)</li>
<li>Add llama 4 scaling support (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #28145)</li>
<li>[Chore] eliminate duplicated and unconditional object serialization in anthropic messages api (@vicoooo26 #27792)</li>
<li>[ROCm] triton fp8 kernel (@maleksan85 #27058)</li>
<li>[Doc]: Make extraInit containers fully configurable in helm chart (@HanFa #27497)</li>
<li>[Test] Add non-MoE DP test coverage (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28235)</li>
<li>[BugFix] Fix FusedMoELoRA + ModularKernel Integration (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28237)</li>
<li>Fix failing test for CRadio (<a class="user-mention notranslate" href="https://github.com/BloodAxe">@BloodAxe</a> #27738)</li>
<li>Speed up mm processor kwargs per request by spliting dynamic and static kwargs (@LJH-LBJ #26483)</li>
<li>[Multimodal][torch.compile] Add compilation config field for turning off ViT/MM compile (<a class="user-mention notranslate" href="https://github.com/Lucaskabela">@Lucaskabela</a> #28242)</li>
<li>[CI/Build] Loosen STT LoRA Translate Check (Flaky Test) (@alex-jw-brooks #28247)</li>
<li>Add runai model streamer e2e test for GCS (@amacaskill #28079)</li>
<li>Fix issues from #28242 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28257)</li>
<li>[amd][gptoss] Perf gain because of block alignment (@smitkadvani #28024)</li>
<li>[Bug] Fix missing token_ids for reasoning parser models in chat completions   #28246 (<a class="user-mention notranslate" href="https://github.com/baonudesifeizhai">@baonudesifeizhai</a> #28256)</li>
<li>[CI] Reduce Blackwell Fusion test runtime by filtering tests and only run all tests in nightly (@Copilot #28074)</li>
<li>[Kernel] LoRA triton kernels support PDL (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #27402)</li>
<li>[Perf] Introduce FlattenLogprobs to store logprobs results to reduce GC overhead (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #28171)</li>
<li>[FixBug]Aeala/ShareGPT_Vicuna_unfiltered marked as multimodal benchmark (@princepride #28265)</li>
<li>[CPU]Avoid repeated random sample compile (@xiangze-arm #28260)</li>
<li>[Misc][Model][Refactor] Pass the prefix into Linear layers (@MengqingCao #28259)</li>
<li>[fix] Revert "fixing mm placeholder replacement issue with gemma3" (@khluu #28285)</li>
<li>[Core][MM] Add mechanism to configure multimodal fields which should stay on CPU (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28168)</li>
<li>[Bugfix] Use latency MOE backend as default for Flashinfer and other misc fixes (<a class="user-mention notranslate" href="https://github.com/pavanimajety">@pavanimajety</a> #27439)</li>
<li>[CLI] add --max-tokens to <code>vllm complete</code> (@Iceber #28109)</li>
<li>[Feature] Default <code>ignore_eos</code> True for <code>random</code> dataset (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28227)</li>
<li>[Log] update shm wait time msg (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #28255)</li>
<li>Revert "[PerfFix] Avoid separate thread for MP executor shm spin (#28012)" (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28289)</li>
<li>[README] Add Arm CPUs to the list of supported targets (<a class="user-mention notranslate" href="https://github.com/fadara01">@fadara01</a> #28290)</li>
<li>[doc] add guide about the provided PTX was compiled with an unsupported toolchain (<a class="user-mention notranslate" href="https://github.com/youkaichao">@youkaichao</a> #28305)</li>
<li>[Build] Fix release pipeline failing annotation (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> #28272)</li>
<li>[Bugfix] Fix and add tests for GptOss reasoning parser (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #28000)</li>
<li>[Core] Rework handling of async scheduling config (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28250)</li>
<li>[PerfFix] Avoid separate thread for MP executor shm spin (take 2) (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28319)</li>
<li>Update Flashinfer from <code>v0.4.1</code> to <code>v0.5.2</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27952)</li>
<li>[XPU] Enable Expert parallel for MoE models (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #28263)</li>
<li>remove resolve_op_overloads and use splitting_ops directly (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #28081)</li>
<li>[Bugfix][LoRA][Spec Decode] Support LoRA with speculative decoding (@xiaohongchen1991 #21068)</li>
<li>Update gpu.rocm.inc.md to add support for AMD Ryzen AI MAX / AI 300 Series (gfx1151, gfx1150) (@hammmmy #28308)</li>
<li>[Perf][DeepSeek] Add sigmoid+bias fusion to fused_grouped_topk from TRTLLM (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28124)</li>
<li>Bump arctic-inference requirement (@aurickq #28174)</li>
<li>[bugfix] support eagle with lora cudagraph specialization (@gnovack #28318)</li>
<li>[Model] Consolidate Deepseek-MoE implementation with DeepSeek-v2 (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28101)</li>
<li>Refactor CPU/GPU extension targets for CMake build (@ashahba #28026)</li>
<li>[flashinfer][fix] do not check nvcc availability when using pre-downloaded cubins (@mxz297 #27990)</li>
<li>[Attention] Remove max cudagraph size limit of 992 (<a class="user-mention notranslate" href="https://github.com/22quinn">@22quinn</a> #27840)</li>
<li><code>reasoning_content</code> -&gt; <code>reasoning</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27752)</li>
<li>[Bugfix] Update device name for H200 detection (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> #28349)</li>
<li>[Bugfix] Spec decode + structured output + spec model max len edge case (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #28298)</li>
<li>[DCP] Support dcp kv_cache interleave size &gt; 1 (@zhangsicheng5 #26696)</li>
<li>Enhance run_cluster.sh for multi-NIC support (@evberrypi #28328)</li>
<li>[Feat] Drop-in Torch CUDA Profiler (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #27841)</li>
<li>Remove setuptools upper bound constraint (&lt;80) (@ColeMurray #28337)</li>
<li>[Bugfix] Fix test fused quant layernorm tests (<a class="user-mention notranslate" href="https://github.com/ElizaWszola">@ElizaWszola</a> #27865)</li>
<li>[Performance][gpt-oss] Revert gpt-oss max cudagraph size to 1024 (@mmangkad #28345)</li>
<li>[chore] Move some wikimedia images to S3 (@khluu #28351)</li>
<li>fix: close issue 28338 by fixed python version (<a class="user-mention notranslate" href="https://github.com/yihong0618">@yihong0618</a> #28339)</li>
<li>[Misc] fix typo and add detailed log (@andyxning #28178)</li>
<li>[ROCm] Add env to enable/disable aiter triton gemm (@sarckk #28321)</li>
<li>[Misc] Add some comments in qwen3-next (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28267)</li>
<li>[CI] Fix flaky <code>test_eagle_correctness</code> test (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28364)</li>
<li>[Core] Simplify async KV output aggregation (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28327)</li>
<li>[Core] Separate out attention metadata building logic from prepare inputs (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #26764)</li>
<li>[BugFix] Fix cu_num_generated_tokens slicing logic in LogprobsLists.slice() method (@usberkeley #28214)</li>
<li>[CI/Build] Temporary fix to LM Eval Small Models (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28324)</li>
<li>[Kernel] Fix fused_gdn_gating (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28343)</li>
<li>[ROCm][Platform] Add RX7900XTX device id in _ROCM_DEVICE_ID_NAME_MAP (@JartX #28279)</li>
<li>[CI] lora/test_mixtral.py : Add additional expected outputs due to flakiness (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28322)</li>
<li>[Hardware][AMD][Model] Add Triton MoE tuning support and optimized configs for Qwen3 omni for MI308X (@sammysun0711 #28373)</li>
<li>[V0 deprecation] Remove no longer used <code>get_metadata_cls</code> (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28370)</li>
<li>Restore PlaMo2 unit test as <code>pfnet/plamo-2-1b</code> now supports <code>transformers &gt;=4.56</code> (@Alnusjaponica #28019)</li>
<li>[Metrics] Refactor LoRA state tracking (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #26801)</li>
<li>[bugfix] fix siglip batch text output error (@piood #28365)</li>
<li>[Fix] optimize visual token mask with caching and multi-token support (@bo-ke #28374)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> to codeowner for ROCm and multi-modal (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #28360)</li>
<li>[Rocm][fused_moe][fp4] view weight to torch.float4_e2m1fn_x2 when running aiter fused moe for fp4 model (@zejunchen-zejun #27474)</li>
<li>[Kernel] Optimization of the mm_k operator. (@caozuoba #28280)</li>
<li>[RFC][ROCm][AITER] Keep all AITER kernels in <code>_aiter_ops</code> class like <code>_custom_ops</code> and <code>_ipex_ops</code> (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #24490)</li>
<li>[V0 Deprecation] Remove unused <code>context_len</code> and <code>seq_len</code> from M-RoPE (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28395)</li>
<li>[Bugfix] Fix persistent_masked_m_silu_mul_quant tests (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28366)</li>
<li>[Performance] Support FP8 flashinfer TRTLLM MOE on Qwen3 and Qwen-3next (@jiahanc #27492)</li>
<li>[Bugfix] Fix llguidance backend, rollback when EOS was encountered (@Flechman #25905)</li>
<li>[FA/Chore] Bump FA version for FP8 two-level accumulation  (<a class="user-mention notranslate" href="https://github.com/jmkuebler">@jmkuebler</a> #27889)</li>
<li>[Bugfix][EPLB] Disabled shared expert overlap when EPLB is enabled (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28377)</li>
<li>[Misc] Add more scoping for improved trace (@frank-wei #28329)</li>
<li>[BugFix] Fix DeepGEMM over-allocating workspace (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28254)</li>
<li>[Frontend][2/n] remove empty content from _parse_tool_calls_from_content (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #28331)</li>
<li>[CI] Fix Plugin Tests Tests (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> #28413)</li>
<li>[ROCm] Add missing gemm_a8w8_blockscale import (@sarckk #28378)</li>
<li>[PERF] Allreduce fusion. Support torch native matching. Tuning of the thresholds (@ilmarkov #24248)</li>
<li>[Perf] Move gc.freeze logic from EngineCoreProc to EngineCore for better coverage (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #27896)</li>
<li>[Bugfix] Ensure calculated KV scales are applied in attention. (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> #27232)</li>
<li>[Test] Remove old non-varlen FA2 test (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28420)</li>
<li>[Feature] Refactor batch invariant fp8 DeepGEMM (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27606)</li>
<li>[CI/Test Fix] Fix CP tests on Blackwell (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28404)</li>
<li>[Feature] Add env var <code>VLLM_MOE_USE_DEEP_GEMM</code> (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28422)</li>
<li>Only register rocm_aiter_ops if aiter is found (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28428)</li>
<li>Fix rotary embedding benchmark script (@xyang16 #28323)</li>
<li>[Misc] FlattenLogprobs -&gt; FlatLogprobs (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28335)</li>
<li>[Frontend] Add sagemaker_standards dynamic lora adapter and stateful session management decorators to vLLM OpenAI API server (@zhaozuy #27892)</li>
<li>[Bugfix] Fix Stream Sync for Shared Expert Overlap (<a class="user-mention notranslate" href="https://github.com/robertgshaw2-redhat">@robertgshaw2-redhat</a> #28430)</li>
<li>[Doc] Sleep mode documentation  (@iAmir97 #28357)</li>
<li>[BugFix] Avoid calling KV connector layer APIs when metadata is unset (<a class="user-mention notranslate" href="https://github.com/sdavidbd">@sdavidbd</a> #28253)</li>
<li>[Bugfix] Fix max image size for PaddleOCR-VL (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28442)</li>
<li>[EPLB] Refactor balance_packing to use numpy and optimize GPU-CPU transfers in EPLB (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28369)</li>
<li>[Bugfix] fix qwen3-next crash (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28202)</li>
<li>[BugFix] 'DeepseekV2Config' object has no attribute 'use_mla'`  (@faaany #28387)</li>
<li>[Model][Qwen3VL] Slighly speedup <code>fast_pos_embed_interpolate</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28434)</li>
<li>Multi turn benchmark progress bar for synthetic conversation generation (@segevido #28394)</li>
<li>[CI] Add mergify rules for <code>nvidia</code> label (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28417)</li>
<li>[Attention] Refactor CUDA attention backend selection logic (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #24794)</li>
<li>Fix Fused MoE LoRA Triton kernel bug (@chaojun-zhang #28450)</li>
<li>[Model] Pass <code>mm_features</code> directly into <code>get_mrope_input_positions</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28399)</li>
<li>Add request timeout override for multi-turn benchmarks (@segevido #28386)</li>
<li>[Docs] Fix grammar in CPU installation guide (@maryamtahhan #28461)</li>
<li>[Kernels] Split up fused_moe/layer.py, isolate more modular kernel code (<a class="user-mention notranslate" href="https://github.com/bnellnm">@bnellnm</a> #28064)</li>
<li>[BugFix] Fix Failing Ruff Check (@jvlunteren #28469)</li>
<li>Add <a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> to CODEOWNERS for Observability (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #28457)</li>
<li>[BugFix] Fix RuntimeError in PixtralHFAttention on CPU/XPU (@faaany #28444)</li>
<li>[BugFix] Add test_outputs.py to CI pipeline (@usberkeley #28466)</li>
<li>[Doc] Fix typo in serving docs (@the-codeboy #28474)</li>
<li>Remove weight_scale.T special case for SM90 Block FP8 CUTLASS kernel (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28431)</li>
<li>[NIXL] Generalize block-first backend layouts (FlashInfer-like) (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28282)</li>
<li>[Kernel][Perf] fuse QK Norm and RoPE into one cuda kernel for Qwen Model (@izhuhaoran #27165)</li>
<li>[ROCm][Quantization] extend AMD Quark to support mixed-precision quantized model (@xuebwang-amd #24239)</li>
<li>[Quantization] fix attention quantization of gpt_oss model (@xuebwang-amd #27334)</li>
<li>[CI/Build] Refactor Attention backend for test_prefix_prefill from xformers to SDPA (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28424)</li>
<li>Prefer FlashAttention MLA as default over FlashMLA (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #27363)</li>
<li>[Kernel] Optimize rms_norm kernel (@xyang16 #27931)</li>
<li>[BugFix] Fix Siglip2Attention on XPU (@faaany #28448)</li>
<li>[Misc] Remove unused attention prefix prefill ops functions (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #26971)</li>
<li>[Perf] Use np.ndarray instead of list[list[int]] to reduce GC overhead (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #28245)</li>
<li>[V0 deprecation] Clean up num_prefill_tokens logic for V0 (@gcanlin #28203)</li>
<li>[Misc] fix typo in DCP comment (@Livinfly #28389)</li>
<li>[LoRA][1/N]Remove LoRA extra vocab (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #28382)</li>
<li>[TPU] Rename path to tpu platform (@kyuyeunk #28452)</li>
<li>[Misc] Cleanup Executor interface (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28441)</li>
<li>Add Zurich vLLM Meetup (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28488)</li>
<li>[Bugfix] Disable shared expert overlap if Marlin MoE is used (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28410)</li>
<li>[Feature] Allow configuring FlashInfer workspace size (@maxyanghu #28269)</li>
<li>Use FLASHINFER MLA backend when testing fp8_kv_scale_compile (<a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> #28491)</li>
<li>[BugFix] Graceful handling of torch symm mem errors. (@ilmarkov #27671)</li>
<li>[Frontend] Change CompilationMode to a proper Enum (@gmagogsfm #28165)</li>
<li>[Performance] Cache loaded custom logitsprocs to avoid overheads (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28462)</li>
<li>[[V0 deprecation]]Remove VLLM_USE_V1 env (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28204)</li>
<li>[CPU] Refactor CPU attention backend (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #27954)</li>
<li><code>VLLM_USE_TRITON_FLASH_ATTN</code> V0 variable deprecation (@AndreasKaratzas #27611)</li>
<li>[Model][Qwen3VL] Simplify <code>get_mrope_input_positions</code> using numpy (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28302)</li>
<li>[Core] Encoder separation for Encode-Prefill-Decode Disaggregation (@fake0fan #25233)</li>
<li>[BugFix] Add fallback path in <code>apply_rotary_pos_emb_flashattn</code> for non-cuda platforms (@faaany #28447)</li>
<li>[Benchmark] Add retry support to fix workload bias in multi-turn benchmark (@ai-jz #28493)</li>
<li>[Core] Cache <code>vllm_is_batch_invariant</code> (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28304)</li>
<li>[CI/Build] Fix crash due to removed VLLM_USE_V1 attribute in EPD (@fake0fan #28521)</li>
<li>[CI] Introduce autorun_on_main feature (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #27836)</li>
<li>[BugFix]: --enable-lora with model granite-4.0-micro crash (<a class="user-mention notranslate" href="https://github.com/yyzxw">@yyzxw</a> #27733)</li>
<li>[Model] fix glm4_moe_mtp load weights with GLM-4.6 checkpoint. (@wuyaoxuehun #27597)</li>
<li>[XPU]Fix crash due to removed VLLM_USE_V1 attribute (@chaojun-zhang #28520)</li>
<li>[KVConnector] Enable get_block_ids_with_load_errors() in LMCache connector  (@ziruiliu #27978)</li>
<li>add cpu option for p/d in nixl_connector (@ZhengHongming888 #28356)</li>
<li>[ROCm] [Bugfix] Fix <code>fused_qknorm_rope_kernel</code> rocm compatibility (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #28500)</li>
<li>[Bugfix] Fix gpt_oss packed_modules_mapping (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #28536)</li>
<li>[V0 deprecation] Deprecate use_v1 parameter (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #28112)</li>
<li>Fix pre-commit (and XPU) on <code>main</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28556)</li>
<li>[Performance][Hopper] Avoid M dim padding to 4x for most cases (due to cuda graphs paddings) (@alexm-redhat #28492)</li>
<li>[Refactor] Remove redundant TP gather/split in split_qkv in QwenVL (@gcanlin #28271)</li>
<li>[Misc] Refactor Attention kv transfer methods into decorator (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #27816)</li>
<li>Remove deprecated fields from <code>CompilationConfig</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27593)</li>
<li>[Perf] Refactor cudagraph_support to enable full CUDA graphs for spec decoding with FlashInfer (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #28479)</li>
<li>Implement ARC KV cache eviction policy (@albertoperdomo2 #27039)</li>
<li>[EPLB][ROCm]: support EPBL for ROCm backend (@PerryZhang01 #27731)</li>
<li>[Model] [Config] Correctly identify granite-4.0-micro as non-hybrid model (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> #28563)</li>
<li>[CI] Skip "Multi-Modal Models Test (Extended) 3" test that's broken in current Transformers (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28559)</li>
<li>[KV connector][WIP] KV cache proxy based on LMCache multi-process mode (@ApostaC #27902)</li>
<li>[BugFix] Priority scheduling and spec tokens preemption (<a class="user-mention notranslate" href="https://github.com/andylolu2">@andylolu2</a> #28558)</li>
<li>[Misc]Fix typo in llm_engine.py (@frank-wei #28584)</li>
<li>[Performance][B200] Fix deepgemm prologue (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #27897)</li>
<li>[ROCM] Fix ROCm warnings, environment flag access, and GEMM kernel naming for consistency in <code>_aiter_ops.py</code> (<a class="user-mention notranslate" href="https://github.com/vllmellm">@vllmellm</a> #28464)</li>
<li>[TPU] Support GCS path in VLLM_TORCH_PROFILER_DIR (@QiliangCui #28487)</li>
<li>[Bugfix] Adjust Marlin CUDA arch selection to 8.0+PTX;9.0+PTX (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28294)</li>
<li>[Core][AMD] Migrate fully transparent sleep mode to ROCm platform (@HollowMan6 #12695)</li>
<li>[MoE][Kernel][Perf] Improve Shared Expert Stream Overlap (@alexm-redhat #28406)</li>
<li>Skip models that cannot currently init on Transformers v5 (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28471)</li>
<li>[Docs] Update meetups.md description (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28583)</li>
<li>[ROCm][Bugfix] Revert removing setuptools version restriction (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #28592)</li>
<li>[platform] Move get_cu_count to utils (<a class="user-mention notranslate" href="https://github.com/wangxiyuan">@wangxiyuan</a> #27005)</li>
<li>[Bugfix] Fix SM100 gpt-oss regression due to faulty attn sink support (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28561)</li>
<li>[BugFix] Fix <code>mm_encoder_attn_backend</code> arg type checking (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28599)</li>
<li>[Docs] Add some details about what the MoE block needs for the Transformers backend (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28588)</li>
<li>Rename clashing method names for vLLM model protocol (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #27583)</li>
<li>[n-gen] DO NOT repeatedly return finished child requests (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #28591)</li>
<li>[Frontend] split append tool output (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #28333)</li>
<li>[Frontend][responsesAPI][1/n] convert responses API tool input to chat completions tool format (<a class="user-mention notranslate" href="https://github.com/qandrew">@qandrew</a> #28231)</li>
<li>[BugFix][ROCm] Fix <code>get_cu_count</code> missing variable error (@ganyi1996ppo #28608)</li>
<li>[XPU] Support Triton path for LoRA operations on XPU   (@faaany #28511)</li>
<li>Support DeepEP for Kimi-k2-thinking through enabling gemm selection for compressed-tensor marlin wna16 (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #28574)</li>
<li>[build][cmake]: Bundle static ACL and torch libgomp for CPU extension builds (@Radu2k #28059)</li>
<li>[ROCm][BugFix] Remove the usage of <code>device_info</code> from aiter (@ganyi1996ppo #28383)</li>
<li>[Bugfix] Prevent crash on empty grammar string (@tjandy98 #28210)</li>
<li>Use official xformers-0.0.33 built for PT 2.9 (<a class="user-mention notranslate" href="https://github.com/huydhn">@huydhn</a> #28600)</li>
<li>Add NUMA node validation for CPU thread binding (@usberkeley #28555)</li>
<li>[Bugfix] fix kimi-linear crash (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28445)</li>
<li>[Frontend] supports interleaved thinking (<a class="user-mention notranslate" href="https://github.com/chaunceyjiang">@chaunceyjiang</a> #28531)</li>
<li>Support all interleaved layer types (@sarckk #28485)</li>
<li>Fix: Correctly filter special tokens in benchmark_prefix_caching (@dw2761 #28615)</li>
<li>[BugFix] Fix type error when assign a trition kernel tensor to a torch.nn.Parameter (@liuzijing2014 #28603)</li>
<li>Fix io processor pooling  #28273 (<a class="user-mention notranslate" href="https://github.com/baonudesifeizhai">@baonudesifeizhai</a> #28484)</li>
<li>[XPU] add sym params to IPEXConfig (@zufangzhu #28611)</li>
<li>[Bugfix] Fix FPS value type for Qwen2.5-Omni video processing (@faaany #28630)</li>
<li>[Hardware][PowerPC] Fix fp16 compilation error for Power in cpu attention backend and bump oneDNN version (@Akashcodes732 #28535)</li>
<li>[ROCm][BugFix]Fix <code>get_cu_count</code> in rocm_aiter_fa.py (@ganyi1996ppo #28618)</li>
<li>[CI/Build] Install uv for AMD MI300: Language Models Tests (Hybrid) %N (@amdfaa #28142)</li>
<li>[CI Failure] Fix backend selection for encoder-only models (<a class="user-mention notranslate" href="https://github.com/hl475">@hl475</a> #28534)</li>
<li>[BugFix] DeepSeek-OCR: apply NoRepeatNGramLogitsProcessor to greedy path (@YuanpingSong #28617)</li>
<li>Fix <code>get_num_experts</code> when config sets it explicitly to <code>None</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28652)</li>
<li>[Misc] Turn off encoder torch compile by default (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28634)</li>
<li>Rewrite C++ meta funcs to Python (@janeyx99 #28595)</li>
<li>[BugFix] Ensure <code>EngineArgs.create_engine_config</code> is idempotent (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28515)</li>
<li>[TPU] patch TPU wheel build script to resolve metadata issue (<a class="user-mention notranslate" href="https://github.com/jcyang43">@jcyang43</a> #27279)</li>
<li>[Performance][B200] silu_mul_quant: pack scales in int32 (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28358)</li>
<li>[Bugfix] Fix validate model input for decoder models (<a class="user-mention notranslate" href="https://github.com/yannicks1">@yannicks1</a> #27099)</li>
<li>[Attention][Bugfix] Fix FA sink support (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28660)</li>
<li>[Perf] Support stream interval for reducing host overhead (<a class="user-mention notranslate" href="https://github.com/elvischenv">@elvischenv</a> #27869)</li>
<li>[bugfix] correct local_chunk_len for DCP in reorg_kvcache with long context (@pisceskkk #28526)</li>
<li>[Bugfix] Eliminate tuple inputs to submodules in graph partitioning (@gmagogsfm #28533)</li>
<li>[Bugfix] [CPU] bump torch to 2.9.0 for Darwin to fix segmentation fault (@kebe7jun #27791)</li>
<li>[Misc] Update CODEOWNERS for simon-mo and comaniac (<a class="user-mention notranslate" href="https://github.com/simon-mo">@simon-mo</a> #28675)</li>
<li>[CI] Bug: Fix ci entrypoint pooling (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28684)</li>
<li>[KV Connector] Test async mode in scheduler tests (<a class="user-mention notranslate" href="https://github.com/markmc">@markmc</a> #28550)</li>
<li>Mirrored test group definitions for AMD (2025-11-11) (<a class="user-mention notranslate" href="https://github.com/Alexei-V-Ivanov-AMD">@Alexei-V-Ivanov-AMD</a> #28573)</li>
<li>[quantization][config] enable override existing quant_config (<a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> #28510)</li>
<li>[ROCm] Bump up the version of amd-smi to 6.4.3 (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28680)</li>
<li>[CPU][Bugfix] Fix Apple Silicon M1 compilation failure (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28681)</li>
<li>[ci][amd] fix basic models extra init test (@bradleyhd #28676)</li>
<li>[Misc] Remove <code>warn_for_unimplemented_methods</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28613)</li>
<li>[XPU][CI]disable lm cache uts (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #28696)</li>
<li>[Misc] Update xformers to 0.33.0.post1 (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28678)</li>
<li>[Misc] add ignore mapper for quark quantization (@haoyangli-amd #28275)</li>
<li>[Bugfix][CI/Test][Spec Decode] Fix illegal memory access in offline_inference/spec_decode.py (Issue  27619) (@rasmith #28432)</li>
<li>[BugFix][CI/Build][ROCM] Fix import error and apply assert in appropriate case in test_struct_output_generate (@rasmith #28311)</li>
<li>use default CCL_ZE_IPC_EXCHANGE (@yma11 #28700)</li>
<li>[Bugfix] fix dots.ocr pp support (<a class="user-mention notranslate" href="https://github.com/ZJY0516">@ZJY0516</a> #28705)</li>
<li>[BugFix] Fix multi-modal async scheduling race condition (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28706)</li>
<li>Add output token counting to gsm8k eval (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28594)</li>
<li>[Minor] avoid register new custom and just import silly_attn (<a class="user-mention notranslate" href="https://github.com/BoyuanFeng">@BoyuanFeng</a> #28578)</li>
<li>[Misc] fix comment in test_envs (@xingliu14 #28529)</li>
<li>[feat]: log number of preempted requests (@610lyn #28522)</li>
<li>[Frontend] Added chat-style multimodal support to /classify. (@WorldExplored #27516)</li>
<li>[Model][MM] Extract conv layer as CustomOp (@shen-shanshan #28455)</li>
<li>[DCP] Support Decode Context Parallel (DCP) for GQA with Flashinfer (@gjc0824 #25438)</li>
<li>Fix KV sharing fast prefill with cudagraph enabled (@sarckk #28537)</li>
<li>[BugFix] Fix FA3 IMA with FULL_AND_PIECEWISE and cascade attention (default) (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28702)</li>
<li>[Doc] Fix macOS installation dependency resolution issue (@shahfasal #26721)</li>
<li>[Model] Fix bailing_moe accuracy problem (@zhaozx-cn #28277)</li>
<li>[Bugfix][Nixl] Fix kernel physical&lt;&gt;logical block_size issue  (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #28677)</li>
<li>[Config] Clean up SchedulerConfig initialization (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28665)</li>
<li>[Kernels] Enable FlashInfer FP8 Blockscale on SM90 (for TEP DSR1) (@djmmoss #27134)</li>
<li>[Fix] improve aspect ratio in dummy image generation and add common  VLM tests for PaddleOCR-VL (@dongbo910220 #28711)</li>
<li>[Docs] Update the name of <code>Transformers backend</code> -&gt; <code>Transformers modeling backend</code> (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28725)</li>
<li>[CI][CPU] Smoke test for Apple Silicon using GHA MacOS runner (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28688)</li>
<li>[DisaggEverything] Tokens in&lt;&gt;out <code>/generate</code> endpoint (<a class="user-mention notranslate" href="https://github.com/NickLucche">@NickLucche</a> #24261)</li>
<li>[Attention] Bump FA for removed method (<a class="user-mention notranslate" href="https://github.com/MatthewBonanni">@MatthewBonanni</a> #28429)</li>
<li>Fix typo in comment: existance -&gt; existence (@OthmanMohammad #28737)</li>
<li>Remove audio optional dependency for mistral-common (<a class="user-mention notranslate" href="https://github.com/juliendenize">@juliendenize</a> #28722)</li>
<li>[kernel] Improve FP8 PTPC on Hopper for larger shapes (@czhu-cohere #28692)</li>
<li>docs(lora_resolvers): clarify multi-resolver order and storage path requirement (@wangchen615 #28153)</li>
<li>LLaMA4 LoRA Adapter Enablement (<a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> #28602)</li>
<li>[Bugfix] [ROCm] [AITER]: Fix aiter block quant not compatible with torch compile dynamo (<a class="user-mention notranslate" href="https://github.com/tjtanaa">@tjtanaa</a> #28716)</li>
<li>[Docs] Enable some more markdown lint rules for the docs (<a class="user-mention notranslate" href="https://github.com/hmellor">@hmellor</a> #28731)</li>
<li>[Chore] Rename <code>SchedulerConfig.chunked_prefill_enabled</code> (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28735)</li>
<li>[Bugfix] resolve Qwen3-VL GPTQModel quantized model loading failure (@GuanH #28663)</li>
<li>[BugFix] Fix misprint introduced by modular_kernel refactoring. (@halyavin #28728)</li>
<li>[ROCm][Bugfix] Fix compilation errors with fused_qknorm_rope_kernel.cu (<a class="user-mention notranslate" href="https://github.com/SageMoore">@SageMoore</a> #28682)</li>
<li>[CI] Fix macos smoke test uv cache issue (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28736)</li>
<li>[Bugfix] TypeError: 'NoneType' object is not callable (@mostrowskix #27410)</li>
<li>[ROCm][CI/Build] Change install location of uv (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #28741)</li>
<li>Avoid bytecode hook and simplify TorchCompileWrapperWithCustomDipatch (@laithsakka #25110)</li>
<li>[Bugfix] Fix incorrect use of hidden_states for shared_experts due to do_naive_dispatch_combine (@alexm-redhat #28740)</li>
<li>[Bugfix] Fix ChunkedLocalAttention CUDA Graph setting (<a class="user-mention notranslate" href="https://github.com/benchislett">@benchislett</a> #28739)</li>
<li>[Hybrid] [Kernel] Fix chunk scan kernel when BLOCK_SIZE_DSTATE &gt; 128 (<a class="user-mention notranslate" href="https://github.com/tdoublep">@tdoublep</a> #28295)</li>
<li>[Log] Save profiler results to file instead of stdout (@rasmith #28144)</li>
<li>[ROCm][CI/Build] Upgrade to ROCm 7.1 and AITER main (<a class="user-mention notranslate" href="https://github.com/gshtras">@gshtras</a> #28753)</li>
<li>[Test] Rework e2e async scheduling tests (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28744)</li>
<li>[Core] Performance: Use list[np.ndarray] instead of list[list[int]] for output tokens for GC optimization (<a class="user-mention notranslate" href="https://github.com/Jialin">@Jialin</a> #26368)</li>
<li>[TPU] Fix import error in tpu launch (@QiliangCui #28758)</li>
<li>[Model][Qwen3VL] Use <code>mm_position</code> to compute mrope positions (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28730)</li>
<li>[Bugfix] Build hadacore kernels on &gt;SM90 (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28748)</li>
<li>Revert "[Core] Performance: Use list[np.ndarray] instead of list[list‚Ä¶ (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28773)</li>
<li>Fix IntermediateTensors initialization and add type hints (@OthmanMohammad #28743)</li>
<li>[NIXL] heterogeneous block_size support (<a class="user-mention notranslate" href="https://github.com/xuechendi">@xuechendi</a> #26759)</li>
<li>[Performance][DeepGEMM] Estimate expected_m (<a class="user-mention notranslate" href="https://github.com/varun-sundar-rabindranath">@varun-sundar-rabindranath</a> #28694)</li>
<li>[Redo] #26368 (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28771)</li>
<li>[RL] [V1] Remove unused device argument from reset_kv_cache (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28766)</li>
<li>Use narrow over indexing in <code>hadacore_transform</code> to prep for ABI stable (@janeyx99 #28756)</li>
<li>[Kernel][Moe Configs] llama4 maverick fp8 moe config tp8 on mi325 (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28709)</li>
<li>[Misc] Make <code>SchedulerConfig.max_model_len</code> init-only (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28733)</li>
<li>[PERF] Remove TRTLLM Gen attn kernel limitation <code>max_seq_len &lt;=131072</code> (<a class="user-mention notranslate" href="https://github.com/vadiklyutiy">@vadiklyutiy</a> #28755)</li>
<li>[compile] Enable sequence parallelism matching w/o custom ops enabled  (<a class="user-mention notranslate" href="https://github.com/angelayi">@angelayi</a> #27126)</li>
<li>Allow Gemma3 to take image embeddings (@tingtingtangmeta #28483)</li>
<li>[Doc] Fix failing doc build (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28772)</li>
<li>[Model] Fix lmhead init bug of bailing_moe (@hwhaokun #28777)</li>
<li>Add support for Eagle with separate lm-head and embed_tokens layers (@eldarkurtic #28549)</li>
<li>[CI] Fix broken pipeline (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28781)</li>
<li>[Model][Qwen3VL] Cache positional embedding indices  (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28475)</li>
<li>[Doc]: fix typos in various files (@didier-durand #28567)</li>
<li>[BugFix] Fix <code>AssertionError: DCP not support reorder_batch_threshold &gt; 1 now.</code>  (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28751)</li>
<li>Adding a benchmark for batch invariance (<a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> #28161)</li>
<li>[Benchmark] Fix client seed synchronization in multi-turn benchmark (@ai-jz #28512)</li>
<li>[Model] Allow users to control skip reading cache per request. (<a class="user-mention notranslate" href="https://github.com/noooop">@noooop</a> #28194)</li>
<li>[V1] Support MP Executor for multi node distributed inference (<a class="user-mention notranslate" href="https://github.com/luccafong">@luccafong</a> #23691)</li>
<li>Fixed gpt-oss _load_weights_other() parameter position bug (@River12 #28715)</li>
<li>[Bugfix] Fix host and port join for ipv6 in bench serve (@scottzh8 #28679)</li>
<li>Fix gpt oss weight loading with EP + bf16 (@ashors1 #28765)</li>
<li>[Doc]: fix typos in various files (@didier-durand #28811)</li>
<li>fix comment typo (@andyxning #28802)</li>
<li>[Model][QwenVL] Optimize <code>Qwen2_5_VisionAttention</code> q,k preparation (<a class="user-mention notranslate" href="https://github.com/lgeiger">@lgeiger</a> #28769)</li>
<li>Feature: Support Relu2 in FusedMoE fp8 cutlass path (@amirkl94 #27261)</li>
<li>[BugFix] Fix async scheduling + chunked prefill + preemption (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28787)</li>
<li>[Performance][Fix] update nvfp4 code to support renorm routing (@jiahanc #28569)</li>
<li>[NIXL][XPU] update install script of NIXL (<a class="user-mention notranslate" href="https://github.com/zhenwei-intel">@zhenwei-intel</a> #28778)</li>
<li>[ROCm][Qwen3-32B] Fix AITER MHA accuracy issue cause by #25763 (@sammysun0711 #28670)</li>
<li>[Bugfix][Model] Prevent special token leakage in KimiK2ToolParser streaming mode (@jscaldwell55 #28543)</li>
<li>[Doc] Add llama4 LoRA tag (<a class="user-mention notranslate" href="https://github.com/jeejeelee">@jeejeelee</a> #28825)</li>
<li>[CPU][Bugfix] Fix _to_list in CPU model runner (<a class="user-mention notranslate" href="https://github.com/bigPYJ1151">@bigPYJ1151</a> #28824)</li>
<li>[BugFix] Fix glm4_moe_mtp load weights bug (@wuyaoxuehun #28805)</li>
<li>[Metrics] Fix KV cache usage percent metric multiproc (@jaywonchung #28792)</li>
<li>[XPU] work around for sp, avoid custom op import error (<a class="user-mention notranslate" href="https://github.com/jikunshang">@jikunshang</a> #28822)</li>
<li>[BugFix] Temporary fix for IMA with MTP = 2 and full-cg (<a class="user-mention notranslate" href="https://github.com/LucasWilkinson">@LucasWilkinson</a> #28315)</li>
<li>[Bugfix][Perf] Revert applying HF processor on text-only inputs for multimodal models  (<a class="user-mention notranslate" href="https://github.com/ywang96">@ywang96</a> #28858)</li>
<li>Cast return value to int64_t for cache size (@tiehexue #28814)</li>
<li>[Bugfix] Fix GPT-OSS on AMD after #28603 (<a class="user-mention notranslate" href="https://github.com/zhewenl">@zhewenl</a> #28816)</li>
<li>[Core] Async Scheduling X Spec Decoding Compatibility (@Ronald1995 #24799)</li>
<li>[BugFix] Fix PP performance and PP kv connector output regression  (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28768)</li>
<li>[Quantization] [Eagle] Add complete quantization support to the draft model in Eagle (@shreyas269 #28435)</li>
<li>[Test] Batch Invariant: Rename and organize tests (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #27421)</li>
<li>[Model] Add Afmoe architecture implementation (@pranav4501 #28332)</li>
<li>[BugFix] Corner case that could cause out-of-sync with external launcher mode and dp &gt;1 (@bangshengtang #28774)</li>
<li>[Misc] Fix wrong comment in scheduler (<a class="user-mention notranslate" href="https://github.com/zhuohan123">@zhuohan123</a> #28880)</li>
<li>[Bugfix] Fix Kimi-K2 tool parser concatenated tool calls parsing (@bbartels #28831)</li>
<li>Run macos smoke test workflow on main commit (<a class="user-mention notranslate" href="https://github.com/mgoin">@mgoin</a> #28752)</li>
<li>[ROCm][Quantization] add apply_vllm_mapper in quark config for models like gpt-oss (@xuebwang-amd #28638)</li>
<li>[Refactor] Remove Unused Func in Batch Invariant (<a class="user-mention notranslate" href="https://github.com/yewentao256">@yewentao256</a> #28881)</li>
<li>[Bugfix] Fix wrong CLI defaults for dynamic <code>SchedulerConfig</code> fields (<a class="user-mention notranslate" href="https://github.com/DarkLight1337">@DarkLight1337</a> #28872)</li>
<li>[Doc]: fix typos in various files (@didier-durand #28863)</li>
<li>[Misc] Remove unnecessary parentheses from log statements (@andyxning #28897)</li>
<li>[CI] Fix async scheduling + spec decoding test flake (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28902)</li>
<li>[MISC] Remove format.sh (@KuntaiDu #28906)</li>
<li>[CI/Build] Replace wikipedia url with local server ones (<a class="user-mention notranslate" href="https://github.com/Isotr0py">@Isotr0py</a> #28908)</li>
<li>[BugFix] Fix PP/async scheduling with pooling models (<a class="user-mention notranslate" href="https://github.com/njhill">@njhill</a> #28899)</li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a class="user-mention notranslate" href="https://github.com/bwasti">@bwasti</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25603">#25603</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Renovamen">@Renovamen</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25796">#25796</a></li>
<li><a class="user-mention notranslate" href="https://github.com/patrick-toulme">@patrick-toulme</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25084">#25084</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kingsmad">@kingsmad</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25825">#25825</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yingjun-mou">@yingjun-mou</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25827">#25827</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhoukezi">@zhoukezi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25854">#25854</a></li>
<li><a class="user-mention notranslate" href="https://github.com/leejnau">@leejnau</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25706">#25706</a></li>
<li><a class="user-mention notranslate" href="https://github.com/adabeyta">@adabeyta</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25513">#25513</a></li>
<li><a class="user-mention notranslate" href="https://github.com/acisseJZhong">@acisseJZhong</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25912">#25912</a></li>
<li><a class="user-mention notranslate" href="https://github.com/a120092009">@a120092009</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25942">#25942</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Anionex">@Anionex</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25354">#25354</a></li>
<li><a class="user-mention notranslate" href="https://github.com/DrStone1971">@DrStone1971</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25843">#25843</a></li>
<li><a class="user-mention notranslate" href="https://github.com/certainly-param">@certainly-param</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25935">#25935</a></li>
<li><a class="user-mention notranslate" href="https://github.com/natoscott">@natoscott</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26007">#26007</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kmaehashi">@kmaehashi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26005">#26005</a></li>
<li><a class="user-mention notranslate" href="https://github.com/leo-pony">@leo-pony</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25470">#25470</a></li>
<li><a class="user-mention notranslate" href="https://github.com/huijjj">@huijjj</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24947">#24947</a></li>
<li><a class="user-mention notranslate" href="https://github.com/levunet">@levunet</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24768">#24768</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Egor-Krivov">@Egor-Krivov</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25668">#25668</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sixiang-google">@sixiang-google</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25992">#25992</a></li>
<li><a class="user-mention notranslate" href="https://github.com/astralord">@astralord</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26027">#26027</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jasl">@jasl</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26098">#26098</a></li>
<li><a class="user-mention notranslate" href="https://github.com/nrghosh">@nrghosh</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26148">#26148</a></li>
<li><a class="user-mention notranslate" href="https://github.com/southfreebird">@southfreebird</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25974">#25974</a></li>
<li><a class="user-mention notranslate" href="https://github.com/soldni">@soldni</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26054">#26054</a></li>
<li><a class="user-mention notranslate" href="https://github.com/yuafng">@yuafng</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26219">#26219</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ILikeIneine">@ILikeIneine</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25823">#25823</a></li>
<li><a class="user-mention notranslate" href="https://github.com/jasonlizhengjian">@jasonlizhengjian</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25998">#25998</a></li>
<li><a class="user-mention notranslate" href="https://github.com/elieserr">@elieserr</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26177">#26177</a></li>
<li><a class="user-mention notranslate" href="https://github.com/orangeng">@orangeng</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26266">#26266</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ymoslem">@ymoslem</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26258">#26258</a></li>
<li><a class="user-mention notranslate" href="https://github.com/abhisheksheth28">@abhisheksheth28</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25521">#25521</a></li>
<li><a class="user-mention notranslate" href="https://github.com/seven-mile">@seven-mile</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26231">#26231</a></li>
<li><a class="user-mention notranslate" href="https://github.com/cfRod">@cfRod</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26289">#26289</a></li>
<li><a class="user-mention notranslate" href="https://github.com/atalhens">@atalhens</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26265">#26265</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gholmes829">@gholmes829</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25164">#25164</a></li>
<li><a class="user-mention notranslate" href="https://github.com/dcampora">@dcampora</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25945">#25945</a></li>
<li><a class="user-mention notranslate" href="https://github.com/antrec">@antrec</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26340">#26340</a></li>
<li><a class="user-mention notranslate" href="https://github.com/plliao">@plliao</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26325">#26325</a></li>
<li><a class="user-mention notranslate" href="https://github.com/morrison-turnansky">@morrison-turnansky</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26113">#26113</a></li>
<li><a class="user-mention notranslate" href="https://github.com/isharif168">@isharif168</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26347">#26347</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Barry-Delaney">@Barry-Delaney</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25931">#25931</a></li>
<li><a class="user-mention notranslate" href="https://github.com/utkarshsharma1">@utkarshsharma1</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26279">#26279</a></li>
<li><a class="user-mention notranslate" href="https://github.com/Aydin-ab">@Aydin-ab</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25283">#25283</a></li>
<li><a class="user-mention notranslate" href="https://github.com/therealnaveenkamal">@therealnaveenkamal</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25103">#25103</a></li>
<li><a class="user-mention notranslate" href="https://github.com/QierLi">@QierLi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24926">#24926</a></li>
<li><a class="user-mention notranslate" href="https://github.com/zhiyuan1i">@zhiyuan1i</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24486">#24486</a></li>
<li><a class="user-mention notranslate" href="https://github.com/iwzbi">@iwzbi</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/16601">#16601</a></li>
<li><a class="user-mention notranslate" href="https://github.com/roikoren755">@roikoren755</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25947">#25947</a></li>
<li><a class="user-mention notranslate" href="https://github.com/luis5tb">@luis5tb</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25593">#25593</a></li>
<li><a class="user-mention notranslate" href="https://github.com/wangxiongts">@wangxiongts</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/25550">#25550</a></li>
<li><a class="user-mention notranslate" href="https://github.com/sangho-vision">@sangho-vision</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26563">#26563</a></li>
<li><a class="user-mention notranslate" href="https://github.com/muzian666">@muzian666</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26562">#26562</a></li>
<li><a class="user-mention notranslate" href="https://github.com/HsChen-sys">@HsChen-sys</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/22100">#22100</a></li>
<li><a class="user-mention notranslate" href="https://github.com/FENP">@FENP</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26574">#26574</a></li>
<li><a class="user-mention notranslate" href="https://github.com/gjgjos">@gjgjos</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26339">#26339</a></li>
<li><a class="user-mention notranslate" href="https://github.com/andycandy">@andycandy</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26629">#26629</a></li>
<li><a class="user-mention notranslate" href="https://github.com/aitsvet">@aitsvet</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26713">#26713</a></li>
<li><a class="user-mention notranslate" href="https://github.com/cyb70289">@cyb70289</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26698">#26698</a></li>
<li><a class="user-mention notranslate" href="https://github.com/kfhfar">@kfhfar</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26538">#26538</a></li>
<li><a class="user-mention notranslate" href="https://github.com/n1ck-guo">@n1ck-guo</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/24024">#24024</a></li>
<li><a class="user-mention notranslate" href="https://github.com/ryanli">@ryanli</a> first commit is <a class="issue-link js-issue-link" href="https://github.com/vllm-project/vllm/pull/26758">#26758</a></li>
<li><a class="user-mention notranslate" href="https://github.com/VladOS95-cyber">@VladOS95-cyber</a> first commit is #26726</li>
<li>@zklapow first commit is #26818</li>
<li>@HDCharles first commit is #26820</li>
<li>@Dhruvilbhatt first commit is #26837</li>
<li>@madongfly first commit is #26853</li>
<li>@li2haipeng first commit is #26319</li>
<li>@pdasigi first commit is #26143</li>
<li>@cern1710 first commit is #26637</li>
<li>@inc-jeong first commit is #26225</li>
<li>@bogdanminko first commit is #27008</li>
<li>@mandy-li first commit is #26883</li>
<li>@kimbochen first commit is #26943</li>
<li>@staghado first commit is #26916</li>
<li>@rkarhila-amd first commit is #25586</li>
<li>@hyongtao-code first commit is #27101</li>
<li>@jianyuh first commit is #27159</li>
<li>@uyzhang first commit is #27012</li>
<li>@shivampr first commit is #26268</li>
<li>@helunwencser first commit is #26832</li>
<li>@dagrayvid first commit is #27196</li>
<li>@ExtReMLapin first commit is #27253</li>
<li>@ReinForce-II first commit is #26789</li>
<li>@LiuLi1998 first commit is #22627</li>
<li>@sagiahrac first commit is #27211</li>
<li>@fangpings first commit is #27133</li>
<li>@jonathanc-n first commit is #27372</li>
<li>@bradleyhd first commit is #27124</li>
<li>@Navya1707 first commit is #27156</li>
<li>@piood first commit is #27324</li>
<li>@xxxxyu first commit is #26092</li>
<li>@usberkeley first commit is #27419</li>
<li>@strinczer first commit is #26706</li>
<li>@hjh0119 first commit is #27469</li>
<li>@wpc first commit is #27328</li>
<li>@yeshsurya first commit is #27188</li>
<li>@rogeryoungh first commit is #27535</li>
<li>@dcmaddix first commit is #27291</li>
<li>@tingtingtangmeta first commit is #27538</li>
<li>@minatoaquaMK2 first commit is #27323</li>
<li>@wangln19 first commit is #27565</li>
<li>@junpuf first commit is #27596</li>
<li>@sammshen first commit is #27600</li>
<li>@mpashkovskii first commit is #26886</li>
<li>@KevinCheung2259 first commit is #27670</li>
<li>@sammysun0711 first commit is #27623</li>
<li>@dumb0002 first commit is #24176</li>
<li>@sairampillai first commit is #25775</li>
<li>@FlamingoPg first commit is #27794</li>
<li>@SumanthRH first commit is #27789</li>
<li>@PaulZhang12 first commit is #27660</li>
<li>@jakub-sochacki first commit is #26919</li>
<li>@RobMulla first commit is #27824</li>
<li>@yugong333 first commit is #27818</li>
<li>@ai-jz first commit is #27850</li>
<li>@xiaohajiayou first commit is #26779</li>
<li>@biswapanda first commit is #27728</li>
<li>@efimki first commit is #24905</li>
<li>@zhang-prog first commit is #27758</li>
<li>@xiangze-arm first commit is #27240</li>
<li>@yt0428 first commit is #27521</li>
<li>@ganyi1996ppo first commit is #25763</li>
<li>@nadavkluger first commit is #28048</li>
<li>@toulzx first commit is #27740</li>
<li>@frost-intel first commit is #28004</li>
<li>@jjzhang first commit is #28127</li>
<li>@walterbm first commit is #28075</li>
<li>@dayeol first commit is #22496</li>
<li>@cmpute first commit is #27780</li>
<li>@seungduk-yanolja first commit is #27946</li>
<li>@aditew01 first commit is #28130</li>
<li>@milpuz01 first commit is #26018</li>
<li>@StanHatko first commit is #27953</li>
<li>@vicoooo26 first commit is #27792</li>
<li>@HanFa first commit is #27497</li>
<li>@amacaskill first commit is #28079</li>
<li>@smitkadvani first commit is #28024</li>
<li>@xiaohongchen1991 first commit is #21068</li>
<li>@hammmmy first commit is #28308</li>
<li>@ashahba first commit is #28026</li>
<li>@zhangsicheng5 first commit is #26696</li>
<li>@evberrypi first commit is #28328</li>
<li>@ColeMurray first commit is #28337</li>
<li>@bo-ke first commit is #28374</li>
<li>@caozuoba first commit is #28280</li>
<li>@zhaozuy first commit is #27892</li>
<li>@maryamtahhan first commit is #28461</li>
<li>@the-codeboy first commit is #28474</li>
<li>@xuebwang-amd first commit is #24239</li>
<li>@Livinfly first commit is #28389</li>
<li>@AndreasKaratzas first commit is #27611</li>
<li>@wuyaoxuehun first commit is #27597</li>
<li>@ziruiliu first commit is #27978</li>
<li>@ZhengHongming888 first commit is #28356</li>
<li>@albertoperdomo2 first commit is #27039</li>
<li>@PerryZhang01 first commit is #27731</li>
<li>@Radu2k first commit is #28059</li>
<li>@tjandy98 first commit is #28210</li>
<li>@dw2761 first commit is #28615</li>
<li>@zufangzhu first commit is #28611</li>
<li>@amdfaa first commit is #28142</li>
<li>@YuanpingSong first commit is #28617</li>
<li>@janeyx99 first commit is #28595</li>
<li>@xingliu14 first commit is #28529</li>
<li>@610lyn first commit is #28522</li>
<li>@WorldExplored first commit is #27516</li>
<li>@gjc0824 first commit is #25438</li>
<li>@shahfasal first commit is #26721</li>
<li>@zhaozx-cn first commit is #28277</li>
<li>@OthmanMohammad first commit is #28737</li>
<li>@GuanH first commit is #28663</li>
<li>@halyavin first commit is #28728</li>
<li>@mostrowskix first commit is #27410</li>
<li>@laithsakka first commit is #25110</li>
<li>@hwhaokun first commit is #28777</li>
<li>@River12 first commit is #28715</li>
<li>@scottzh8 first commit is #28679</li>
<li>@ashors1 first commit is #28765</li>
<li>@jscaldwell55 first commit is #28543</li>
<li>@tiehexue first commit is #28814</li>
<li>@Ronald1995 first commit is #24799</li>
<li>@shreyas269 first commit is #28435</li>
<li>@pranav4501 first commit is #28332</li>
</ul>
<p><strong>Full Changelog</strong>: <a class="commit-link" href="https://github.com/vllm-project/vllm/compare/v0.11.0...v0.11.1"><tt>v0.11.0...v0.11.1</tt></a></p>

---

### [v0.11.1rc7](https://github.com/vllm-project/vllm/releases/tag/v0.11.1rc7)

**Êù•Ê∫ê**: vLLM GitHub Releases

**ÊëòË¶Å**: <p>[compile] Enable sequence parallelism matching w/o custom ops enabled‚Ä¶</p>

---

## Operating Systems

### [Fedora Linux 43 election results](https://lwn.net/Articles/1053333/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>The Fedora Project has <a href="https://communityblog.fedoraproject.org/f43-election-results/">announced</a>
the results of the Fedora&#160;43 election cycle. Five seats were open
on the <a href="https://docs.fedoraproject.org/en-US/fesco/">Fedora Engineering
Steering Committee</a> (FESCo), and the <a href="https://elections.fedoraproject.org/about/f43-fesco">winners</a>
are Kevin Fenzi, Zbigniew Jƒôdrzejewski-Szmek, Timoth√©e Ravier, Dave
Cantrell, and M√°ir√≠n Duffy.</p>

<p></p>

---

### [Gentoo looks back on 2025](https://lwn.net/Articles/1053289/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>Gentoo Linux has published a <a href="https://www.gentoo.org/news/2026/01/05/new-year.html">2025
project retrospective</a> that looks at how the community has evolved,
changes to the distribution, infrastructure, and finances for the
Gentoo Foundation.</p>

<blockquote class="bq">
<p>Gentoo currently consists of 31663 ebuilds for 19174 different
packages. For amd64 (x86-64), there are 89 GBytes of binary packages
available on the mirrors. Gentoo each week builds 154 distinct <a href="https://www.gentoo.org/downloads/">installation stages</a> for
different processor architectures and system configurations, with an
overwhelming part of these fully up-to-date.</p>

<p>The number of commits to the <a href="https://gitweb.gentoo.org/repo/gentoo.git/">main ::gentoo</a>
repository has remained at an overall high level in 2025, with a
slight decrease from 123942 to 112927. The number of commits by
external contributors was 9396, now across 377 unique external
authors.</p>
</blockquote>

---

### [[$] SFC v. VIZIO: who can enforce the GPL?](https://lwn.net/Articles/1052734/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: <p>
The
<a href="https://sfconservancy.org/">
Software Freedom Conservancy</a> (SFC) is
<a href="https://lwn.net/Articles/873415/">
suing</a>
<a href="https://www.vizio.com/en/home">
VIZIO</a> over smart TVs that
include software licensed under the GPL and LGPL (including the Linux kernel,
FFmpeg, systemd, and others).
VIZIO didn't provide the source code along with the device, and on request they
only provided some of it. Unlike a typical lawsuit about enforcing the GPL, the
SFC isn't suing as a copyright holder; it's suing as
a normal owner of the TV
in question. This approach opens some important legal questions, and after years
of pre-trial maneuvering (most recently resulting in
<a href="https://lwn.net/Articles/1051984/">a ruling related to signing keys</a> that
<a href="https://lwn.net/Articles/1052842/">is the subject of a separate article</a>),
we might finally obtain some answers when the case goes
to trial on January&#160;12. As things stand, it seems likely that the judge in
the case will rule that that the GPL-enforcement lawsuits can be a matter of
contract law, not just copyright law, which would be a major change to how GPL
enforcement works.
</p>

---

### [[$] GPLv2 and installation requirements](https://lwn.net/Articles/1052842/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: On December 24 2025, Linus Torvalds posted <a href="https://social.kernel.org/notice/B1aR6QFuzksLVSyBZQ">a strongly
worded message</a> celebrating a <a href="https://lwn.net/Articles/1051984/">ruling</a> in
the ongoing <a href="https://lwn.net/Articles/1052734/">GPL-compliance lawsuit</a> filed
against VIZIO by the Software Freedom Conservancy (SFC).  This case and
Torvalds's response have put a spotlight on an old debate over the extent
to which the source-code requirements of the <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GNU
General Public License (version&#160;2)</a> extend to keys and other data
needed to successfully install modified software on a device.  It is worth
looking at whether this requirement exists, the subtleties in
interpretation that cloud the issue, and the extent to which, if any, the
SFC is demanding that information.

---

### [Two new stable kernels](https://lwn.net/Articles/1053279/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Greg Kroah-Hartman has released the <a href="https://lwn.net/Articles/1053280/">6.18.4</a> and <a href="https://lwn.net/Articles/1053278/">6.12.64</a> stable kernels. As always, each
contains important fixes throughout the tree. Users are advised to
upgrade.</p>

<p></p>

---

### [Security updates for Thursday](https://lwn.net/Articles/1053277/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Security updates have been issued by <b>AlmaLinux</b> (gcc-toolset-14-binutils, gcc-toolset-15-binutils, httpd, kernel, libpng, mariadb, mingw-libpng, poppler, python3.12, and ruby:3.3), <b>Debian</b> (foomuuri and libsodium), <b>Fedora</b> (python-pdfminer and wget2), <b>Oracle</b> (audiofile, bind, gcc-toolset-15-binutils, libpng, mariadb, mariadb10.11, mariadb:10.11, mariadb:10.5, mingw-libpng, poppler, and python3.12), <b>Red Hat</b> (git-lfs, kernel, libpng, libpq, mariadb:10.3, osbuild-composer, postgresql, postgresql:13, and postgresql:15), <b>Slackware</b> (curl), <b>SUSE</b> (c-ares-devel, capstone, curl, gpsd, ImageMagick, libpcap, log4j, python311-filelock, and python314), and <b>Ubuntu</b> (libcaca, libxslt, and net-snmp).

---

### [[$] LWN.net Weekly Edition for January 8, 2026](https://lwn.net/Articles/1051994/)

**Êù•Ê∫ê**: LWN.net (Linux Weekly News)

**ÊëòË¶Å**: Inside this week's LWN.net Weekly Edition:
        <p>
        <ul>
<li> <a href="https://lwn.net/Articles/1051994/">Front</a>: What to expect in 2026; LAVD scheduler; libpathrs; Questions for the TAB; Graphite; 2025 timeline.
            <li> <a href="https://lwn.net/Articles/1051996/">Briefs</a>: shadow-utils 4.19.0; Android releases; IPFire 2.29-199; Manjaro 26.0; curl strcpy(); GNU ddrescue 1.30; Ruby 4.0; Partial GPL ruling; Quotes; ...
            <li> <a href="https://lwn.net/Articles/1051997/">Announcements</a>: Newsletters, conferences, security updates, patches, and more.
            </ul>

---

## Systems

### [A data model for Git (and other docs updates)](https://jvns.ca/blog/2026/01/08/a-data-model-for-git/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Git ÁöÑÊï∞ÊçÆÊ®°ÂûãÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÁâàÊú¨ÊéßÂà∂Á≥ªÁªüÁöÑÂ∫ïÂ±ÇÂ≠òÂÇ®Êú∫Âà∂ÔºåÈÄöËøáËß£Êûê Git Â¶Ç‰Ωï‰ΩøÁî®ÂØπË±°„ÄÅÂºïÁî®ÂíåÂìàÂ∏åÊ†ëÊù•ÁÆ°ÁêÜ‰ª£Á†Å‰ªìÂ∫ì„ÄÇ‰ΩúËÄÖËØ¶ÁªÜÈòêËø∞‰∫Ü Git ÂØπË±°Ôºàblob„ÄÅtree„ÄÅcommitÔºâÁöÑÂÜÖÈÉ®ÁªìÊûÑÂíåÂÖ≥ËÅîÊñπÂºèÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÁêÜËß£ Git ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊéåÊè°ÁâàÊú¨ÊéßÂà∂ÁöÑÊ†∏ÂøÉÊ¶ÇÂøµÂíåÊäÄÊúØÁªÜËäÇ„ÄÇÂØπ‰∫éÊÉ≥Ë¶ÅÊ∑±ÂÖ•ÁêÜËß£ Git ÂÜÖÈÉ®ÂéüÁêÜÁöÑÁ®ãÂ∫èÂëòÊù•ËØ¥ÔºåËøôÊòØ‰∏ÄÁØáÊûÅÂÖ∑ÊäÄÊúØÊ¥ûÂØüÂäõÁöÑËß£ËØªÊñáÁ´†„ÄÇ

---

### [Notes on switching to Helix from vim](https://jvns.ca/blog/2025/10/10/notes-on-switching-to-helix-from-vim/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Helix ÊòØ‰∏Ä‰∏™Áé∞‰ª£ÂåñÁöÑÊ®°ÊÄÅÁºñËæëÂô®ÔºåÁõ∏ÊØî‰º†Áªü VimÔºåÂÆÉÂú®ËØ≠Ë®ÄÊúçÂä°Âô®ÈõÜÊàê„ÄÅ‰ª£Á†ÅÈÄâÊã©ÂíåÁºñËæëÊ®°ÂºèÁ≠âÊñπÈù¢Êèê‰æõ‰∫ÜÊõ¥Áõ¥ËßÇÂíåÈ´òÊïàÁöÑ‰ΩìÈ™å„ÄÇ‰ΩúËÄÖÂàÜ‰∫´‰∫Ü‰ªé Vim ËøÅÁßªÂà∞ Helix ÁöÑ‰∏™‰∫∫ÂÆûË∑µÔºåÈáçÁÇπ‰ªãÁªç‰∫Ü Helix Âú®‰ª£Á†ÅÂØºËà™„ÄÅË°•ÂÖ®ÂíåÈáçÊûÑÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇÂØπ‰∫éËøΩÊ±ÇÈ´òÊïàÁºñÁ†ÅÂ∑•‰ΩúÊµÅÁöÑÂºÄÂèëËÄÖÊù•ËØ¥ÔºåHelix ÊòØ‰∏Ä‰∏™ÂÄºÂæóÂ∞ùËØïÁöÑÁé∞‰ª£ÁºñËæëÂô®ÈÄâÊã©„ÄÇ

---

### [New zine: The Secret Rules of the Terminal](https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: „ÄäÁªàÁ´ØÁöÑÁßòÂØÜËßÑÂàô„ÄãÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÂëΩ‰ª§Ë°åÊìç‰ΩúÁöÑÈöêËóèÊú∫Âà∂ÂíåÊúÄ‰Ω≥ÂÆûË∑µÔºåÈÄöËøáËØ¶ÁªÜËß£ÊûêÁªàÁ´Ø‰∫§‰∫íÁöÑÂ∫ïÂ±ÇÂéüÁêÜÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÂíåÁ≥ªÁªüÁÆ°ÁêÜÂëòÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊéåÊè°ÂëΩ‰ª§Ë°åÊäÄÊúØ„ÄÇËØ•zine‰ª•ÈÄö‰øóÊòìÊáÇÁöÑÊñπÂºèÊè≠Á§∫‰∫ÜÁªàÁ´ØÊìç‰Ωú‰∏≠ÁöÑÂÖ≥ÈîÆÊäÄÂ∑ßÂíåÈ≤ú‰∏∫‰∫∫Áü•ÁöÑÁªÜËäÇÔºå‰∏∫ËØªËÄÖÊèê‰æõ‰∫ÜÊèêÂçáÂëΩ‰ª§Ë°åÊïàÁéáÂíåÊ∑±ÂÖ•ÁêÜËß£Á≥ªÁªü‰∫§‰∫íÁöÑÂÆùË¥µÊåáÂçó„ÄÇ

---

### [Using `make` to compile C programs (for non-C-programmers)](https://jvns.ca/blog/2025/06/10/how-to-compile-a-c-program/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Êú¨ÊñáËØ¶ÁªÜÈòêËø∞‰∫Ü‰ΩøÁî® `make` Â∑•ÂÖ∑ÁºñËØë C Á®ãÂ∫èÁöÑÂü∫Êú¨ÊµÅÁ®ãÂíåÊäÄÂ∑ßÔºåÁâπÂà´ÈíàÂØπÈùû C ËØ≠Ë®ÄÁ®ãÂ∫èÂëò„ÄÇÈÄöËøáËß£Èáä Makefile ÁöÑËØ≠Ê≥ï„ÄÅ‰æùËµñÂÖ≥Á≥ªÂíåÁºñËØëËßÑÂàôÔºå‰ΩúËÄÖÂ∏ÆÂä©ËØªËÄÖÁêÜËß£Ëá™Âä®ÂåñÊûÑÂª∫ËøáÁ®ãÔºåÂπ∂Êèê‰æõ‰∫ÜÂÆûÁî®ÁöÑÁºñËØëÊäÄÊúØÂíåÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇÂØπ‰∫éÊÉ≥Ë¶ÅÂÖ•Èó® C ËØ≠Ë®ÄÁºñÁ®ãÂíåÊûÑÂª∫Á≥ªÁªüÁöÑÂºÄÂèëËÄÖÊù•ËØ¥ÔºåËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÂÆûÁî®ÁöÑÊäÄÊúØÊåáÂçó„ÄÇ

---

### [Standards for ANSI escape codes](https://jvns.ca/blog/2025/03/07/escape-code-standards/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: ANSIËΩ¨‰πâÁ†ÅÊ†áÂáÜÊòØÊéßÂà∂ÁªàÁ´ØÊòæÁ§∫Âíå‰∫§‰∫íÁöÑÂÖ≥ÈîÆÊäÄÊúØËßÑËåÉÔºåÊñáÁ´†Ê∑±ÂÖ•Êé¢ËÆ®‰∫Ü‰∏çÂêåÊìç‰ΩúÁ≥ªÁªüÂíåÁªàÁ´ØÁéØÂ¢É‰∏≠ËΩ¨‰πâÁ†ÅÁöÑÂÖºÂÆπÊÄßÂíåÂÆûÁé∞ÁªÜËäÇ„ÄÇÈÄöËøáÊ¢≥ÁêÜÂéÜÂè≤ÂèëÂ±ïÂíåÂΩìÂâçÊ†áÂáÜÔºå‰ΩúËÄÖÊè≠Á§∫‰∫ÜÁªàÁ´ØÊéßÂà∂Á†ÅÁöÑÂ§çÊùÇÊÄßÔºå‰∏∫ÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÁêÜËß£ÂíåÊ≠£Á°Æ‰ΩøÁî®ÁªàÁ´ØÊéßÂà∂Â∫èÂàóÁöÑÈáçË¶ÅÊäÄÊúØÊ¥ûÂØü„ÄÇ

---

### [How to add a directory to your PATH](https://jvns.ca/blog/2025/02/13/how-to-add-a-directory-to-your-path/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Êú¨ÊñáËØ¶ÁªÜËß£Êûê‰∫ÜÂú®‰∏çÂêåÊìç‰ΩúÁ≥ªÁªü‰∏≠Â∞ÜÁõÆÂΩïÊ∑ªÂä†Âà∞Á≥ªÁªüPATHÁéØÂ¢ÉÂèòÈáèÁöÑÊñπÊ≥ïÔºåÂåÖÊã¨‰∏¥Êó∂ÂíåÊ∞∏‰πÖÊÄßÈÖçÁΩÆÊäÄÂ∑ß„ÄÇ‰ΩúËÄÖÈÄöËøáÂÆûÁî®ÁöÑÂëΩ‰ª§ÂíåÈÖçÁΩÆÁ§∫‰æãÔºåÂ∏ÆÂä©ËØªËÄÖÁêÜËß£PATHÁöÑÂ∑•‰ΩúÂéüÁêÜÔºåÂπ∂Êèê‰æõ‰∫ÜË∑®Âπ≥Âè∞(Linux„ÄÅmacOSÂíåWindows)ÁöÑÂÖ∑‰ΩìÊìç‰ΩúÊåáÂçó„ÄÇÂØπ‰∫éÂ∏åÊúõÊèêÈ´òÂëΩ‰ª§Ë°åÊïàÁéáÂíåÁ≥ªÁªüÈÖçÁΩÆËÉΩÂäõÁöÑÂºÄÂèëËÄÖÂíåÁ≥ªÁªüÁÆ°ÁêÜÂëòÊù•ËØ¥ÔºåËøôÊòØ‰∏ÄÁØáÈùûÂ∏∏ÂÆûÁî®ÁöÑÊäÄÊúØÊåáÂçó„ÄÇ

---

### [Some terminal frustrations](https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: ËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫éÁªàÁ´ØÔºàTerminalÔºâ‰ΩøÁî®‰ΩìÈ™åÂíåÁóõÁÇπÁöÑÊäÄÊúØÂçöÂÆ¢Ôºå‰ΩúËÄÖÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÂëΩ‰ª§Ë°å‰∫§‰∫í‰∏≠Â≠òÂú®ÁöÑ‰∏Ä‰∫õËÆæËÆ°ÂíåÂäüËÉΩÁº∫Èô∑„ÄÇÊñáÁ´†ÈáçÁÇπÂàÜÊûê‰∫ÜÁªàÁ´ØÂú®ËæìÂá∫Ê†ºÂºè„ÄÅÈîôËØØÂ§ÑÁêÜÂíåÁî®Êà∑‰∫§‰∫íÁ≠âÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÂπ∂ÊèêÂá∫‰∫ÜÊîπËøõÂª∫ËÆÆÔºåÂØπ‰∫éÂ∏åÊúõÊ∑±ÂÖ•ÁêÜËß£ÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑ÂíåÁªàÁ´ØËÆæËÆ°ÁöÑÂºÄÂèëËÄÖÂíåÁ≥ªÁªüÁÆ°ÁêÜÂëòÂÖ∑ÊúâÈáçË¶ÅÂèÇËÄÉ‰ª∑ÂÄº„ÄÇÈÄöËøáÊåáÂá∫Áé∞ÊúâÁªàÁ´ØÊäÄÊúØÁöÑÂ±ÄÈôêÊÄßÔºå‰ΩúËÄÖÂëºÂêÅÂºÄÂèëËÄÖÂíåÁ§æÂå∫ÂÖ±ÂêåÊé®Âä®ÁªàÁ´Ø‰∫§‰∫í‰ΩìÈ™åÁöÑÊåÅÁª≠‰ºòÂåñ„ÄÇ

---

### [What's involved in getting a "modern" terminal setup?](https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Êú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÁé∞‰ª£ÁªàÁ´ØÁéØÂ¢ÉÁöÑÈÖçÁΩÆÂíå‰ºòÂåñÁ≠ñÁï•ÔºåÈáçÁÇπ‰ªãÁªç‰∫ÜÂåÖÊã¨ÁªàÁ´ØÊ®°ÊãüÂô®„ÄÅShell„ÄÅÊèêÁ§∫Á¨¶ÂíåÂ∑•ÂÖ∑ÈìæÁ≠âÂÖ≥ÈîÆÁªÑ‰ª∂ÁöÑÈÄâÊã©ÂíåÈÖçÁΩÆÊñπÊ≥ï„ÄÇ‰ΩúËÄÖ‰ªéÂÆûË∑µËßíÂ∫¶ÂàÜ‰∫´‰∫Ü‰∏™‰∫∫ÊûÑÂª∫È´òÊïà„ÄÅÁé∞‰ª£ÂåñÁªàÁ´ØÂºÄÂèëÁéØÂ¢ÉÁöÑÁªèÈ™åÔºå‰∏∫ËØªËÄÖÊèê‰æõ‰∫ÜÂÖ∑‰ΩìÁöÑÊäÄÊúØÂª∫ËÆÆÂíåÂèØÂèÇËÄÉÁöÑÈÖçÁΩÆÊÄùË∑ØÔºåÊúâÂä©‰∫éÂºÄÂèëËÄÖÊòæËëóÊèêÂçáÂëΩ‰ª§Ë°åÂ∑•‰ΩúÊïàÁéáÂíå‰ΩøÁî®‰ΩìÈ™å„ÄÇ

---

### ["Rules" that terminal programs follow](https://jvns.ca/blog/2024/11/26/terminal-rules/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Êú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÁªàÁ´ØÁ®ãÂ∫èÁöÑÈÄöÁî®ËÆæËÆ°ËßÑÂàôÂíåÊúÄ‰Ω≥ÂÆûË∑µÔºåÊè≠Á§∫‰∫ÜÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑Âú®ËæìÂá∫„ÄÅÈîôËØØÂ§ÑÁêÜÂíå‰∫§‰∫íÊÄßÊñπÈù¢Â∫îÈÅµÂæ™ÁöÑÂÖ≥ÈîÆÂéüÂàô„ÄÇ‰ΩúËÄÖÈÄöËøáËØ¶ÁªÜÂàÜÊûêÂíåÂÆû‰æãÔºåÈòêËø∞‰∫ÜÂ¶Ç‰ΩïÊûÑÂª∫Áî®Êà∑ÂèãÂ•Ω„ÄÅË°å‰∏∫‰∏ÄËá¥ÁöÑÁªàÁ´ØÁ®ãÂ∫èÔºå‰∏∫ÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑËÆæËÆ°ÊåáÂØº„ÄÇÂØπ‰∫éËá¥Âäõ‰∫éÂºÄÂèëÈ´òË¥®ÈáèÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑ÁöÑÁ®ãÂ∫èÂëòÊù•ËØ¥ÔºåÊñáÁ´†Êèê‰æõ‰∫ÜÂÖ∑ÊúâÂÆûË∑µ‰ª∑ÂÄºÁöÑÊäÄÊúØÊ¥ûÂØü„ÄÇ

---

### [Why pipes sometimes get "stuck": buffering](https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/)

**Êù•Ê∫ê**: Julia Evans

**ÊëòË¶Å**: Êú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜUnixÁÆ°ÈÅì‰∏≠ÁºìÂÜ≤Êú∫Âà∂ÁöÑÂ∑•‰ΩúÂéüÁêÜÂíåÊΩúÂú®ÈòªÂ°ûÈóÆÈ¢òÔºåËØ¶ÁªÜËß£Êûê‰∫ÜËøõÁ®ãÈó¥Êï∞ÊçÆ‰º†ËæìÊó∂ÁºìÂÜ≤Âå∫Â§ßÂ∞è„ÄÅÈòªÂ°ûÂíåÈùûÈòªÂ°ûÊ®°ÂºèÂØπÁÆ°ÈÅìÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÈÄöËøáÂâñÊûêLinuxÁ≥ªÁªüË∞ÉÁî®ÂíåÂÜÖÊ†∏Ë°å‰∏∫Ôºå‰ΩúËÄÖÊè≠Á§∫‰∫ÜÁÆ°ÈÅìÈòªÂ°ûÁöÑÊ†πÊú¨ÂéüÂõ†Ôºå‰∏∫ÂºÄÂèëËÄÖÁêÜËß£Á≥ªÁªüÁ∫ßI/OÊìç‰ΩúÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊäÄÊúØÊ¥ûÂØüÔºåÊúâÂä©‰∫é‰ºòÂåñËøõÁ®ãÈó¥ÈÄö‰ø°ÂíåÊï∞ÊçÆÊµÅÂ§ÑÁêÜ„ÄÇ

---

### [2025 Recap: so many projects](https://fasterthanli.me/articles/2025-recap)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: ÈùûÂ∏∏Êä±Ê≠âÔºåÊàëÊó†Ê≥ïËÆøÈóÆÂÖ∑‰ΩìÁöÑÊñáÁ´†ÈìæÊé•ÂíåÂÜÖÂÆπ„ÄÇË¶ÅÁîüÊàêÂáÜÁ°ÆÁöÑÊëòË¶ÅÔºåÊàëÈúÄË¶ÅÊñáÁ´†ÁöÑÂÆûÈôÖÂÜÖÂÆπ„ÄÇÂ¶ÇÊûúÊÇ®ËÉΩÁõ¥Êé•Êèê‰æõÊñáÁ´†ÊñáÊú¨ÔºåÊàëÂ∞ÜÈùûÂ∏∏‰πêÊÑèÂ∏ÆÊÇ®Êí∞ÂÜô‰∏Ä‰∏™ÁÆÄÊ¥ÅÁ≤æÁÇºÁöÑ‰∏≠ÊñáÊëòË¶Å„ÄÇ

---

### [Introducing arborium, a tree-sitter distribution](https://fasterthanli.me/articles/introducing-arborium)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: ArboriumÊòØ‰∏Ä‰∏™ÂàõÊñ∞ÁöÑtree-sitterÂèëË°åÁâàÔºåÊó®Âú®ÁÆÄÂåñËØ≠Ê≥ïËß£ÊûêÂíå‰ª£Á†ÅÂàÜÊûêÂ∑•‰Ωú„ÄÇÂÆÉÈÄöËøáÈ¢ÑÊûÑÂª∫Âíå‰ºòÂåñÁöÑËØ≠Ê≥ïËß£ÊûêÂô®Ôºå‰∏∫ÂºÄÂèëËÄÖÊèê‰æõÊõ¥È´òÊïà„ÄÅÊõ¥‰æøÊç∑ÁöÑ‰ª£Á†ÅËØ≠Ê≥ïËß£ÊûêÂ∑•ÂÖ∑„ÄÇÂØπ‰∫éÈúÄË¶ÅËøõË°å‰ª£Á†ÅÈùôÊÄÅÂàÜÊûê„ÄÅËØ≠Ê≥ïÈ´ò‰∫ÆÊàñ‰ª£Á†ÅÁêÜËß£ÁöÑÂºÄÂèëËÄÖÊù•ËØ¥ÔºåArboriumÊòØ‰∏Ä‰∏™ÂÄºÂæóÂÖ≥Ê≥®ÁöÑÊäÄÊúØËß£ÂÜ≥ÊñπÊ°à„ÄÇ

---

### [Does Dioxus spark joy?](https://fasterthanli.me/articles/does-dioxus-spark-joy)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: Dioxus ÊòØ‰∏Ä‰∏™Êñ∞ÂÖ¥ÁöÑ Rust Web Ê°ÜÊû∂ÔºåÊó®Âú®Êèê‰æõÁ±ª‰ºº React ÁöÑÂºÄÂèë‰ΩìÈ™åÔºåÂπ∂ÊîØÊåÅÂ§öÁ´ØÔºàWeb„ÄÅÊ°åÈù¢ÂíåÁßªÂä®ÔºâÊ∏≤Êüì„ÄÇ‰ΩúËÄÖÈÄöËøáËØ¶ÁªÜÁöÑÂÆûË∑µÂíå‰ª£Á†ÅÂàÜÊûêÔºåÊ∑±ÂÖ•Êé¢ËÆ®‰∫Ü Dioxus Âú®ÂºÄÂèëÊïàÁéá„ÄÅÊÄßËÉΩÂíåÂºÄÂèë‰ΩìÈ™åÊñπÈù¢ÁöÑ‰ºòÂä£Ôºå‰∏∫ÂØπ Rust ÂâçÁ´ØÊ°ÜÊû∂ÊÑüÂÖ¥Ë∂£ÁöÑÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊäÄÊúØÊ¥ûÂØüÂíåÂÆûË∑µÂèÇËÄÉ„ÄÇÂØπ‰∫éÊÉ≥Â∞ùËØï‰ΩøÁî® Rust ÊûÑÂª∫Ë∑®Âπ≥Âè∞Â∫îÁî®ÁöÑÂºÄÂèëËÄÖÊù•ËØ¥ÔºåËøôÊòØ‰∏ÄÁØáÂÄºÂæóÊ∑±ÂÖ•ÈòÖËØªÁöÑÊäÄÊúØËØÑÊµãÊñáÁ´†„ÄÇ

---

### [Engineering a Rust optimization quiz](https://fasterthanli.me/articles/engineering-a-rust-optimization-quiz)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: RustÊÄßËÉΩ‰ºòÂåñÊé¢Á¥¢Ê∑±ÂÖ•ÂâñÊûê‰∫ÜÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÁöÑÊÄßËÉΩÊµãËØïÂíåÂàÜÊûêÊäÄÊúØÔºåÊè≠Á§∫‰∫Ü‰ª£Á†Å‰ºòÂåñÁöÑÂÖ≥ÈîÆÁ≠ñÁï•ÂíåÊÄùÁª¥ÊñπÊ≥ï„ÄÇ‰ΩúËÄÖ‰ª•‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÊÄßËÉΩÊµãÈ™å‰∏∫‰æãÔºåËØ¶ÁªÜÂ±ïÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®Âü∫ÂáÜÊµãËØï„ÄÅÊÄßËÉΩÂàÜÊûêÂ∑•ÂÖ∑ÂíåÊ±áÁºñÁ∫ßÂà´ÁöÑÊ£ÄÊü•Êù•ËØÜÂà´ÂíåÊîπËøõRust‰ª£Á†ÅÁöÑÊÄßËÉΩÁì∂È¢à„ÄÇÂØπ‰∫éËøΩÊ±ÇÊûÅËá¥ÊÄßËÉΩÁöÑRustÂºÄÂèëËÄÖÊù•ËØ¥ÔºåÊú¨ÊñáÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊÄßËÉΩ‰ºòÂåñÂÆûË∑µÊåáÂçóÂíåÊ∑±ÂÖ•ÁöÑÊäÄÊúØÊ¥ûÂØü„ÄÇ

---

### [Making our own spectrogram](https://fasterthanli.me/articles/making-our-own-spectrogram)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: Êú¨ÊñáËØ¶ÁªÜÈòêËø∞‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®RustËØ≠Ë®Ä‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞Èü≥È¢ëÈ¢ëË∞±ÂõæÔºàSpectrogramÔºâÁöÑÁîüÊàêËøáÁ®ãÔºåÈÄöËøáÊ∑±ÂÖ•Ëß£ÊûêÂø´ÈÄüÂÇÖÈáåÂè∂ÂèòÊç¢ÔºàFFTÔºâÂíåÈü≥È¢ëÊï∞ÊçÆÂ§ÑÁêÜÊäÄÊúØÔºåÂ±ïÁ§∫‰∫ÜÈü≥È¢ëÂèØËßÜÂåñÁöÑÂ∫ïÂ±ÇÂÆûÁé∞ÂéüÁêÜ„ÄÇ‰ΩúËÄÖ‰ª•ÂÆûË∑µÈ©±Âä®ÁöÑÊñπÂºèÔºåÁ≥ªÁªüÂú∞‰ªãÁªç‰∫ÜÈ¢ëË∞±ÂõæÁªòÂà∂ÁöÑÂÖ≥ÈîÆÊäÄÊúØÁªÜËäÇÔºå‰∏∫ËØªËÄÖÊèê‰æõ‰∫ÜÈü≥È¢ë‰ø°Âè∑Â§ÑÁêÜÂíåÂõæÂΩ¢Ê∏≤ÊüìÁöÑÂÖ∑‰ΩìÁºñÁ®ãËåÉ‰æãÔºåÂØπÊÉ≥Ë¶ÅÊ∑±ÂÖ•ÁêÜËß£Èü≥È¢ëÂèØËßÜÂåñÊäÄÊúØÁöÑÂºÄÂèëËÄÖÂÖ∑ÊúâÂæàÈ´òÁöÑÂ≠¶‰π†‰ª∑ÂÄº„ÄÇ

---

### [crates.io phishing attempt](https://fasterthanli.me/articles/crates-io-phishing-attempt)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: Êú¨ÊñáÊè≠Á§∫‰∫ÜÈíàÂØπRustÁîüÊÄÅÁ≥ªÁªüÂåÖÁÆ°ÁêÜÂπ≥Âè∞crates.ioÁöÑ‰∏ÄÊ¨°Á≤æÂøÉÁ≠ñÂàíÁöÑÁΩëÁªúÈíìÈ±ºÊîªÂáªÔºåËØ¶ÁªÜÂàÜÊûê‰∫ÜÊîªÂáªËÄÖÂ¶Ç‰ΩïÈÄöËøáÊ≥®ÂÜåÁõ∏‰ººÁöÑÂåÖÂêçÂíåÂà©Áî®‰æùËµñÂÖ≥Á≥ªÊù•ËØ±ÂØºÂºÄÂèëËÄÖ‰∏ãËΩΩÊÅ∂ÊÑè‰ª£Á†Å„ÄÇ‰ΩúËÄÖÈÄöËøáÊ∑±ÂÖ•ËøΩË∏™ÊîªÂáªÁªÜËäÇÔºåÂ±ïÁ§∫‰∫ÜÂºÄÊ∫êÁîüÊÄÅÁ≥ªÁªü‰∏≠ÊΩúÂú®ÁöÑÂÆâÂÖ®È£éÈô©ÔºåÂπ∂Âº∫Ë∞É‰∫ÜÂºÄÂèëËÄÖÂú®ÈÄâÊã©Âíå‰ΩøÁî®Á¨¨‰∏âÊñπ‰æùËµñÊó∂ÈúÄË¶Å‰øùÊåÅÈ´òÂ∫¶Ë≠¶ÊÉïÂíåË∞®ÊÖé„ÄÇËøôÁØáÊäÄÊúØÂàÜÊûêÂØπ‰∫éÁêÜËß£ÂºÄÊ∫êËΩØ‰ª∂‰æõÂ∫îÈìæÊîªÂáªÂÖ∑ÊúâÈáçË¶ÅÁöÑÂèÇËÄÉ‰ª∑ÂÄºÔºåÊúâÂä©‰∫éÊèêÈ´òÂºÄÂèëËÄÖÁöÑÂÆâÂÖ®ÊÑèËØÜ„ÄÇ

---

### [color npm package compromised](https://fasterthanli.me/articles/color-npm-package-compromised)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: Êú¨ÊñáÊè≠Á§∫‰∫Ü npm ÁîüÊÄÅÁ≥ªÁªü‰∏≠ "color" ÂåÖÈÅ≠ÂèóÁöÑÂÆâÂÖ®ÊºèÊ¥ûÔºåËØ¶ÁªÜÂàÜÊûê‰∫ÜÊîªÂáªËÄÖÂ¶Ç‰ΩïÈÄöËøáÊÅ∂ÊÑè‰ª£Á†ÅÁâàÊú¨Âä´ÊåÅÁî®Êà∑Á≥ªÁªüÔºåÂπ∂ÈòêËø∞‰∫ÜÂºÄÊ∫êËΩØ‰ª∂‰æõÂ∫îÈìæÊîªÂáªÁöÑ‰∏•ÈáçÈ£éÈô©„ÄÇ‰ΩúËÄÖÈÄöËøáÊ∑±ÂÖ•ËøΩË∏™ÂíåÂàÜÊûêÊºèÊ¥ûÁªÜËäÇÔºåÂº∫Ë∞É‰∫ÜÂºÄÂèëËÄÖÂú®ÈÄâÊã©Âíå‰ΩøÁî®Á¨¨‰∏âÊñπ‰æùËµñÊó∂ÂøÖÈ°ª‰øùÊåÅÈ´òÂ∫¶Ë≠¶ÊÉïÂíåË∞®ÊÖé„ÄÇËøô‰∏ÄÊ°à‰æãÁîüÂä®Âú∞Â±ïÁ§∫‰∫ÜËΩØ‰ª∂‰æõÂ∫îÈìæÂÆâÂÖ®ÁöÑÂ§çÊùÇÊÄßÔºå‰∏∫ÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÂÆâÂÖ®ÊÑèËØÜÂíåÈò≤ËåÉÂª∫ËÆÆ„ÄÇ

---

### [The science of loudness](https://fasterthanli.me/articles/the-science-of-loudness)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: Êú¨ÊñáÊ∑±ÂÖ•Êé¢ËÆ®‰∫ÜÂ£∞Èü≥ÂìçÂ∫¶(loudness)ÁöÑÁßëÂ≠¶ÂéüÁêÜÂíåÊÑüÁü•Êú∫Âà∂ÔºåËØ¶ÁªÜËß£Êûê‰∫Ü‰∫∫ËÄ≥ÂØπÂ£∞Èü≥Âº∫Â∫¶ÁöÑÂ§çÊùÇÊÑüÁü•ËøáÁ®ãÔºå‰ª•Âèä‰∏çÂêåÂìçÂ∫¶Ê†áÂáÜÂíåÊµãÈáèÊñπÊ≥ï„ÄÇ‰ΩúËÄÖÈÄöËøáÊäÄÊúØËßÜËßíÂíåÂÖ∑‰ΩìÂÆûË∑µÔºåÊè≠Á§∫‰∫ÜÂ£∞Èü≥ÂìçÂ∫¶ËÉåÂêéÁöÑÁâ©ÁêÜÂíåÂøÉÁêÜÂ≠¶ÂéüÁêÜÔºå‰∏∫Èü≥È¢ëÂ∑•Á®ãÂ∏àÂíåÈü≥È¢ëÁà±Â•ΩËÄÖÊèê‰æõ‰∫ÜÊ∑±ÂÖ•ÁêÜËß£Â£∞Èü≥ÊÑüÁü•ÁöÑ‰∏ì‰∏öÊ¥ûÂØü„ÄÇÊñáÁ´†‰∏ç‰ªÖÈòêËø∞‰∫ÜÊäÄÊúØÁªÜËäÇÔºåËøòÂ∏ÆÂä©ËØªËÄÖÁêÜËß£Â£∞Èü≥ÊÑüÁü•ÁöÑ‰∏ªËßÇÊÄßÂíåÂ§çÊùÇÊÄßÔºåÂØπÈü≥È¢ëÈ¢ÜÂüüÁöÑ‰∏ì‰∏ö‰∫∫Â£´ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂèÇËÄÉ‰ª∑ÂÄº„ÄÇ

---

### [Summer fasterthanlime update](https://fasterthanli.me/articles/summer-fasterthanlime-update)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: ËøôÊòØÂÖ≥‰∫é‰ΩúËÄÖ Amos Âú® Rust ÁîüÊÄÅÁ≥ªÁªüÂíå‰∏™‰∫∫ÊäÄÊúØÈ°πÁõÆÁöÑÊúÄÊñ∞ËøõÂ±ïÊÄªÁªìÔºåÈáçÁÇπÂàÜ‰∫´‰∫Ü‰ªñÂú® Rust ÊûÑÂª∫Á≥ªÁªü„ÄÅÊÄßËÉΩ‰ºòÂåñÂíåÂºÄÊ∫êÂ∑•ÂÖ∑ÂºÄÂèëÊñπÈù¢ÁöÑÊé¢Á¥¢‰∏éÂøÉÂæó„ÄÇÊñáÁ´†ËØ¶ÁªÜÈòêËø∞‰∫Ü‰ªñÂú® cargo-binstall„ÄÅcargo-dist Á≠âÂ∑•ÂÖ∑ÁöÑÊîπËøõÔºå‰ª•ÂèäÂØπ Rust ÁîüÊÄÅÁ≥ªÁªüÊûÑÂª∫ÂíåÂàÜÂèëÊµÅÁ®ãÁöÑÊ∑±ÂÖ•ÊÄùËÄÉÔºå‰∏∫ÂØπ Rust ÊäÄÊúØÊÑüÂÖ¥Ë∂£ÁöÑÂºÄÂèëËÄÖÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÊäÄÊúØÊ¥ûÂØüÂíåÂÆûË∑µÁªèÈ™å„ÄÇÂØπ‰∫éÊÉ≥Ê∑±ÂÖ•‰∫ÜËß£ Rust ÁîüÊÄÅÁ≥ªÁªüÂ∑•Á®ãÂÆûË∑µÁöÑËØªËÄÖÔºåËøôÁØáÊñáÁ´†Êèê‰æõ‰∫ÜÁã¨ÁâπÁöÑ‰∏ÄÊâãÊäÄÊúØËßÜËßí

---

### [All color is best-effort](https://fasterthanli.me/articles/all-color-is-best-effort)

**Êù•Ê∫ê**: Amos (fasterthanlime)

**ÊëòË¶Å**: <p>I do not come to you with answers today, but rather some observations and a lot of questions.</p>

<a class="anchor" href="https://fasterthanli.me/index.xml#the-weird-glitch" id="the-weird-glitch"><h2>The weird glitch</h2></a>
<p>Recently I was editing some video and I noticed this:</p>

<p>



<source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/dvr-circles@2x~3c031f1f42693336.jxl" title="A screenshot of the video, there are visible circles at various places in the image. Some of them are black, some of them are white. The image itself shows some blue and white text composited on some blurry background, which doesn‚Äôt really matter for this, and there‚Äôs a red line horizontal up in the image. It‚Äôs very confusing." width="1966" /></p>

<p>Not what the finger is pointing at ‚Äî the dots.</p>

<p>Here are the separate layers this image is made up of: the background is a stock image
I‚Äôve licensed from Envato Elements:</p>

<p><source media="(max-width: 400px)" type="image/jxl" /><source media="(max-width: 900px)" type="image/jxl" /><source type="image/jxl" /><source media="(max-width: 400px)" type="image/avif" /><source media="(max-width: 900px)" type="image/avif" /><source type="image/avif" /><source media="(max-width: 400px)" type="image/webp" /><source media="(max-width: 900px)" type="image/webp" /><source type="image/webp" /><img alt="A picture of a canyon, darker than you‚Äôd expect." height="1126" src="https://cdn.fasterthanli.me/content/articles/all-color-is-best-effort/canyon-background@2x~dcfb5771209ddbd5.jxl" title="A picture of a canyon, darker than you‚Äôd expect." width="1966" /></p>

<p>Because I use it as a background image, I‚Äôve cranked down the exposition in the Color tab:</p>













































































<a class="anchor" href="https://fasterthanli.me/index.xml#playing-with-color-spaces" id="playing-with-color-spaces"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#cie-chromaticity-diagram" id="cie-chromaticity-diagram"></a>


























































<a class="anchor" href="https://fasterthanli.me/index.xml#our-first-transfer-function" id="our-first-transfer-function"></a>






















































<a class="anchor" href="https://fasterthanli.me/index.xml#parade-scope" id="parade-scope"></a>






























<a class="anchor" href="https://fasterthanli.me/index.xml#more-transfer-functions" id="more-transfer-functions"></a>































































































<a class="anchor" href="https://fasterthanli.me/index.xml#how-white-is-your-white" id="how-white-is-your-white"></a>









































<a class="anchor" href="https://fasterthanli.me/index.xml#conclusion" id="conclusion"></a>

---

## Tech News

### [What Happened To WebAssembly](https://emnudge.dev/blog/what-happened-to-webassembly/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/elj9pq/what_happened_webassembly">Comments</a></p>

---

### [How I program in AWK](https://maximullaris.com/program-in-awk.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/bufcbw/how_i_program_awk">Comments</a></p>

---

### [Tailwind Labs loses 80% of revenue and 75% of engineering due to AI](https://adams-morning-walk.transistor.fm/episodes/we-had-six-months-left)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/dzcugr/tailwind_labs_loses_80_revenue_75">Comments</a></p>

---

### [jujutsu v0.37.0 released](https://github.com/jj-vcs/jj/releases/tag/v0.37.0)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/fou2tb/jujutsu_v0_37_0_released">Comments</a></p>

---

### [Dialogue Between a Developer and a Kid](https://riggraz.dev/dialogue-developer.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/g3boya/dialogue_between_developer_kid">Comments</a></p>

---

### [A data model for Git](https://jvns.ca/blog/2026/01/08/a-data-model-for-git/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/a2clkv/data_model_for_git">Comments</a></p>

---

### [How Terminals Work](https://how-terminals-work.vercel.app/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/pza074/how_terminals_work">Comments</a></p>

---

### [Clang Hardening Cheat Sheet - Ten Years Later](https://blog.quarkslab.com/clang-hardening-cheat-sheet-ten-years-later.html)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p><a href="https://lobste.rs/s/y0tzkt/clang_hardening_cheat_sheet_ten_years">Comments</a></p>

---

### [FreeBSD Home NAS, part 8: Backing up NFS and Samba data with restic](https://rtfm.co.ua/en/freebsd-home-nas-part-8-nfs-and-samba-data-backups-using-restic/)

**Êù•Ê∫ê**: Lobsters (Tech News)

**ÊëòË¶Å**: <p>Backups on FreeBSD and Linux with restic: repositories and snapshots, backup and restore workflows, and using restic with AWS S3 and Google Drive via rclone.</p>
<p><a href="https://lobste.rs/s/ge1lq7/freebsd_home_nas_part_8_backing_up_nfs">Comments</a></p>

---


---

## üìö Â¶Ç‰Ωï‰ΩøÁî®

1. ÊµèËßàÊÑüÂÖ¥Ë∂£ÁöÑÊ†áÈ¢ò
2. ÈòÖËØªAIÁîüÊàêÁöÑÊëòË¶ÅÂø´ÈÄü‰∫ÜËß£ÂÜÖÂÆπ
3. ÁÇπÂáªÈìæÊé•Ê∑±ÂÖ•ÈòÖËØª
4. Êúâ‰ª∑ÂÄºÁöÑÂÜÖÂÆπÂèØ‰ª•Êï¥ÁêÜÂà∞ÂØπÂ∫îÁöÑ‰∏ªÈ¢òÁõÆÂΩï

## üîß ÈÖçÁΩÆ

‰øÆÊîπ `config/sources.yaml` ÂèØ‰ª•:
- Ê∑ªÂä†/Âà†Èô§RSSËÆ¢ÈòÖÊ∫ê
- Ë∞ÉÊï¥HackerNewsÊúÄÂ∞èÂàÜÊï∞ÈòàÂÄº
- ÈÖçÁΩÆÂÜÖÂÆπËøáÊª§ÂÖ≥ÈîÆËØç

*Êú¨ÊñáÊ°£Áî± [daily-digestËÑöÊú¨](../scripts/generate_digest.py) Ëá™Âä®ÁîüÊàê*