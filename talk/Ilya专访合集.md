# Ilya专访合集

## 1. Lex Fridman Podcast

1. Deep learning via Hessian-free optimization的论文第一次无需预训练实现了10层神经网络的训练
2. 过拟合不算是一个很糟糕的问题, 因为有各种数据增强和防止过拟合的手段
3. 代价函数是神经网络不可或缺的一环
4. 人的大脑在学习过程中存在STDP过程, 用来改变神经元之间的连接强度
5. 在大型神经网络中隐藏构建的知识库, 从现在来看本质上就是GPT
6. transformer统一CV和NLP领域, 也就是说所有的任务都是通过一个统一的模型架构解决. 而RL和它们都不太相同, 它更加类似于一个组件



## 2. Dwarkesh Podcast

1. AI在某些Eval中表现非常好, 但是在一些简单的例子是重复犯错?
   1. 可能强化学习导致模型single-minded和narrowly focused
   2. 预训练阶段是利用所有的数据, 没有专门的挑选, 模型具有更好的泛化性; 但是对于强化学习, 很多公司为了在某项指标上更加好看, 会挑选一些专门的RL环境进行eval, 对某些特定的能力进行强化, 从而导致了泛化性的塌缩