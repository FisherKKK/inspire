#!/usr/bin/env python3
"""
æ¯æ—¥æŠ€æœ¯æ–°é—»èšåˆè„šæœ¬
è‡ªåŠ¨ä»HackerNewså’ŒRSSæºæŠ“å–å†…å®¹ï¼Œä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦
"""

import os
import sys
import json
import yaml
import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from pathlib import Path
import anthropic


class DailyDigestGenerator:
    def __init__(self, config_path: str = "config/sources.yaml"):
        """åˆå§‹åŒ–"""
        self.config = self._load_config(config_path)
        self.articles = []

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        llm_config = self.config.get('llm', {})
        api_key = os.getenv(llm_config.get('api_key_env', 'ANTHROPIC_AUTH_TOKEN'))
        base_url = os.getenv(llm_config.get('base_url_env', 'ANTHROPIC_BASE_URL'))

        if api_key:
            self.llm_client = anthropic.Anthropic(
                api_key=api_key,
                base_url=base_url if base_url else None
            )
        else:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LLM APIå¯†é’¥ï¼Œå°†è·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            self.llm_client = None

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def fetch_hackernews(self) -> List[Dict]:
        """æŠ“å–HackerNewsçƒ­é—¨æ–‡ç« """
        print("ğŸ“° æ­£åœ¨æŠ“å– HackerNews...")

        hn_config = self.config.get('hacker_news', {})
        if not hn_config.get('enabled', False):
            print("  â­ï¸  HackerNewså·²ç¦ç”¨ï¼Œè·³è¿‡")
            return []

        try:
            # è·å–çƒ­é—¨æ–‡ç« IDåˆ—è¡¨
            response = requests.get(hn_config['api_url'], timeout=10)
            story_ids = response.json()[:hn_config.get('max_items', 30)]

            articles = []
            min_score = hn_config.get('min_score', 100)

            for story_id in story_ids:
                # è·å–æ–‡ç« è¯¦æƒ…
                item_url = hn_config['item_url'].format(story_id)
                item_response = requests.get(item_url, timeout=10)
                item = item_response.json()

                # è¿‡æ»¤ä½åˆ†æ–‡ç« 
                if item.get('score', 0) < min_score:
                    continue

                # è·³è¿‡æ²¡æœ‰URLçš„æ–‡ç« ï¼ˆAsk HNç­‰ï¼‰
                if 'url' not in item:
                    continue

                articles.append({
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'score': item.get('score', 0),
                    'comments': f"https://news.ycombinator.com/item?id={story_id}",
                    'source': 'HackerNews',
                    'category': 'HackerNews'
                })

            print(f"  âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles

        except Exception as e:
            print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
            return []

    def fetch_rss_feeds(self) -> List[Dict]:
        """æŠ“å–RSSè®¢é˜…æº"""
        print("\nğŸ“¡ æ­£åœ¨æŠ“å– RSS è®¢é˜…...")

        all_articles = []
        feeds = self.config.get('rss_feeds', [])

        for feed_config in feeds:
            if not feed_config.get('enabled', True):
                continue

            feed_name = feed_config['name']
            feed_url = feed_config['url']

            print(f"  ğŸ“¥ {feed_name}...")

            try:
                feed = feedparser.parse(feed_url)

                # åªå–æœ€è¿‘24å°æ—¶çš„æ–‡ç« 
                cutoff_time = datetime.now() - timedelta(days=1)

                for entry in feed.entries[:10]:  # æ¯ä¸ªæºæœ€å¤šå–10ç¯‡
                    # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                    if hasattr(entry, 'published_parsed'):
                        pub_time = datetime(*entry.published_parsed[:6])
                        if pub_time < cutoff_time:
                            continue

                    all_articles.append({
                        'title': entry.get('title', ''),
                        'url': entry.get('link', ''),
                        'summary': entry.get('summary', ''),
                        'source': feed_name,
                        'category': feed_config.get('category', 'Tech')
                    })

            except Exception as e:
                print(f"    âŒ å¤±è´¥: {e}")
                continue

        print(f"  âœ… RSSè®¢é˜…å…±æŠ“å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles

    def generate_summary(self, article: Dict) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ–‡ç« æ‘˜è¦"""
        if not self.llm_client:
            return article.get('summary', 'æš‚æ— æ‘˜è¦')

        llm_config = self.config.get('llm', {})
        url = article['url']
        title = article['title']

        try:
            # æ„é€ æç¤ºè¯
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æŠ€æœ¯æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰ï¼Œé‡ç‚¹è¯´æ˜ï¼š
1. æ–‡ç« çš„æ ¸å¿ƒä¸»é¢˜
2. å…³é”®æŠ€æœ¯ç‚¹æˆ–åˆ›æ–°ç‚¹
3. å¯¹è¯»è€…çš„ä»·å€¼

æ–‡ç« æ ‡é¢˜: {title}
æ–‡ç« é“¾æ¥: {url}

è¦æ±‚:
- ç”¨ç®€æ´çš„ä¸­æ–‡è¡¨è¾¾
- çªå‡ºæŠ€æœ¯è¦ç‚¹
- 2-3å¥è¯å³å¯
- ä¸è¦ä½¿ç”¨"è¿™ç¯‡æ–‡ç« "ç­‰å¼€å¤´"""

            message = self.llm_client.messages.create(
                model=llm_config.get('model', 'claude-3-5-haiku-20241022'),
                max_tokens=llm_config.get('max_tokens', 200),
                temperature=llm_config.get('temperature', 0.3),
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            return message.content[0].text.strip()

        except Exception as e:
            print(f"    âš ï¸  æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return article.get('summary', 'æš‚æ— æ‘˜è¦')

    def generate_markdown(self, date_str: str) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æ¯æ—¥æ‘˜è¦"""
        print("\nğŸ“ æ­£åœ¨ç”ŸæˆMarkdownæ–‡æ¡£...")

        # æŒ‰ç±»åˆ«åˆ†ç»„
        categorized = {}
        for article in self.articles:
            category = article.get('category', 'Other')
            if category not in categorized:
                categorized[category] = []
            categorized[category].append(article)

        # ç”ŸæˆMarkdownå†…å®¹
        md_lines = [
            f"# æ¯æ—¥æŠ€æœ¯æ‘˜è¦ - {date_str}",
            "",
            f"> è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"> å…±æ”¶é›† {len(self.articles)} ç¯‡æ–‡ç« ",
            "",
            "## ğŸ“‘ ç›®å½•",
            ""
        ]

        # æ·»åŠ ç›®å½•
        for category in sorted(categorized.keys()):
            md_lines.append(f"- [{category}](#{category.lower().replace('/', '').replace(' ', '-')}) ({len(categorized[category])}ç¯‡)")

        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")

        # æ·»åŠ å„ç±»åˆ«å†…å®¹
        for category in sorted(categorized.keys()):
            md_lines.append(f"## {category}")
            md_lines.append("")

            articles = categorized[category]

            # HackerNewsç‰¹æ®Šå¤„ç†ï¼ˆæ˜¾ç¤ºåˆ†æ•°ï¼‰
            if category == 'HackerNews':
                # æŒ‰åˆ†æ•°æ’åº
                articles.sort(key=lambda x: x.get('score', 0), reverse=True)

                for article in articles:
                    md_lines.append(f"### [{article['title']}]({article['url']})")
                    md_lines.append(f"")
                    md_lines.append(f"**åˆ†æ•°**: {article.get('score', 0)} | [è®¨è®º]({article.get('comments', '')})")
                    md_lines.append("")

                    # ç”Ÿæˆæ‘˜è¦
                    if self.llm_client:
                        print(f"  ğŸ¤– æ­£åœ¨ä¸º '{article['title'][:50]}...' ç”Ÿæˆæ‘˜è¦")
                        summary = self.generate_summary(article)
                        md_lines.append(f"**æ‘˜è¦**: {summary}")

                    md_lines.append("")
                    md_lines.append("---")
                    md_lines.append("")

            else:
                # å…¶ä»–RSSæº
                for article in articles:
                    md_lines.append(f"### [{article['title']}]({article['url']})")
                    md_lines.append(f"")
                    md_lines.append(f"**æ¥æº**: {article.get('source', 'Unknown')}")
                    md_lines.append("")

                    # ç”Ÿæˆæ‘˜è¦
                    if self.llm_client:
                        print(f"  ğŸ¤– æ­£åœ¨ä¸º '{article['title'][:50]}...' ç”Ÿæˆæ‘˜è¦")
                        summary = self.generate_summary(article)
                        md_lines.append(f"**æ‘˜è¦**: {summary}")
                    elif article.get('summary'):
                        md_lines.append(f"**æ‘˜è¦**: {article['summary'][:200]}...")

                    md_lines.append("")
                    md_lines.append("---")
                    md_lines.append("")

        # æ·»åŠ é¡µè„š
        md_lines.extend([
            "",
            "---",
            "",
            "## ğŸ“š å¦‚ä½•ä½¿ç”¨",
            "",
            "1. æµè§ˆæ„Ÿå…´è¶£çš„æ ‡é¢˜",
            "2. é˜…è¯»AIç”Ÿæˆçš„æ‘˜è¦å¿«é€Ÿäº†è§£å†…å®¹",
            "3. ç‚¹å‡»é“¾æ¥æ·±å…¥é˜…è¯»",
            "4. æœ‰ä»·å€¼çš„å†…å®¹å¯ä»¥æ•´ç†åˆ°å¯¹åº”çš„ä¸»é¢˜ç›®å½•",
            "",
            "## ğŸ”§ é…ç½®",
            "",
            "ä¿®æ”¹ `config/sources.yaml` å¯ä»¥:",
            "- æ·»åŠ /åˆ é™¤RSSè®¢é˜…æº",
            "- è°ƒæ•´HackerNewsæœ€å°åˆ†æ•°é˜ˆå€¼",
            "- é…ç½®å†…å®¹è¿‡æ»¤å…³é”®è¯",
            "",
            f"*æœ¬æ–‡æ¡£ç”± [daily-digestè„šæœ¬](../scripts/generate_digest.py) è‡ªåŠ¨ç”Ÿæˆ*"
        ])

        return "\n".join(md_lines)

    def save_digest(self, content: str, date_str: str):
        """ä¿å­˜æ¯æ—¥æ‘˜è¦åˆ°æ–‡ä»¶"""
        output_config = self.config.get('output', {})
        output_dir = Path(output_config.get('directory', 'daily-digest'))
        output_dir.mkdir(exist_ok=True)

        filename = output_config.get('filename_format', 'digest-{date}.md').format(date=date_str)
        filepath = output_dir / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nâœ… æ¯æ—¥æ‘˜è¦å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def cleanup_old_files(self):
        """æ¸…ç†æ—§çš„æ‘˜è¦æ–‡ä»¶"""
        output_config = self.config.get('output', {})
        output_dir = Path(output_config.get('directory', 'daily-digest'))
        max_days = output_config.get('max_days_to_keep', 30)

        if not output_dir.exists():
            return

        cutoff_date = datetime.now() - timedelta(days=max_days)
        deleted_count = 0

        for file in output_dir.glob('digest-*.md'):
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            try:
                date_str = file.stem.replace('digest-', '')
                file_date = datetime.strptime(date_str, '%Y-%m-%d')

                if file_date < cutoff_date:
                    file.unlink()
                    deleted_count += 1
            except:
                continue

        if deleted_count > 0:
            print(f"ğŸ—‘ï¸  å·²æ¸…ç† {deleted_count} ä¸ªæ—§æ–‡ä»¶")

    def run(self):
        """ä¸»æµç¨‹"""
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæ¯æ—¥æŠ€æœ¯æ‘˜è¦\n")
        print("=" * 60)

        # æŠ“å–å†…å®¹
        hn_articles = self.fetch_hackernews()
        rss_articles = self.fetch_rss_feeds()

        self.articles = hn_articles + rss_articles

        if not self.articles:
            print("\nâš ï¸  æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–‡ç« ")
            return

        # ç”Ÿæˆæ‘˜è¦æ–‡æ¡£
        date_str = datetime.now().strftime('%Y-%m-%d')
        markdown_content = self.generate_markdown(date_str)

        # ä¿å­˜æ–‡ä»¶
        filepath = self.save_digest(markdown_content, date_str)

        # æ¸…ç†æ—§æ–‡ä»¶
        self.cleanup_old_files()

        print("\n" + "=" * 60)
        print(f"âœ¨ å®Œæˆ! å…±å¤„ç† {len(self.articles)} ç¯‡æ–‡ç« ")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {filepath}")


if __name__ == "__main__":
    try:
        generator = DailyDigestGenerator()
        generator.run()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
